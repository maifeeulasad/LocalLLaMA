{
  "kind": "Listing",
  "data": {
    "after": "t3_1mgn94g",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "and it's better than Flux Kontext Pro (according to their benchmarks). That's insane. Really looking forward to it. ",
          "author_fullname": "t2_ghr4m7l1n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "QWEN-IMAGE is released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhdig",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 740,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 740,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=a720ed50938a0f7d099fa5095eeaa524819f6b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754323135,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;and it&amp;#39;s better than Flux Kontext Pro (according to their benchmarks). That&amp;#39;s insane. Really looking forward to it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen-Image",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?auto=webp&amp;s=7578d1430d51fe3898437256626f5bd7f9c643b5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55a19a341313ab08b43f3737ad0171a6dc27a3a6",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=561673e4e6ac3694e8d08fb8f3b50de1d4d7bafc",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5972624b3a9fe2057d6c44275f111b27e3e66505",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a65aaca919e956009709dd069f70c0c907403912",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b308acfa4755323a0b8731404fd7599940db7ca",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fcc7db6ec2f7f9365fb1224503b8e2a33a798c7",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhhdig",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheIncredibleHem",
          "discussion_type": null,
          "num_comments": 168,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen-Image",
          "subreddit_subscribers": 510259,
          "created_utc": 1754323135,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4ois9219",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Sam Altman watching Qwen drop model after model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhgu6t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 630,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 630,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/2ksLlK4qMpHN0wJRX622eitQNrDUtW9RxXFJtAx_L2U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754321919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/g7t8cmgrv0hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?auto=webp&amp;s=77f4646870c66b9ec3797f5deb047528f80e4e1f",
                  "width": 1170,
                  "height": 662
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44351caaa0ea4099b9344000e708dfe04a848bdc",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a20f236ad7e68ab4eef6370b9ec71122c5dedbb",
                    "width": 216,
                    "height": 122
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9652e2e8eda7d2e28de7a1b107b0a8adc8c5dbf",
                    "width": 320,
                    "height": 181
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=460e01fe2091b6bc2347e41a918d258022a40353",
                    "width": 640,
                    "height": 362
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=83bf7b9bcffae519199437557b6b6d459d2e6727",
                    "width": 960,
                    "height": 543
                  },
                  {
                    "url": "https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28f72356208bc50b38ca8b396360ae4b3b217050",
                    "width": 1080,
                    "height": 611
                  }
                ],
                "variants": {},
                "id": "_zkJ38t23EZXuiZZ8jVwSAIkImC1wga8Qejk17AVZuM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mhgu6t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheRealSerdra",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/",
          "stickied": false,
          "url": "https://i.redd.it/g7t8cmgrv0hf1.jpeg",
          "subreddit_subscribers": 510259,
          "created_utc": 1754321919,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1952398250121756992\n\nIt's better than Flux Kontext, gpt-image level",
          "author_fullname": "t2_58t8ty6v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen-Image is out",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhiqqn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 461,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4077mfg081hf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/4077mfg081hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4077mfg081hf1/DASHPlaylist.mpd?a=1756948483%2CNDYyOWEyYzM0NzM3M2QyMWYwMjc3YzYyNzUzOWJjNWFiMzgyNTk2ZDc0MjQ5N2FiZGQ4NzE1N2U4ZTZhYmQ2Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 116,
              "hls_url": "https://v.redd.it/4077mfg081hf1/HLSPlaylist.m3u8?a=1756948483%2CYzVmMTNlYTZjODBkN2U0NTgzOWVjZDU3YjIzYmQ3MTczOTc5NmZmMmZmYTFlNjA0MzA2NDc2MDgxZGFhMjk5Mw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 461,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=55f564e25aed19bc0608b4b7b3a6e94a2cb852a6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754326154,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1952398250121756992\"&gt;https://x.com/Alibaba_Qwen/status/1952398250121756992&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s better than Flux Kontext, gpt-image level&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/4077mfg081hf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?format=pjpg&amp;auto=webp&amp;s=f2e05f42033e081c6d088d575d93180ae723d268",
                  "width": 1080,
                  "height": 607
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e13de73d5d88711e89158aadf32e2b1b9d0f8cbe",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c028dc1b30127eeeea0df5fa246f97fe6e74c36e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=19d2e5447a0dd5f36d7e648492262c90d292c666",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aa9773d48043db6c700f05cbcf3ad034dcb15761",
                    "width": 640,
                    "height": 359
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8d02cdd020b1b671aea766bcf6625b2e345dc93a",
                    "width": 960,
                    "height": 539
                  },
                  {
                    "url": "https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=30e6c8ef3172bb477014b575290651e650238236",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhiqqn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BoJackHorseMan53",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/",
          "stickied": false,
          "url": "https://v.redd.it/4077mfg081hf1",
          "subreddit_subscribers": 510259,
          "created_utc": 1754326154,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4077mfg081hf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/4077mfg081hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4077mfg081hf1/DASHPlaylist.mpd?a=1756948483%2CNDYyOWEyYzM0NzM3M2QyMWYwMjc3YzYyNzUzOWJjNWFiMzgyNTk2ZDc0MjQ5N2FiZGQ4NzE1N2U4ZTZhYmQ2Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 116,
              "hls_url": "https://v.redd.it/4077mfg081hf1/HLSPlaylist.m3u8?a=1756948483%2CYzVmMTNlYTZjODBkN2U0NTgzOWVjZDU3YjIzYmQ3MTczOTc5NmZmMmZmYTFlNjA0MzA2NDc2MDgxZGFhMjk5Mw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.\n\n🔍 Key Highlights:\n\n🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese\n\n🔹 In-pixel text generation — no overlays, fully integrated\n\n🔹 Bilingual support, diverse fonts, complex layouts\n\n🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.\n\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Meet Qwen-Image",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 83,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhctd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 483,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 483,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MksEcgMoK6Jmwoikg8X4-cDPzaFA1Qb7KAVt6Vkaa8o.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754323091,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt;\n\n&lt;p&gt;🔍 Key Highlights:&lt;/p&gt;\n\n&lt;p&gt;🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt;\n\n&lt;p&gt;🔹 In-pixel text generation — no overlays, fully integrated&lt;/p&gt;\n\n&lt;p&gt;🔹 Bilingual support, diverse fonts, complex layouts&lt;/p&gt;\n\n&lt;p&gt;🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/7a463it8z0hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?auto=webp&amp;s=3d345126dacc633d373b82475dde325d90c4c76f",
                  "width": 3665,
                  "height": 2181
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa3d443b56ce6f98c27acfab23a04897cc07af2c",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de833da505be2db0cdc52261408e16c27ab8f2db",
                    "width": 216,
                    "height": 128
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=099ee55093c2e0a097645bf6c077a0a7f1218cfa",
                    "width": 320,
                    "height": 190
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a9bc46837ad4e1dac08bb6879d131c650ac6476",
                    "width": 640,
                    "height": 380
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1bcf4277e31c7930cf3baf2cb6e844086778af3",
                    "width": 960,
                    "height": 571
                  },
                  {
                    "url": "https://preview.redd.it/7a463it8z0hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f2c3dc19a952ac3c12699f79444b9c742d1caf7",
                    "width": 1080,
                    "height": 642
                  }
                ],
                "variants": {},
                "id": "SSvyPfajItjWlZ5XZvl958Hd8gBJ9Y2EIdDLuhYSzlg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhhctd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/",
          "stickied": false,
          "url": "https://i.redd.it/7a463it8z0hf1.jpeg",
          "subreddit_subscribers": 510259,
          "created_utc": 1754323091,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "r/LocalLLaMA right now",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhe1rl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "ups": 532,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 532,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/PPKozUiF3SYW1-d8ApDHQ6cvdG7JpTuJ10ijiFT_rnE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754315546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/f0xr7mshc0hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/f0xr7mshc0hf1.png?auto=webp&amp;s=406d23785d295484462e63db9c1321b110db9e37",
                  "width": 968,
                  "height": 1280
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e4fb1ed4f97bf1ef6b619b68d73f264e3545abe",
                    "width": 108,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=36a72274c84378acb2518b49234bdf66ec250ea3",
                    "width": 216,
                    "height": 285
                  },
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3f3479dd1b60048227ed41014880f4578f9763e",
                    "width": 320,
                    "height": 423
                  },
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0d701cbdf33b1ec6ea47dd4b4874202dbea647e",
                    "width": 640,
                    "height": 846
                  },
                  {
                    "url": "https://preview.redd.it/f0xr7mshc0hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9cadb3c3eb6f9eefb04092b385b82e9f357ad09e",
                    "width": 960,
                    "height": 1269
                  }
                ],
                "variants": {},
                "id": "3GbmLMftfHxRNuEdGZh7axZUKeKfa45TSKaX6bMQoro"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhe1rl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/",
          "stickied": false,
          "url": "https://i.redd.it/f0xr7mshc0hf1.png",
          "subreddit_subscribers": 510259,
          "created_utc": 1754315546,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen Models Today!!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 54,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhbpmo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 676,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 676,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TROn1uPQcH0PujeybIidpkc9G7nZ0H_qibt1MPjtmMI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754309520,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qemmgysvuzgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qemmgysvuzgf1.png?auto=webp&amp;s=42888269e8a8be81d80e8a6d5692747211e04c55",
                  "width": 1220,
                  "height": 476
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f9e5dff4613eb055af874621d1a213848bf522f",
                    "width": 108,
                    "height": 42
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f6672b49f5b95cc45a9b23e3598d09d05496d7",
                    "width": 216,
                    "height": 84
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=28a355bc0adcfc06eaf4b216b3ef61b9d652f5eb",
                    "width": 320,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e45a424e82bdde4384e3d7ba1be6631b2a25639",
                    "width": 640,
                    "height": 249
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd0a3cc19923cbf380af7306ff3f4d5335556fb8",
                    "width": 960,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f267cdf32b56403571256c354e628f69fadd7b15",
                    "width": 1080,
                    "height": 421
                  }
                ],
                "variants": {},
                "id": "34KTkl_1uxrvHPhAnaWXTjSZ6bmw11ut0GxXsPRfDZY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhbpmo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 102,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/",
          "stickied": false,
          "url": "https://i.redd.it/qemmgysvuzgf1.png",
          "subreddit_subscribers": 510259,
          "created_utc": 1754309520,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for GLM 4.5 family of models has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhlkyx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "ups": 199,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 199,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=b40f582039621f51c445ac0fcfdb827b930d7f2c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754332231,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?auto=webp&amp;s=083930ea54b88a7f6eaadda136c2185460baf66e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=78369c4a613d24a26f628c7b0d0788fbd02727b4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7d7893c234acd63db0445e0010c29d3054bf72a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=be66666d37f3f19b8f252dc5f32ba0b7be39e97c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e04ffa9cbd0a435f87d74eaf876a5853c1e06023",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=658e3d76ab28bc884f80a72e29bf1040fe464132",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91b76178fba350b1d785fbd980fb39979366d649",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhlkyx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "subreddit_subscribers": 510259,
          "created_utc": 1754332231,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;format=png&amp;auto=webp&amp;s=5234584d7973049a12fc3c428b50a1d35e48858f\n\nhttps://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;format=png&amp;auto=webp&amp;s=f06cc61180cfced07aa66367d23e552e605c0f75\n\nQwen image is ready to drop:https://github.com/huggingface/diffusers/pull/12055",
          "author_fullname": "t2_u398xzta",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen image 20B is coming!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "uxg8kr5ej0hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2fcfc8d951cc2bccbf0ade68de7f27e62dfeeab9"
                },
                {
                  "y": 120,
                  "x": 216,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=27e928f28592fb88701fa631cfd74d00fd77612e"
                },
                {
                  "y": 178,
                  "x": 320,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2c331eeefd2b5dfb25080342a3f21dca5499a5b"
                },
                {
                  "y": 356,
                  "x": 640,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=62b9a2502c11596faff798266ea7cf2bc24914fa"
                },
                {
                  "y": 535,
                  "x": 960,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a39f911d55049fa1c419c36f6714ca64a12d529"
                },
                {
                  "y": 602,
                  "x": 1080,
                  "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a6538e66dc7da30f1dcc6d6ad85f1e81705d3cf"
                }
              ],
              "s": {
                "y": 928,
                "x": 1664,
                "u": "https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;format=png&amp;auto=webp&amp;s=f06cc61180cfced07aa66367d23e552e605c0f75"
              },
              "id": "uxg8kr5ej0hf1"
            },
            "3n3tfhqbj0hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 143,
                  "x": 108,
                  "u": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=146b14f2f3fc2707a993a9a97f3a60bb2723d2b6"
                },
                {
                  "y": 286,
                  "x": 216,
                  "u": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ca82d65ca0223fb27a32b5d739c376bf79ef56f"
                },
                {
                  "y": 424,
                  "x": 320,
                  "u": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec51a5877ed2a2ed6c5670705517c391266e83d3"
                }
              ],
              "s": {
                "y": 702,
                "x": 529,
                "u": "https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;format=png&amp;auto=webp&amp;s=5234584d7973049a12fc3c428b50a1d35e48858f"
              },
              "id": "3n3tfhqbj0hf1"
            }
          },
          "name": "t3_1mhf0kl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 309,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 309,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=562eb305e5946c82a293b905b50f03961ef833dd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754317809,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5234584d7973049a12fc3c428b50a1d35e48858f\"&gt;https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5234584d7973049a12fc3c428b50a1d35e48858f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f06cc61180cfced07aa66367d23e552e605c0f75\"&gt;https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f06cc61180cfced07aa66367d23e552e605c0f75&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen image is ready to drop:&lt;a href=\"https://github.com/huggingface/diffusers/pull/12055\"&gt;https://github.com/huggingface/diffusers/pull/12055&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?auto=webp&amp;s=c2f942543ac8d9ccfd9ae80646c3b7a133f8ba08",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5be1c7fe4a021337cd223c3f63b471eaee167539",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=190517067fa2ac3d0abfa3102bd8d0e9195a2419",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2787be181adcc2160884957e19b14738dfdf391",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b2d120bc77d25aceff54c01b358c0fa229b74ef",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd70e8035ced6265cd47615c4892322de1172577",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a4c65249b77ca0b2996bdcb0af318f2a83e8f33",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhf0kl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sunshinecheung",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754317809,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://x.com/OfficialLoganK/status/1952430214375493808](https://x.com/OfficialLoganK/status/1952430214375493808)",
          "author_fullname": "t2_1uzhw00y56",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemini 3 is coming?..",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 24,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhl5yo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 136,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 136,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/EDf-MI0GF8x5ZjyG0YABbe4z3xjcAqC0nMWmMq6CEE0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754331340,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/OfficialLoganK/status/1952430214375493808\"&gt;https://x.com/OfficialLoganK/status/1952430214375493808&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/59joqndkn1hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/59joqndkn1hf1.png?auto=webp&amp;s=728c888953031ef3f33df909853396c378ea05ee",
                  "width": 671,
                  "height": 116
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/59joqndkn1hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=318f53c18e18eee36e48194ccf41cec21ffc52db",
                    "width": 108,
                    "height": 18
                  },
                  {
                    "url": "https://preview.redd.it/59joqndkn1hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3754309bb4fb6ba5a78d3ebb019551b59ec630e8",
                    "width": 216,
                    "height": 37
                  },
                  {
                    "url": "https://preview.redd.it/59joqndkn1hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7e6fd5243afbed6f2958a397457b412f0bc4e31",
                    "width": 320,
                    "height": 55
                  },
                  {
                    "url": "https://preview.redd.it/59joqndkn1hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e89c768daaac9653e4f2ad00c6a0ee5f6412107",
                    "width": 640,
                    "height": 110
                  }
                ],
                "variants": {},
                "id": "uhYd1Q9dnVcQrpLekV8fk4gEIwtVrTg3elgEcFfUKqM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhl5yo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SlerpE",
          "discussion_type": null,
          "num_comments": 48,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/",
          "stickied": false,
          "url": "https://i.redd.it/59joqndkn1hf1.png",
          "subreddit_subscribers": 510259,
          "created_utc": 1754331340,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "FINALLY",
          "author_fullname": "t2_uptissiz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 GGUFs are coming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mht910",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 52,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 52,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=e4e7a0c9fa14f2888f5f07598ff2ff6958188b94",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754349968,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;FINALLY&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?auto=webp&amp;s=4f29cdd9d86d464f3541c6b772fcf35782c87ded",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13c5f6e652403be83b71873e1bbc87da605d3006",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=534e020aa7468d38197ed4c44174cf3a57584f8b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a9cae5191f82a2f0f0af1bc09cc8d9f635c7ec6",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cbaa78bb76999536a7337e9b0c9e2f578691b200",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d454810ac3eaf0025e5d8ae947c98b915310812f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=33f3ae9e2091370aa36282d51b361680fb28f807",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mht910",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pro-editor-1105",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/",
          "stickied": false,
          "url": "https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF",
          "subreddit_subscribers": 510259,
          "created_utc": 1754349968,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_qqgbes3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Huawei released weights of Pangu Ultra,a 718B model.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhctvk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 280,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 70,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 280,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/b4vAhRXu0QISFGzKNI7MMEeFrdQG1UWQqC8GhQPUCNU.png?width=70&amp;height=70&amp;crop=70:70,smart&amp;auto=webp&amp;s=42d2a16045706201098a626b24ce7402818223f6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754312524,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ai.gitcode.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ai.gitcode.com/ascend-tribe/openpangu-ultra-moe-718b-model/blob/main/README_EN.md",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/b4vAhRXu0QISFGzKNI7MMEeFrdQG1UWQqC8GhQPUCNU.png?auto=webp&amp;s=da6a5e01cd36a70882b4a98dbc5b14b02b19a809",
                  "width": 96,
                  "height": 96
                },
                "resolutions": [],
                "variants": {},
                "id": "b4vAhRXu0QISFGzKNI7MMEeFrdQG1UWQqC8GhQPUCNU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhctvk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Overflow_al",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhctvk/huawei_released_weights_of_pangu_ultraa_718b_model/",
          "stickied": false,
          "url": "https://ai.gitcode.com/ascend-tribe/openpangu-ultra-moe-718b-model/blob/main/README_EN.md",
          "subreddit_subscribers": 510259,
          "created_utc": 1754312524,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.\n\n🔍 Key Highlights:\n\n🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese\n\n🔹 In-pixel text generation — no overlays, fully integrated\n\n🔹 Bilingual support, diverse fonts, complex layouts\n\n🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.\n\nBlog: https://qwenlm.github.io/blog/qwen-image/[Blog](https://qwenlm.github.io/blog/qwen-image/)\n\nHugging Face: [huggingface.co/Qwen/Qwen-Image](http://huggingface.co/Qwen/Qwen-Image)",
          "author_fullname": "t2_e9mfhlg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen-Image — a 20B MMDiT model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhhpi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 105,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 105,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754323671,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754323369,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt;\n\n&lt;p&gt;🔍 Key Highlights:&lt;/p&gt;\n\n&lt;p&gt;🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt;\n\n&lt;p&gt;🔹 In-pixel text generation — no overlays, fully integrated&lt;/p&gt;\n\n&lt;p&gt;🔹 Bilingual support, diverse fonts, complex layouts&lt;/p&gt;\n\n&lt;p&gt;🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"https://qwenlm.github.io/blog/qwen-image/%5BBlog%5D(https://qwenlm.github.io/blog/qwen-image/)\"&gt;https://qwenlm.github.io/blog/qwen-image/[Blog](https://qwenlm.github.io/blog/qwen-image/)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"http://huggingface.co/Qwen/Qwen-Image\"&gt;huggingface.co/Qwen/Qwen-Image&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhhhpi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xhehab_",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754323369,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This model is insane!   I have been testing the ongoing llama.cpp PR and this morning has been amazing!    GLM can spit out LOOOOOOOOOOOOOOOOOONG tokens!     The original was a beast, and the new one is even better.   I gave it 2500 lines of python code, told it to refactor it, it do so without dropping anything!   Then I told it to translate it to ruby and it did so completely.   The model is very coherent across long contexts, the quality so far is great.   The model is fast!  Full loaded on 3090's, It starts out at 45tk/sec and this is with llama.cpp.\n\nI have only driven it for about an hour and this is the smaller model air, not the big one!  I'm very convinced that this will replace deepseek-r1/chimera/v3/ernie-300b/kimi-k2 for me.\n\nIs this better than sonnet/opus/gemini/openai?   For me yup!  I don't use closed models, so I really can't tell, but this so far is looking like the best damn model locally.  I have only thrown code generation at it, so I can't tell how it would perform in creative writing, role play, other sorts of generation etc.   I haven't played at all with tool calling, instruction following, etc, but based on how well it's responding, I think it's going to be great.  The only short coming I see is the 128k context window.\n\nIt's fast too, 50k+ token, 16.44 tk/sec\n\nslot      release: id  0 | task 42155 | stop processing: n\\_past = 51785, truncated = 0\n\nslot print\\_timing: id  0 | task 42155 |\n\nprompt eval time =     421.72 ms /    35 tokens (   12.05 ms per token,    82.99 tokens per second)\n\neval time =  983525.01 ms / 16169 tokens (   60.83 ms per token,    16.44 tokens per second)\n\nEdit:  \nq4 quants down to 67.85gb  \nI decide to run q4, offload only shared experts to 1 3090 GPU and the rest to system ram (ddr4 2400mhz quad channel on dual x99 platform).  The entire shared experts for 47 layers takes about 4gb of vram, that means you can put all of the shared expert on your 8gb GPU.  I decide to not load any other tensor but just these and see how it performs.  It start out at 10tk/sec.   I'm going to run q3\\_k\\_l on a 3060 and P40 and put up the results later.",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Get ready for GLM-4-5 local gguf woot woot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhg8rt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 117,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 117,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754326570,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754320611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This model is insane!   I have been testing the ongoing llama.cpp PR and this morning has been amazing!    GLM can spit out LOOOOOOOOOOOOOOOOOONG tokens!     The original was a beast, and the new one is even better.   I gave it 2500 lines of python code, told it to refactor it, it do so without dropping anything!   Then I told it to translate it to ruby and it did so completely.   The model is very coherent across long contexts, the quality so far is great.   The model is fast!  Full loaded on 3090&amp;#39;s, It starts out at 45tk/sec and this is with llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;I have only driven it for about an hour and this is the smaller model air, not the big one!  I&amp;#39;m very convinced that this will replace deepseek-r1/chimera/v3/ernie-300b/kimi-k2 for me.&lt;/p&gt;\n\n&lt;p&gt;Is this better than sonnet/opus/gemini/openai?   For me yup!  I don&amp;#39;t use closed models, so I really can&amp;#39;t tell, but this so far is looking like the best damn model locally.  I have only thrown code generation at it, so I can&amp;#39;t tell how it would perform in creative writing, role play, other sorts of generation etc.   I haven&amp;#39;t played at all with tool calling, instruction following, etc, but based on how well it&amp;#39;s responding, I think it&amp;#39;s going to be great.  The only short coming I see is the 128k context window.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fast too, 50k+ token, 16.44 tk/sec&lt;/p&gt;\n\n&lt;p&gt;slot      release: id  0 | task 42155 | stop processing: n_past = 51785, truncated = 0&lt;/p&gt;\n\n&lt;p&gt;slot print_timing: id  0 | task 42155 |&lt;/p&gt;\n\n&lt;p&gt;prompt eval time =     421.72 ms /    35 tokens (   12.05 ms per token,    82.99 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;eval time =  983525.01 ms / 16169 tokens (   60.83 ms per token,    16.44 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;br/&gt;\nq4 quants down to 67.85gb&lt;br/&gt;\nI decide to run q4, offload only shared experts to 1 3090 GPU and the rest to system ram (ddr4 2400mhz quad channel on dual x99 platform).  The entire shared experts for 47 layers takes about 4gb of vram, that means you can put all of the shared expert on your 8gb GPU.  I decide to not load any other tensor but just these and see how it performs.  It start out at 10tk/sec.   I&amp;#39;m going to run q3_k_l on a 3060 and P40 and put up the results later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhg8rt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 78,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754320611,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_17g3lg5snf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen model has vision",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhdnye",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 134,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 134,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/qwP5df-vG6P0fFg9M5s5Bb9e4eBggj8ue2wrQPXOkpY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754314625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vypcvak2a0hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?auto=webp&amp;s=7b3f188c42679012c6d075a4fcc5a071d61204d7",
                  "width": 1080,
                  "height": 2400
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=812d44c02d8ace8217583425393f7b465984241c",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb25ae2781a0c7777e59497293a17f1774ff05dc",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=992e05e9602caaa9daf551b46544dbc837f44c4b",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb260c7a9be4af5c73e35111d5c25d23acd339bd",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5818ada9a4af9e0c0af26645f62509d35e647bd4",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/vypcvak2a0hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35d69a7f561bc9c11b13d04b86bbec74533e0e24",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "t3HB7rEFEpA_c84oL5rMK_DhP-86PeLTNUvQX50FdoU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhdnye",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Relative_Rope4234",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhdnye/new_qwen_model_has_vision/",
          "stickied": false,
          "url": "https://i.redd.it/vypcvak2a0hf1.jpeg",
          "subreddit_subscribers": 510259,
          "created_utc": 1754314625,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here is the original blog post: [https://blog.google/technology/ai/kaggle-game-arena/](https://blog.google/technology/ai/kaggle-game-arena/) \n\nAbout the benchmark, I personally prefer game as a head-to-head benchmark to LMArena. At least if they do benchmaxxing, we might have models that's more intelligent comparing to the more glazing effect of LMArena. \n\n  \nAbout the exhibition stream, it's funny to see they let Deepseek R1 play against o4-mini and Grok 4 play against gemini flash. Kimi-K2 vs O3 would be fun though. \n\nhttps://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814\n\n",
          "author_fullname": "t2_6mjqz0at",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google introduces a new Benchmark: Game Arena and they're streaming your favorite open weight models playing chess against close source models.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "83xmndz6q1hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbb73ebf70ed7f8fe4edb4b29ff277e291b77a6d"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d34e0673562d914366016c116db33e499f3ce9aa"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bead8eabd9003c8382152c86297522e0510c667d"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd802cf4c986633ec0b89a53831c9d72912f09d6"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=094f632a5252c9911b447458f7f11504d60b9251"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef3ab4f4ba1906268baacf02f2b46029ee03f84b"
                }
              ],
              "s": {
                "y": 720,
                "x": 1280,
                "u": "https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814"
              },
              "id": "83xmndz6q1hf1"
            }
          },
          "name": "t3_1mhlo6g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=8eae0b8d98140f6575481359160da04e2933f01e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754332426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is the original blog post: &lt;a href=\"https://blog.google/technology/ai/kaggle-game-arena/\"&gt;https://blog.google/technology/ai/kaggle-game-arena/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;About the benchmark, I personally prefer game as a head-to-head benchmark to LMArena. At least if they do benchmaxxing, we might have models that&amp;#39;s more intelligent comparing to the more glazing effect of LMArena. &lt;/p&gt;\n\n&lt;p&gt;About the exhibition stream, it&amp;#39;s funny to see they let Deepseek R1 play against o4-mini and Grok 4 play against gemini flash. Kimi-K2 vs O3 would be fun though. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814\"&gt;https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?auto=webp&amp;s=bb83dc4652169f42acbd54e2777ee5ed8aa93432",
                  "width": 1300,
                  "height": 731
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6c4c1e50bf3d40b8b76be77b34dbecd15f1ff79",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=653d78e943b4fecfa3184638cf2aa4fc2cc2ecb0",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e08d2c25b02ab639e27806ae3a4d28a7be6bd144",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc604dc195cda31f472811e38b2354a3cb7b4e27",
                    "width": 640,
                    "height": 359
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d9db50afa75d54d2a2dfc8f4debd87a0c7879e7",
                    "width": 960,
                    "height": 539
                  },
                  {
                    "url": "https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=900c76acd835190ef74ca8257cf1c3213a79312d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhlo6g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mtmttuan",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754332426,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TLDR:** I built this **open source** and **local** app that lets your local models **watch your screen** and do stuff! It is now **suuuper easy to install** and use, to make local AI **accessible to** **everybody**!\n\nHey r/LocalLLaMA! I'm back with some Observer updates c: first of all **Thank You** so much for all of your support and feedback, i've been working hard to take this project to this current state. I added the app installation which is a significant QOL improvement for ease of use for first time users!! The docker-compose option is still supported and viable for people wanting a more specific and custom install.\n\nThe new app tools are a **game-changer**!! You can now have direct system-level pop ups or notifications that come up right **up to your face** hahaha. And sorry to everyone who tried out SMS and WhatsApp and were frustrated because you weren't getting notifications, Meta started blocking my account thinking i was just spamming messages to you guys.\n\nBut the pushover and discord notifications work perfectly well!\n\nIf you have any feedback please reach out through the discord, i'm really open to suggestions.\n\nThis is the projects [Github](https://github.com/Roy3838/Observer) (completely open source)  \nAnd the discord: [https://discord.gg/wnBb7ZQDUC](https://discord.gg/wnBb7ZQDUC)\n\nIf you have any questions i'll be hanging out here for a while!",
          "author_fullname": "t2_p443m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to use your Local Models to watch your screen. Open Source and Completely Free!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhrx3m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g3pod2zlw2hf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g3pod2zlw2hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g3pod2zlw2hf1/DASHPlaylist.mpd?a=1756948483%2CZDAyNGQ4ODBkNzgyMzA4YTQ5ZmU5YzFiN2FhMDEyYzhmNWM2MTQwODU0ZGRjNTgyYzAwZTUzZjZjOTJlMWIzMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 248,
              "hls_url": "https://v.redd.it/g3pod2zlw2hf1/HLSPlaylist.m3u8?a=1756948483%2COTM5Njg0MDk5ZTJmMzU1ZGQxYWFjMzljZTllODYwZDk3YTRmMGFjNjFiMTk1MjRjYzAwNGIwMmY5YWE1YzhlNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=6b85a1bb34e9e7db698603fcebc73b8fd360d283",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754346605,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; I built this &lt;strong&gt;open source&lt;/strong&gt; and &lt;strong&gt;local&lt;/strong&gt; app that lets your local models &lt;strong&gt;watch your screen&lt;/strong&gt; and do stuff! It is now &lt;strong&gt;suuuper easy to install&lt;/strong&gt; and use, to make local AI &lt;strong&gt;accessible to&lt;/strong&gt; &lt;strong&gt;everybody&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;Hey r/LocalLLaMA! I&amp;#39;m back with some Observer updates c: first of all &lt;strong&gt;Thank You&lt;/strong&gt; so much for all of your support and feedback, i&amp;#39;ve been working hard to take this project to this current state. I added the app installation which is a significant QOL improvement for ease of use for first time users!! The docker-compose option is still supported and viable for people wanting a more specific and custom install.&lt;/p&gt;\n\n&lt;p&gt;The new app tools are a &lt;strong&gt;game-changer&lt;/strong&gt;!! You can now have direct system-level pop ups or notifications that come up right &lt;strong&gt;up to your face&lt;/strong&gt; hahaha. And sorry to everyone who tried out SMS and WhatsApp and were frustrated because you weren&amp;#39;t getting notifications, Meta started blocking my account thinking i was just spamming messages to you guys.&lt;/p&gt;\n\n&lt;p&gt;But the pushover and discord notifications work perfectly well!&lt;/p&gt;\n\n&lt;p&gt;If you have any feedback please reach out through the discord, i&amp;#39;m really open to suggestions.&lt;/p&gt;\n\n&lt;p&gt;This is the projects &lt;a href=\"https://github.com/Roy3838/Observer\"&gt;Github&lt;/a&gt; (completely open source)&lt;br/&gt;\nAnd the discord: &lt;a href=\"https://discord.gg/wnBb7ZQDUC\"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you have any questions i&amp;#39;ll be hanging out here for a while!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g3pod2zlw2hf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?format=pjpg&amp;auto=webp&amp;s=c244884240671677f86749f32fc9389093c74e36",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0cafd483e776962ac85274492a6320c14f4f96e7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a8ecd52b64c1d70725e4ee9f8d68e4818913af28",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=289118bfb31cf4da7f18b52f49d4b5323299468e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bf3944d90b3c752805d02808f6d8a1f7bb9aa87a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=57af74c0dbffe776156ed3cfe8ff487f087da0d4",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=18a084198c43fec7cfd85211109df2e00512f2ff",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mhrx3m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roy3838",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/",
          "stickied": false,
          "url": "https://v.redd.it/g3pod2zlw2hf1",
          "subreddit_subscribers": 510259,
          "created_utc": 1754346605,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g3pod2zlw2hf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g3pod2zlw2hf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g3pod2zlw2hf1/DASHPlaylist.mpd?a=1756948483%2CZDAyNGQ4ODBkNzgyMzA4YTQ5ZmU5YzFiN2FhMDEyYzhmNWM2MTQwODU0ZGRjNTgyYzAwZTUzZjZjOTJlMWIzMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 248,
              "hls_url": "https://v.redd.it/g3pod2zlw2hf1/HLSPlaylist.m3u8?a=1756948483%2COTM5Njg0MDk5ZTJmMzU1ZGQxYWFjMzljZTllODYwZDk3YTRmMGFjNjFiMTk1MjRjYzAwNGIwMmY5YWE1YzhlNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of Qwen 2508 do you want tonight? ;)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 28,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhbvig",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 121,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 121,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/xW5rjzTOCVnresjRs9FDgm4fGUjo6_kF-kfsc1LJrK8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754309979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3f5by1b8wzgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3f5by1b8wzgf1.png?auto=webp&amp;s=8943dfd99bf22f0cc0c02f506e7d3fd14a2f8352",
                  "width": 1216,
                  "height": 246
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c79933ab77e3d643053f37ab6382908ac1eb9af",
                    "width": 108,
                    "height": 21
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=768aae7713836c3c2a3aea894b45a4f967d9fa2f",
                    "width": 216,
                    "height": 43
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b20bde67926e3e8040aaf57a6389e737abd4fc69",
                    "width": 320,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f4a0d87b3973237c269d2cef31fbc18fbe655b1",
                    "width": 640,
                    "height": 129
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2eec3557cccdfd47534f0ee7386653e12cd322ea",
                    "width": 960,
                    "height": 194
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db751d3024ba27f2cf55f1c5324f9b111e96ef8c",
                    "width": 1080,
                    "height": 218
                  }
                ],
                "variants": {},
                "id": "COyPpURqpsrEIUBXoEpCUbhYmsp15c4NUFjXeHFSIu8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhbvig",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/",
          "stickied": false,
          "url": "https://i.redd.it/3f5by1b8wzgf1.png",
          "subreddit_subscribers": 510259,
          "created_utc": 1754309979,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The results are a mix of real and made up characters. The signs are meaningless gibberish. ",
          "author_fullname": "t2_4xzh04rz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Qwen Image Japanese and Chinese text generation test",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ytw3w1q571hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/ytw3w1q571hf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5455a0d1240e8f3818f12f1ae7162e499ba0bacd"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/ytw3w1q571hf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2fc02c1db9517e5a3da80f98f9395e9665e953bf"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/ytw3w1q571hf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd776cee52c45fce24047b1c61ea8cb8a002373f"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/ytw3w1q571hf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e912f9c364bf84dfd18ebfc583902a6113a3adda"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/ytw3w1q571hf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=328e3f42a43d64d8a0c68cc9b1231fdf54eab76b"
                }
              ],
              "s": {
                "y": 1024,
                "x": 1024,
                "u": "https://preview.redd.it/ytw3w1q571hf1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=a00f5d79be8d1e6d371fcd9edceb99f1701e5cb6"
              },
              "id": "ytw3w1q571hf1"
            },
            "j5n4h1q571hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/j5n4h1q571hf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b7c3c17d352083caf44a51e5d8a01b7deab494a"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/j5n4h1q571hf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4192d8acab9a5ce70f8cff755f91b9c6d33b156"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/j5n4h1q571hf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f121ff1743de8d996748ff009a0e207b75e12e8"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/j5n4h1q571hf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e052a4ec51850bd8ea218b80d0af7a354f988519"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/j5n4h1q571hf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ed50d538c8e621889d77bdf2a08a2c3655fa98e"
                }
              ],
              "s": {
                "y": 1024,
                "x": 1024,
                "u": "https://preview.redd.it/j5n4h1q571hf1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=b34dfe2da20e42d3a1110d5737b72709dc72b894"
              },
              "id": "j5n4h1q571hf1"
            }
          },
          "name": "t3_1mhimmj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 51,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "j5n4h1q571hf1",
                "id": 721117274
              },
              {
                "media_id": "ytw3w1q571hf1",
                "id": 721117275
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/qgQuHunQQo7mnCA7jscRiLJC9qwUEnTBUGvNSAP0Kgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754325897,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The results are a mix of real and made up characters. The signs are meaningless gibberish. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mhimmj",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhimmj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shokuninstudio",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhimmj/qwen_image_japanese_and_chinese_text_generation/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mhimmj",
          "subreddit_subscribers": 510259,
          "created_utc": 1754325897,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen-Image · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhh6se",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 72,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 72,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=a720ed50938a0f7d099fa5095eeaa524819f6b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754322707,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen-Image",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?auto=webp&amp;s=7578d1430d51fe3898437256626f5bd7f9c643b5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55a19a341313ab08b43f3737ad0171a6dc27a3a6",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=561673e4e6ac3694e8d08fb8f3b50de1d4d7bafc",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5972624b3a9fe2057d6c44275f111b27e3e66505",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a65aaca919e956009709dd069f70c0c907403912",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b308acfa4755323a0b8731404fd7599940db7ca",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fcc7db6ec2f7f9365fb1224503b8e2a33a798c7",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhh6se",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhh6se/qwenqwenimage_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen-Image",
          "subreddit_subscribers": 510259,
          "created_utc": 1754322707,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Current status:\n\nhttps://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036\n\nEveryone get ready to fire up your GPUs...",
          "author_fullname": "t2_1utnp17o3h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 llama.cpp PR is nearing completion",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhb5el",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 100,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754307842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current status:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036\"&gt;https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Everyone get ready to fire up your GPUs...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhb5el",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistanceSolar1449",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754307842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Style control removed.\n\n|Rank (UB)|Model|Score|95% CI (±)|Votes|Company|License|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|gemini-2.5-pro|1470|±5|26,019|Google|Closed|\n|2|grok-4-0709|1435|±6|13,058|xAI|Closed|\n|2|glm-4.5|1435|±9|4,112|[Z.ai](http://Z.ai)|MIT|\n|2|chatgpt-4o-latest-20250326|1430|±5|30,777|Closed AI|Closed|\n|2|o3-2025-04-16|1429|±5|32,033|Closed AI|Closed|\n|2|deepseek-r1-0528|1427|±6|18,284|DeepSeek|MIT|\n|2|qwen3-235b-a22b-instruct-2507|1427|±9|4,154|Alibaba|Apache 2.0|\n\n[https://x.com/lmarena\\_ai/status/1952402506497020330](https://x.com/lmarena_ai/status/1952402506497020330)\n\n[https://lmarena.ai/leaderboard/text](https://lmarena.ai/leaderboard/text)",
          "author_fullname": "t2_m40tjcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM ranks #2 for chat according to lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhix7d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754326721,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754326551,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Style control removed.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Rank (UB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Score&lt;/th&gt;\n&lt;th align=\"left\"&gt;95% CI (±)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Votes&lt;/th&gt;\n&lt;th align=\"left\"&gt;Company&lt;/th&gt;\n&lt;th align=\"left\"&gt;License&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;gemini-2.5-pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;1470&lt;/td&gt;\n&lt;td align=\"left\"&gt;±5&lt;/td&gt;\n&lt;td align=\"left\"&gt;26,019&lt;/td&gt;\n&lt;td align=\"left\"&gt;Google&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok-4-0709&lt;/td&gt;\n&lt;td align=\"left\"&gt;1435&lt;/td&gt;\n&lt;td align=\"left\"&gt;±6&lt;/td&gt;\n&lt;td align=\"left\"&gt;13,058&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;glm-4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;1435&lt;/td&gt;\n&lt;td align=\"left\"&gt;±9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,112&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;MIT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;chatgpt-4o-latest-20250326&lt;/td&gt;\n&lt;td align=\"left\"&gt;1430&lt;/td&gt;\n&lt;td align=\"left\"&gt;±5&lt;/td&gt;\n&lt;td align=\"left\"&gt;30,777&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;o3-2025-04-16&lt;/td&gt;\n&lt;td align=\"left\"&gt;1429&lt;/td&gt;\n&lt;td align=\"left\"&gt;±5&lt;/td&gt;\n&lt;td align=\"left\"&gt;32,033&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;deepseek-r1-0528&lt;/td&gt;\n&lt;td align=\"left\"&gt;1427&lt;/td&gt;\n&lt;td align=\"left\"&gt;±6&lt;/td&gt;\n&lt;td align=\"left\"&gt;18,284&lt;/td&gt;\n&lt;td align=\"left\"&gt;DeepSeek&lt;/td&gt;\n&lt;td align=\"left\"&gt;MIT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3-235b-a22b-instruct-2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1427&lt;/td&gt;\n&lt;td align=\"left\"&gt;±9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,154&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache 2.0&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;a href=\"https://x.com/lmarena_ai/status/1952402506497020330\"&gt;https://x.com/lmarena_ai/status/1952402506497020330&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lmarena.ai/leaderboard/text\"&gt;https://lmarena.ai/leaderboard/text&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhix7d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Terminator857",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhix7d/glm_ranks_2_for_chat_according_to_lmarena/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhix7d/glm_ranks_2_for_chat_according_to_lmarena/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754326551,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just tested the new **Qwen-Image** model from Alibaba using 🤗 Diffusers with bfloat16 + dual-GPU memory config (4090 + 3060). Prompted it to generate a **cyberpunk night market scene**—complete with neon signs, rainy pavement, futuristic street food vendors, and a monorail in the background.\n\nRan at `1472x832`, 32 steps, `true_cfg_scale=3.0`. No LoRA, no refiner—just straight from the base checkpoint.\n\nFull prompt and code below. Let me know what you think of the result or if you’ve got prompt ideas to push it further.\n\n\\`\\`\\`\n\nfrom diffusers import DiffusionPipeline\n\nimport torch, gc\n\npipe = DiffusionPipeline.from\\_pretrained(\n\n\"Qwen/Qwen-Image\",\n\ntorch\\_dtype=torch.bfloat16,\n\ndevice\\_map=\"balanced\",\n\nmax\\_memory={0: \"23GiB\", 1: \"11GiB\"},\n\n)\n\npipe.enable\\_attention\\_slicing()\n\npipe.enable\\_vae\\_tiling()\n\nprompt = (\n\n\"A bustling cyberpunk night market street scene. Neon signs in Chinese hang above steaming food stalls. \"\n\n\"A robotic vendor is grilling skewers while a crowd of futuristic characters—some wearing glowing visors, \"\n\n\"some holding umbrellas under a light drizzle—gathers around. Bright reflections on the wet pavement. \"\n\n\"In the distance, a monorail passes by above the alley. Ultra HD, 4K, cinematic composition.\"\n\n)\n\nnegative\\_prompt = (\n\n\"low quality, blurry, distorted, bad anatomy, text artifacts, poor lighting\"\n\n)\n\nimg = pipe(\n\nprompt=prompt,\n\nnegative\\_prompt=negative\\_prompt,\n\nwidth=1472, height=832,\n\nnum\\_inference\\_steps=32,\n\ntrue\\_cfg\\_scale=3.0,\n\ngenerator=torch.Generator(\"cuda\").manual\\_seed(8899)\n\n).images\\[0\\]\n\nimg.save(\"qwen\\_cyberpunk\\_market.png\")\n\ndel pipe; gc.collect(); torch.cuda.empty\\_cache()\n\n\\`\\`\\`\n\nhttps://preview.redd.it/wizhuiymi2hf1.png?width=1472&amp;format=png&amp;auto=webp&amp;s=bede6c7a666d675a037a6216b9819b609fe6a2de\n\n",
          "author_fullname": "t2_1tp8zldw5g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Quick Qwen Image Gen with 4090+3060",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wizhuiymi2hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/wizhuiymi2hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b49bd0cdf9fd3ae805b04bd043f24358062e1aac"
                },
                {
                  "y": 122,
                  "x": 216,
                  "u": "https://preview.redd.it/wizhuiymi2hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=35a400a90c55230ce2a3db3b1dfb6aac99b93580"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/wizhuiymi2hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c110c546aa9cd2fba169e1fb8b33198b61becb8"
                },
                {
                  "y": 361,
                  "x": 640,
                  "u": "https://preview.redd.it/wizhuiymi2hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3b2e2f7232220c81e89a71c6c3d0dd13e8dd82d"
                },
                {
                  "y": 542,
                  "x": 960,
                  "u": "https://preview.redd.it/wizhuiymi2hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b77b212e93a6e8d43192049d4a8cdf8f708e777"
                },
                {
                  "y": 610,
                  "x": 1080,
                  "u": "https://preview.redd.it/wizhuiymi2hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc0e9a61c5104f49dc0e3ca7ddd410be40065897"
                }
              ],
              "s": {
                "y": 832,
                "x": 1472,
                "u": "https://preview.redd.it/wizhuiymi2hf1.png?width=1472&amp;format=png&amp;auto=webp&amp;s=bede6c7a666d675a037a6216b9819b609fe6a2de"
              },
              "id": "wizhuiymi2hf1"
            }
          },
          "name": "t3_1mhpm02",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3Ie8QfO5QUIJHiVOsdbZ1YC7qWfnOubml09yVMrrLOg.jpg",
          "edited": 1754341722,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754341198,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just tested the new &lt;strong&gt;Qwen-Image&lt;/strong&gt; model from Alibaba using 🤗 Diffusers with bfloat16 + dual-GPU memory config (4090 + 3060). Prompted it to generate a &lt;strong&gt;cyberpunk night market scene&lt;/strong&gt;—complete with neon signs, rainy pavement, futuristic street food vendors, and a monorail in the background.&lt;/p&gt;\n\n&lt;p&gt;Ran at &lt;code&gt;1472x832&lt;/code&gt;, 32 steps, &lt;code&gt;true_cfg_scale=3.0&lt;/code&gt;. No LoRA, no refiner—just straight from the base checkpoint.&lt;/p&gt;\n\n&lt;p&gt;Full prompt and code below. Let me know what you think of the result or if you’ve got prompt ideas to push it further.&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;from diffusers import DiffusionPipeline&lt;/p&gt;\n\n&lt;p&gt;import torch, gc&lt;/p&gt;\n\n&lt;p&gt;pipe = DiffusionPipeline.from_pretrained(&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Qwen/Qwen-Image&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;torch_dtype=torch.bfloat16,&lt;/p&gt;\n\n&lt;p&gt;device_map=&amp;quot;balanced&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;max_memory={0: &amp;quot;23GiB&amp;quot;, 1: &amp;quot;11GiB&amp;quot;},&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;pipe.enable_attention_slicing()&lt;/p&gt;\n\n&lt;p&gt;pipe.enable_vae_tiling()&lt;/p&gt;\n\n&lt;p&gt;prompt = (&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A bustling cyberpunk night market street scene. Neon signs in Chinese hang above steaming food stalls. &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A robotic vendor is grilling skewers while a crowd of futuristic characters—some wearing glowing visors, &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;some holding umbrellas under a light drizzle—gathers around. Bright reflections on the wet pavement. &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;In the distance, a monorail passes by above the alley. Ultra HD, 4K, cinematic composition.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;negative_prompt = (&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;low quality, blurry, distorted, bad anatomy, text artifacts, poor lighting&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;img = pipe(&lt;/p&gt;\n\n&lt;p&gt;prompt=prompt,&lt;/p&gt;\n\n&lt;p&gt;negative_prompt=negative_prompt,&lt;/p&gt;\n\n&lt;p&gt;width=1472, height=832,&lt;/p&gt;\n\n&lt;p&gt;num_inference_steps=32,&lt;/p&gt;\n\n&lt;p&gt;true_cfg_scale=3.0,&lt;/p&gt;\n\n&lt;p&gt;generator=torch.Generator(&amp;quot;cuda&amp;quot;).manual_seed(8899)&lt;/p&gt;\n\n&lt;p&gt;).images[0]&lt;/p&gt;\n\n&lt;p&gt;img.save(&amp;quot;qwen_cyberpunk_market.png&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;del pipe; gc.collect(); torch.cuda.empty_cache()&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wizhuiymi2hf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bede6c7a666d675a037a6216b9819b609fe6a2de\"&gt;https://preview.redd.it/wizhuiymi2hf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bede6c7a666d675a037a6216b9819b609fe6a2de&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhpm02",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fp4guru",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754341198,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**I’ve worked really hard and launched a FREE resource with 30+ detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.**\n\nThe tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.\n\nThe response so far has been incredible! (the repo got nearly 10,000 stars in one month from launch - all organic) This is part of my broader effort to create high-quality open source educational material. I already have over 130 code tutorials on GitHub with over 50,000 stars.\n\nI hope you find it useful. The tutorials are available here: [https://github.com/NirDiamant/agents-towards-production](https://github.com/NirDiamant/agents-towards-production)\n\n(most of the tutorials can be run locally, but some of them don't, so please enjoy those who are and don't hate me for those how aren't :D )\n\nThe content is organized into these categories:\n\n1. Orchestration\n2. Tool integration\n3. Observability\n4. Deployment\n5. Memory\n6. UI &amp; Frontend\n7. Agent Frameworks\n8. Model Customization\n9. Multi-agent Coordination\n10. Security\n11. Evaluation\n12. Tracing &amp; Debugging\n13. Web Scraping",
          "author_fullname": "t2_4x84zf5l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A free goldmine of tutorials for the components you need to create production-level agents\nExtensive open source resource with tutorials for creating robust AI agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhy47",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754324380,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;I’ve worked really hard and launched a FREE resource with 30+ detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.&lt;/p&gt;\n\n&lt;p&gt;The response so far has been incredible! (the repo got nearly 10,000 stars in one month from launch - all organic) This is part of my broader effort to create high-quality open source educational material. I already have over 130 code tutorials on GitHub with over 50,000 stars.&lt;/p&gt;\n\n&lt;p&gt;I hope you find it useful. The tutorials are available here: &lt;a href=\"https://github.com/NirDiamant/agents-towards-production\"&gt;https://github.com/NirDiamant/agents-towards-production&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(most of the tutorials can be run locally, but some of them don&amp;#39;t, so please enjoy those who are and don&amp;#39;t hate me for those how aren&amp;#39;t :D )&lt;/p&gt;\n\n&lt;p&gt;The content is organized into these categories:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Orchestration&lt;/li&gt;\n&lt;li&gt;Tool integration&lt;/li&gt;\n&lt;li&gt;Observability&lt;/li&gt;\n&lt;li&gt;Deployment&lt;/li&gt;\n&lt;li&gt;Memory&lt;/li&gt;\n&lt;li&gt;UI &amp;amp; Frontend&lt;/li&gt;\n&lt;li&gt;Agent Frameworks&lt;/li&gt;\n&lt;li&gt;Model Customization&lt;/li&gt;\n&lt;li&gt;Multi-agent Coordination&lt;/li&gt;\n&lt;li&gt;Security&lt;/li&gt;\n&lt;li&gt;Evaluation&lt;/li&gt;\n&lt;li&gt;Tracing &amp;amp; Debugging&lt;/li&gt;\n&lt;li&gt;Web Scraping&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?auto=webp&amp;s=d017e4b3980f41b097659de2d8b747fc4e9c4c69",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45b07e7616995751a757eb80d771bad2ee406619",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=562290004967cb312a5c268f53c8c35b73c5f15f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b3c364fe34ee3f00d4c026b7767af05aee0562fa",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bd6cf48cc2c23488347c25c8e9102da79a44ec5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f66d0734736f318d392878aec339b4d288894401",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2ceace9004b5c00980685a03a3ff9c4aa2227ed",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "23K9xyykQzGugCXJyC20OMixBbwPZe-S5vv1or7jJHM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhhy47",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nir777",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhy47/a_free_goldmine_of_tutorials_for_the_components/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhhy47/a_free_goldmine_of_tutorials_for_the_components/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754324380,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tescent has released new models (llama.cpp support is already merged!)\n\n[https://huggingface.co/tencent/Hunyuan-7B-Instruct](https://huggingface.co/tencent/Hunyuan-7B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-4B-Instruct](https://huggingface.co/tencent/Hunyuan-4B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-1.8B-Instruct](https://huggingface.co/tencent/Hunyuan-1.8B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-0.5B-Instruct](https://huggingface.co/tencent/Hunyuan-0.5B-Instruct)\n\n# Model Introduction\n\nHunyuan is Tencent's open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.\n\nWe have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.\n\n# \n\n# Key Features and Advantages\n\n* **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n* **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n* **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.\n* **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\nUPDATE\n\npretrain models\n\n[https://huggingface.co/tencent/Hunyuan-7B-Pretrain](https://huggingface.co/tencent/Hunyuan-7B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-4B-Pretrain](https://huggingface.co/tencent/Hunyuan-4B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain](https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain](https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain)\n\nGGUFs\n\n[https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new Hunyuan Instruct 7B/4B/1.8B/0.5B models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh3s7q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 256,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 256,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754292850,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754280980,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tescent has released new models (llama.cpp support is already merged!)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-7B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-7B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-4B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-4B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-1.8B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-0.5B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Model Introduction&lt;/h1&gt;\n\n&lt;p&gt;Hunyuan is Tencent&amp;#39;s open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.&lt;/p&gt;\n\n&lt;p&gt;We have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.&lt;/p&gt;\n\n&lt;h1&gt;Key Features and Advantages&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Hybrid Reasoning Support&lt;/strong&gt;: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ultra-Long Context Understanding&lt;/strong&gt;: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced Agent Capabilities&lt;/strong&gt;: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;UPDATE&lt;/p&gt;\n\n&lt;p&gt;pretrain models&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-7B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-7B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-4B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-4B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GGUFs&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?auto=webp&amp;s=6f622eceb5c359f202e3b99375e5e58502d2ec49",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d7d208147546310820cea26a2856210455054de",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3afe2af4a5c98a2eeb74d6b8f4e87d8e7d67379",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b15fc318112698d6e74e7ae2a79fe2b70cfdb24b",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=12addf5b08bd142a26edf444c94debefdd48b9e6",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1df2789194ea707b707209c04d7485c60a743d3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=858c8b8c5cba69037df907c1924a66347161a674",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh3s7q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754280980,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hunyuan just released 4 new dense models. It’s a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.\n\nLove to see more small models as I'm developing an iOS local chat called [Locally AI](https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692). Will look to add them but since it's new architecture it will need to be ported to Apple MLX.\n\nThe choice of size here is perfect:\n\n- 0.5B, 1.8B and 4B great for all iPhones models\n- 7B great for iPad with M chip",
          "author_fullname": "t2_1jgkfm9u25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New small models from Hunyuan (0.5B, 1.8B, 4B, 7B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kgm0t9q6gygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab02c39ede4b3675d5617c5a8ddb2fb3f55006f4"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f682f7d72c17329d449b47977069ca02c39fa1e6"
                },
                {
                  "y": 226,
                  "x": 320,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6612e1a30d154e85c954c2f4bf92f9692eb8a014"
                },
                {
                  "y": 452,
                  "x": 640,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e0ac6cc6275921def7a509a1781af2b65af8bde6"
                },
                {
                  "y": 678,
                  "x": 960,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a477236ce1c7be739f9e2ebe072ecc9b8fd2d81"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7550d21b9e1e389fe9d001a54e3fd11a2930b7e2"
                }
              ],
              "s": {
                "y": 1017,
                "x": 1440,
                "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=a8519dca9785d55ee61e5339bdaeb27132f847db"
              },
              "id": "kgm0t9q6gygf1"
            },
            "gjb5n6r6gygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa3891d5431eebe6ada8791f8182fd2d48e67ee5"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8334b244623b60f52c4f15cb7063d53958162e33"
                },
                {
                  "y": 226,
                  "x": 320,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d1ba8e6a77f342fc6a563af8de0367ed05e2cd0"
                },
                {
                  "y": 452,
                  "x": 640,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d76dab340c1fa53b85a369ea6e8b95108c9bde7"
                },
                {
                  "y": 678,
                  "x": 960,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8154e0ac9db723bf05c41f09b4e569480e065627"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e967a101a45103a134b4fc9b01af49525bbf803"
                }
              ],
              "s": {
                "y": 1017,
                "x": 1440,
                "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=3be7da2fbfc3a5e6e7e4a66e1e0acf38c12b3156"
              },
              "id": "gjb5n6r6gygf1"
            }
          },
          "name": "t3_1mh6z16",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 144,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "kgm0t9q6gygf1",
                "id": 720841480
              },
              {
                "media_id": "gjb5n6r6gygf1",
                "id": 720841481
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 144,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/r2IMyxKAeZgSqc_fmrMhOP7SMYeTy8apLYYgezLqpRg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754292468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hunyuan just released 4 new dense models. It’s a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.&lt;/p&gt;\n\n&lt;p&gt;Love to see more small models as I&amp;#39;m developing an iOS local chat called &lt;a href=\"https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692\"&gt;Locally AI&lt;/a&gt;. Will look to add them but since it&amp;#39;s new architecture it will need to be ported to Apple MLX.&lt;/p&gt;\n\n&lt;p&gt;The choice of size here is perfect:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;0.5B, 1.8B and 4B great for all iPhones models&lt;/li&gt;\n&lt;li&gt;7B great for iPad with M chip&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mh6z16",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh6z16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adrgrondin",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh6z16/new_small_models_from_hunyuan_05b_18b_4b_7b/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mh6z16",
          "subreddit_subscribers": 510259,
          "created_utc": 1754292468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I got frustrated trying to use GitHub Copilot in restricted environments, airgapped networks, legacy IDEs, places with no extensions allowed. So I built [**QuietPrompt**](https://github.com/viktorfaubl/QuietPrompt), a local-first Copilot-style helper. It comes with an installer to Windows too, [Installer](https://github.com/viktorfaubl/QuietPrompt/releases/tag/v1.0.1).\n\nIt reads your screen (via hotkey OCR), listens to your mic (Whisper), or grabs from clipboard, and pipes it to your own LLM, totally offline. Works with Qwen 30B but requires a heavy PC, 32GB RAM min and a 3060 Ti GPU min.\n\nOpen-source, free for personal use, Windows-only for now. Happy to hear what you'd improve or add.\n\n[Overlay](https://preview.redd.it/a90esne532hf1.png?width=2559&amp;format=png&amp;auto=webp&amp;s=c6d31471235423f1e83f353e55037317583fc851)\n\n[System Tray](https://preview.redd.it/71ydew6732hf1.png?width=311&amp;format=png&amp;auto=webp&amp;s=e3d8d2b6b12fcd6196f2d3067bcd90ab7cd17f46)\n\n",
          "author_fullname": "t2_p9c3a6hj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built my own Copilot-style assistant that works offline, with screen/mic/text input",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "a90esne532hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 21,
                  "x": 108,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8e09852c31ff8a16d99e87089223139b887d9de"
                },
                {
                  "y": 42,
                  "x": 216,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a98b61371abe809adc084f08f122a6f671efb6d3"
                },
                {
                  "y": 63,
                  "x": 320,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=70d910a057e05136ff023ea4f6c9ccaef2caad1d"
                },
                {
                  "y": 127,
                  "x": 640,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7974b21f949296cc70eae62258417250d1d92e96"
                },
                {
                  "y": 190,
                  "x": 960,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55ee0575d5a35cf66cdc12daa130d6305c4bfb4e"
                },
                {
                  "y": 214,
                  "x": 1080,
                  "u": "https://preview.redd.it/a90esne532hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=169f8f054401fb8bf3bc396dc82bb8c832fedbed"
                }
              ],
              "s": {
                "y": 508,
                "x": 2559,
                "u": "https://preview.redd.it/a90esne532hf1.png?width=2559&amp;format=png&amp;auto=webp&amp;s=c6d31471235423f1e83f353e55037317583fc851"
              },
              "id": "a90esne532hf1"
            },
            "71ydew6732hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 123,
                  "x": 108,
                  "u": "https://preview.redd.it/71ydew6732hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab83cbc84dcebef553b7cf60564c471f070581c2"
                },
                {
                  "y": 246,
                  "x": 216,
                  "u": "https://preview.redd.it/71ydew6732hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b433c06ca8095004a5eb0359efb72e0eb88db21"
                }
              ],
              "s": {
                "y": 355,
                "x": 311,
                "u": "https://preview.redd.it/71ydew6732hf1.png?width=311&amp;format=png&amp;auto=webp&amp;s=e3d8d2b6b12fcd6196f2d3067bcd90ab7cd17f46"
              },
              "id": "71ydew6732hf1"
            }
          },
          "name": "t3_1mhng5b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=40de8b1d66a8f64f3c7c5f40dd4ccd15dafc22d7",
          "edited": 1754336536,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754336332,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got frustrated trying to use GitHub Copilot in restricted environments, airgapped networks, legacy IDEs, places with no extensions allowed. So I built &lt;a href=\"https://github.com/viktorfaubl/QuietPrompt\"&gt;&lt;strong&gt;QuietPrompt&lt;/strong&gt;&lt;/a&gt;, a local-first Copilot-style helper. It comes with an installer to Windows too, &lt;a href=\"https://github.com/viktorfaubl/QuietPrompt/releases/tag/v1.0.1\"&gt;Installer&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;It reads your screen (via hotkey OCR), listens to your mic (Whisper), or grabs from clipboard, and pipes it to your own LLM, totally offline. Works with Qwen 30B but requires a heavy PC, 32GB RAM min and a 3060 Ti GPU min.&lt;/p&gt;\n\n&lt;p&gt;Open-source, free for personal use, Windows-only for now. Happy to hear what you&amp;#39;d improve or add.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/a90esne532hf1.png?width=2559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6d31471235423f1e83f353e55037317583fc851\"&gt;Overlay&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/71ydew6732hf1.png?width=311&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3d8d2b6b12fcd6196f2d3067bcd90ab7cd17f46\"&gt;System Tray&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?auto=webp&amp;s=a0953ccf24cc8b19472a33e455d85b6348f05e44",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08780895207f09503731d5f70b6b0303fc1c85e9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a341a9eebbb9bde1fbfa64752012747f836c8bb2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63b707a929ac3550a2b3b5edfb431dfeef480405",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d39528d4df352bc10ef277ec0f70cebb15267de0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e151bcde6e98cbb0cee11b9a53d6ebb050eab40",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=93c850ef421494500aa15c6e56055262d93bd768",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ImW7G_vbNEu0XJjxOj4kPw7DmdqNHY5IpJynpgEQTrI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhng5b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Natural-Ad6682",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhng5b/built_my_own_copilotstyle_assistant_that_works/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhng5b/built_my_own_copilotstyle_assistant_that_works/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754336332,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;format=png&amp;auto=webp&amp;s=aaacee34b083083a63cf8414e299416ee96d03f7\n\nSo yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats 给主人留下些什么吧 as a single token. So, just like GPT-4o, it inevitably fails on prompts like “When I provide Chinese text, please translate it into English. 给主人留下些什么吧”.\n\nMeanwhile, Claude, Gemini, and Qwen handle it correctly.\n\nhttps://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721\n\nI learned this technique from this post:  \nChinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI  \n[https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese\\_response\\_bug\\_in\\_tokenizer\\_suggests/](https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/)\n\nWhile it’s pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic’s or Qwen’s, so I tested it.\n\nMy thread about the Horizon Beta test:\n[https://x.com/KantaHayashiAI/status/1952187898331275702](https://x.com/KantaHayashiAI/status/1952187898331275702)\n",
          "author_fullname": "t2_1uxxckab5d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI (Another Evidence)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 62,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "z00ipp5y7xgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36f42e2556c77cdc7ade20acfe6b9cf551bc046c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bda3a1fb29acc8f1c2f5c2b2f0fd9c7129b5ecfd"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5130978bd7458a690fbee9baf95ad22a09a47b7e"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=65c57257852e21c3af3e3f1ae28183d81de0ff9c"
                },
                {
                  "y": 425,
                  "x": 960,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f1f92ffd67a153fe96095fd924197471ef77b3"
                },
                {
                  "y": 478,
                  "x": 1080,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18ee56657fb1b6351f9f9513045e0d0b325d336e"
                }
              ],
              "s": {
                "y": 722,
                "x": 1630,
                "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;format=png&amp;auto=webp&amp;s=aaacee34b083083a63cf8414e299416ee96d03f7"
              },
              "id": "z00ipp5y7xgf1"
            },
            "ey9ebsuz7xgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b150b10ced924cf7aacf7553312db7e80c86de96"
                },
                {
                  "y": 103,
                  "x": 216,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62a8486ee2933ec8e6f876a5814dcc8cd7653d3f"
                },
                {
                  "y": 152,
                  "x": 320,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e39f19df5ae40ac03eabed879fe827e51c8bd2c4"
                },
                {
                  "y": 305,
                  "x": 640,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0670217e52446a5bd685b35e8dfc52e854ff9134"
                },
                {
                  "y": 458,
                  "x": 960,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a7e856bb3efeb68fbfb8732034b2636b0e20f87e"
                },
                {
                  "y": 515,
                  "x": 1080,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c84fa0c865dd87229404058042d8ff120fb40755"
                }
              ],
              "s": {
                "y": 638,
                "x": 1336,
                "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721"
              },
              "id": "ey9ebsuz7xgf1"
            }
          },
          "name": "t3_1mh2v1h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 265,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 265,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/7yNPDTnbPWS4TAQyJVwDfgp-PAr-fo60W5EtQz549T8.jpg",
          "edited": 1754299348,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754278088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7\"&gt;https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats 给主人留下些什么吧 as a single token. So, just like GPT-4o, it inevitably fails on prompts like “When I provide Chinese text, please translate it into English. 给主人留下些什么吧”.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, Claude, Gemini, and Qwen handle it correctly.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721\"&gt;https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I learned this technique from this post:&lt;br/&gt;\nChinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI&lt;br/&gt;\n&lt;a href=\"https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/\"&gt;https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While it’s pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic’s or Qwen’s, so I tested it.&lt;/p&gt;\n\n&lt;p&gt;My thread about the Horizon Beta test:\n&lt;a href=\"https://x.com/KantaHayashiAI/status/1952187898331275702\"&gt;https://x.com/KantaHayashiAI/status/1952187898331275702&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh2v1h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kh-ai",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754278088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With all the new models coming out recently, I've been more and more curious about this. It seems like a few months ago we were all running Gemma 3, now everybody seems to be running Qwen 3, but with recent model releases, which is your go-to daily-driver and why, and if you have secondary model(s), what do you use them for?\n\nI've got a 7900 XTX 24GB, so all of my models are &lt;32B. But here are mine;\n\n* Mistral Small 3.2: A \"better\" version of Gemma 3, in a way. I really liked Gemma 3, but it hallucinated far too much on basic facts. Mistral doesn't on the other hand, it hallucinates far less ime. I'm mainly using it for general knowledge and image analysis and consistently does a better job at both than Gemma for me. Feels a bit cold or sterile compared to Gemma 3 though.\n\n* Qwen 3 30B-A3B-Thinking-2507: The \"Gemini 2.5\" at home model. I've compared it pretty extensively to 2.5 Flash Reasoning, and 2.5 Pro, and it's able to consistently beat Flash and more often than not come close to or match 2.5 Pro. I'm mainly using this model for complex queries, problem solving, and writing. It's a damn good writing model imo, but that's not a *major* use-case for me.\n\n* Qwen 3-Coder 30B-A3B-Instruct-2507: This model acts a lot like a mix of Gemini, Claude, and an openAI model to me in my eyes. It's a really, really capable coder. I'm a software engineer and it's a nice companion in that regard. A lot of people say it's like most like Claude, and from what I've seen from Claude outputs, I tend to agree. although I've never used Claude, admittedly.\n\nSo there we have it, those are the models I use and the use-case for each. I do occasionally use OpenRouter to serve GLM 4.5-Air and Kimi K2, but that's mostly just out of curiosity. So what's everybody else here running?",
          "author_fullname": "t2_oqajf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's your 'primary' model and why? Do you run a secondary model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhp2e5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754339961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With all the new models coming out recently, I&amp;#39;ve been more and more curious about this. It seems like a few months ago we were all running Gemma 3, now everybody seems to be running Qwen 3, but with recent model releases, which is your go-to daily-driver and why, and if you have secondary model(s), what do you use them for?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a 7900 XTX 24GB, so all of my models are &amp;lt;32B. But here are mine;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Mistral Small 3.2: A &amp;quot;better&amp;quot; version of Gemma 3, in a way. I really liked Gemma 3, but it hallucinated far too much on basic facts. Mistral doesn&amp;#39;t on the other hand, it hallucinates far less ime. I&amp;#39;m mainly using it for general knowledge and image analysis and consistently does a better job at both than Gemma for me. Feels a bit cold or sterile compared to Gemma 3 though.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Qwen 3 30B-A3B-Thinking-2507: The &amp;quot;Gemini 2.5&amp;quot; at home model. I&amp;#39;ve compared it pretty extensively to 2.5 Flash Reasoning, and 2.5 Pro, and it&amp;#39;s able to consistently beat Flash and more often than not come close to or match 2.5 Pro. I&amp;#39;m mainly using this model for complex queries, problem solving, and writing. It&amp;#39;s a damn good writing model imo, but that&amp;#39;s not a &lt;em&gt;major&lt;/em&gt; use-case for me.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Qwen 3-Coder 30B-A3B-Instruct-2507: This model acts a lot like a mix of Gemini, Claude, and an openAI model to me in my eyes. It&amp;#39;s a really, really capable coder. I&amp;#39;m a software engineer and it&amp;#39;s a nice companion in that regard. A lot of people say it&amp;#39;s like most like Claude, and from what I&amp;#39;ve seen from Claude outputs, I tend to agree. although I&amp;#39;ve never used Claude, admittedly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So there we have it, those are the models I use and the use-case for each. I do occasionally use OpenRouter to serve GLM 4.5-Air and Kimi K2, but that&amp;#39;s mostly just out of curiosity. So what&amp;#39;s everybody else here running?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhp2e5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ayylmaonade",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754339961,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/ggml-org/llama.cpp/pull/14939](https://github.com/ggml-org/llama.cpp/pull/14939)",
          "author_fullname": "t2_11m4x2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looks like GGUF for GLM 4.5 may be getting closer to a reality.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mheij9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754316632,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939\"&gt;https://github.com/ggml-org/llama.cpp/pull/14939&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?auto=webp&amp;s=083930ea54b88a7f6eaadda136c2185460baf66e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=78369c4a613d24a26f628c7b0d0788fbd02727b4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7d7893c234acd63db0445e0010c29d3054bf72a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=be66666d37f3f19b8f252dc5f32ba0b7be39e97c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e04ffa9cbd0a435f87d74eaf876a5853c1e06023",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=658e3d76ab28bc884f80a72e29bf1040fe464132",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91b76178fba350b1d785fbd980fb39979366d649",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mheij9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jeffwadsworth",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mheij9/looks_like_gguf_for_glm_45_may_be_getting_closer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mheij9/looks_like_gguf_for_glm_45_may_be_getting_closer/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754316632,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA, I am considering buying an M3 mac studio for local LLM server needs\n\nThe needs are as follows\n\n\\&gt;run LLM models LOCALLY (locality is non-negotiable)\n\n\\&gt;stream files, videos across multiple computers, emails and other basic server operations\n\nThe big limitation is, currently, we don't have the infrastructure to host larger servers, and for the time being, the LLM models the M3 studio can run are the main priorities.\n\nIf the mac studio can be sufficient as a server that we can safely, remotely log into, as well as download files, or stream files from, then it works great as we have an offer from a seller. If the M3 can work, under the current constraints, it would be perfect, but not sure how macOS would function as a small server for LLMs.\n\nIf not, we will focus on eliminating our current constraints and consider other options.\n\nThanks!",
          "author_fullname": "t2_s784can9o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Maxed out M3 Mac studio as an LLM server for local employees?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhqgv1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754343447,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754343152,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, I am considering buying an M3 mac studio for local LLM server needs&lt;/p&gt;\n\n&lt;p&gt;The needs are as follows&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;run LLM models LOCALLY (locality is non-negotiable)&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;stream files, videos across multiple computers, emails and other basic server operations&lt;/p&gt;\n\n&lt;p&gt;The big limitation is, currently, we don&amp;#39;t have the infrastructure to host larger servers, and for the time being, the LLM models the M3 studio can run are the main priorities.&lt;/p&gt;\n\n&lt;p&gt;If the mac studio can be sufficient as a server that we can safely, remotely log into, as well as download files, or stream files from, then it works great as we have an offer from a seller. If the M3 can work, under the current constraints, it would be perfect, but not sure how macOS would function as a small server for LLMs.&lt;/p&gt;\n\n&lt;p&gt;If not, we will focus on eliminating our current constraints and consider other options.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhqgv1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Manderbillt2000",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754343152,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve just finished building a desk-side powerhouse and I want to run a community-driven series of inference latency benchmarks (in ms) on the latest high-performance open-source LLMs (&gt; 70 B parameters), including models like QWEN, GLM 4.5, and others you recommend. \n\nI’m also keen to try any RAM/CPU tricks for models that may not fit entirely in GPU VRAM—e.g. K-Transformers, paged-KV llama.cpp, host-offload, thread-affinity hacks, etc.\n\nIf your interested in a benchmark send me: \n\n- Model (&gt; 70 B): NUQUEN-xxx, GLM 4.5,etc\n- Quantization: Q4_K, Q5_K, BF16, FP16, 4-bit, etc.\n- Engine: vLLM, k-Transformers, llama.cpp (paged KV), Text Generation Inference (TGI), etc.\n- Context length: e.g. 32 K, 100 K tokens\n- Batch size: default 1 unless you specify otherwise\nAny Tricks: any settings or hacks to leverage host RAM, CPU offload, etc.\n\nI’ll report back with:\n- Generation latency (e.g. tg128)\n- Prompt-processing throughput (e.g. pp512)\n\nMy system specs:\n\nCPU: AMD EPYC 9255 (24 C / 48 T)\nRAM: 12 × 64 GB DDR5-6000 ≈ 760 GiB\nGPUs: 2 × RTX PRO 6000 Blackwell Max-Q (96 GB VRAM each)\nOS: Pop!_OS",
          "author_fullname": "t2_5l4zmzcw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Offering Benchmarks on my New RIG for Larger Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhoaxs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754338256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve just finished building a desk-side powerhouse and I want to run a community-driven series of inference latency benchmarks (in ms) on the latest high-performance open-source LLMs (&amp;gt; 70 B parameters), including models like QWEN, GLM 4.5, and others you recommend. &lt;/p&gt;\n\n&lt;p&gt;I’m also keen to try any RAM/CPU tricks for models that may not fit entirely in GPU VRAM—e.g. K-Transformers, paged-KV llama.cpp, host-offload, thread-affinity hacks, etc.&lt;/p&gt;\n\n&lt;p&gt;If your interested in a benchmark send me: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Model (&amp;gt; 70 B): NUQUEN-xxx, GLM 4.5,etc&lt;/li&gt;\n&lt;li&gt;Quantization: Q4_K, Q5_K, BF16, FP16, 4-bit, etc.&lt;/li&gt;\n&lt;li&gt;Engine: vLLM, k-Transformers, llama.cpp (paged KV), Text Generation Inference (TGI), etc.&lt;/li&gt;\n&lt;li&gt;Context length: e.g. 32 K, 100 K tokens&lt;/li&gt;\n&lt;li&gt;Batch size: default 1 unless you specify otherwise\nAny Tricks: any settings or hacks to leverage host RAM, CPU offload, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I’ll report back with:\n- Generation latency (e.g. tg128)\n- Prompt-processing throughput (e.g. pp512)&lt;/p&gt;\n\n&lt;p&gt;My system specs:&lt;/p&gt;\n\n&lt;p&gt;CPU: AMD EPYC 9255 (24 C / 48 T)\nRAM: 12 × 64 GB DDR5-6000 ≈ 760 GiB\nGPUs: 2 × RTX PRO 6000 Blackwell Max-Q (96 GB VRAM each)\nOS: Pop!_OS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhoaxs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Infamous_Jaguar_2151",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754338256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been debating the use of quantized KV cache with llama.cpp (no less than q8) for a long time, but I still can't tell if it's a good idea:\n\n* On one hand, the [original PR](https://github.com/ggml-org/llama.cpp/pull/7412) mentions that perplexity is more sensitive to model weight quants than to KV cache. Or in other words, quantizing kv cache is worth it if it frees memory for a slightly less quantized model (eg q4kL vs q4kM).\n* On the other hand, many commenters in this community seem to strongly suggest against it.\n* My own experience hasn't been conclusive one way or another. In theory, more quantization = less quality, but I don't have a concrete measure of the degradation introduced by quantized kv cache to rule out the idea entirely.\n\nSo I'm here to gauge everyone's experience with this, hopefully someone has a strong argument with or against that feature.\n\nThank you!",
          "author_fullname": "t2_nc2u4f7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the verdict on using quantized KV cache?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhlj69",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754335757,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754332125,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been debating the use of quantized KV cache with llama.cpp (no less than q8) for a long time, but I still can&amp;#39;t tell if it&amp;#39;s a good idea:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On one hand, the &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/7412\"&gt;original PR&lt;/a&gt; mentions that perplexity is more sensitive to model weight quants than to KV cache. Or in other words, quantizing kv cache is worth it if it frees memory for a slightly less quantized model (eg q4kL vs q4kM).&lt;/li&gt;\n&lt;li&gt;On the other hand, many commenters in this community seem to strongly suggest against it.&lt;/li&gt;\n&lt;li&gt;My own experience hasn&amp;#39;t been conclusive one way or another. In theory, more quantization = less quality, but I don&amp;#39;t have a concrete measure of the degradation introduced by quantized kv cache to rule out the idea entirely.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So I&amp;#39;m here to gauge everyone&amp;#39;s experience with this, hopefully someone has a strong argument with or against that feature.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?auto=webp&amp;s=9747fc98599981e69ef3dbabd18ac955767b61a1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=719eefafd72ad9dfefe61307fd88e8316866de92",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ae4f338ba2a27a6abf4d22dbfd20af9b475a6e6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8782f872b86a598db0a3a6e2778f7fb6d2731edb",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6c1e124ade737ee359be91a0766310ce074cadd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0453b80458e72b57f76e7907738a09419b3abbb2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a6f64546df415a5fb04416ad5ef8205b6fb933c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhlj69",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ParaboloidalCrest",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754332125,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bolt Graphics’ Zeus GPU Makes Bold Claim of Outperforming NVIDIA’s RTX 5090 by 10x in Rendering Workloads, That Too Using Laptop-Grade Memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhfbsi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=e3db4f91381596fd7b067db702881b90371ab295",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754318547,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/bolt-graphics-zeus-gpu-makes-bold-claim-of-outperforming-rtx-5090-by-10x-in-rendering-workloads/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?auto=webp&amp;s=5baaa86e6e026ac34488570fe1ff2f085cdbd5e1",
                  "width": 728,
                  "height": 409
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7bb50bf5bad3f11662fc8e79df150764f625417a",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=409e4417ae6631361d5a51c8a9bc3820aea75d3c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c019f1c0adf4ee6e58ca0c4b449ae4acc19760f6",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ae5fe39ac56dc9dbf574d38ebae1b5f7a328588",
                    "width": 640,
                    "height": 359
                  }
                ],
                "variants": {},
                "id": "NOVvMZCeb6MJYdVCo6dYfR7H6AIJZt8iOvly0YwXZZ0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhfbsi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhfbsi/bolt_graphics_zeus_gpu_makes_bold_claim_of/",
          "stickied": false,
          "url": "https://wccftech.com/bolt-graphics-zeus-gpu-makes-bold-claim-of-outperforming-rtx-5090-by-10x-in-rendering-workloads/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754318547,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After being frustrated with Ollama about their slow new model support thair configs etc, \n\nI built Mindforge, a tiny Python tool that runs Hugging Face and GGUF models locally with an OpenAI-compatible API. It’s like a minimal Ollama but in Python, using transformers and llama.cpp.\n\nHighlights:\n\n• Run HF (transformer) or GGUF (llama.cpp) models locally\n• OpenAI-compatible endpoints: /v1/models, /v1/completions, /v1/chat/completions, /v1/embeddings\n• Simple CLI (interactive REPL, pull/list/rm/create)\n• Modelfile support (FROM/TAGS/PARAMS/SYSTEM) for custom models\n\nExample GGUF: mindforge run unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q4_K_M\n\n[Repo](https://github.com/Exw27/mindforge)\n\nWould love feedback!\n\nNote. \nIt is still experimental, and some things might not work.",
          "author_fullname": "t2_2fnzmgph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mindforge — Ollama-like local LLM runner with HF + GGUF, OpenAI-compatible API",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhmgci",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754334135,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After being frustrated with Ollama about their slow new model support thair configs etc, &lt;/p&gt;\n\n&lt;p&gt;I built Mindforge, a tiny Python tool that runs Hugging Face and GGUF models locally with an OpenAI-compatible API. It’s like a minimal Ollama but in Python, using transformers and llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;Highlights:&lt;/p&gt;\n\n&lt;p&gt;• Run HF (transformer) or GGUF (llama.cpp) models locally\n• OpenAI-compatible endpoints: /v1/models, /v1/completions, /v1/chat/completions, /v1/embeddings\n• Simple CLI (interactive REPL, pull/list/rm/create)\n• Modelfile support (FROM/TAGS/PARAMS/SYSTEM) for custom models&lt;/p&gt;\n\n&lt;p&gt;Example GGUF: mindforge run unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q4_K_M&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Exw27/mindforge\"&gt;Repo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback!&lt;/p&gt;\n\n&lt;p&gt;Note. \nIt is still experimental, and some things might not work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?auto=webp&amp;s=c2468c4a9fbe9613a83b6f3af40878f060d08308",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d06f68b2553cb89685c7d7b8d1c373c7cba8d93e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4fd1dd51ef98e008d443a8a43e313099da5d592",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1383cd994f4f1aa261cd7434c48feb1b27a327b1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=75a5bc866f173516cb7ac9b2235c800f040e793d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e5b14f24507b7186bab893742aa0e2de25a6259",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a65296d3503db19920c7585291152b38ed7424a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "EZzTTg1hdOCFMDwS0zyBmN2ARykDd0eCrL3pSZxiib8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhmgci",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Exw00",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhmgci/mindforge_ollamalike_local_llm_runner_with_hf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhmgci/mindforge_ollamalike_local_llm_runner_with_hf/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754334135,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "NexNotes AI is an AI-powered tool that helps you streamline your study and learning process. With a suite of features including mind maps, study plans, flowcharts, summaries, and quizzes, NexNotes AI empowers you to grasp complex information quickly and effectively. Whether you're a student, professional, or lifelong learner, this versatile platform can transform the way you approach your studies and boost your knowledge retention. It also gives awards for solving questions. I need feedback from you about My tool overall\n[NexNotesAI ](https://nexnotes-ai.pages.dev)",
          "author_fullname": "t2_1utt8r8ixm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a one stop AI powered study solution",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhv99h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754355346,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NexNotes AI is an AI-powered tool that helps you streamline your study and learning process. With a suite of features including mind maps, study plans, flowcharts, summaries, and quizzes, NexNotes AI empowers you to grasp complex information quickly and effectively. Whether you&amp;#39;re a student, professional, or lifelong learner, this versatile platform can transform the way you approach your studies and boost your knowledge retention. It also gives awards for solving questions. I need feedback from you about My tool overall\n&lt;a href=\"https://nexnotes-ai.pages.dev\"&gt;NexNotesAI &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mhv99h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pls_Do_not_ban",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhv99h/i_built_a_one_stop_ai_powered_study_solution/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhv99h/i_built_a_one_stop_ai_powered_study_solution/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754355346,
          "num_crossposts": 9,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nhttps://youtu.be/YR0KYO1YxsM?si=PEZJci3xJXITSuHM&amp;utm_source=ZTQxO",
          "author_fullname": "t2_8c7clfk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 - 7B has a rival - Hunyuan.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhchdb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/V3Dgg9PfGn8ZeCKm1IkrYHrrLdHHjEj3QDpsDaMHbYc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754311641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://youtu.be/YR0KYO1YxsM?si=PEZJci3xJXITSuHM&amp;amp;utm_source=ZTQxO\"&gt;https://youtu.be/YR0KYO1YxsM?si=PEZJci3xJXITSuHM&amp;amp;utm_source=ZTQxO&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/sfrqq83710hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?auto=webp&amp;s=c172e2b98a6859f2b372038cbc1c42fa64c28a98",
                  "width": 1080,
                  "height": 622
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=480e2bd14f64df792e7eaa9981b21f6d929c3d90",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1b6e98127b512019566b074ca83014406f284c3",
                    "width": 216,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=795b60e1c577441ffc10cd1765de8635cdae6e1a",
                    "width": 320,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc1c4d80ba59b73371c7a600daa85753805381da",
                    "width": 640,
                    "height": 368
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2f3cd8deea253229adc2a8f284762a9d98277e22",
                    "width": 960,
                    "height": 552
                  },
                  {
                    "url": "https://preview.redd.it/sfrqq83710hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fffabaf71378a58e7172bf0288389f1a7c31cbe6",
                    "width": 1080,
                    "height": 622
                  }
                ],
                "variants": {},
                "id": "CeOPnrkdAVTWcw3RfH95g_byZ-S--H7GKlR46QmXQLU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhchdb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Current-Stop7806",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhchdb/qwen_3_7b_has_a_rival_hunyuan/",
          "stickied": false,
          "url": "https://i.redd.it/sfrqq83710hf1.jpeg",
          "subreddit_subscribers": 510259,
          "created_utc": 1754311641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA! I'm struggling to get decent performance from the new GLM-4.5-Air model and could use some help finding the optimal config.\n\n**Hardware:**\n\n* RTX 4090 (24GB VRAM)\n* 64GB DDR4 RAM\n* Using latest llama.cpp build (6088 with clang 19.1.5)\n\n**Model:**\n\n* DevQuasar/zai-org.GLM-4.5-Air-GGUF (Q4\\_K\\_M, 6 shards, \\~67GB total)\n* 110B parameters, MoE with 128 experts (8 active)\n\n**Current working config:**\n\n    llama-server -hf DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf -fa -c 4096 -ngl 99 -ot \".ffn_.*_exps.=CPU\" --port 8081\n\n**Performance issues:**\n\n* Only getting **3.37 tokens/sec** generation\n* **1.53 tokens/sec** prompt processing\n* GPU utilization only **12%** (should be much higher!)\n* GPU memory usage: 7.7GB/24GB (tons of headroom)\n* System RAM: 62.9GB/64GB (98% full - this seems to be the bottleneck)\n\n**What I've tried:**\n\n* Without expert offloading (`-ot` flag) → OOMs trying to allocate 66GB on 24GB GPU\n* Higher `-ngl` values without expert offloading → System freezes/crashes after 30min loading\n* Different batch sizes → No improvement\n* For comparison, 12B dense models get 40-50 TPS on this setup",
          "author_fullname": "t2_uptissiz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help optimizing GLM-4.5-Air 110B (Q4_K_M) on RTX 4090 + 64GB RAM - Getting only 3.37 TPS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhsyv9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754349244,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! I&amp;#39;m struggling to get decent performance from the new GLM-4.5-Air model and could use some help finding the optimal config.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;RTX 4090 (24GB VRAM)&lt;/li&gt;\n&lt;li&gt;64GB DDR4 RAM&lt;/li&gt;\n&lt;li&gt;Using latest llama.cpp build (6088 with clang 19.1.5)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DevQuasar/zai-org.GLM-4.5-Air-GGUF (Q4_K_M, 6 shards, ~67GB total)&lt;/li&gt;\n&lt;li&gt;110B parameters, MoE with 128 experts (8 active)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Current working config:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server -hf DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf -fa -c 4096 -ngl 99 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; --port 8081\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance issues:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Only getting &lt;strong&gt;3.37 tokens/sec&lt;/strong&gt; generation&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;1.53 tokens/sec&lt;/strong&gt; prompt processing&lt;/li&gt;\n&lt;li&gt;GPU utilization only &lt;strong&gt;12%&lt;/strong&gt; (should be much higher!)&lt;/li&gt;\n&lt;li&gt;GPU memory usage: 7.7GB/24GB (tons of headroom)&lt;/li&gt;\n&lt;li&gt;System RAM: 62.9GB/64GB (98% full - this seems to be the bottleneck)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve tried:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Without expert offloading (&lt;code&gt;-ot&lt;/code&gt; flag) → OOMs trying to allocate 66GB on 24GB GPU&lt;/li&gt;\n&lt;li&gt;Higher &lt;code&gt;-ngl&lt;/code&gt; values without expert offloading → System freezes/crashes after 30min loading&lt;/li&gt;\n&lt;li&gt;Different batch sizes → No improvement&lt;/li&gt;\n&lt;li&gt;For comparison, 12B dense models get 40-50 TPS on this setup&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhsyv9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pro-editor-1105",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754349244,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I downloaded Qwen3-Coder-30B-A3B-Instruct this morning and it surprised me. The model wrote a working Snake game on the first try.\n\nHere's what I did:\n\n1. Converted the model to MLX format with one command: `mlx_lm.convert --hf-path Qwen/Qwen3-Coder-30B-A3B-Instruct --mlx-path ~/models/Qwen3-Coder-30B-A3B-Instruct.mlx --q-group-size 64` (EDIT: --q-group-size is not needed for full precision. Only if quantizing. But it seemed to have no ill effect.)\n2. Set up a symlink for LM Studio (you can also use mlx\\_lm.chat)\n3. Gave it a simple prompt: \"Write a snake game in python.\"\n4. Created a Python environment and ran the code: `python3 -m venv ./snake &amp;&amp; . ./snake/bin/activate &amp;&amp; pip install pygame &amp;&amp; python ./snake`\n\nThe results:\n\n* 56 tokens per second at full 16-bit precision\n* 0.17 seconds to first token\n* Total time to complete game: 24 seconds\n* The game worked perfectly on the first run\n\nThe code included some nice graphical touches like a grid overlay and a distinct snake head. Six months ago, this would have been tough for most models.\n\nYes, Snake game examples probably exist in the training data. But running a 60GB model at full precision on a laptop at this speed still feels remarkable. I ran this prompt multiple times and it never failed to produce working pygame code, though the features and graphics varied slightly.\n\nSetup: MacBook Pro M4 Max with 128GB RAM\n\n[Screenshot of Game Over screen with score from a single short prompt.](https://preview.redd.it/ecwmh5acn1hf1.png?width=1604&amp;format=png&amp;auto=webp&amp;s=80fc551198145487c877d02ee099304fe1c58a40)",
          "author_fullname": "t2_jh5lk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-30B nailed Snake game in one shot on my MacBook",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 110,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ecwmh5acn1hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 84,
                  "x": 108,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc6d897752248a91312ae58d9eed61b1d872df51"
                },
                {
                  "y": 169,
                  "x": 216,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb14dd66ec113f158e4deb9ffb6864bc1eba14e8"
                },
                {
                  "y": 251,
                  "x": 320,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1ce125b795512f57e241c084d583e3a2456a16a"
                },
                {
                  "y": 503,
                  "x": 640,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b17c385bd563df8e36085bdcbb0111960400a8c7"
                },
                {
                  "y": 755,
                  "x": 960,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a042ec97cefc1f5eab73c88a06679ac30409ea89"
                },
                {
                  "y": 849,
                  "x": 1080,
                  "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49db9fb40d886a2d4c69563f1a2c7bc760f3d7cd"
                }
              ],
              "s": {
                "y": 1262,
                "x": 1604,
                "u": "https://preview.redd.it/ecwmh5acn1hf1.png?width=1604&amp;format=png&amp;auto=webp&amp;s=80fc551198145487c877d02ee099304fe1c58a40"
              },
              "id": "ecwmh5acn1hf1"
            }
          },
          "name": "t3_1mhl49l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/R6uYGl8qMhNCvvGDq584gvz1OcqCyGM1AH7v0jQ2LQg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754331237,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded Qwen3-Coder-30B-A3B-Instruct this morning and it surprised me. The model wrote a working Snake game on the first try.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I did:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Converted the model to MLX format with one command: &lt;code&gt;mlx_lm.convert --hf-path Qwen/Qwen3-Coder-30B-A3B-Instruct --mlx-path ~/models/Qwen3-Coder-30B-A3B-Instruct.mlx --q-group-size 64&lt;/code&gt; (EDIT: --q-group-size is not needed for full precision. Only if quantizing. But it seemed to have no ill effect.)&lt;/li&gt;\n&lt;li&gt;Set up a symlink for LM Studio (you can also use mlx_lm.chat)&lt;/li&gt;\n&lt;li&gt;Gave it a simple prompt: &amp;quot;Write a snake game in python.&amp;quot;&lt;/li&gt;\n&lt;li&gt;Created a Python environment and ran the code: &lt;code&gt;python3 -m venv ./snake &amp;amp;&amp;amp; . ./snake/bin/activate &amp;amp;&amp;amp; pip install pygame &amp;amp;&amp;amp; python ./snake&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The results:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;56 tokens per second at full 16-bit precision&lt;/li&gt;\n&lt;li&gt;0.17 seconds to first token&lt;/li&gt;\n&lt;li&gt;Total time to complete game: 24 seconds&lt;/li&gt;\n&lt;li&gt;The game worked perfectly on the first run&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The code included some nice graphical touches like a grid overlay and a distinct snake head. Six months ago, this would have been tough for most models.&lt;/p&gt;\n\n&lt;p&gt;Yes, Snake game examples probably exist in the training data. But running a 60GB model at full precision on a laptop at this speed still feels remarkable. I ran this prompt multiple times and it never failed to produce working pygame code, though the features and graphics varied slightly.&lt;/p&gt;\n\n&lt;p&gt;Setup: MacBook Pro M4 Max with 128GB RAM&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ecwmh5acn1hf1.png?width=1604&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80fc551198145487c877d02ee099304fe1c58a40\"&gt;Screenshot of Game Over screen with score from a single short prompt.&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhl49l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "txgsync",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhl49l/qwen3coder30b_nailed_snake_game_in_one_shot_on_my/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhl49l/qwen3coder30b_nailed_snake_game_in_one_shot_on_my/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754331237,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3cle71ya",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU-enabled Llama3 inference in Java now runs Qwen3, Phi-3, Mistral and Llama3 models in FP16, Q8 and Q4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhiw2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7S3n210Mbo3m_5SVfjJqbryTPGhWTE95IrYGDnPhDiI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754323440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/r887j3c401hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/r887j3c401hf1.png?auto=webp&amp;s=c32b99267b50dc24bd615d3eb72cbb44eb463397",
                  "width": 640,
                  "height": 960
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/r887j3c401hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=edc47e70ed41dfecf98c112cda83ad38681ebd5c",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/r887j3c401hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d246a2c0b810b5612aa6d0d385d12c4301a1662c",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://preview.redd.it/r887j3c401hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73dd415bcaa0b793a7e3587a34d751df41412d86",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/r887j3c401hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a526327b790e21780f771391119fea341975354",
                    "width": 640,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "T-dzijjLHxhok6UBJPX3mgRpxypAdwfyi4rcFToq4gE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhhiw2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mikebmx1",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhiw2/gpuenabled_llama3_inference_in_java_now_runs/",
          "stickied": false,
          "url": "https://i.redd.it/r887j3c401hf1.png",
          "subreddit_subscribers": 510259,
          "created_utc": 1754323440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Reading [https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after\\_6\\_months\\_of\\_fiddling\\_with\\_local\\_ai\\_heres\\_my/](https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/) it occurred to me...\n\nThere should be a BitTorrent tracker on the internet which has torrents of the models on HF.\n\n\n\n\n\nCreating torrents &amp; initial seeding can be automated to a point of only needing a monitoring &amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...\n\n\n\nIt's what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they'd download super fast.\n\nAnyone interested to work on this?",
          "author_fullname": "t2_45gug1j9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "BItTorrent tracker that mirrors HuggingFace",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh4r0s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754284264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reading &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/&lt;/a&gt; it occurred to me...&lt;/p&gt;\n\n&lt;p&gt;There should be a BitTorrent tracker on the internet which has torrents of the models on HF.&lt;/p&gt;\n\n&lt;p&gt;Creating torrents &amp;amp; initial seeding can be automated to a point of only needing a monitoring &amp;amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they&amp;#39;d download super fast.&lt;/p&gt;\n\n&lt;p&gt;Anyone interested to work on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh4r0s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lurkystrike",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754284264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It seems they're updating their meta-benchmark with some less saturated ones which is good.\n\nA bit strange to continue using MMLU pro as it's quite saturated.\n\nThis update will make comparisons across time invalid.\n\nGrok and o3 are now tied. It's not clear if they are done updating.",
          "author_fullname": "t2_syq52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Artificial analysis meta-benchmark update",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhrke2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=140&amp;height=77&amp;crop=140:77,smart&amp;auto=webp&amp;s=938c7c54c5afab9fd0496cba4e5d012b557db44d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754345749,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "artificialanalysis.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems they&amp;#39;re updating their meta-benchmark with some less saturated ones which is good.&lt;/p&gt;\n\n&lt;p&gt;A bit strange to continue using MMLU pro as it&amp;#39;s quite saturated.&lt;/p&gt;\n\n&lt;p&gt;This update will make comparisons across time invalid.&lt;/p&gt;\n\n&lt;p&gt;Grok and o3 are now tied. It&amp;#39;s not clear if they are done updating.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://artificialanalysis.ai/models/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhrke2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nomorebuttsplz",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhrke2/artificial_analysis_metabenchmark_update/",
          "stickied": false,
          "url": "https://artificialanalysis.ai/models/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754345749,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This might be somewhat unusual, but if the goal of model quantization is to reduce the model size, what about quantizing only the text encoder? I found that the Qwen Image model consists of text encoders(Qwen2.5 VL) and diffusion transformers. If the text encoder is more robust to quantization than the diffusion, wouldn't it make sense to quantize only the text encoder while keeping the image generation part intact?\n\n  \nDoes this idea make no sense at all, or is it theoretically possible?",
          "author_fullname": "t2_73xg2fw4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Image quantization idea",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhtjqo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754350727,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This might be somewhat unusual, but if the goal of model quantization is to reduce the model size, what about quantizing only the text encoder? I found that the Qwen Image model consists of text encoders(Qwen2.5 VL) and diffusion transformers. If the text encoder is more robust to quantization than the diffusion, wouldn&amp;#39;t it make sense to quantize only the text encoder while keeping the image generation part intact?&lt;/p&gt;\n\n&lt;p&gt;Does this idea make no sense at all, or is it theoretically possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhtjqo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExcuseAccomplished97",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhtjqo/qwen_image_quantization_idea/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhtjqo/qwen_image_quantization_idea/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754350727,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nVector Space now runs Qwen3 0.6B with up to 100 token/second on Apple Neural Engine. \n\nThe Neural Engine is a new kind of hardware unlike GPU or CPU that requires extensive changes to model architecture to make the model run on it - but we could get a significant speed gain and 1/4 energy consumption. \n\n🎉 Try it now on TestFlight:  \nhttps://testflight.apple.com/join/HXyt2bjU\n\n\n⚠️ First-time model load takes ~2 minutes (one-time setup).  \nAfter that, it’s just 1–2 seconds.",
          "author_fullname": "t2_w5xu1ep7l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Run 0.6B LLM 100token/s locally on iPhone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhl06m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/PeBg1vkGuYw6HMk_eEaN_xBxBQFb8kxcuQQi-a12uPk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754330997,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Vector Space now runs Qwen3 0.6B with up to 100 token/second on Apple Neural Engine. &lt;/p&gt;\n\n&lt;p&gt;The Neural Engine is a new kind of hardware unlike GPU or CPU that requires extensive changes to model architecture to make the model run on it - but we could get a significant speed gain and 1/4 energy consumption. &lt;/p&gt;\n\n&lt;p&gt;🎉 Try it now on TestFlight:&lt;br/&gt;\n&lt;a href=\"https://testflight.apple.com/join/HXyt2bjU\"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;⚠️ First-time model load takes ~2 minutes (one-time setup).&lt;br/&gt;\nAfter that, it’s just 1–2 seconds.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lls41nzqm1hf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?auto=webp&amp;s=a9757bad7ed1660066f3a946468a210f0589895f",
                  "width": 1320,
                  "height": 1615
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bda34bbec8fdf3925252d345c21af2d53ce861e2",
                    "width": 108,
                    "height": 132
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=018fe4f3ce7c0ab9be2f8ee0ae281dd7bdf76719",
                    "width": 216,
                    "height": 264
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0a6353d63539bf16245aaa38749c972b9798e12",
                    "width": 320,
                    "height": 391
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=26b6959bbf0837900160d9c038af01d00c3f46fb",
                    "width": 640,
                    "height": 783
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=56d243fc4618bd59acfd62bf3368d2cc47b206e1",
                    "width": 960,
                    "height": 1174
                  },
                  {
                    "url": "https://preview.redd.it/lls41nzqm1hf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b42845624fe8bec3999ce79cee1a524c2c871d10",
                    "width": 1080,
                    "height": 1321
                  }
                ],
                "variants": {},
                "id": "pzO2nUXm5T7ElM7k5qjpe9WHFUaEPttFlE2rHWKrYK0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mhl06m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glad-Speaker3006",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhl06m/run_06b_llm_100tokens_locally_on_iphone/",
          "stickied": false,
          "url": "https://i.redd.it/lls41nzqm1hf1.jpeg",
          "subreddit_subscribers": 510259,
          "created_utc": 1754330997,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_9b9s4a7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Profanity: QwenCode... but is Devstral in the background. And it works. Just slower than Coder-30b-a3b... but it works.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhbrr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/8gUCH4MJD2WmMBZraicAK7p72XubGXeMH1jGLOBdkBk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754323027,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wa8tbrp0z0hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?auto=webp&amp;s=2bd18a2eb9714d944827ae11190b0ee083fa0930",
                  "width": 2600,
                  "height": 1518
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec77a68e170a79bdacd792c9b8fc588ef1dc83b7",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f20e2b3c25bf73bcd3ab1fd96f6c6f224e8ef826",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9be5e63324a7bfc20b041ce16d7f07b9cdc92acb",
                    "width": 320,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d6c812e6f8b133e97cedd6562f5b6358b92a0bc",
                    "width": 640,
                    "height": 373
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48fa7636b7828cb4380ae2e49075840247b2e512",
                    "width": 960,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/wa8tbrp0z0hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=08d9b6f9a38fc50d111ee53aff8dc4d9b61381b8",
                    "width": 1080,
                    "height": 630
                  }
                ],
                "variants": {},
                "id": "jiRDjFD0hzFt2Lxgzc9ZLtFEvRnbsiVy-1nmLKMO1W0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhhbrr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JLeonsarmiento",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhbrr/profanity_qwencode_but_is_devstral_in_the/",
          "stickied": false,
          "url": "https://i.redd.it/wa8tbrp0z0hf1.png",
          "subreddit_subscribers": 510259,
          "created_utc": 1754323027,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all, I've been experimenting with various LLM apps and have an idea for a small open-source project to address a frustration I'm hitting repeatedly. But before I dive deep, I wanted to quickly check if it already exists (fingers crossed)!\n\n**My Pain Point:**  \nI'm tired of being stuck with linear conversations. When exploring complex problems, like debugging or research, I often want to:\n\n* Ask side-questions without polluting the main conversation\n* Explore multiple paths (e.g., testing two possible solutions simultaneously)\n\nRight now, these side explorations clutter my main context, inflate token usage/costs, and make responses less relevant.\n\n**My Idea (OS)**: Small self-hosted micro-service + API that lets you:\n\n1. Branch a conversation\n2. Toggle past messages (i.e. ability to pick and choose which message are included in the context to minimize tokens and boost relevance)\n3. Get an optimized JSON context output, which you then feed into your existing LLM connector or custom client (thinking it makes the most sense to avoid direct complexity of sending messages directly to Local LLM, OpenAI, Anthropic, etc.)\n\n**Does something like this already exist?**  \nDoes this bother anyone else, is it just me, or am I missing something obvious?\n\nThanks so much for any candid feedback!\n\n**TLDR: Sick of linear LLM chats causing wasted tokens and cluttered context. Considering making an open-source tool/service for branching conversations + explicit message toggling, returning optimized JSON contexts for easy integration. Does this exist? Good idea, bad idea?**",
          "author_fullname": "t2_7xaxpi27",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool for chat branching &amp; selective-context control exist?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhlxe1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754334058,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754332985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve been experimenting with various LLM apps and have an idea for a small open-source project to address a frustration I&amp;#39;m hitting repeatedly. But before I dive deep, I wanted to quickly check if it already exists (fingers crossed)!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Pain Point:&lt;/strong&gt;&lt;br/&gt;\nI&amp;#39;m tired of being stuck with linear conversations. When exploring complex problems, like debugging or research, I often want to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ask side-questions without polluting the main conversation&lt;/li&gt;\n&lt;li&gt;Explore multiple paths (e.g., testing two possible solutions simultaneously)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now, these side explorations clutter my main context, inflate token usage/costs, and make responses less relevant.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Idea (OS)&lt;/strong&gt;: Small self-hosted micro-service + API that lets you:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Branch a conversation&lt;/li&gt;\n&lt;li&gt;Toggle past messages (i.e. ability to pick and choose which message are included in the context to minimize tokens and boost relevance)&lt;/li&gt;\n&lt;li&gt;Get an optimized JSON context output, which you then feed into your existing LLM connector or custom client (thinking it makes the most sense to avoid direct complexity of sending messages directly to Local LLM, OpenAI, Anthropic, etc.)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Does something like this already exist?&lt;/strong&gt;&lt;br/&gt;\nDoes this bother anyone else, is it just me, or am I missing something obvious?&lt;/p&gt;\n\n&lt;p&gt;Thanks so much for any candid feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR: Sick of linear LLM chats causing wasted tokens and cluttered context. Considering making an open-source tool/service for branching conversations + explicit message toggling, returning optimized JSON contexts for easy integration. Does this exist? Good idea, bad idea?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhlxe1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IsWired",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754332985,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5's slides the past few days... and wow, I actually might prefer it now.\n\nHere is example of same topic:\n\nGLM 4.5 AI Slides:  \n[https://chat.z.ai/space/u01ja6suarb0-ppt](https://chat.z.ai/space/u01ja6suarb0-ppt)\n\nhttps://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player\n\nGEMINI 2.5 Pro DR:  \n[https://gemini.google.com/share/ca95257c1a48](https://gemini.google.com/share/ca95257c1a48)\n\nhttps://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player\n\n",
          "author_fullname": "t2_3fg55rsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 AI Sliders vs Gemini 2.5 Pro Deep Research Infographics",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "gmg5vfk2eygf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mh6zja/asset/gmg5vfk2eygf1/DASHPlaylist.mpd?a=1756948483%2CMTczNzI5OWRmZWE5ZDY0YmEyMWM3MDU5ZmMwZTNiZmYyZmExNjNjODBiMzJmOTVhOWE1YTExNmIyYjcyNWU3ZA%3D%3D&amp;v=1&amp;f=sd",
              "x": 1278,
              "y": 720,
              "hlsUrl": "https://v.redd.it/link/1mh6zja/asset/gmg5vfk2eygf1/HLSPlaylist.m3u8?a=1756948483%2CNWQ4MzZlNzU5NjIzMWY3Yjk4Mjk5MTc2NTU5YWY1ZjExZmU1ZTIzOGI5MGIwODE5YTQzZmY0MGE3NmE3ZTQ2OQ%3D%3D&amp;v=1&amp;f=sd",
              "id": "gmg5vfk2eygf1",
              "isGif": false
            },
            "0kgfqae7gygf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mh6zja/asset/0kgfqae7gygf1/DASHPlaylist.mpd?a=1756948483%2CMTU2ZjdmYjRmNjk2MjU3ZTIyN2Y5NDFiNDgyNmJlYzI4ZWQyOGVmMGFmYmI3NDU1MzhkY2RjMTM2OWYxYmU0Yw%3D%3D&amp;v=1&amp;f=sd",
              "x": 1278,
              "y": 720,
              "hlsUrl": "https://v.redd.it/link/1mh6zja/asset/0kgfqae7gygf1/HLSPlaylist.m3u8?a=1756948483%2CZTE4NzM3YzM5YmY0MTU0OTU4N2ZiMmE4NDA0NmQ4NzY3N2YzYzJmNzg4ODFiNTU2M2JkM2Y2ZWMzZWU0YTYxMQ%3D%3D&amp;v=1&amp;f=sd",
              "id": "0kgfqae7gygf1",
              "isGif": false
            }
          },
          "name": "t3_1mh6zja",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=be50e0facc73717d00af311d55a802e1d4f67b46",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754292525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5&amp;#39;s slides the past few days... and wow, I actually might prefer it now.&lt;/p&gt;\n\n&lt;p&gt;Here is example of same topic:&lt;/p&gt;\n\n&lt;p&gt;GLM 4.5 AI Slides:&lt;br/&gt;\n&lt;a href=\"https://chat.z.ai/space/u01ja6suarb0-ppt\"&gt;https://chat.z.ai/space/u01ja6suarb0-ppt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player\"&gt;https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GEMINI 2.5 Pro DR:&lt;br/&gt;\n&lt;a href=\"https://gemini.google.com/share/ca95257c1a48\"&gt;https://gemini.google.com/share/ca95257c1a48&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player\"&gt;https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?auto=webp&amp;s=06f19448d458a949198ac72d6d7c73d5e6463785",
                  "width": 400,
                  "height": 400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=731547beb9c0ce796d8f8edd4b883c564da2c39b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a6eef195d7537bf441a643dbcaf760056822a2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abcfb3d145a4837cd123c1d5c55d56b5eaefd529",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mh6zja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z1xto",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754292525,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm planning a new build primarily for running a 72B model using vLLM (qwen 72b vl) with FP8.I think about  using four RTX 5090s for the highest throughput. I was also thinking about one rtx pro 6000 but the inference speed is much slower and the cost similar. I get about 3x the throughput with 4x 5090 (tested via [runpod.io](http://runpod.io)), but I've hit a wall concerning the case/chassis and the physical connectivity.  Also my budget is limited, I would like to keep it below 15k. \n\nI'd appreciate a sanity check on the whole build, especially on the feasibility of housing and connecting four of these GPUs. Any comments, critiques, or suggestions are more than welcome!\n\n**The Goal**\n\n* **Primary:** Run a 72B parameter model with FP8 quantisation using vLLM on a dedicated set of GPUs.\n* **Secondary:** Use separate GPUs for other tasks\n\n**Core System Components**\n\n* **CPU:** AMD EPYC 7402P (24 Cores, 48 Threads, 180W TDP) (\\~300 CHF)\n* **Motherboard:** ASRock Rack ROMED8-2T (Socket SP3, 8x DDR4, 7x PCIe 4.0 x16 slots) (\\~750 CHF) (  I chose a motherboard with pcie 4 because i will not use the setup for fine tuning and the pricing of the cpu + motherboard is much cheaper and i already have ram for it. )\n* **RAM:** 512 GB DDR4 ECC (Already have)\n* **Storage:** 2TB - 4TB PCIe 4.0 NVMe SSD (\\~300 CHF)\n* **CPU Cooler:** A high-performance air cooler compatible with AMD SP3, like a Noctua TR4-SP3. (\\~100 CHF)\n* **PSU:** Fortron Cannon Pro 2500W (80+ Platinum). Should be enough even with all GPUs. Could also add a second in case not or the ASUS Pro WS 3000W if its released soon (\\~500 CHF)\n\n**GPU Configuration**\n\n* **For the LLM:**\n   * **4x NVIDIA RTX 5090** (\\~8,000 CHF)\n   * I plan to **power-limit each RTX 5090 to 300-350W**. \n* **For other tasks**\n   * 2x NVIDIA RTX 5080 (\\~2,400 CHF total)\n\n**My Main Questions &amp; Problems**\n\n1. Case / Chassis\\*\\*:\\*\\* This is my biggest problem. What's the best way to house 4x RTX 5090s (plus 2x 5080s)? An open-air frame seems likely, but which ones work well for this many cards? Is there a server chassis that could handle the spacing, where it would eventually also be possible to add more later ?\n2. GPU Connectivity: The motherboard has 7x PCIe 4.0 x16 slots. I'll need to use 6 of them, whats the best way to connect them, pcie riser cables, are there any problems fixing the gpu just in front or do i need  any kind of gpu support or is the pcie bracket strong enough?\n3. General Sanity Check: Looking at the whole picture, does this setup look ok? \n4. Do you think using the first of the cases below would work with riser cables and the above listed hardware ? \n\nThanks in advance for your time!\n\nFor the cases so far i thought about: \n\n1. [https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr\\_1\\_1?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-1](https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-1)\n2. [ttps://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr\\_1\\_6?crid=7NH56ZDDBUQV&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754348992&amp;sprefix=mining+ri%2Caps%2C116&amp;sr=8-6](https://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754348992&amp;sprefix=mining+ri%2Caps%2C116&amp;sr=8-6)\n3. [https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr\\_1\\_3?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-3](https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-3)",
          "author_fullname": "t2_oaw1i0pr4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help building an price efficient inference server (no fine tuning) + multi 5090 setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhu9tx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754352667,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning a new build primarily for running a 72B model using vLLM (qwen 72b vl) with FP8.I think about  using four RTX 5090s for the highest throughput. I was also thinking about one rtx pro 6000 but the inference speed is much slower and the cost similar. I get about 3x the throughput with 4x 5090 (tested via &lt;a href=\"http://runpod.io\"&gt;runpod.io&lt;/a&gt;), but I&amp;#39;ve hit a wall concerning the case/chassis and the physical connectivity.  Also my budget is limited, I would like to keep it below 15k. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate a sanity check on the whole build, especially on the feasibility of housing and connecting four of these GPUs. Any comments, critiques, or suggestions are more than welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Goal&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Primary:&lt;/strong&gt; Run a 72B parameter model with FP8 quantisation using vLLM on a dedicated set of GPUs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Secondary:&lt;/strong&gt; Use separate GPUs for other tasks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Core System Components&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD EPYC 7402P (24 Cores, 48 Threads, 180W TDP) (~300 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Rack ROMED8-2T (Socket SP3, 8x DDR4, 7x PCIe 4.0 x16 slots) (~750 CHF) (  I chose a motherboard with pcie 4 because i will not use the setup for fine tuning and the pricing of the cpu + motherboard is much cheaper and i already have ram for it. )&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 512 GB DDR4 ECC (Already have)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 2TB - 4TB PCIe 4.0 NVMe SSD (~300 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; A high-performance air cooler compatible with AMD SP3, like a Noctua TR4-SP3. (~100 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Fortron Cannon Pro 2500W (80+ Platinum). Should be enough even with all GPUs. Could also add a second in case not or the ASUS Pro WS 3000W if its released soon (~500 CHF)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU Configuration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;For the LLM:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;4x NVIDIA RTX 5090&lt;/strong&gt; (~8,000 CHF)&lt;/li&gt;\n&lt;li&gt;I plan to &lt;strong&gt;power-limit each RTX 5090 to 300-350W&lt;/strong&gt;. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;For other tasks&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2x NVIDIA RTX 5080 (~2,400 CHF total)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;My Main Questions &amp;amp; Problems&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Case / Chassis**:** This is my biggest problem. What&amp;#39;s the best way to house 4x RTX 5090s (plus 2x 5080s)? An open-air frame seems likely, but which ones work well for this many cards? Is there a server chassis that could handle the spacing, where it would eventually also be possible to add more later ?&lt;/li&gt;\n&lt;li&gt;GPU Connectivity: The motherboard has 7x PCIe 4.0 x16 slots. I&amp;#39;ll need to use 6 of them, whats the best way to connect them, pcie riser cables, are there any problems fixing the gpu just in front or do i need  any kind of gpu support or is the pcie bracket strong enough?&lt;/li&gt;\n&lt;li&gt;General Sanity Check: Looking at the whole picture, does this setup look ok? &lt;/li&gt;\n&lt;li&gt;Do you think using the first of the cases below would work with riser cables and the above listed hardware ? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for your time!&lt;/p&gt;\n\n&lt;p&gt;For the cases so far i thought about: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-1\"&gt;https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-1&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754348992&amp;amp;sprefix=mining+ri%2Caps%2C116&amp;amp;sr=8-6\"&gt;ttps://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754348992&amp;amp;sprefix=mining+ri%2Caps%2C116&amp;amp;sr=8-6&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-3\"&gt;https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-3&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?auto=webp&amp;s=9b598b3fe915c4e340a1d2be347d6ada11f361b7",
                  "width": 2400,
                  "height": 1260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2ec29473ad9a43f57f6de38e719603168628711",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fbe80d3cfbb71c2262379cd9b070f60a3559377",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=756815133007c791068d5b797184c842b074ab75",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4feb605e519121401324c6cc277e71f0c83948d",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad4525174d8ac6007a90f9bd6e65c6a7aa6a406c",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fece91f94e327af10d6bd9f2be0f89eeec62d4cb",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhu9tx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Civil-Image5411",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754352667,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "YuE: Open Full-song Music Generation Foundation Model, something similar to [Suno.ai](http://Suno.ai) but open",
          "author_fullname": "t2_etmr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Music Foundation Models for Full-Song Generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mha439",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754304531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "map-yue.github.io",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;YuE: Open Full-song Music Generation Foundation Model, something similar to &lt;a href=\"http://Suno.ai\"&gt;Suno.ai&lt;/a&gt; but open&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://map-yue.github.io/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mha439",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phone_radio_tv",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mha439/open_music_foundation_models_for_fullsong/",
          "stickied": false,
          "url": "https://map-yue.github.io/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754304531,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:\n\n* Some requests randomly timed out or took way longer than others, even with the same provider\n* Logs didn’t show much, and tracing failures across providers was difficult\n* When we tried running it behind a load balancer, we ran into strange behavior with state\n* Fallbacks didn’t always trigger reliably when a provider was down or rate-limited\n* We tried plugging in Prometheus, but visibility into request flow was limited\n\nThe architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.\n\nWhile looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?",
          "author_fullname": "t2_1p9vds0za6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LiteLLM started breaking down for us past 300 RPS, what are folks using in prod?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh99hu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754301478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some requests randomly timed out or took way longer than others, even with the same provider&lt;/li&gt;\n&lt;li&gt;Logs didn’t show much, and tracing failures across providers was difficult&lt;/li&gt;\n&lt;li&gt;When we tried running it behind a load balancer, we ran into strange behavior with state&lt;/li&gt;\n&lt;li&gt;Fallbacks didn’t always trigger reliably when a provider was down or rate-limited&lt;/li&gt;\n&lt;li&gt;We tried plugging in Prometheus, but visibility into request flow was limited&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.&lt;/p&gt;\n\n&lt;p&gt;While looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh99hu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Otherwise_Flan7339",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754301478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "NVIDIA AI-Q, a blueprint for building AI agents with advanced reasoning skills, is now the top-rated open, portable AI agent in the **LLM with search category** on Hugging Face’s [Deep Research Bench leaderboard](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard). \n\nAI-Q, the top open and portable AI agent for deep research, uses NVIDIA Llama Nemotron for advanced reasoning and NeMo Retriever for RAG, informed by an organization’s internal data. With AI-Q, organizations can build AI agents that operate anywhere—data center and cloud. AI-Q enables the creation of fully customizable, local AI agents with reduced latency that meet security requirements for regulated industries.\n\nHear more from NVIDIA product leader, Adel El Hallak 📺 [https://www.youtube.com/shorts/dd\\_pJchCxTg](https://www.youtube.com/shorts/dd_pJchCxTg)\n\nDevelopers - get started here  ➡️ [https://build.nvidia.com/nvidia/aiq](https://build.nvidia.com/nvidia/aiq)\n\nhttps://i.redd.it/jgv068r4h1hf1.gif\n\n",
          "author_fullname": "t2_1vf7k06t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA AI-Q Achieves Top Score for Open, Portable AI Deep Research (LLM with Search Category)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "jgv068r4h1hf1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 63,
                  "x": 108,
                  "u": "https://preview.redd.it/jgv068r4h1hf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=41ecd26c4ebfb0888b6b090467afa0998ae2c023"
                },
                {
                  "y": 126,
                  "x": 216,
                  "u": "https://preview.redd.it/jgv068r4h1hf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=415fa49038eccc7eea61affaa6d07fede972065f"
                },
                {
                  "y": 188,
                  "x": 320,
                  "u": "https://preview.redd.it/jgv068r4h1hf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=e12c4df6af6e828ac7d05dd0ba31b034ab18e10d"
                },
                {
                  "y": 376,
                  "x": 640,
                  "u": "https://preview.redd.it/jgv068r4h1hf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=1305c534a3741fa2776638f279c1c2db02ba503f"
                }
              ],
              "s": {
                "y": 470,
                "gif": "https://i.redd.it/jgv068r4h1hf1.gif",
                "mp4": "https://preview.redd.it/jgv068r4h1hf1.gif?format=mp4&amp;s=04e2364284527d682e9f9bb0a7601502685d606c",
                "x": 800
              },
              "id": "jgv068r4h1hf1"
            }
          },
          "name": "t3_1mhk4it",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/axz8MpGbRHTMyaCQ3GQH5ICNpeA557SBPWKFqbOeoHY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754329131,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NVIDIA AI-Q, a blueprint for building AI agents with advanced reasoning skills, is now the top-rated open, portable AI agent in the &lt;strong&gt;LLM with search category&lt;/strong&gt; on Hugging Face’s &lt;a href=\"https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard\"&gt;Deep Research Bench leaderboard&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;AI-Q, the top open and portable AI agent for deep research, uses NVIDIA Llama Nemotron for advanced reasoning and NeMo Retriever for RAG, informed by an organization’s internal data. With AI-Q, organizations can build AI agents that operate anywhere—data center and cloud. AI-Q enables the creation of fully customizable, local AI agents with reduced latency that meet security requirements for regulated industries.&lt;/p&gt;\n\n&lt;p&gt;Hear more from NVIDIA product leader, Adel El Hallak 📺 &lt;a href=\"https://www.youtube.com/shorts/dd_pJchCxTg\"&gt;https://www.youtube.com/shorts/dd_pJchCxTg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Developers - get started here  ➡️ &lt;a href=\"https://build.nvidia.com/nvidia/aiq\"&gt;https://build.nvidia.com/nvidia/aiq&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/jgv068r4h1hf1.gif\"&gt;https://i.redd.it/jgv068r4h1hf1.gif&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mhk4it",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PDXcoder2000",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhk4it/nvidia_aiq_achieves_top_score_for_open_portable/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhk4it/nvidia_aiq_achieves_top_score_for_open_portable/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754329131,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there a way to export or backup an AnythingLLM workspace/RAG? I have one that is well developed and want to deploy it so others can mess around with it, but since it keeps track of chat history, I want a backup and a way to import it into a new workspace if the interaction changes its dynamic too much.\n\nAlso just in general want to back up the workspaces as checkpoints periodically. Any advice appreciated including if I should be using something besides AnythingLLM to suit my needs better.",
          "author_fullname": "t2_36loo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Export/Backup AnythingLLM Workspace?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhrey9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754345379,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to export or backup an AnythingLLM workspace/RAG? I have one that is well developed and want to deploy it so others can mess around with it, but since it keeps track of chat history, I want a backup and a way to import it into a new workspace if the interaction changes its dynamic too much.&lt;/p&gt;\n\n&lt;p&gt;Also just in general want to back up the workspaces as checkpoints periodically. Any advice appreciated including if I should be using something besides AnythingLLM to suit my needs better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhrey9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nuvious",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhrey9/exportbackup_anythingllm_workspace/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhrey9/exportbackup_anythingllm_workspace/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754345379,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am in quest of finding SOTA document parser for PDF/Docx files. I have about 100k pages with tables, text, images(with text) that I want to convert to markdown format.\n\nWhat is the best open source document parser available right now? That reaches near to Azure document intelligence accruacy.\n\nI have explored\n\n* Doclin\n* Marker\n* Pymupdf\n\nWhich one would be best to use in production?\n\n",
          "author_fullname": "t2_1d34fueda3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best document parser",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhe2h9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754315591,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in quest of finding SOTA document parser for PDF/Docx files. I have about 100k pages with tables, text, images(with text) that I want to convert to markdown format.&lt;/p&gt;\n\n&lt;p&gt;What is the best open source document parser available right now? That reaches near to Azure document intelligence accruacy.&lt;/p&gt;\n\n&lt;p&gt;I have explored&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Doclin&lt;/li&gt;\n&lt;li&gt;Marker&lt;/li&gt;\n&lt;li&gt;Pymupdf&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Which one would be best to use in production?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhe2h9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aiwtl",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhe2h9/best_document_parser/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhe2h9/best_document_parser/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754315591,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "is this normal? what's happening?",
          "author_fullname": "t2_10ht66",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Mistral-Small3.1-24B-Instruct with Open-WebUI and got this",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhprfk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/q6BWsH-KvJTpvwlyck_74vRNRFaH4daHSQc5f90f3sA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754341536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is this normal? what&amp;#39;s happening?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/pbw43rp0i2hf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/pbw43rp0i2hf1.png?auto=webp&amp;s=52d1741047ef0a7eacc835dc94fc56ff87517dab",
                  "width": 1370,
                  "height": 740
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=637c5a65b67429fd45aa9db51e2e1e85724fd95c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=30e5f3801fdc684c2f52aa184c6a96872f777711",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0017a34f4e1c768c1c0ed5b857d4ed080cb6149e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dec9bddad59cf0ab62244b5cd3b50ce24b0b0606",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=56206801dbad385c1997e40bd4bea26c2583798b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://preview.redd.it/pbw43rp0i2hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22026bb62a35e0dfd07cd2bd645185074b1f18e6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XFVhJi6M0vTP6FPybQEHVyZ6TDvWBudwA77SB9h0H50"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhprfk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Juanouo",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhprfk/tried_mistralsmall3124binstruct_with_openwebui/",
          "stickied": false,
          "url": "https://i.redd.it/pbw43rp0i2hf1.png",
          "subreddit_subscribers": 510259,
          "created_utc": 1754341536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\n\nHorizon Beta is OpenAI",
          "author_fullname": "t2_h0z59zgo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 40,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "m30x07j54vgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 31,
                  "x": 108,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=588d4a9c04c05e66d8b3c8ba8b93d7c827f14b20"
                },
                {
                  "y": 62,
                  "x": 216,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=446e608e2e3bf06827203dc313f98d8ec1704b9d"
                },
                {
                  "y": 92,
                  "x": 320,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f5cab485a7892fcf45b7a6bf1ecfa12925d59cf"
                },
                {
                  "y": 184,
                  "x": 640,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fda4acd30425eae2bddb9d9f82ce6241eaf9641"
                },
                {
                  "y": 276,
                  "x": 960,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e8f560871bebab02cfbd9ae3af3f79ac6d3819a"
                },
                {
                  "y": 310,
                  "x": 1080,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6c803879173681737db389051fa4d599d2f19a0"
                }
              ],
              "s": {
                "y": 364,
                "x": 1266,
                "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a"
              },
              "id": "m30x07j54vgf1"
            }
          },
          "name": "t3_1mgtboa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 174,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 174,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/inyC6dLBuynY6QZPK34zrtaIVdVEKly7ofVVWlBmYW4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754252212,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\"&gt;https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Horizon Beta is OpenAI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgtboa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MiddleLobster9191",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754252212,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the best approach for this? Tried it in open webui with ollama backend but it's too slow.\n\nAll docs are pdf, all done with ocr so it's all just text. Ingestion to knowledgebase is the blocker.\n\nAnybody done this and what was the best approach for you?",
          "author_fullname": "t2_mu8eykc30",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG with 30k documents, some with 300 pages each.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mha1g1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754328080,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754304270,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best approach for this? Tried it in open webui with ollama backend but it&amp;#39;s too slow.&lt;/p&gt;\n\n&lt;p&gt;All docs are pdf, all done with ocr so it&amp;#39;s all just text. Ingestion to knowledgebase is the blocker.&lt;/p&gt;\n\n&lt;p&gt;Anybody done this and what was the best approach for you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mha1g1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dennisitnet",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mha1g1/rag_with_30k_documents_some_with_300_pages_each/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mha1g1/rag_with_30k_documents_some_with_300_pages_each/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754304270,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys!\n\nSo recently as a learning exercise I tuned a Qwen3 model for coding task. I was now interested in understanding how to properly benchmark these tunes models using wellknown benchmarks. But, I'm a bit unsure about how this is exactly done, and was curious about how this is typically done in the industry.\n\nDo each of these big tech companies usually have their own internal benchmarking frameworks/strategies? Or are there popular tools or frameworks that are widely used across both the community and in industry? Since I'm a bit new to the field would like to know what you guys think, what you've used and seen during your learning, etc. Thanks a lot!! :))",
          "author_fullname": "t2_1uzbjk5ofl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do people in industry benchmark models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhieis",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754325391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!&lt;/p&gt;\n\n&lt;p&gt;So recently as a learning exercise I tuned a Qwen3 model for coding task. I was now interested in understanding how to properly benchmark these tunes models using wellknown benchmarks. But, I&amp;#39;m a bit unsure about how this is exactly done, and was curious about how this is typically done in the industry.&lt;/p&gt;\n\n&lt;p&gt;Do each of these big tech companies usually have their own internal benchmarking frameworks/strategies? Or are there popular tools or frameworks that are widely used across both the community and in industry? Since I&amp;#39;m a bit new to the field would like to know what you guys think, what you&amp;#39;ve used and seen during your learning, etc. Thanks a lot!! :))&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhieis",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Spiritual_Process575",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhieis/how_do_people_in_industry_benchmark_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhieis/how_do_people_in_industry_benchmark_models/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754325391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Its listed at 0 Cost , but all my chats with have incurred a charge  . Any one else facing the same issues  ? Is it normal, i am new to this , am i missing something obvious here\n\n[Charged for chat usage](https://preview.redd.it/5kjvnxyla2hf1.png?width=1352&amp;format=png&amp;auto=webp&amp;s=c5ff5aab4d6b7e2bfc49efcb17717f9b7e429058)\n\n[Zero Price Shown](https://preview.redd.it/iahlrp1ha2hf1.png?width=1308&amp;format=png&amp;auto=webp&amp;s=cea90795f67ec7a06d376b3952551496d287f7b7)",
          "author_fullname": "t2_1hssure7vp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta Free or not on Openrouter",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 31,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "iahlrp1ha2hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 37,
                  "x": 108,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d51017ab88a27516cf2dcc2084dd46cccbc24a1b"
                },
                {
                  "y": 75,
                  "x": 216,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f237dd54d71dc0acf955b6e923be85eeebf41d56"
                },
                {
                  "y": 111,
                  "x": 320,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f70b9d422c2a6c33a3b9c773c19f43ab80392597"
                },
                {
                  "y": 223,
                  "x": 640,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc55f6b3073f641f9013a9c91a84bfa03b1eee3e"
                },
                {
                  "y": 335,
                  "x": 960,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad913e55bef2644d69fdfe5d25fb35fdd933c6f0"
                },
                {
                  "y": 377,
                  "x": 1080,
                  "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=74533f2ab0814df19511eda1702d88530dcea25b"
                }
              ],
              "s": {
                "y": 457,
                "x": 1308,
                "u": "https://preview.redd.it/iahlrp1ha2hf1.png?width=1308&amp;format=png&amp;auto=webp&amp;s=cea90795f67ec7a06d376b3952551496d287f7b7"
              },
              "id": "iahlrp1ha2hf1"
            },
            "5kjvnxyla2hf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 24,
                  "x": 108,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=96fd790a4623b3276477b4f524ead18e4b6951a3"
                },
                {
                  "y": 49,
                  "x": 216,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7143cd80ade2b4bd45933fce80454d84032fe256"
                },
                {
                  "y": 72,
                  "x": 320,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1efc7a6227fe3b506499af729e91bffb9d7bc401"
                },
                {
                  "y": 145,
                  "x": 640,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9584be7ca124f495764a2075a44e7aee64a2b82"
                },
                {
                  "y": 218,
                  "x": 960,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ddce746dfb93dd097db757b3921cf2df168372b2"
                },
                {
                  "y": 246,
                  "x": 1080,
                  "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57de2844d6c5980c4e84a70d7864b6f94eea94b1"
                }
              ],
              "s": {
                "y": 308,
                "x": 1352,
                "u": "https://preview.redd.it/5kjvnxyla2hf1.png?width=1352&amp;format=png&amp;auto=webp&amp;s=c5ff5aab4d6b7e2bfc49efcb17717f9b7e429058"
              },
              "id": "5kjvnxyla2hf1"
            }
          },
          "name": "t3_1mhok5i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-4sl9nlNlThRhCXHlpD0quR49YKqs0-ZQWK2eZGeDNs.jpg",
          "edited": 1754339031,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754338835,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its listed at 0 Cost , but all my chats with have incurred a charge  . Any one else facing the same issues  ? Is it normal, i am new to this , am i missing something obvious here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5kjvnxyla2hf1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ff5aab4d6b7e2bfc49efcb17717f9b7e429058\"&gt;Charged for chat usage&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/iahlrp1ha2hf1.png?width=1308&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea90795f67ec7a06d376b3952551496d287f7b7\"&gt;Zero Price Shown&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhok5i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Training-Surround228",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhok5i/horizon_beta_free_or_not_on_openrouter/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhok5i/horizon_beta_free_or_not_on_openrouter/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754338835,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally got to finish a weekend project from a couple of months ago. \n\nThis is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).\n\nFull source and configuration guide is on GitHub: [https://github.com/av/unhype](https://github.com/av/unhype) ",
          "author_fullname": "t2_o7p5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Use local LLM to neutralise the headers on the web",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkiti",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bd9e9e",
          "ups": 493,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756948483%2CNzgxNjA1NmUxNTE3YTEzNTk2NWJjN2NlNDVjNjllNDI3MjNkNTkzZDExMmI1YzdiN2FlYWI5ZjBlYTM0NTFhOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756948483%2CNGM0MTk3MjRkMTdkZDAzM2RkOGVhNDE3Nzc2NWQzYjRiNGM5YzU1OWIwNTQxZjUxNDczM2M4NmJhNGQ1ZDlmMA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 493,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=140&amp;height=89&amp;crop=140:89,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f692b7d05bb54cb77646e5499a279385018af33e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Alpaca"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754230985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally got to finish a weekend project from a couple of months ago. &lt;/p&gt;\n\n&lt;p&gt;This is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).&lt;/p&gt;\n\n&lt;p&gt;Full source and configuration guide is on GitHub: &lt;a href=\"https://github.com/av/unhype\"&gt;https://github.com/av/unhype&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/niaha18uctgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?format=pjpg&amp;auto=webp&amp;s=e3f711ff053fccc48764a295b7ef68533d8f7f9a",
                  "width": 1688,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=47b5550c7dd6476d0d3e442d2ca5c2a382f74e92",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8828ff7d3eb9b270e88f2ec7ff4973818f896e8e",
                    "width": 216,
                    "height": 138
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bc01544a8c25b4174c98c4519803a567aa98c887",
                    "width": 320,
                    "height": 204
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9c9498bdb1588d50d25b53d82635a1092a82f4ef",
                    "width": 640,
                    "height": 409
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=402ac5eba7ed08a00bc9e012b6a4bd69a46e4608",
                    "width": 960,
                    "height": 614
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=374d1819675e3e0f60cf4790383435325e543d5f",
                    "width": 1080,
                    "height": 690
                  }
                ],
                "variants": {},
                "id": "NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Alpaca",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgkiti",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Everlier",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/",
          "stickied": false,
          "url": "https://v.redd.it/niaha18uctgf1",
          "subreddit_subscribers": 510259,
          "created_utc": 1754230985,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756948483%2CNzgxNjA1NmUxNTE3YTEzNTk2NWJjN2NlNDVjNjllNDI3MjNkNTkzZDExMmI1YzdiN2FlYWI5ZjBlYTM0NTFhOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756948483%2CNGM0MTk3MjRkMTdkZDAzM2RkOGVhNDE3Nzc2NWQzYjRiNGM5YzU1OWIwNTQxZjUxNDczM2M4NmJhNGQ1ZDlmMA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Lately I started using more local LLMs again but after playing around with the latest Qwen MOE with A3B I found out the hard way how fast it falls apart due to hallucination and similar, especially when context gets a little bit longer (were talking \\~1k t). Might be because the model is just not good, because of the quant or the quant provider. In any case, I wanna stop with this \"vibe testing\" and have some up-to-date eval I can use to at least compare the basics. I know there are datasets and eval libs but i was looking for more for a \"full pacakge\" (that uses these eval libs).\n\nDoes anyone has a nice project already, ideally python, to share?\n\nSome requirements:  \n\n* Goal would be really to compare local models and their quants, not make general tests - here we have enough benchmarks already\n* Works with localmodels and their apis (e.g. Ollama/litellm) - I dont mind something foundational for the \"LLM as a judge\" though\n* Dataset as mentioned should check the foundations, like reasoning, halluzinations, instruction following... nothing too wild but with focus on longer contexts not just simple questions\n* datasets shouldn't be too big as I dont want to spend too much on running incl judge LLMs\n* Its not professional - doesnt mean  it cant use professional libs if its not overkill\n\n  \nI was actually working in different areas with datasets and evals (eg i like \"inspect ai\") but datasets were often very special or technical for certain cases. Or others try to solve everything and half of it is not working (lm evaluation harness). Its generally surprising how many datasets just suck or have issues.  \n  \nAnd there  must be someone better (hopefully) putting their working code out already. Otherwise I will probably try to get something going (again)",
          "author_fullname": "t2_1bpvzzmckh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Evalproject for Local LLMs &amp; Quants",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mho569",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754337887,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately I started using more local LLMs again but after playing around with the latest Qwen MOE with A3B I found out the hard way how fast it falls apart due to hallucination and similar, especially when context gets a little bit longer (were talking ~1k t). Might be because the model is just not good, because of the quant or the quant provider. In any case, I wanna stop with this &amp;quot;vibe testing&amp;quot; and have some up-to-date eval I can use to at least compare the basics. I know there are datasets and eval libs but i was looking for more for a &amp;quot;full pacakge&amp;quot; (that uses these eval libs).&lt;/p&gt;\n\n&lt;p&gt;Does anyone has a nice project already, ideally python, to share?&lt;/p&gt;\n\n&lt;p&gt;Some requirements:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Goal would be really to compare local models and their quants, not make general tests - here we have enough benchmarks already&lt;/li&gt;\n&lt;li&gt;Works with localmodels and their apis (e.g. Ollama/litellm) - I dont mind something foundational for the &amp;quot;LLM as a judge&amp;quot; though&lt;/li&gt;\n&lt;li&gt;Dataset as mentioned should check the foundations, like reasoning, halluzinations, instruction following... nothing too wild but with focus on longer contexts not just simple questions&lt;/li&gt;\n&lt;li&gt;datasets shouldn&amp;#39;t be too big as I dont want to spend too much on running incl judge LLMs&lt;/li&gt;\n&lt;li&gt;Its not professional - doesnt mean  it cant use professional libs if its not overkill&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I was actually working in different areas with datasets and evals (eg i like &amp;quot;inspect ai&amp;quot;) but datasets were often very special or technical for certain cases. Or others try to solve everything and half of it is not working (lm evaluation harness). Its generally surprising how many datasets just suck or have issues.  &lt;/p&gt;\n\n&lt;p&gt;And there  must be someone better (hopefully) putting their working code out already. Otherwise I will probably try to get something going (again)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mho569",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nore_se_kra",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mho569/evalproject_for_local_llms_quants/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mho569/evalproject_for_local_llms_quants/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754337887,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen3 32b on a 3090Ti = 38tps\n\nI was expecting more? Like at least 50tps and more like 60? Am I tripping?\n\n    C:\\&gt;llama-bench.exe -m Qwen_Qwen3-32B-GGUF\\Qwen_Qwen3-32B-Q4_K_L.gguf --flash-attn 1\n    ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no\n    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    ggml_cuda_init: found 1 CUDA devices:\n    Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\n    \n    | model | size | params | backend | ngl | fa | test | t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | CUDA,RPC | 99 | 1 | pp512 | 1442.69 ± 17.38 |\n    | qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | CUDA,RPC | 99 | 1 | tg128 | 38.48 ± 0.06 |\n    build: 5aa1105d (6082)",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "3090Ti - 38 tokens/sec?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhhpy9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754332894,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754323876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 32b on a 3090Ti = 38tps&lt;/p&gt;\n\n&lt;p&gt;I was expecting more? Like at least 50tps and more like 60? Am I tripping?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;C:\\&amp;gt;llama-bench.exe -m Qwen_Qwen3-32B-GGUF\\Qwen_Qwen3-32B-Q4_K_L.gguf --flash-attn 1\nggml_cuda_init: GGML_CUDA_FORCE_MMQ: no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\nDevice 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\n\n| model | size | params | backend | ngl | fa | test | t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | CUDA,RPC | 99 | 1 | pp512 | 1442.69 ± 17.38 |\n| qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | CUDA,RPC | 99 | 1 | tg128 | 38.48 ± 0.06 |\nbuild: 5aa1105d (6082)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhhpy9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhhpy9/3090ti_38_tokenssec/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhhpy9/3090ti_38_tokenssec/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754323876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think OpenAI figured something out with this indentation in Codex (KISS). \n\nThe instructions are in english, but when overlooking, it is literally \"pseudo code\" with scopes, if and else clauses, \"finally\" clauses...  \n  \nPrompts are pseudo code. Nested indentation plays crucial role in Codex's success IMO.  \nUsing \"-\", \"\\\\t\" and \"\\\\n\" is pretty efficient. Also, The way \\_CODING GUIDELINES\\_ is highlighted is interesting. Reminds of Anthropic's XML tags in Claude, but less elegant.  \n  \nThis is currently one of the most powerful agents.  \n  \nKeep It Simple? Something to have in mind...",
          "author_fullname": "t2_5p5o5yxn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Keep It Simple Pseudo Code (That's what Codex does)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh0ltj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/J7QgKm1guKOSxZb3MVoof9r_uFvQZduBeUBP32dzpB8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754271438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think OpenAI figured something out with this indentation in Codex (KISS). &lt;/p&gt;\n\n&lt;p&gt;The instructions are in english, but when overlooking, it is literally &amp;quot;pseudo code&amp;quot; with scopes, if and else clauses, &amp;quot;finally&amp;quot; clauses...  &lt;/p&gt;\n\n&lt;p&gt;Prompts are pseudo code. Nested indentation plays crucial role in Codex&amp;#39;s success IMO.&lt;br/&gt;\nUsing &amp;quot;-&amp;quot;, &amp;quot;\\t&amp;quot; and &amp;quot;\\n&amp;quot; is pretty efficient. Also, The way _CODING GUIDELINES_ is highlighted is interesting. Reminds of Anthropic&amp;#39;s XML tags in Claude, but less elegant.  &lt;/p&gt;\n\n&lt;p&gt;This is currently one of the most powerful agents.  &lt;/p&gt;\n\n&lt;p&gt;Keep It Simple? Something to have in mind...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nk1a76nkpwgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?auto=webp&amp;s=3217bb71a80edc1997a73840a0e09d7fc2900229",
                  "width": 800,
                  "height": 831
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5183ed1f8eb9f7bba1469735154f6de46534a86",
                    "width": 108,
                    "height": 112
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=825eaaf03b32601b5d9c4f95ada2060e2cd39a42",
                    "width": 216,
                    "height": 224
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a680301e72bbf5c4139e647a71d653e4faf9d90b",
                    "width": 320,
                    "height": 332
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7f08a511a8b4c55448cf10557e9558c548047b1",
                    "width": 640,
                    "height": 664
                  }
                ],
                "variants": {},
                "id": "pBM8p2fA5M4u9sC1Ro6a-lYp0_kbuHXbrSbVRuqVQ7Q"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh0ltj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cov_id19",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh0ltj/keep_it_simple_pseudo_code_thats_what_codex_does/",
          "stickied": false,
          "url": "https://i.redd.it/nk1a76nkpwgf1.jpeg",
          "subreddit_subscribers": 510259,
          "created_utc": 1754271438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a web developer who has just started to dabble with running LLMs locally, I feel like I have a high-level understanding of how they work and how to run and train them but I still feel very clueless on they actually work. I know it would be extremely difficult to be on the bleeding edge of building them, such as developing new transformer architecture, but is it possible to understand how they work enough to contribute to existing models or even build one of my own? Or would I need years worth of classes on statistics and machine learning?  I've read some of the papers on architecture and they do feel way over my head at this point. The question is, is it thousands of miles over my head or is it attainable without making it my career.\n\nSorry if it's a dumb question but I would love to get an idea of whether it's even worth it to try learning on the side.",
          "author_fullname": "t2_mw9ag8b4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it difficult to get into the field of building LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhjk0z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754328352,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754327904,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a web developer who has just started to dabble with running LLMs locally, I feel like I have a high-level understanding of how they work and how to run and train them but I still feel very clueless on they actually work. I know it would be extremely difficult to be on the bleeding edge of building them, such as developing new transformer architecture, but is it possible to understand how they work enough to contribute to existing models or even build one of my own? Or would I need years worth of classes on statistics and machine learning?  I&amp;#39;ve read some of the papers on architecture and they do feel way over my head at this point. The question is, is it thousands of miles over my head or is it attainable without making it my career.&lt;/p&gt;\n\n&lt;p&gt;Sorry if it&amp;#39;s a dumb question but I would love to get an idea of whether it&amp;#39;s even worth it to try learning on the side.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhjk0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Traditional_Bet8239",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhjk0z/is_it_difficult_to_get_into_the_field_of_building/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhjk0z/is_it_difficult_to_get_into_the_field_of_building/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754327904,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Spent a few days finishing the evaluation for Qwen3-30B-A3B-Instruct-2507's quant instead of vibe checking the performance of the DWQ. It turns out the 4bit DWQ is quite close to the 8bit, even though the DWQ is still in an experimental phase, it's quite solid.\n\nhttps://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390\n\n",
          "author_fullname": "t2_aqcxxu50",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX 4bit DWQ vs 8bit eval",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kj8dz3orrygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=130a77acaab502d0c212b2aac3139c953759382d"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e9e750f61b69dc716d34ce5c7f64c5e73c749d88"
                },
                {
                  "y": 158,
                  "x": 320,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9026ae7698238e5701b096b17fbb73f91f6b8f4f"
                },
                {
                  "y": 317,
                  "x": 640,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff6a7dde9b00eba2574dc564c24eb08fa9de9617"
                },
                {
                  "y": 476,
                  "x": 960,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a6d8ffabd6439c7bb37a77b9a3bf6a2230f4535"
                },
                {
                  "y": 536,
                  "x": 1080,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01ec93da10c6235803c27b984d4d6f9e597482c4"
                }
              ],
              "s": {
                "y": 790,
                "x": 1590,
                "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390"
              },
              "id": "kj8dz3orrygf1"
            }
          },
          "name": "t3_1mh7yud",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/skKvv4aQvCUeHEuMqErewlo-Pi0FnE1rG_pbppcoUSo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754296426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Spent a few days finishing the evaluation for Qwen3-30B-A3B-Instruct-2507&amp;#39;s quant instead of vibe checking the performance of the DWQ. It turns out the 4bit DWQ is quite close to the 8bit, even though the DWQ is still in an experimental phase, it&amp;#39;s quite solid.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390\"&gt;https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh7yud",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tiny_Judge_2119",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754296426,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently using ollama from the command line in linux and it works well. But one thing I miss from chatgpt is having the model supplement its knowledge with web search. How can I allow a local model to search the web when it thinks it would be helpful?",
          "author_fullname": "t2_sm168dt0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can I allow a local model to search the web?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhcyu0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754312880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently using ollama from the command line in linux and it works well. But one thing I miss from chatgpt is having the model supplement its knowledge with web search. How can I allow a local model to search the web when it thinks it would be helpful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhcyu0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMrsPotts",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhcyu0/how_can_i_allow_a_local_model_to_search_the_web/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhcyu0/how_can_i_allow_a_local_model_to_search_the_web/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754312880,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new into the local AI world, what kind of pc specs would i need to run a useful ai agent specialized in coding? ",
          "author_fullname": "t2_1h8xfc6x3f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "is there an actually useful ai model for coding tasks and workflows?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhrryp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754346257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new into the local AI world, what kind of pc specs would i need to run a useful ai agent specialized in coding? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhrryp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Comfortable-Smoke672",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhrryp/is_there_an_actually_useful_ai_model_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhrryp/is_there_an_actually_useful_ai_model_for_coding/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754346257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I’ve shifted focus to the infra side, specifically LLM gateways.\n\nMost of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.\n\nSome quick observations from what I tried:\n\n* [Bifrost](https://getmax.im/2frost) (Go, self-hosted): Surprisingly fast even under high load. Saw around 11µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.\n* [Portkey](https://portkey.ai/features/ai-gateway): Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.\n* **Kong** and [Gloo](https://www.solo.io/products/gloo-ai-gateway): These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn’t feel natural. Not LLM-aware.\n* **Cloudflare’s AI Gateway**: Pretty good for lightweight routing if you're already using Cloudflare. But it’s a black box, not much visibility or customization.\n* **Aisera’s Gateway**: Geared toward enterprise support use cases. More of a vertical solution. Didn’t feel suitable for general-purpose LLM infra.\n* [LiteLLM](https://www.litellm.ai/): Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.\n\nWould love to hear what others are running in production, especially if you’re doing failover, traffic splitting, or anything more advanced.",
          "author_fullname": "t2_1tizhpru5u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best LLM gateway?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh9r0z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754303241,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I’ve shifted focus to the infra side, specifically LLM gateways.&lt;/p&gt;\n\n&lt;p&gt;Most of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.&lt;/p&gt;\n\n&lt;p&gt;Some quick observations from what I tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://getmax.im/2frost\"&gt;Bifrost&lt;/a&gt; (Go, self-hosted): Surprisingly fast even under high load. Saw around 11µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://portkey.ai/features/ai-gateway\"&gt;Portkey&lt;/a&gt;: Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kong&lt;/strong&gt; and &lt;a href=\"https://www.solo.io/products/gloo-ai-gateway\"&gt;Gloo&lt;/a&gt;: These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn’t feel natural. Not LLM-aware.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cloudflare’s AI Gateway&lt;/strong&gt;: Pretty good for lightweight routing if you&amp;#39;re already using Cloudflare. But it’s a black box, not much visibility or customization.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Aisera’s Gateway&lt;/strong&gt;: Geared toward enterprise support use cases. More of a vertical solution. Didn’t feel suitable for general-purpose LLM infra.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.litellm.ai/\"&gt;LiteLLM&lt;/a&gt;: Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear what others are running in production, especially if you’re doing failover, traffic splitting, or anything more advanced.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mh9r0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational-Bison786",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754303241,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vcawomd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 110,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgt2om",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 123,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e1348s852vgf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 910,
              "scrubber_media_url": "https://v.redd.it/e1348s852vgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e1348s852vgf1/DASHPlaylist.mpd?a=1756948483%2COTdkZThmN2YxYTFhNTVmNzM5ZWUzMjY1ODg1ZTU3ZWY1M2I3ZDRlNDcxZWIzYTI5YWQwM2YxODFiZmQ3MGU1Yw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 36,
              "hls_url": "https://v.redd.it/e1348s852vgf1/HLSPlaylist.m3u8?a=1756948483%2CMDkwNzNmYjVkMzYyM2Y1MjdjYzQzY2NlMDQ1ODQ4MGEwMDRiNjhjZWY1ZmUzZWRkNjcwODAwZGRiNDk0MTQ1MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 123,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=140&amp;height=110&amp;crop=140:110,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=abdd574d8f67b63e995dc11eb3cd4f8e74e11bb9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754251636,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e1348s852vgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?format=pjpg&amp;auto=webp&amp;s=0e7a1d12b1a2a1acf19fdfc0f91b16be267107fa",
                  "width": 1350,
                  "height": 1068
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8aff50a70e5cb53b7b38c9b35a93218ab06b529e",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5e621992a0c2958c031c79fe79c2a214c0cc8d5",
                    "width": 216,
                    "height": 170
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1edee9ae25cab3880fe29e9ac723b8339f6cebcf",
                    "width": 320,
                    "height": 253
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5bb59fb0f5c8e7a4e248e1068b8aa841bfa37998",
                    "width": 640,
                    "height": 506
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d85e99a79f17e7ec33873066c7eb6394c4cc7a5e",
                    "width": 960,
                    "height": 759
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ee9e27ab710d36f97c624d0fec564a4f4b5b127",
                    "width": 1080,
                    "height": 854
                  }
                ],
                "variants": {},
                "id": "dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mgt2om",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "onil_gova",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/",
          "stickied": false,
          "url": "https://v.redd.it/e1348s852vgf1",
          "subreddit_subscribers": 510259,
          "created_utc": 1754251636,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e1348s852vgf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 910,
              "scrubber_media_url": "https://v.redd.it/e1348s852vgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e1348s852vgf1/DASHPlaylist.mpd?a=1756948483%2COTdkZThmN2YxYTFhNTVmNzM5ZWUzMjY1ODg1ZTU3ZWY1M2I3ZDRlNDcxZWIzYTI5YWQwM2YxODFiZmQ3MGU1Yw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 36,
              "hls_url": "https://v.redd.it/e1348s852vgf1/HLSPlaylist.m3u8?a=1756948483%2CMDkwNzNmYjVkMzYyM2Y1MjdjYzQzY2NlMDQ1ODQ4MGEwMDRiNjhjZWY1ZmUzZWRkNjcwODAwZGRiNDk0MTQ1MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "They said they're refining it months ago. Possibly timing to coincide with OpenAI's drop? Would be epic, I'm a fan of both. Especially if OpenAI's is not a reasoning model.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When DeepSeek r2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgny8p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "transparent",
          "ups": 215,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 215,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/V9pPhmLOwBjBig1Mp88kf2vrbPkb0OVif8im0hRtZXs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754239452,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They said they&amp;#39;re refining it months ago. Possibly timing to coincide with OpenAI&amp;#39;s drop? Would be epic, I&amp;#39;m a fan of both. Especially if OpenAI&amp;#39;s is not a reasoning model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/dz0i0w1j2ugf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?auto=webp&amp;s=6cf4a209d32163902a3fb91d5a06108b4ebb9e61",
                  "width": 1080,
                  "height": 654
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bab256437dc4b2ab50c0bfefc751721bae4de7c5",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9916573c0160c0ddd9eb66a28ab87d51d07207d6",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a55f67b3df278451d7c98ae260464229b3ad11fa",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aaef5f3c86c4f7340d2367eea7fce60751451a94",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3d7f058cca6ec2b2a9fcbd009eadd099423f494",
                    "width": 960,
                    "height": 581
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f8bd24556666c1de0f3c211632b5ace09b00eb99",
                    "width": 1080,
                    "height": 654
                  }
                ],
                "variants": {},
                "id": "lqAfsgD1hr66i0LHoQ3Cw_QWosWkvNCmE0O1wxkWap4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgny8p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/",
          "stickied": false,
          "url": "https://i.redd.it/dz0i0w1j2ugf1.jpeg",
          "subreddit_subscribers": 510259,
          "created_utc": 1754239452,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "😶‍🌫️",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the largest openweights LLM? non-MoE and MoE?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhqt77",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754343944,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;😶‍🌫️&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhqt77",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhqt77/whats_the_largest_openweights_llm_nonmoe_and_moe/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhqt77/whats_the_largest_openweights_llm_nonmoe_and_moe/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754343944,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is it simply the website chatbot? Or do I need to go to open router and use the free chat there . \n\nAlso I am new to AI chatbots , what is API? And if deepseek is free what are all these tokens and prices ??\n\nAm I using the best model (R1 0528) In the deepseek chatbot on the website ?? Or am I getting a weaker version on the site and I need to do some api stuff ??\n\nDo I need to click on (DEEPTHINK R1) button for me to get R1 0528??",
          "author_fullname": "t2_2c195d4e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to use  Deepseek R1 0528?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhp3to",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754340049,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it simply the website chatbot? Or do I need to go to open router and use the free chat there . &lt;/p&gt;\n\n&lt;p&gt;Also I am new to AI chatbots , what is API? And if deepseek is free what are all these tokens and prices ??&lt;/p&gt;\n\n&lt;p&gt;Am I using the best model (R1 0528) In the deepseek chatbot on the website ?? Or am I getting a weaker version on the site and I need to do some api stuff ??&lt;/p&gt;\n\n&lt;p&gt;Do I need to click on (DEEPTHINK R1) button for me to get R1 0528??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhp3to",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DryMistake",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhp3to/how_to_use_deepseek_r1_0528/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhp3to/how_to_use_deepseek_r1_0528/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754340049,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am just getting started with serious research, I wanted to work on MOE models. Here are my assumptions and thinking of buying hardware based on that. \n\nCurrent hardware: i7(13th gen 8 cores) + 64 RAM + RTX 4060. Current GPU hardware is pretty limited 8GB VRAM - not suited for any serious work. Also I do not reside in US, and most of the high end GPUs are 1.5x-2x price if I could find one in first place. Luckily most of my friend circle travel from US to my country, so I can get it from there - used 3090 with 24 GB is a good option but I will fall into serious risk if it stops working after a while, so I want to invest on 5090 at 2.4k possible upgrade if my work goes well.\n\nAssumptions: With MOE architecture system RAM + VRAM can work hand in hand enabling users work on best models locally.   \nVRAM contains active experts + gating network.   \nSystem RAM contains whole MOE model. Based on input tokens -  active parameters are selected. - if everything is in VRAM inference is no brainer.   \n  \nBut my question is how realistic is to expect Higher possibly 128 GB ram + 5090 can I expect to run models like GLM-Air 106B - 12B active parameters. \n\nAlso I was open to M3-Ultra but based on my research - due to lack of Cuda like architecture even 512 GB is not suitable for fine tuning - can someone correct me on this. \n\nPS: I'm actually planning to work full-time on this, so any help is appreciated.",
          "author_fullname": "t2_u5scsvlj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestion for upgrading hardware for MOE inference and fine-tuning.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhitwa",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754326350,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am just getting started with serious research, I wanted to work on MOE models. Here are my assumptions and thinking of buying hardware based on that. &lt;/p&gt;\n\n&lt;p&gt;Current hardware: i7(13th gen 8 cores) + 64 RAM + RTX 4060. Current GPU hardware is pretty limited 8GB VRAM - not suited for any serious work. Also I do not reside in US, and most of the high end GPUs are 1.5x-2x price if I could find one in first place. Luckily most of my friend circle travel from US to my country, so I can get it from there - used 3090 with 24 GB is a good option but I will fall into serious risk if it stops working after a while, so I want to invest on 5090 at 2.4k possible upgrade if my work goes well.&lt;/p&gt;\n\n&lt;p&gt;Assumptions: With MOE architecture system RAM + VRAM can work hand in hand enabling users work on best models locally.&lt;br/&gt;\nVRAM contains active experts + gating network.&lt;br/&gt;\nSystem RAM contains whole MOE model. Based on input tokens -  active parameters are selected. - if everything is in VRAM inference is no brainer.   &lt;/p&gt;\n\n&lt;p&gt;But my question is how realistic is to expect Higher possibly 128 GB ram + 5090 can I expect to run models like GLM-Air 106B - 12B active parameters. &lt;/p&gt;\n\n&lt;p&gt;Also I was open to M3-Ultra but based on my research - due to lack of Cuda like architecture even 512 GB is not suitable for fine tuning - can someone correct me on this. &lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;m actually planning to work full-time on this, so any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhitwa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Icy_Gas8807",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754326350,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you’re working with local LLMs or agents, you’ve probably dealt with this pain:\n\n* Stateless sessions that lose context\n* RAG pipelines that break or leak info\n* No clean way to store/retrieve memory scoped per user/project\n\nWe built [**Recallio**](https://recallio.ai) to fix it:  \nA simple API that gives you persistent, scoped, and compliant memory - no vector DB maintenance, no brittle chains.\n\n# What it does:\n\n* `POST /memory` – scoped writes with TTL, consent, tags\n* `POST /recall` – semantic recall + optional summarization\n* Graph memory API – structure and query relationships\n\nWorks with:\n\n* **LlamaIndex**, **LangChain**, **Open-source models**, and even your own agent stack.\n* Add to local LLM workflows or serve as memory for multi-agent setups\n\n  \nWould love feedback from anyone building personal agents, AI OS tools, or private copilots.\n\n→ [https://recallio.ai](https://recallio.ai)",
          "author_fullname": "t2_sz8vdlhdc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building local LLMs that remember? Here’s a memory layer that doesn’t suck.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mho26i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754337699,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you’re working with local LLMs or agents, you’ve probably dealt with this pain:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Stateless sessions that lose context&lt;/li&gt;\n&lt;li&gt;RAG pipelines that break or leak info&lt;/li&gt;\n&lt;li&gt;No clean way to store/retrieve memory scoped per user/project&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We built &lt;a href=\"https://recallio.ai\"&gt;&lt;strong&gt;Recallio&lt;/strong&gt;&lt;/a&gt; to fix it:&lt;br/&gt;\nA simple API that gives you persistent, scoped, and compliant memory - no vector DB maintenance, no brittle chains.&lt;/p&gt;\n\n&lt;h1&gt;What it does:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;POST /memory&lt;/code&gt; – scoped writes with TTL, consent, tags&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;POST /recall&lt;/code&gt; – semantic recall + optional summarization&lt;/li&gt;\n&lt;li&gt;Graph memory API – structure and query relationships&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Works with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;, &lt;strong&gt;LangChain&lt;/strong&gt;, &lt;strong&gt;Open-source models&lt;/strong&gt;, and even your own agent stack.&lt;/li&gt;\n&lt;li&gt;Add to local LLM workflows or serve as memory for multi-agent setups&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love feedback from anyone building personal agents, AI OS tools, or private copilots.&lt;/p&gt;\n\n&lt;p&gt;→ &lt;a href=\"https://recallio.ai\"&gt;https://recallio.ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/VQsVngyDjYxfnIA8BZbKvwP_4QObPRr0Duzu51eNaJA.png?auto=webp&amp;s=5fcd993fb39d908fa277be4646d34812fc5053da",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/VQsVngyDjYxfnIA8BZbKvwP_4QObPRr0Duzu51eNaJA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=63077f1a187a2f57f1972c25ac8a7974a563f1ad",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/VQsVngyDjYxfnIA8BZbKvwP_4QObPRr0Duzu51eNaJA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d2c5c1781426639d74e7b77eb081aab96cdca9a",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/VQsVngyDjYxfnIA8BZbKvwP_4QObPRr0Duzu51eNaJA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a18dd4efeeb9301ebac5cfcff47e3d38ce76d167",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/VQsVngyDjYxfnIA8BZbKvwP_4QObPRr0Duzu51eNaJA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=50b768b68984a043e3fa5688f3cc668174743dbe",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/VQsVngyDjYxfnIA8BZbKvwP_4QObPRr0Duzu51eNaJA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=66c818829fced4e5b054d34bca98d43e531a973a",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/VQsVngyDjYxfnIA8BZbKvwP_4QObPRr0Duzu51eNaJA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5b43585ebef1869d683e90b60443c12fb86097a8",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "VQsVngyDjYxfnIA8BZbKvwP_4QObPRr0Duzu51eNaJA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mho26i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GardenCareless5991",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mho26i/building_local_llms_that_remember_heres_a_memory/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mho26i/building_local_llms_that_remember_heres_a_memory/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754337699,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.\n\n[https://huggingface.co/trillionlabs/Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)",
          "author_fullname": "t2_1ug5bi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This might be the largest un-aligned open-source model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgky8g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 222,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 222,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754232080,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?auto=webp&amp;s=b45653eadbcba17c38d2f4f2e15f6ebd41ec7c72",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7a80c31c557591f18bda1f387961a8fe38f053e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=13c5c597d54b6cfe9cc7d137d17b8c65f8c6db95",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=10f045816aeb8c03203c2c64cc8f2065cb392da4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cad5dd9413d8196f56dd930d3b4333ee0351d472",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37853a1aca067c019b4d7c06a11c66190e6bc001",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f4fc9332f1425fd05814a4fb77159385e0f2d89",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgky8g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jshin49",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754232080,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fp657",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmx8w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=64baffecac79129329accd5dfef00560cde30027",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754236913,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/randombk/chatterbox-vllm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?auto=webp&amp;s=f061762d17683d4e88608dbfe355d57e45d90ad5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d660bf476941abc2978684579558acc15bd0d2e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f08036e019ea53656d9f429e46be530e96c0ed23",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eac38ddf9a5c589c5a861f8aafada6fe73427034",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=50d687fd0b27b1fc30b1175e432b4518d7d1f15d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=185870ef47db93a269dea9dfe0a0629b16de6b64",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df00e364b0a49f0fb368440e176f32ee5e39b351",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgmx8w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dlp_randombk",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/",
          "stickied": false,
          "url": "https://github.com/randombk/chatterbox-vllm",
          "subreddit_subscribers": 510259,
          "created_utc": 1754236913,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "it just seems a bit ridiculous to use existing LLMs to output fine-tuning data  \nlike how are you getting the full set of data of what you need for fine-tuning?  \ndo you just set temperature to high? ",
          "author_fullname": "t2_1ek3ly58xd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "how are you guys getting data for fine-tuning?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhnhol",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754336429,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;it just seems a bit ridiculous to use existing LLMs to output fine-tuning data&lt;br/&gt;\nlike how are you getting the full set of data of what you need for fine-tuning?&lt;br/&gt;\ndo you just set temperature to high? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhnhol",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "backlinkbento",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhnhol/how_are_you_guys_getting_data_for_finetuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhnhol/how_are_you_guys_getting_data_for_finetuning/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754336429,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are they really gonna train a model that's absolutely useless to give to us?",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why doesn't \"OpenAI\" just release one of the models they already have? Like 3.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgiyg4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 266,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 266,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754226851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are they really gonna train a model that&amp;#39;s absolutely useless to give to us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgiyg4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 191,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754226851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🧠 Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I’ve implemented after Gemma 🚀. This was a major milestone for me, especially since there’s no open-source implementation of Qwen 2 available online (at least none I could find).\n\nWhat makes this build special:\n✅ Implemented without access to source code\n📖 Based entirely on the Qwen 1 &amp; Qwen 2 research papers\n🧱 Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n⚠️ Does not support Mixture of Experts (MoE) yet\n\nThis project pushed my understanding of transformer architectures even further, and I’m excited to keep going.\nIf you're into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!\n\nSource code: https://github.com/introlix/Swiftlet\nKaggle: https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet",
          "author_fullname": "t2_6qpq9avr5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reimplemention of Qwen 2 from scratch",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgpb8t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 110,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 110,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754242723,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🧠 Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I’ve implemented after Gemma 🚀. This was a major milestone for me, especially since there’s no open-source implementation of Qwen 2 available online (at least none I could find).&lt;/p&gt;\n\n&lt;p&gt;What makes this build special:\n✅ Implemented without access to source code\n📖 Based entirely on the Qwen 1 &amp;amp; Qwen 2 research papers\n🧱 Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n⚠️ Does not support Mixture of Experts (MoE) yet&lt;/p&gt;\n\n&lt;p&gt;This project pushed my understanding of transformer architectures even further, and I’m excited to keep going.\nIf you&amp;#39;re into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/introlix/Swiftlet\"&gt;https://github.com/introlix/Swiftlet&lt;/a&gt;\nKaggle: &lt;a href=\"https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet\"&gt;https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?auto=webp&amp;s=519a96c79f13a619e42513cc0f904d9b36729fa1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09e4e357ff6f03ec51f0d4d875169c2822efb899",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a804dcd73acb0eafcbf3c98a337b04b0330590",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6460444e14680b3ebc1e71bebe27f5eaaab08b28",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=142de401ddac2c07588c3df4af6a9a88a8c43665",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=730308dc3a494b5e493c8cc6e298f0435aba1498",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=798ec017daa864fb62d31aa0cd92c8316b840da9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgpb8t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CodingWithSatyam",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754242723,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey\n\nDoes anybody have some experience working with those newer Ryzen AI Chips in case of running Models (up to 70B Q4 ? )\nFine tuning LLMs using LoRA or converting models from/into GGUF?\n\nSaw those are more affordable than going for a maxed out mac book pro and would be quite interested in their performance and semi professional use with fine tuning and converting.\n\nShare your experiences, happy to read 😁👍",
          "author_fullname": "t2_i00m20zzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running, fine tuning and converting LLMs on new Ryzen AI 7 or 9 APUs - 64-128GB RAM - 75% VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhc31j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754310562,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey&lt;/p&gt;\n\n&lt;p&gt;Does anybody have some experience working with those newer Ryzen AI Chips in case of running Models (up to 70B Q4 ? )\nFine tuning LLMs using LoRA or converting models from/into GGUF?&lt;/p&gt;\n\n&lt;p&gt;Saw those are more affordable than going for a maxed out mac book pro and would be quite interested in their performance and semi professional use with fine tuning and converting.&lt;/p&gt;\n\n&lt;p&gt;Share your experiences, happy to read 😁👍&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhc31j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IngloriousBastrd7908",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhc31j/running_fine_tuning_and_converting_llms_on_new/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhc31j/running_fine_tuning_and_converting_llms_on_new/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754310562,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "7900 XTX  24Gb or RTX Pro 4000 24GB blackwell ?  \nAMD is 303TDP about 800€ and RTX is 140W TDP about 1200 euros, not yet available much?\n\nvLLM or Ollama like gemma3?\n\nCan anyone estimate? I have 5090 and 7900 XTX and in Ollama 5090 gives 66t/s while 7900 xtx 29t/s in gemma3-27b.  \nI guess RTX pro 4000 at least twice slower than 5090, so maybe quite close to 7900 XTX?\n\nI am just thinking should I get rid of 7900 XTX and switch to blackwell, but 7900 XTX works pretty well in vLLM meanwhile havent yet been able to get 5090 even working with vLLM. \n\nI need 1 slot cards, thats why rtx pro 4000 comes to question but I can live also with 2,5x cards",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which one is faster in LLM inference, 7900 XTX or RTX Pro 4000 ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhf5jq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754318142,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;7900 XTX  24Gb or RTX Pro 4000 24GB blackwell ?&lt;br/&gt;\nAMD is 303TDP about 800€ and RTX is 140W TDP about 1200 euros, not yet available much?&lt;/p&gt;\n\n&lt;p&gt;vLLM or Ollama like gemma3?&lt;/p&gt;\n\n&lt;p&gt;Can anyone estimate? I have 5090 and 7900 XTX and in Ollama 5090 gives 66t/s while 7900 xtx 29t/s in gemma3-27b.&lt;br/&gt;\nI guess RTX pro 4000 at least twice slower than 5090, so maybe quite close to 7900 XTX?&lt;/p&gt;\n\n&lt;p&gt;I am just thinking should I get rid of 7900 XTX and switch to blackwell, but 7900 XTX works pretty well in vLLM meanwhile havent yet been able to get 5090 even working with vLLM. &lt;/p&gt;\n\n&lt;p&gt;I need 1 slot cards, thats why rtx pro 4000 comes to question but I can live also with 2,5x cards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhf5jq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhf5jq/which_one_is_faster_in_llm_inference_7900_xtx_or/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhf5jq/which_one_is_faster_in_llm_inference_7900_xtx_or/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754318142,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.\n\nI think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it's about [commoditizing your complement](https://gwern.net/complement). But for many of these companies, which are pure play AI Labs, this simply does not hold.\n\nSo the question remains, why are they doing this?\n\nOne theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is [a precedent](https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838) of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.\n\nIf this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don't find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.\n\nWhat prompted this question, was the [recent interview](https://youtu.be/mYDSSRS-B5U?t=2203) with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don't find his response very compelling).\n\n\\---\n\nOne aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?",
          "author_fullname": "t2_garopjsj1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are Chinese LLM companies effectively price dumping?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjlek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 196,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 196,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754232034,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754228603,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.&lt;/p&gt;\n\n&lt;p&gt;I think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it&amp;#39;s about &lt;a href=\"https://gwern.net/complement\"&gt;commoditizing your complement&lt;/a&gt;. But for many of these companies, which are pure play AI Labs, this simply does not hold.&lt;/p&gt;\n\n&lt;p&gt;So the question remains, why are they doing this?&lt;/p&gt;\n\n&lt;p&gt;One theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is &lt;a href=\"https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838\"&gt;a precedent&lt;/a&gt; of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.&lt;/p&gt;\n\n&lt;p&gt;If this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don&amp;#39;t find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.&lt;/p&gt;\n\n&lt;p&gt;What prompted this question, was the &lt;a href=\"https://youtu.be/mYDSSRS-B5U?t=2203\"&gt;recent interview&lt;/a&gt; with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don&amp;#39;t find his response very compelling).&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;One aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?auto=webp&amp;s=537c5cea0430f04039f256f31d55847ec27c39b6",
                  "width": 1238,
                  "height": 1400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3562c0af61eb68438add188a0ebb448f79b54657",
                    "width": 108,
                    "height": 122
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51a1f9e5bee46295e1d5ac9dc517b0097b9fa489",
                    "width": 216,
                    "height": 244
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=112817eb7860f41d0e5f471023d0200983bb9f91",
                    "width": 320,
                    "height": 361
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=577450661689dd64b4c310d0803093687dd84688",
                    "width": 640,
                    "height": 723
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcdc0e42b1e5a63bb47ad4fa640fbc0fc006cd5c",
                    "width": 960,
                    "height": 1085
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dc589f590b2d3181d4aca8b2fcc7ea75fd5cda5e",
                    "width": 1080,
                    "height": 1221
                  }
                ],
                "variants": {},
                "id": "Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgjlek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "uutnt",
          "discussion_type": null,
          "num_comments": 218,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754228603,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?\n\nI have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?\n\nI don't mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.\n\nShould I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn't really matter to me too much. I'm happy to wait. \n\nThis is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. \n\nWhat llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. \n\nWhat quants?\n\nI want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? \n\nAny ideas, thoughts? Anyone tried to maximise long context processing?",
          "author_fullname": "t2_dmg3f1uq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to maximise Prompt processing speed for long context usage?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhbp73",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754309485,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.&lt;/p&gt;\n\n&lt;p&gt;Should I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn&amp;#39;t really matter to me too much. I&amp;#39;m happy to wait. &lt;/p&gt;\n\n&lt;p&gt;This is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. &lt;/p&gt;\n\n&lt;p&gt;What llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. &lt;/p&gt;\n\n&lt;p&gt;What quants?&lt;/p&gt;\n\n&lt;p&gt;I want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? &lt;/p&gt;\n\n&lt;p&gt;Any ideas, thoughts? Anyone tried to maximise long context processing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhbp73",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Kangaroo6055",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754309485,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there any unbiased llm benchmakr for specific tasks like OCR, audio understanding etc. rather than resoning, solving maths and coding.",
          "author_fullname": "t2_h5du2zzgx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is There Any GooD Bencmarking Website for LLMs for Specific Tasks???",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhjyt8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754328789,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any unbiased llm benchmakr for specific tasks like OCR, audio understanding etc. rather than resoning, solving maths and coding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhjyt8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iamaseem1",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhjyt8/is_there_any_good_bencmarking_website_for_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhjyt8/is_there_any_good_bencmarking_website_for_llms/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754328789,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgnwnx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 100,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=e35cde9b2565c154a06b4a4ec2a056512f465e28",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754239344,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?auto=webp&amp;s=c1ed5f3fc1bf6e79ce85d0034c3222d7952acbd1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=165ab4b12c90bfe025b46debae13b140652c63d2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=47d3bbf5bd07f507b8f71659fd9562dc4d194b90",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b91013b0a9041e7c6c25c022439d1503d608d9af",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7344e67fe48dc6a6f67623605b3dc51b204d189",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=996d51aa65f7fae5b2ee3e5ffbe787ce0b8cfdcd",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4499831a246669c1b31de509f9c28678094e6e6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgnwnx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/",
          "stickied": false,
          "url": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4",
          "subreddit_subscribers": 510259,
          "created_utc": 1754239344,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am looking for a setup that can run 30b local models no problem and preferably 70b, even if they’re pretty slow. Any recommendations for what graphics cards to get and how many? Also good places to look for used ones? Also any insights into how much ram, vram, hdd/ssd space etc. I’m fairly new. Thanks!",
          "author_fullname": "t2_9htoyuti",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of setup should I get?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhjtvv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754328503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a setup that can run 30b local models no problem and preferably 70b, even if they’re pretty slow. Any recommendations for what graphics cards to get and how many? Also good places to look for used ones? Also any insights into how much ram, vram, hdd/ssd space etc. I’m fairly new. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhjtvv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SnooLemons5892",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhjtvv/what_kind_of_setup_should_i_get/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhjtvv/what_kind_of_setup_should_i_get/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754328503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to make a model that analyzes Handwritten Prescriptions and converts them to Text. But I am having a hard time in what to use ? Should I go with an OCR or should I go with a VLM like ColQwen ?  \nAlso I don't have the ground truth for these Prescriptions so how can I verify them ?  \n  \nAdditionally should I use something like a layout model or should I use something else ?\n\nThe image provided is from a Kaggle Dataset so no issue of privacy - \n\n[https://ibb.co/whkQp56T](https://ibb.co/whkQp56T)\n\nIn this should an OCR be used to convert this to text or should VLM be used to understand this whole document ? I am actually quite confused  \nIn the end I want result as a JSON with fields like name, medicine, frequency, tests, diagnosis etc.",
          "author_fullname": "t2_7m88zu40",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Handwritten Prescription to Text",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhj1cr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754326810,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to make a model that analyzes Handwritten Prescriptions and converts them to Text. But I am having a hard time in what to use ? Should I go with an OCR or should I go with a VLM like ColQwen ?&lt;br/&gt;\nAlso I don&amp;#39;t have the ground truth for these Prescriptions so how can I verify them ?  &lt;/p&gt;\n\n&lt;p&gt;Additionally should I use something like a layout model or should I use something else ?&lt;/p&gt;\n\n&lt;p&gt;The image provided is from a Kaggle Dataset so no issue of privacy - &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ibb.co/whkQp56T\"&gt;https://ibb.co/whkQp56T&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this should an OCR be used to convert this to text or should VLM be used to understand this whole document ? I am actually quite confused&lt;br/&gt;\nIn the end I want result as a JSON with fields like name, medicine, frequency, tests, diagnosis etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lTAzdTn-1K5Dl5JRUESU8hj75sJCED-FuJIqnwwHKsg.jpg?auto=webp&amp;s=7863e8a6cc45e200785e09699d26960480d003db",
                  "width": 2400,
                  "height": 3600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lTAzdTn-1K5Dl5JRUESU8hj75sJCED-FuJIqnwwHKsg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=13b5ca96fac11a3ca06a4eabbe530e77d9f56ced",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/lTAzdTn-1K5Dl5JRUESU8hj75sJCED-FuJIqnwwHKsg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d38351db66a86a65cfd7e40916a8cf25a7fe73b8",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://external-preview.redd.it/lTAzdTn-1K5Dl5JRUESU8hj75sJCED-FuJIqnwwHKsg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0535231b6e0cf5b448fa8b84c1faf91117bcfc5b",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/lTAzdTn-1K5Dl5JRUESU8hj75sJCED-FuJIqnwwHKsg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2ad67281db83263de1eeefd92f49c23fd4fedf5",
                    "width": 640,
                    "height": 960
                  },
                  {
                    "url": "https://external-preview.redd.it/lTAzdTn-1K5Dl5JRUESU8hj75sJCED-FuJIqnwwHKsg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d2ff31627958e0e3d4d3a1f2b4d9051968d13dee",
                    "width": 960,
                    "height": 1440
                  },
                  {
                    "url": "https://external-preview.redd.it/lTAzdTn-1K5Dl5JRUESU8hj75sJCED-FuJIqnwwHKsg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b1511ed332e51c312d03cb74ed8094cc2c384abd",
                    "width": 1080,
                    "height": 1620
                  }
                ],
                "variants": {},
                "id": "udT4QZgiVbOTaDnIXiD8_EZNt_N6762SRpnpYtnpNlw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhj1cr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rukelele_Dixit21",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhj1cr/handwritten_prescription_to_text/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhj1cr/handwritten_prescription_to_text/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754326810,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on:  I tried to reproduce the Flappy Bird game featured in the [z.ai/blog/glm-4.5](http://z.ai/blog/glm-4.5) blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there's no way through.\n\nI gave up on the model for a while, thinking that it was due to the 3-bit quant.  But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt.  This not only eliminated the \"thinking\" part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.\n\nCan anyone with 4, 6 or 8 bit mlx version verify if they have this problem?  Here's the exact prompt: \"Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.\"\n\nPS.  I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio.  Also, Qwen3-Coder-30B-A3B (unlsoth Q8\\_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.",
          "author_fullname": "t2_3xif6p3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Air Produces Better Code Without Thinking, Using 3-bit MLX (/nothink)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv53t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256522,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on:  I tried to reproduce the Flappy Bird game featured in the &lt;a href=\"http://z.ai/blog/glm-4.5\"&gt;z.ai/blog/glm-4.5&lt;/a&gt; blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there&amp;#39;s no way through.&lt;/p&gt;\n\n&lt;p&gt;I gave up on the model for a while, thinking that it was due to the 3-bit quant.  But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt.  This not only eliminated the &amp;quot;thinking&amp;quot; part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.&lt;/p&gt;\n\n&lt;p&gt;Can anyone with 4, 6 or 8 bit mlx version verify if they have this problem?  Here&amp;#39;s the exact prompt: &amp;quot;Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;PS.  I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio.  Also, Qwen3-Coder-30B-A3B (unlsoth Q8_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgv53t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jcmyang",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754256522,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just recently updated text generation web ui and am running Deepseek Distill Llama 3.3 70b with llama.cpp. Its one of those imatrix quants or whatever but it’s labeled Q4\\_KM I think. I am utilizing the full 131k context length and making it possible with K\\_V cache quantization. The weird part is the same exact configuration in LM Studio with the same exact gguf file was giving me like 15-16 t/s. Running the weights split between a 3090TI and a 3090, k\\_v quantized to q4 and ran on cpu and full context. In text generation web ui, i average at 3-4 t/s which is painfully slower. Anyone have any advice or insight? I apologize if this post wasn’t super coherent and detailed I just woke up but this issue is really bothering me. Thanks all.\n\nEdit: when loading a 70b model with the described setup, this is what the console reports:\n\n`13:03:04-177711 INFO     Loading \"Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf\"`\n\n`13:03:04-226187 INFO     Using gpu_layers=81 | ctx_size=131072 | cache_type=q4_0`\n\n`ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no`\n\n`ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no`\n\n`ggml_cuda_init: found 2 CUDA devices:`\n\n  `Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes`\n\n  `Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes`\n\n`build: 1 (9008328) with MSVC 19.44.35211.0 for x64`\n\n`system info: n_threads = 16, n_threads_batch = 16, total_threads = 32`\n\n\n\n`system_info: n_threads = 16 (n_threads_batch = 16) / 32 | CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |`\n\n\n\n`Web UI is disabled`\n\n`main: binding port with default address family`\n\n`main: HTTP server is listening, hostname:` [`127.0.0.1`](http://127.0.0.1)`, port: 55856, http threads: 31`\n\n`main: loading model`\n\n`llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090 Ti) - 23287 MiB free`\n\n`llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 3090) - 23306 MiB free`\n\n`llama_model_loader: loaded meta data with 39 key-value pairs and 724 tensors from user_data\\models\\Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf (version GGUF V3 (latest))`\n\n`llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.`\n\n`llama_model_loader: - kv   0:                       general.architecture str              = llama`\n\n`llama_model_loader: - kv   1:                               general.type str              = model`\n\n`llama_model_loader: - kv   2:`                               [`general.name`](http://general.name) `str              = Llama 3.3 70B Vulpecula R1`\n\n`llama_model_loader: - kv   3:                       general.organization str              = Sao10K`\n\n`llama_model_loader: - kv   4:                           general.finetune str              = Vulpecula-r1`\n\n`llama_model_loader: - kv   5:                           general.basename str              = Llama-3.3`\n\n`llama_model_loader: - kv   6:                         general.size_label str              = 70B`\n\n`llama_model_loader: - kv   7:                            general.license str              = llama3.3`\n\n`llama_model_loader: - kv   8:                   general.base_model.count u32              = 1`\n\n`llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.3 70B Instruct`\n\n`llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama`\n\n`llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...`\n\n`llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [\"en\"]`\n\n`llama_model_loader: - kv  13:                          llama.block_count u32              = 80`\n\n`llama_model_loader: - kv  14:                       llama.context_length u32              = 131072`\n\n`llama_model_loader: - kv  15:                     llama.embedding_length u32              = 8192`\n\n`llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 28672`\n\n`llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 64`\n\n`llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 8`\n\n`llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 500000.000000`\n\n`llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010`\n\n`llama_model_loader: - kv  21:                 llama.attention.key_length u32              = 128`\n\n`llama_model_loader: - kv  22:               llama.attention.value_length u32              = 128`\n\n`llama_model_loader: - kv  23:                           llama.vocab_size u32              = 128256`\n\n`llama_model_loader: - kv  24:                 llama.rope.dimension_count u32              = 128`\n\n`llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2`\n\n`llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = llama-bpe`\n\n`llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...`\n\n`llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...`\n\n`llama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...`\n\n`llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 128000`\n\n`llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 128009`\n\n`llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...`\n\n`llama_model_loader: - kv  33:               general.quantization_version u32              = 2`\n\n`llama_model_loader: - kv  34:                          general.file_type u32              = 15`\n\n`llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/Llama-3.3-70B-Vulpecula-r...`\n\n`llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt`\n\n`llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 560`\n\n`llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 125`\n\n`llama_model_loader: - type  f32:  162 tensors`\n\n`llama_model_loader: - type q4_K:  441 tensors`\n\n`llama_model_loader: - type q5_K:   40 tensors`\n\n`llama_model_loader: - type q6_K:   81 tensors`\n\n`print_info: file format = GGUF V3 (latest)`\n\n`print_info: file type   = Q4_K - Medium`\n\n`print_info: file size   = 39.59 GiB (4.82 BPW)`\n\n`load: special tokens cache size = 256`\n\n`load: token to piece cache size = 0.7999 MB`\n\n`print_info: arch             = llama`\n\n`print_info: vocab_only       = 0`\n\n`print_info: n_ctx_train      = 131072`\n\n`print_info: n_embd           = 8192`\n\n`print_info: n_layer          = 80`\n\n`print_info: n_head           = 64`\n\n`print_info: n_head_kv        = 8`\n\n`print_info: n_rot            = 128`\n\n`print_info: n_swa            = 0`\n\n`print_info: is_swa_any       = 0`\n\n`print_info: n_embd_head_k    = 128`\n\n`print_info: n_embd_head_v    = 128`\n\n`print_info: n_gqa            = 8`\n\n`print_info: n_embd_k_gqa     = 1024`\n\n`print_info: n_embd_v_gqa     = 1024`\n\n`print_info: f_norm_eps       = 0.0e+00`\n\n`print_info: f_norm_rms_eps   = 1.0e-05`\n\n`print_info: f_clamp_kqv      = 0.0e+00`\n\n`print_info: f_max_alibi_bias = 0.0e+00`\n\n`print_info: f_logit_scale    = 0.0e+00`\n\n`print_info: f_attn_scale     = 0.0e+00`\n\n`print_info: n_ff             = 28672`\n\n`print_info: n_expert         = 0`\n\n`print_info: n_expert_used    = 0`\n\n`print_info: causal attn      = 1`\n\n`print_info: pooling type     = 0`\n\n`print_info: rope type        = 0`\n\n`print_info: rope scaling     = linear`\n\n`print_info: freq_base_train  = 500000.0`\n\n`print_info: freq_scale_train = 1`\n\n`print_info: n_ctx_orig_yarn  = 131072`\n\n`print_info: rope_finetuned   = unknown`\n\n`print_info: model type       = 70B`\n\n`print_info: model params     = 70.55 B`\n\n`print_info:` [`general.name`](http://general.name)`= Llama 3.3 70B Vulpecula R1`\n\n`print_info: vocab type       = BPE`\n\n`print_info: n_vocab          = 128256`\n\n`print_info: n_merges         = 280147`\n\n`print_info: BOS token        = 128000 '&lt;|begin_of_text|&gt;'`\n\n`print_info: EOS token        = 128009 '&lt;|eot_id|&gt;'`\n\n`print_info: EOT token        = 128009 '&lt;|eot_id|&gt;'`\n\n`print_info: EOM token        = 128008 '&lt;|eom_id|&gt;'`\n\n`print_info: LF token         = 198 'ÄŠ'`\n\n`print_info: EOG token        = 128001 '&lt;|end_of_text|&gt;'`\n\n`print_info: EOG token        = 128008 '&lt;|eom_id|&gt;'`\n\n`print_info: EOG token        = 128009 '&lt;|eot_id|&gt;'`\n\n`print_info: max token length = 256`\n\n`load_tensors: loading model tensors, this can take a while... (mmap = true)`\n\n`load_tensors: offloading 80 repeating layers to GPU`\n\n`load_tensors: offloading output layer to GPU`\n\n`load_tensors: offloaded 81/81 layers to GPU`\n\n`load_tensors:  CUDA0_Split model buffer size = 20036.25 MiB`\n\n`load_tensors:  CUDA1_Split model buffer size = 19938.20 MiB`\n\n`load_tensors:        CUDA0 model buffer size =     2.56 MiB`\n\n`load_tensors:        CUDA1 model buffer size =     2.47 MiB`\n\n`load_tensors:   CPU_Mapped model buffer size =   563.62 MiB`\n\n`......`\n\n`llama_context: constructing llama_context`\n\n`llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache`\n\n`llama_context: n_seq_max     = 1`\n\n`llama_context: n_ctx         = 131072`\n\n`llama_context: n_ctx_per_seq = 131072`\n\n`llama_context: n_batch       = 256`\n\n`llama_context: n_ubatch      = 256`\n\n`llama_context: causal_attn   = 1`\n\n`llama_context: flash_attn    = 1`\n\n`llama_context: kv_unified    = true`\n\n`llama_context: freq_base     = 500000.0`\n\n`llama_context: freq_scale    = 1`\n\n`llama_context:  CUDA_Host  output buffer size =     0.49 MiB`\n\n`llama_kv_cache_unified:        CPU KV buffer size = 11520.00 MiB`\n\n`llama_kv_cache_unified: size = 11520.00 MiB (131072 cells,  80 layers,  1/ 1 seqs), K (q4_0): 5760.00 MiB, V (q4_0): 5760.00 MiB`\n\n`llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility`\n\n`llama_context:      CUDA0 compute buffer size =   368.00 MiB`\n\n`llama_context:      CUDA1 compute buffer size =   240.00 MiB`\n\n`llama_context:  CUDA_Host compute buffer size =   136.00 MiB`\n\n`llama_context: graph nodes  = 2647`\n\n`llama_context: graph splits = 163`\n\n`common_init_from_params: added &lt;|end_of_text|&gt; logit bias = -inf`\n\n`common_init_from_params: added &lt;|eom_id|&gt; logit bias = -inf`\n\n`common_init_from_params: added &lt;|eot_id|&gt; logit bias = -inf`\n\n`common_init_from_params: setting dry_penalty_last_n to ctx_size = 131072`\n\n`common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)`\n\n`main: model loaded`\n\n`main: chat template, chat_template: {{- bos_token }}`\n\n`{%- if custom_tools is defined %}`\n\n`{%- set tools = custom_tools %}`\n\n`{%- endif %}`\n\n`{%- if not tools_in_user_message is defined %}`\n\n`{%- set tools_in_user_message = true %}`\n\n`{%- endif %}`\n\n`{%- if not date_string is defined %}`\n\n`{%- set date_string = \"26 Jul 2024\" %}`\n\n`{%- endif %}`\n\n`{%- if not tools is defined %}`\n\n`{%- set tools = none %}`\n\n`{%- endif %}`\n\n\n\n`{#- This block extracts the system message, so we can slot it into the right place. #}`\n\n`{%- if messages[0]['role'] == 'system' %}`\n\n`{%- set system_message = messages[0]['content']|trim %}`\n\n`{%- set messages = messages[1:] %}`\n\n`{%- else %}`\n\n`{%- set system_message = \"\" %}`\n\n`{%- endif %}`\n\n\n\n`{#- System message + builtin tools #}`\n\n`{{- \"&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n\" }}`\n\n`{%- if builtin_tools is defined or tools is not none %}`\n\n`{{- \"Environment: ipython\\n\" }}`\n\n`{%- endif %}`\n\n`{%- if builtin_tools is defined %}`\n\n`{{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}`\n\n`{%- endif %}`\n\n\n\n`{%- if tools is not none and not tools_in_user_message %}`\n\n`{{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}`\n\n`{{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}`\n\n`{{- \"Do not use variables.\\n\\n\" }}`\n\n`{%- for t in tools %}`\n\n`{{- t | tojson(indent=4) }}`\n\n`{{- \"\\n\\n\" }}`\n\n`{%- endfor %}`\n\n`{%- endif %}`\n\n`{{- system_message }}`\n\n`{{- \"&lt;|eot_id|&gt;\" }}`\n\n\n\n`{#- Custom tools are passed in a user message with some extra guidance #}`\n\n`{%- if tools_in_user_message and not tools is none %}`\n\n`{#- Extract the first user message so we can plug it in here #}`\n\n`{%- if messages | length != 0 %}`\n\n`{%- set first_user_message = messages[0]['content']|trim %}`\n\n`{%- set messages = messages[1:] %}`\n\n`{%- else %}`\n\n`{{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}`\n\n`{%- endif %}`\n\n`{{- '&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n' -}}`\n\n`{{- \"Given the following functions, please respond with a JSON for a function call \" }}`\n\n`{{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}`\n\n`{{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}`\n\n`{{- \"Do not use variables.\\n\\n\" }}`\n\n`{%- for t in tools %}`\n\n`{{- t | tojson(indent=4) }}`\n\n`{{- \"\\n\\n\" }}`\n\n`{%- endfor %}`\n\n`{{- first_user_message + \"&lt;|eot_id|&gt;\"}}`\n\n`{%- endif %}`\n\n\n\n`{%- for message in messages %}`\n\n`{%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}`\n\n`{{- '&lt;|start_header_id|&gt;' + message['role'] + '&lt;|end_header_id|&gt;\\n\\n'+ message['content'] | trim + '&lt;|eot_id|&gt;' }}`\n\n`{%- elif 'tool_calls' in message %}`\n\n`{%- if not message.tool_calls|length == 1 %}`\n\n`{{- raise_exception(\"This model only supports single tool-calls at once!\") }}`\n\n`{%- endif %}`\n\n`{%- set tool_call = message.tool_calls[0].function %}`\n\n`{%- if builtin_tools is defined and tool_call.name in builtin_tools %}`\n\n`{{- '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n' -}}`\n\n`{{- \"&lt;|python_tag|&gt;\" + tool_call.name + \".call(\" }}`\n\n`{%- for arg_name, arg_val in tool_call.arguments | items %}`\n\n`{{- arg_name + '=\"' + arg_val + '\"' }}`\n\n`{%- if not loop.last %}`\n\n`{{- \", \" }}`\n\n`{%- endif %}`\n\n`{%- endfor %}`\n\n`{{- \")\" }}`\n\n`{%- else  %}`\n\n`{{- '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n' -}}`\n\n`{{- '{\"name\": \"' + tool_call.name + '\", ' }}`\n\n`{{- '\"parameters\": ' }}`\n\n`{{- tool_call.arguments | tojson }}`\n\n`{{- \"}\" }}`\n\n`{%- endif %}`\n\n`{%- if builtin_tools is defined %}`\n\n`{#- This means we're in ipython mode #}`\n\n`{{- \"&lt;|eom_id|&gt;\" }}`\n\n`{%- else %}`\n\n`{{- \"&lt;|eot_id|&gt;\" }}`\n\n`{%- endif %}`\n\n`{%- elif message.role == \"tool\" or message.role == \"ipython\" %}`\n\n`{{- \"&lt;|start_header_id|&gt;ipython&lt;|end_header_id|&gt;\\n\\n\" }}`\n\n`{%- if message.content is mapping or message.content is iterable %}`\n\n`{{- message.content | tojson }}`\n\n`{%- else %}`\n\n`{{- message.content }}`\n\n`{%- endif %}`\n\n`{{- \"&lt;|eot_id|&gt;\" }}`\n\n`{%- endif %}`\n\n`{%- endfor %}`\n\n`{%- if add_generation_prompt %}`\n\n`{{- '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n' }}`\n\n`{%- endif %}`\n\n`, example_format: '&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;`\n\n\n\n`You are a helpful assistant&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;`\n\n\n\n`Hello&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;`\n\n\n\n`Hi there&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;`\n\n\n\n`How are you?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;`\n\n\n\n`'`\n\n`main: server is listening on` [`http://127.0.0.1:55856`](http://127.0.0.1:55856) `- starting the main loop`\n\n`13:05:54-605320 INFO     Loaded \"Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf\" in 170.43 seconds.`\n\n`13:05:54-606803 INFO     LOADER: \"llama.cpp\"`\n\n`13:05:54-607751 INFO     TRUNCATION LENGTH: 131072`\n\n`13:05:54-608751 INFO     INSTRUCTION TEMPLATE: \"Custom (obtained from model metadata)\"`\n\n",
          "author_fullname": "t2_1flwpwd3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Poor performance from llama.cpp in text-generation-webui?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhcy5r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754327191,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754312832,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just recently updated text generation web ui and am running Deepseek Distill Llama 3.3 70b with llama.cpp. Its one of those imatrix quants or whatever but it’s labeled Q4_KM I think. I am utilizing the full 131k context length and making it possible with K_V cache quantization. The weird part is the same exact configuration in LM Studio with the same exact gguf file was giving me like 15-16 t/s. Running the weights split between a 3090TI and a 3090, k_v quantized to q4 and ran on cpu and full context. In text generation web ui, i average at 3-4 t/s which is painfully slower. Anyone have any advice or insight? I apologize if this post wasn’t super coherent and detailed I just woke up but this issue is really bothering me. Thanks all.&lt;/p&gt;\n\n&lt;p&gt;Edit: when loading a 70b model with the described setup, this is what the console reports:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:03:04-177711 INFO     Loading &amp;quot;Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:03:04-226187 INFO     Using gpu_layers=81 | ctx_size=131072 | cache_type=q4_0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ggml_cuda_init: found 2 CUDA devices:&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;build: 1 (9008328) with MSVC 19.44.35211.0 for x64&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;system info: n_threads = 16, n_threads_batch = 16, total_threads = 32&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;system_info: n_threads = 16 (n_threads_batch = 16) / 32 | CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Web UI is disabled&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: binding port with default address family&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: HTTP server is listening, hostname:&lt;/code&gt; &lt;a href=\"http://127.0.0.1\"&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/a&gt;&lt;code&gt;, port: 55856, http threads: 31&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: loading model&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090 Ti) - 23287 MiB free&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 3090) - 23306 MiB free&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: loaded meta data with 39 key-value pairs and 724 tensors from user_data\\models\\Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf (version GGUF V3 (latest))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   0:                       general.architecture str              = llama&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   1:                               general.type str              = model&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   2:&lt;/code&gt;                               &lt;a href=\"http://general.name\"&gt;&lt;code&gt;general.name&lt;/code&gt;&lt;/a&gt; &lt;code&gt;str              = Llama 3.3 70B Vulpecula R1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   3:                       general.organization str              = Sao10K&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   4:                           general.finetune str              = Vulpecula-r1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   5:                           general.basename str              = Llama-3.3&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   6:                         general.size_label str              = 70B&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   7:                            general.license str              = llama3.3&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   8:                   general.base_model.count u32              = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.3 70B Instruct&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [&amp;quot;en&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  13:                          llama.block_count u32              = 80&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  14:                       llama.context_length u32              = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  15:                     llama.embedding_length u32              = 8192&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 28672&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 64&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 8&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 500000.000000&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  21:                 llama.attention.key_length u32              = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  22:               llama.attention.value_length u32              = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  23:                           llama.vocab_size u32              = 128256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  24:                 llama.rope.dimension_count u32              = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = llama-bpe&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,128256]  = [&amp;quot;!&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;&amp;#39;&amp;quot;, ...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,280147]  = [&amp;quot;Ä  Ä &amp;quot;, &amp;quot;Ä  Ä Ä Ä &amp;quot;, &amp;quot;Ä Ä  Ä Ä &amp;quot;, &amp;quot;...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 128000&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 128009&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  33:               general.quantization_version u32              = 2&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  34:                          general.file_type u32              = 15&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/Llama-3.3-70B-Vulpecula-r...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 560&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 125&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - type  f32:  162 tensors&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - type q4_K:  441 tensors&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - type q5_K:   40 tensors&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - type q6_K:   81 tensors&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: file format = GGUF V3 (latest)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: file type   = Q4_K - Medium&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: file size   = 39.59 GiB (4.82 BPW)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load: special tokens cache size = 256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load: token to piece cache size = 0.7999 MB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: arch             = llama&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: vocab_only       = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_ctx_train      = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd           = 8192&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_layer          = 80&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_head           = 64&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_head_kv        = 8&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_rot            = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_swa            = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: is_swa_any       = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd_head_k    = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd_head_v    = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_gqa            = 8&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd_k_gqa     = 1024&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd_v_gqa     = 1024&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_norm_eps       = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_norm_rms_eps   = 1.0e-05&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_clamp_kqv      = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_max_alibi_bias = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_logit_scale    = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_attn_scale     = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_ff             = 28672&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_expert         = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_expert_used    = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: causal attn      = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: pooling type     = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: rope type        = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: rope scaling     = linear&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: freq_base_train  = 500000.0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: freq_scale_train = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_ctx_orig_yarn  = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: rope_finetuned   = unknown&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: model type       = 70B&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: model params     = 70.55 B&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info:&lt;/code&gt; &lt;a href=\"http://general.name\"&gt;&lt;code&gt;general.name&lt;/code&gt;&lt;/a&gt;&lt;code&gt;= Llama 3.3 70B Vulpecula R1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: vocab type       = BPE&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_vocab          = 128256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_merges         = 280147&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: BOS token        = 128000 &amp;#39;&amp;lt;|begin_of_text|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOS token        = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOT token        = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOM token        = 128008 &amp;#39;&amp;lt;|eom_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: LF token         = 198 &amp;#39;ÄŠ&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOG token        = 128001 &amp;#39;&amp;lt;|end_of_text|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOG token        = 128008 &amp;#39;&amp;lt;|eom_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOG token        = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: max token length = 256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors: loading model tensors, this can take a while... (mmap = true)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors: offloading 80 repeating layers to GPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors: offloading output layer to GPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors: offloaded 81/81 layers to GPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:  CUDA0_Split model buffer size = 20036.25 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:  CUDA1_Split model buffer size = 19938.20 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:        CUDA0 model buffer size =     2.56 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:        CUDA1 model buffer size =     2.47 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:   CPU_Mapped model buffer size =   563.62 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;......&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: constructing llama_context&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_seq_max     = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_ctx         = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_ctx_per_seq = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_batch       = 256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_ubatch      = 256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: causal_attn   = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: flash_attn    = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: kv_unified    = true&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: freq_base     = 500000.0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: freq_scale    = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context:  CUDA_Host  output buffer size =     0.49 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_kv_cache_unified:        CPU KV buffer size = 11520.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_kv_cache_unified: size = 11520.00 MiB (131072 cells,  80 layers,  1/ 1 seqs), K (q4_0): 5760.00 MiB, V (q4_0): 5760.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context:      CUDA0 compute buffer size =   368.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context:      CUDA1 compute buffer size =   240.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context:  CUDA_Host compute buffer size =   136.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: graph nodes  = 2647&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: graph splits = 163&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: added &amp;lt;|end_of_text|&amp;gt; logit bias = -inf&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: added &amp;lt;|eom_id|&amp;gt; logit bias = -inf&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: added &amp;lt;|eot_id|&amp;gt; logit bias = -inf&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: setting dry_penalty_last_n to ctx_size = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: model loaded&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: chat template, chat_template: {{- bos_token }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if custom_tools is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set tools = custom_tools %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not tools_in_user_message is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set tools_in_user_message = true %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not date_string is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set date_string = &amp;quot;26 Jul 2024&amp;quot; %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not tools is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set tools = none %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- This block extracts the system message, so we can slot it into the right place. #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if messages[0][&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set system_message = messages[0][&amp;#39;content&amp;#39;]|trim %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set messages = messages[1:] %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set system_message = &amp;quot;&amp;quot; %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- System message + builtin tools #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if builtin_tools is defined or tools is not none %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Environment: ipython\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if builtin_tools is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Tools: &amp;quot; + builtin_tools | reject(&amp;#39;equalto&amp;#39;, &amp;#39;code_interpreter&amp;#39;) | join(&amp;quot;, &amp;quot;) + &amp;quot;\\n\\n&amp;quot;}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if tools is not none and not tools_in_user_message %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;You have access to the following functions. To call a function, please respond with JSON for a function call.&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}.&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Do not use variables.\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- for t in tools %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- t | tojson(indent=4) }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endfor %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- system_message }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- Custom tools are passed in a user message with some extra guidance #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if tools_in_user_message and not tools is none %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- Extract the first user message so we can plug it in here #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if messages | length != 0 %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set first_user_message = messages[0][&amp;#39;content&amp;#39;]|trim %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set messages = messages[1:] %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- raise_exception(&amp;quot;Cannot put tools in the first user message when there&amp;#39;s no first user message!&amp;quot;) }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Given the following functions, please respond with a JSON for a function call &amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;with its proper arguments that best answers the given prompt.\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}.&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Do not use variables.\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- for t in tools %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- t | tojson(indent=4) }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endfor %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- first_user_message + &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot;}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- for message in messages %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not (message.role == &amp;#39;ipython&amp;#39; or message.role == &amp;#39;tool&amp;#39; or &amp;#39;tool_calls&amp;#39; in message) %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;&amp;#39; + message[&amp;#39;role&amp;#39;] + &amp;#39;&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39;+ message[&amp;#39;content&amp;#39;] | trim + &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- elif &amp;#39;tool_calls&amp;#39; in message %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not message.tool_calls|length == 1 %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- raise_exception(&amp;quot;This model only supports single tool-calls at once!&amp;quot;) }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set tool_call = message.tool_calls[0].function %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if builtin_tools is defined and tool_call.name in builtin_tools %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|python_tag|&amp;gt;&amp;quot; + tool_call.name + &amp;quot;.call(&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- for arg_name, arg_val in tool_call.arguments | items %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- arg_name + &amp;#39;=&amp;quot;&amp;#39; + arg_val + &amp;#39;&amp;quot;&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not loop.last %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;, &amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endfor %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;)&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else  %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;{&amp;quot;name&amp;quot;: &amp;quot;&amp;#39; + tool_call.name + &amp;#39;&amp;quot;, &amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;quot;parameters&amp;quot;: &amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- tool_call.arguments | tojson }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;}&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if builtin_tools is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- This means we&amp;#39;re in ipython mode #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|eom_id|&amp;gt;&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- elif message.role == &amp;quot;tool&amp;quot; or message.role == &amp;quot;ipython&amp;quot; %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|start_header_id|&amp;gt;ipython&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if message.content is mapping or message.content is iterable %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- message.content | tojson }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- message.content }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endfor %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if add_generation_prompt %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;, example_format: &amp;#39;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;You are a helpful assistant&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Hello&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Hi there&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;How are you?&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: server is listening on&lt;/code&gt; &lt;a href=\"http://127.0.0.1:55856\"&gt;&lt;code&gt;http://127.0.0.1:55856&lt;/code&gt;&lt;/a&gt; &lt;code&gt;- starting the main loop&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:05:54-605320 INFO     Loaded &amp;quot;Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf&amp;quot; in 170.43 seconds.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:05:54-606803 INFO     LOADER: &amp;quot;llama.cpp&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:05:54-607751 INFO     TRUNCATION LENGTH: 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:05:54-608751 INFO     INSTRUCTION TEMPLATE: &amp;quot;Custom (obtained from model metadata)&amp;quot;&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhcy5r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WyattTheSkid",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754312832,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "DWQ seems brand new so I'm wondering if LM Studio just doesn't support it ",
          "author_fullname": "t2_7wdc4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone got LM Studio working with DWQ models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5v49",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754288253,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DWQ seems brand new so I&amp;#39;m wondering if LM Studio just doesn&amp;#39;t support it &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh5v49",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxiedaniels",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5v49/anyone_got_lm_studio_working_with_dwq_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5v49/anyone_got_lm_studio_working_with_dwq_models/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754288253,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " WHAT THE DEVIL?\n\nAnother open model outperforms closed ones!  \nXBai o4 beats OpenAI o3-mini and *confidently* beats Anthropic's Claude Opus.\n\n•Parameters: 32.8 B\n•Training: Long-CoT RL + Process Reward Learning (SPRM)\n•Benchmarks (High-Modus):\n•AIME24: 86.5\n•AIME25: 77.9\n•LiveCodeBench v5: 67.2\n•C-EVAL: 89.7\n\n🔗Open source weights: https://huggingface.co/MetaStoneTec/XBai-o4",
          "author_fullname": "t2_dmji1c74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "XBai-04 Is It Real?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "idu2e1ngesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a2a62a8bef0a6d00f8049f16c8b54e8aa48b974"
                },
                {
                  "y": 124,
                  "x": 216,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=924e4b76f3498ca5e7d4ab97c08a75b6fb377dc1"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82f9fc643d2975e8481a615c1f5a19b5cb453032"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b611aef0c4759ebf456a134bc82a5fd8065b9c5a"
                },
                {
                  "y": 551,
                  "x": 960,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f73f5b3bcc334e135595851ef884c4997ac617"
                },
                {
                  "y": 620,
                  "x": 1080,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e786e475e1a54e20bdba7fb55d1e03e02789639"
                }
              ],
              "s": {
                "y": 704,
                "x": 1226,
                "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1226&amp;format=pjpg&amp;auto=webp&amp;s=5b2baf561ff73bb0e8b4b5708cd383ce9ed6f372"
              },
              "id": "idu2e1ngesgf1"
            },
            "q3imyykgesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8dbd1ac74caccea54654d6ebe571fab617928272"
                },
                {
                  "y": 158,
                  "x": 216,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c6af0dfc3c0aaa092cea1507485efebac28962f"
                },
                {
                  "y": 235,
                  "x": 320,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=602c9e9684804598a1e5221dbb4d23ce3610098a"
                },
                {
                  "y": 471,
                  "x": 640,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e073695ec8c765d1252320b4c7a5e0bd07547b82"
                },
                {
                  "y": 706,
                  "x": 960,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d48c30f79f06380c415eb3dc8e2120642d96e240"
                },
                {
                  "y": 794,
                  "x": 1080,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d39f9af5c4e09886e80918a8161ae1315aa4c7b"
                }
              ],
              "s": {
                "y": 942,
                "x": 1280,
                "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=e8846d514b560df6c44bca9b5375f5eed938412c"
              },
              "id": "q3imyykgesgf1"
            }
          },
          "name": "t3_1mggku0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 197,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "q3imyykgesgf1",
                "id": 720170802
              },
              {
                "caption": "",
                "media_id": "idu2e1ngesgf1",
                "id": 720170803
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 197,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ybTHvU3e25DWWBXsMwOWfbuGc2dzpO5QuzWGZyUC65s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754219245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;WHAT THE DEVIL?&lt;/p&gt;\n\n&lt;p&gt;Another open model outperforms closed ones!&lt;br/&gt;\nXBai o4 beats OpenAI o3-mini and &lt;em&gt;confidently&lt;/em&gt; beats Anthropic&amp;#39;s Claude Opus.&lt;/p&gt;\n\n&lt;p&gt;•Parameters: 32.8 B\n•Training: Long-CoT RL + Process Reward Learning (SPRM)\n•Benchmarks (High-Modus):\n•AIME24: 86.5\n•AIME25: 77.9\n•LiveCodeBench v5: 67.2\n•C-EVAL: 89.7&lt;/p&gt;\n\n&lt;p&gt;🔗Open source weights: &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mggku0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mggku0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ordinary_Mud7430",
          "discussion_type": null,
          "num_comments": 101,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mggku0",
          "subreddit_subscribers": 510259,
          "created_utc": 1754219245,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First time fine tuning a model in \"the cloud\".\nRunpod was suggested. And what should I say - it's a pita since the first few seconds.\nRDP - only via VNC. Accessing volume? No. SCP? No. Accurately telling you how much you used of your persistent volume? No. Figure out yourself while counting by hand. There is 330t TB available, and 222 TB used. It can't show anything below a TB.\nSetting up my pod for work yesterday so I can start without brain damage into the week today.\nAwesome idea - but GPUs are out for this Region. But let us charge you for your persistent volume.\nYou think you can trick run pod, find a solution and just start all over with a new pod in a region where GPUs are available?\nHaha, no way. After you pressed \"deploy\" we will charge you now for both pots. Accessing the platform to work on your pods or manage them? Whole platform not loading since almost 20 minutes - no chance to do anything. But hey, we will charge you 😂\n\nI don't know what anyone is doing with runpod. Am I unlucky? Maybe. Is the platform working? Not anymore. Only for my user btw 🤣\nSpeedtest benchmark? Could still do cloud gaming at 4k 60 fps.\nBut can't access the platform.\n\nFor me this platform is just a pita. I wasted several days trying to fix things and incompatibility issues of their pods after deployment.\n\nThis is by far one of the worst IT experiences in my life.\nCan't build my own server since I am traveling too much.\n\nAny alternatives that are working and can be accessed?\n\n\n\nEdit:\nClearing cache doesn't help. Incognito mode let's me click on login - but the moment I login it's stucked again. Other Laptop with different OS -&gt; same outcome like in incognito mode.\nClearly, my account is bugged or something.\n\n\nEdit 2:\n\nCould access the platform after ~2 hours again, out of nowhere with a respond from support via email asking for my antivirus. (Didn't work on several systems, worked perfectly before)\nI really think my account was bugged",
          "author_fullname": "t2_i00m20zzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Runpod breaks my head - need a working alternative",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh960c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754311303,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754301108,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First time fine tuning a model in &amp;quot;the cloud&amp;quot;.\nRunpod was suggested. And what should I say - it&amp;#39;s a pita since the first few seconds.\nRDP - only via VNC. Accessing volume? No. SCP? No. Accurately telling you how much you used of your persistent volume? No. Figure out yourself while counting by hand. There is 330t TB available, and 222 TB used. It can&amp;#39;t show anything below a TB.\nSetting up my pod for work yesterday so I can start without brain damage into the week today.\nAwesome idea - but GPUs are out for this Region. But let us charge you for your persistent volume.\nYou think you can trick run pod, find a solution and just start all over with a new pod in a region where GPUs are available?\nHaha, no way. After you pressed &amp;quot;deploy&amp;quot; we will charge you now for both pots. Accessing the platform to work on your pods or manage them? Whole platform not loading since almost 20 minutes - no chance to do anything. But hey, we will charge you 😂&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what anyone is doing with runpod. Am I unlucky? Maybe. Is the platform working? Not anymore. Only for my user btw 🤣\nSpeedtest benchmark? Could still do cloud gaming at 4k 60 fps.\nBut can&amp;#39;t access the platform.&lt;/p&gt;\n\n&lt;p&gt;For me this platform is just a pita. I wasted several days trying to fix things and incompatibility issues of their pods after deployment.&lt;/p&gt;\n\n&lt;p&gt;This is by far one of the worst IT experiences in my life.\nCan&amp;#39;t build my own server since I am traveling too much.&lt;/p&gt;\n\n&lt;p&gt;Any alternatives that are working and can be accessed?&lt;/p&gt;\n\n&lt;p&gt;Edit:\nClearing cache doesn&amp;#39;t help. Incognito mode let&amp;#39;s me click on login - but the moment I login it&amp;#39;s stucked again. Other Laptop with different OS -&amp;gt; same outcome like in incognito mode.\nClearly, my account is bugged or something.&lt;/p&gt;\n\n&lt;p&gt;Edit 2:&lt;/p&gt;\n\n&lt;p&gt;Could access the platform after ~2 hours again, out of nowhere with a respond from support via email asking for my antivirus. (Didn&amp;#39;t work on several systems, worked perfectly before)\nI really think my account was bugged&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh960c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IngloriousBastrd7908",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh960c/runpod_breaks_my_head_need_a_working_alternative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh960c/runpod_breaks_my_head_need_a_working_alternative/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754301108,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From my research, it seems that there is no benefit to 512 bs 256 because the next largest model that doesnt fit in 256 also doesnt fit in 512. Is this true, or is there an advantage to getting 512 vs 256. I am looking at Mac Studios. \n\nLastly, whatever the answer is, will that model at that weight at that quantize size at least be close to Sonnet 4 in speed and accuracy? If so id rather pay 500/mo for 1 year to own my LLM rather than paying 500/mo for ever to “rent” it. I can always install a new model as better models come out even if the latest open source model is a smidge behind the paywall ones owned by the antichrist. The 2 models ive heard compare are Qwen3 Coder and GLM 4.5 Air, so I guess asking which of these would best make use of the 256 or the 512 mac studio at what size etc and if it can be close to sonnet 4",
          "author_fullname": "t2_ufbr1m7p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best model (to replace Sonnet 4) that fits in 256GB Vram and the best that fits in 512gb of Vram?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh0n5h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754271548,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From my research, it seems that there is no benefit to 512 bs 256 because the next largest model that doesnt fit in 256 also doesnt fit in 512. Is this true, or is there an advantage to getting 512 vs 256. I am looking at Mac Studios. &lt;/p&gt;\n\n&lt;p&gt;Lastly, whatever the answer is, will that model at that weight at that quantize size at least be close to Sonnet 4 in speed and accuracy? If so id rather pay 500/mo for 1 year to own my LLM rather than paying 500/mo for ever to “rent” it. I can always install a new model as better models come out even if the latest open source model is a smidge behind the paywall ones owned by the antichrist. The 2 models ive heard compare are Qwen3 Coder and GLM 4.5 Air, so I guess asking which of these would best make use of the 256 or the 512 mac studio at what size etc and if it can be close to sonnet 4&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh0n5h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "devshore",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh0n5h/what_is_the_best_model_to_replace_sonnet_4_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh0n5h/what_is_the_best_model_to_replace_sonnet_4_that/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754271548,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've set up Qwen3 30b quant 4 on a home server running a single 3090. It really struggles with tool calls and can't seem to interact with the Cursor APIs effectively. What are some good models (if any) that will fit within 24gb of VRAM but still be able to utilize the Cursor tool calls in agent mode? I'm planning to try devstral 24b next.",
          "author_fullname": "t2_mw9ag8b4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local model for using with Cursor",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mhg0ts",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754320112,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve set up Qwen3 30b quant 4 on a home server running a single 3090. It really struggles with tool calls and can&amp;#39;t seem to interact with the Cursor APIs effectively. What are some good models (if any) that will fit within 24gb of VRAM but still be able to utilize the Cursor tool calls in agent mode? I&amp;#39;m planning to try devstral 24b next.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhg0ts",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Traditional_Bet8239",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhg0ts/best_local_model_for_using_with_cursor/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhg0ts/best_local_model_for_using_with_cursor/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754320112,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA's \"Highly Optimistic\" DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgis6h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 89,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 89,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=293dff94e206d07e95b1ceb35d306f084972280b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754226361,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?auto=webp&amp;s=761d7765165ce23883e3ea7e5265165d93fd7502",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=462a9746ffb12643f31808d38538f4c0ea76b555",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f437e173d256a303cec220b9cd2f260843de5e3",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f8b731f2beec62abb38c3d104c182153621be67",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09c3744f2431f90355d37d937f1352192cc87780",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee4f08a8c7da0b870a09e2fee6c837603f3ccdca",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f610522848162744871027feef9554990de435d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgis6h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/",
          "stickied": false,
          "url": "https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754226361,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "is it feasable to have a local model run on a pi 5 ? The response time is actually not that important.\n\nAnyone have something like a llama model run on pi5 with 8GB RAM?",
          "author_fullname": "t2_l7ryo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "lcoal llm on raspberry pi",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh8xqp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754300230,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is it feasable to have a local model run on a pi 5 ? The response time is actually not that important.&lt;/p&gt;\n\n&lt;p&gt;Anyone have something like a llama model run on pi5 with 8GB RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh8xqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "overlydelicioustea",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754300230,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i heard some good things about muse writing pose well and not being censored,  just curious if there are other similar open models that are sort of trained from books not part of a very expensive website. \n\nI've been using gemini lately on the website and it does a lot of things right. THe main issue is it just doesn't write like claude does and well claude doesn't think like gpt or gemini do. Then its like wtf you use then.  :)\n\nSo getting out of that just a bit, I've started playing around with novel crafter recently and honeslty it's hard to go back to the chatbots now. Like that free ai at the bottom of the open routerlist did better than perplexity and the 3 main ai sites on fanfiction. of course it and Kimi still censored berserk but I didn't jailbreak it (trying to avoid that so it doesn't make the writing suck hehe) but sometimes like when i was doing perplexity i can just delete and try again. \n\nI just like to try a model that truely is made for writing and not uncensored if it exist. I probably am overplaying muse bet it's censored against berserk too! Are there like any good models that write well and can actually do writing styles and NSFW? ",
          "author_fullname": "t2_gcu2g1163",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any good open source models for NSFW writing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgq8yz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754244875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i heard some good things about muse writing pose well and not being censored,  just curious if there are other similar open models that are sort of trained from books not part of a very expensive website. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using gemini lately on the website and it does a lot of things right. THe main issue is it just doesn&amp;#39;t write like claude does and well claude doesn&amp;#39;t think like gpt or gemini do. Then its like wtf you use then.  :)&lt;/p&gt;\n\n&lt;p&gt;So getting out of that just a bit, I&amp;#39;ve started playing around with novel crafter recently and honeslty it&amp;#39;s hard to go back to the chatbots now. Like that free ai at the bottom of the open routerlist did better than perplexity and the 3 main ai sites on fanfiction. of course it and Kimi still censored berserk but I didn&amp;#39;t jailbreak it (trying to avoid that so it doesn&amp;#39;t make the writing suck hehe) but sometimes like when i was doing perplexity i can just delete and try again. &lt;/p&gt;\n\n&lt;p&gt;I just like to try a model that truely is made for writing and not uncensored if it exist. I probably am overplaying muse bet it&amp;#39;s censored against berserk too! Are there like any good models that write well and can actually do writing styles and NSFW? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgq8yz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LoneyGamer2023",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgq8yz/are_there_any_good_open_source_models_for_nsfw/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgq8yz/are_there_any_good_open_source_models_for_nsfw/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754244875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yet another new model claiming to outperform larger ones:\n\n\n\n**Instruction following** is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.\n\nWe identify **lazy reasoning** during the thinking stage as a key cause of poor instruction adherence.\n\nTo address this, we propose a framework that promotes rigorous reasoning through **previewing and self-checking**.\n\nOur method begins by generating instruction data with **complex constraints**, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.\n\nTraining involves entropy-preserving supervised fine-tuning (**Entropy-SFT**) and token-wise entropy-adaptive reinforcement learning (**TEA-RL**), guided by rule-based multidimensional rewards.\n\nThis approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.\n\nExperiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like **DeepSeek-R1** and closed-source models like **ChatGPT-4o** on challenging instruction-following benchmarks.\n\n[https://huggingface.co/qihoo360/Light-IF-32B](https://huggingface.co/qihoo360/Light-IF-32B)\n\n\n\ntechnical report [https://huggingface.co/papers/2503.10460](https://huggingface.co/papers/2503.10460)\n\n\n\nprevious popular models by this company:\n\n[https://huggingface.co/qihoo360/TinyR1-32B-Preview](https://huggingface.co/qihoo360/TinyR1-32B-Preview)\n\n[https://huggingface.co/qihoo360/Light-R1-32B](https://huggingface.co/qihoo360/Light-R1-32B)\n\n  \nWhat do you think?",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qihoo360/Light-IF-32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mghy1u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": "#bbbdbf",
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QIubNPPJjdRirxQ73mfKHMMmxIY2VZ9TzcjXGXjIohk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another new model claiming to outperform larger ones:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.&lt;/p&gt;\n\n&lt;p&gt;We identify &lt;strong&gt;lazy reasoning&lt;/strong&gt; during the thinking stage as a key cause of poor instruction adherence.&lt;/p&gt;\n\n&lt;p&gt;To address this, we propose a framework that promotes rigorous reasoning through &lt;strong&gt;previewing and self-checking&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Our method begins by generating instruction data with &lt;strong&gt;complex constraints&lt;/strong&gt;, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.&lt;/p&gt;\n\n&lt;p&gt;Training involves entropy-preserving supervised fine-tuning (&lt;strong&gt;Entropy-SFT&lt;/strong&gt;) and token-wise entropy-adaptive reinforcement learning (&lt;strong&gt;TEA-RL&lt;/strong&gt;), guided by rule-based multidimensional rewards.&lt;/p&gt;\n\n&lt;p&gt;This approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.&lt;/p&gt;\n\n&lt;p&gt;Experiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like &lt;strong&gt;DeepSeek-R1&lt;/strong&gt; and closed-source models like &lt;strong&gt;ChatGPT-4o&lt;/strong&gt; on challenging instruction-following benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-IF-32B\"&gt;https://huggingface.co/qihoo360/Light-IF-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;technical report &lt;a href=\"https://huggingface.co/papers/2503.10460\"&gt;https://huggingface.co/papers/2503.10460&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;previous popular models by this company:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/TinyR1-32B-Preview\"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-R1-32B\"&gt;https://huggingface.co/qihoo360/Light-R1-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6vaf0crhrsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6vaf0crhrsgf1.png?auto=webp&amp;s=6ba618920def17d94508256824e2561aba8a6ec9",
                  "width": 1062,
                  "height": 980
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=081b3d7480032b122c209477d47263419358b811",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5fac696d38efe6b971413c8794edd434ae2c9926",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=20af7c29a89f974a98e34d36ff62aa93c6d3e970",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=771c3421ec90e5339a70e9664ac80ef7b729ca73",
                    "width": 640,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=753db7d1045cf75fdd0e7ff4fd0c94209e8cee25",
                    "width": 960,
                    "height": 885
                  }
                ],
                "variants": {},
                "id": "O7-1ZfpSudq0amigVzUb6mHn4tEC8x9xtRLUGpzh3sI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghy1u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/",
          "stickied": false,
          "url": "https://i.redd.it/6vaf0crhrsgf1.png",
          "subreddit_subscribers": 510259,
          "created_utc": 1754223868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Am I the only person who can't stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I'm gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere's 112 Command A Vision if not for the license) - I just can't help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? ",
          "author_fullname": "t2_ap0ra8pe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Daydreaming of a new Gemma model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgm8d3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754235233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am I the only person who can&amp;#39;t stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I&amp;#39;m gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere&amp;#39;s 112 Command A Vision if not for the license) - I just can&amp;#39;t help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mgm8d3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jazzlike_Source_5983",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754235233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a documentation system designed to help LLMs understand codebases more efficiently, and I'd love feedback from this community.\n\nThe Problem:\n- Feeding entire codebases/docs to LLMs wastes tokens and context\n- Most tasks only need a subset of information  \n- Human-written docs aren't structured for LLM consumption\n- Different models have different context windows\n\nThe Approach:\nA system that generates multiple \"zoom levels\" of the same documentation:\n- Overview (500 tokens) - capabilities and high-level structure\n- Architecture (2K tokens) - patterns, flows, integrations  \n- Full Details (8K+ tokens) - complete implementation specs\n\nLLMs can:\n- Start with Overview\n- Request deeper levels only if needed\n- Save 75%+ tokens on many tasks\n\nExample: \"Does this API support webhooks?\" → Overview is sufficient. \"Modify the authentication flow\" → Needs Full Details.\n\nAlso exploring different expression modes (technical/standard/friendly) for human readers, but the LLM optimization is my main focus.\n\nQuestions:\n1. How do you currently handle feeding docs/specs to your local models?\n2. Would progressive loading actually help with your context limits?\n3. What structure would work best for models like Mixtral, Llama, etc?\n4. Is this overengineering vs just better chunking strategies?\n\nNot building a product yet - trying to validate if this solves real problems people have with local models and limited context.\n\nThanks for any thoughts!",
          "author_fullname": "t2_9mh0wady",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Title: Reducing token usage with progressive documentation loading - looking for feedback on approach",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgytca",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754266233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a documentation system designed to help LLMs understand codebases more efficiently, and I&amp;#39;d love feedback from this community.&lt;/p&gt;\n\n&lt;p&gt;The Problem:\n- Feeding entire codebases/docs to LLMs wastes tokens and context\n- Most tasks only need a subset of information&lt;br/&gt;\n- Human-written docs aren&amp;#39;t structured for LLM consumption\n- Different models have different context windows&lt;/p&gt;\n\n&lt;p&gt;The Approach:\nA system that generates multiple &amp;quot;zoom levels&amp;quot; of the same documentation:\n- Overview (500 tokens) - capabilities and high-level structure\n- Architecture (2K tokens) - patterns, flows, integrations&lt;br/&gt;\n- Full Details (8K+ tokens) - complete implementation specs&lt;/p&gt;\n\n&lt;p&gt;LLMs can:\n- Start with Overview\n- Request deeper levels only if needed\n- Save 75%+ tokens on many tasks&lt;/p&gt;\n\n&lt;p&gt;Example: &amp;quot;Does this API support webhooks?&amp;quot; → Overview is sufficient. &amp;quot;Modify the authentication flow&amp;quot; → Needs Full Details.&lt;/p&gt;\n\n&lt;p&gt;Also exploring different expression modes (technical/standard/friendly) for human readers, but the LLM optimization is my main focus.&lt;/p&gt;\n\n&lt;p&gt;Questions:\n1. How do you currently handle feeding docs/specs to your local models?\n2. Would progressive loading actually help with your context limits?\n3. What structure would work best for models like Mixtral, Llama, etc?\n4. Is this overengineering vs just better chunking strategies?&lt;/p&gt;\n\n&lt;p&gt;Not building a product yet - trying to validate if this solves real problems people have with local models and limited context.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgytca",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrsockpicks",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgytca/title_reducing_token_usage_with_progressive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgytca/title_reducing_token_usage_with_progressive/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754266233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nhttps://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2\n\nOn 9th August 2025, I am starting a Small Language Model Workshop. It will be a 5-6 hour live workshop. This is purely for teaching and sharing knowledge. Think of it as a 3 times expanded and live version of Karpathy's repository and video. \n\nIn this workshop, we will build a production ready Small Language Model (SLM) fully from scratch.  \n\nTowards the end of this workshop, we will chain 8 GPUs and actually replicate the results of GPT-2. \n\nIt will be like building GPT-2 fully from scratch, and getting results which OpenAI got in their classical GPT-2 paper. \n\nThe workshop will start from tokenisation and end at multi-GPU programming. \n\nWe will work with 2 datasets: \n\n\\- TinyStories\n\n\\- FineWeb Edu\n\nWe will go through the following: \n\n\\- Loading datasets\n\n\\- Tokenization\n\n\\- Creating input-target pairs\n\n\\- Assembling the entire SLM architecture\n\n\\- Defining the training loop\n\n\\- Running inference\n\n\\- Multi-GPU version of training\n\nRegister for free here: [https://slm-from-scratch.vercel.app/](https://slm-from-scratch.vercel.app/)",
          "author_fullname": "t2_rb9k9zgob",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Build a Small Language Model from Scratch | Free 6 hour live workshop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "2062bdjfszgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=644b6890d9f1d63afe4d1b3649c082853f6b5df9"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=20a92ff4c0705d871443942faa8615498c590153"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2afca8c1a6986c761dee8f582e264c5741a4099b"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d13c836b271b555a60e400b298dd60837ffca1e"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=00aca5ea51a1bee55af64b4c90f31fb80592b388"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d7cf6280d9d0860d2aa3b597532e8c852614086"
                }
              ],
              "s": {
                "y": 4032,
                "x": 3024,
                "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2"
              },
              "id": "2062bdjfszgf1"
            }
          },
          "name": "t3_1mhbhn3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/wYyTcudp-ANH5QEeVy4mcXMJLT3H-mEztMtQMKLbu78.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754308876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2\"&gt;https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On 9th August 2025, I am starting a Small Language Model Workshop. It will be a 5-6 hour live workshop. This is purely for teaching and sharing knowledge. Think of it as a 3 times expanded and live version of Karpathy&amp;#39;s repository and video. &lt;/p&gt;\n\n&lt;p&gt;In this workshop, we will build a production ready Small Language Model (SLM) fully from scratch.  &lt;/p&gt;\n\n&lt;p&gt;Towards the end of this workshop, we will chain 8 GPUs and actually replicate the results of GPT-2. &lt;/p&gt;\n\n&lt;p&gt;It will be like building GPT-2 fully from scratch, and getting results which OpenAI got in their classical GPT-2 paper. &lt;/p&gt;\n\n&lt;p&gt;The workshop will start from tokenisation and end at multi-GPU programming. &lt;/p&gt;\n\n&lt;p&gt;We will work with 2 datasets: &lt;/p&gt;\n\n&lt;p&gt;- TinyStories&lt;/p&gt;\n\n&lt;p&gt;- FineWeb Edu&lt;/p&gt;\n\n&lt;p&gt;We will go through the following: &lt;/p&gt;\n\n&lt;p&gt;- Loading datasets&lt;/p&gt;\n\n&lt;p&gt;- Tokenization&lt;/p&gt;\n\n&lt;p&gt;- Creating input-target pairs&lt;/p&gt;\n\n&lt;p&gt;- Assembling the entire SLM architecture&lt;/p&gt;\n\n&lt;p&gt;- Defining the training loop&lt;/p&gt;\n\n&lt;p&gt;- Running inference&lt;/p&gt;\n\n&lt;p&gt;- Multi-GPU version of training&lt;/p&gt;\n\n&lt;p&gt;Register for free here: &lt;a href=\"https://slm-from-scratch.vercel.app/\"&gt;https://slm-from-scratch.vercel.app/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhbhn3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OtherRaisin3426",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbhn3/build_a_small_language_model_from_scratch_free_6/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbhn3/build_a_small_language_model_from_scratch_free_6/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754308876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It's big enough to be smart without being so huge it steps on OpenAI’s toes. In my experience with the model, here some notes:\n\n* It works *really* well with agents and tools\n* It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.\n* It’s fantastic with languages other than English, a weakness often sees in Chinese models.\n* It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models\n* The way it writes is very similar to OpenAI’s style.\n* Plus, whoever made this has *serious* computing power... they're giving away billions of tokens for \"free\" at a really fast speed\n* The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)\n\nBut ok, lets consider other players:  \n \\- Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly  \n \\- Anthropic: Naah  \n \\- Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don't see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.  \n \\- Amazon or Microsoft: Would be my second guess  \n \\- Google: Naah, they have Aistudio, when wants feedback they launch the model there  \n \\- IBM or Cohere: Hard to think, but they are very capable companies\n\nHonestly, it’s hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren’t fans of CloseAI, but if they say they’re releasing an open-source model, let’s be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.\n\nSo, what you guys think?",
          "author_fullname": "t2_y02rp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If Horizon Models is not from OpenAI, who would be?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgn94g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754237720,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It&amp;#39;s big enough to be smart without being so huge it steps on OpenAI’s toes. In my experience with the model, here some notes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It works &lt;em&gt;really&lt;/em&gt; well with agents and tools&lt;/li&gt;\n&lt;li&gt;It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.&lt;/li&gt;\n&lt;li&gt;It’s fantastic with languages other than English, a weakness often sees in Chinese models.&lt;/li&gt;\n&lt;li&gt;It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models&lt;/li&gt;\n&lt;li&gt;The way it writes is very similar to OpenAI’s style.&lt;/li&gt;\n&lt;li&gt;Plus, whoever made this has &lt;em&gt;serious&lt;/em&gt; computing power... they&amp;#39;re giving away billions of tokens for &amp;quot;free&amp;quot; at a really fast speed&lt;/li&gt;\n&lt;li&gt;The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But ok, lets consider other players:&lt;br/&gt;\n - Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly&lt;br/&gt;\n - Anthropic: Naah&lt;br/&gt;\n - Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don&amp;#39;t see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.&lt;br/&gt;\n - Amazon or Microsoft: Would be my second guess&lt;br/&gt;\n - Google: Naah, they have Aistudio, when wants feedback they launch the model there&lt;br/&gt;\n - IBM or Cohere: Hard to think, but they are very capable companies&lt;/p&gt;\n\n&lt;p&gt;Honestly, it’s hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren’t fans of CloseAI, but if they say they’re releasing an open-source model, let’s be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.&lt;/p&gt;\n\n&lt;p&gt;So, what you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgn94g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AMOVCS",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/",
          "subreddit_subscribers": 510259,
          "created_utc": 1754237720,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}