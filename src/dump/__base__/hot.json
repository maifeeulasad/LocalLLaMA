{
  "kind": "Listing",
  "data": {
    "after": "t3_1mdln75",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;format=png&amp;auto=webp&amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea\n\nThat’s insane — throughout this past July, Chinese companies have been rapidly open-sourcing AI models. First came Kimi-K2, then Qwen3, followed by GLM-4.5. On top of that, there’s Tencent’s HunyuanWorld and Alibaba’s Wan 2.2. Now, most of the trending models on Hugging Face are from China. Meanwhile, according to Zuckerberg, Meta is planning to shift toward a closed-source strategy going forward.\n\n[https://huggingface.co/models](https://huggingface.co/models)",
          "author_fullname": "t2_4zykmpa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 61,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vhlkl99m35gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e150fecc47715fbbf9bab8760e09b7c94192e21e"
                },
                {
                  "y": 94,
                  "x": 216,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b4a3e7bcc6017b88daf46cd61f0b6b012d3bca8"
                },
                {
                  "y": 140,
                  "x": 320,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b6dbe2a8a8ead7c8506de5aae218d40d74a7948"
                },
                {
                  "y": 280,
                  "x": 640,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c20983e9afc560ecd7c94cf907de4147c7a9e4d1"
                },
                {
                  "y": 420,
                  "x": 960,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=60db1a5fad622e3162af97670036ad4d3d9fa422"
                },
                {
                  "y": 473,
                  "x": 1080,
                  "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf166e61b76609870d09e46519c97327c99cf727"
                }
              ],
              "s": {
                "y": 481,
                "x": 1098,
                "u": "https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;format=png&amp;auto=webp&amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea"
              },
              "id": "vhlkl99m35gf1"
            }
          },
          "name": "t3_1mdsjn2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 475,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 475,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Pqx5Ku4b-UvrnWIofuwt9LYnoux9zPw_UBbzkN3H6v4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753937427,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea\"&gt;https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;That’s insane — throughout this past July, Chinese companies have been rapidly open-sourcing AI models. First came Kimi-K2, then Qwen3, followed by GLM-4.5. On top of that, there’s Tencent’s HunyuanWorld and Alibaba’s Wan 2.2. Now, most of the trending models on Hugging Face are from China. Meanwhile, according to Zuckerberg, Meta is planning to shift toward a closed-source strategy going forward.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/models\"&gt;https://huggingface.co/models&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdsjn2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jiawei243",
          "discussion_type": null,
          "num_comments": 82,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753937427,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1mqxxcqio8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chinese models pulling away",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdmsu9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 928,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 928,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WAZkPWKkayjP-D84-JfBNhxMGyjfTxBCkqcnNqASaSM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753920375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/727keqreo3gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/727keqreo3gf1.png?auto=webp&amp;s=fbd047ec9c49dcc4ecc981ac438a33640cf82f64",
                  "width": 500,
                  "height": 659
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/727keqreo3gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6a70ba5db010ef5c37f2d20d7547480395fec85",
                    "width": 108,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/727keqreo3gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4b427a0b64f0cfaebdc6ca4299f1db7633d895d",
                    "width": 216,
                    "height": 284
                  },
                  {
                    "url": "https://preview.redd.it/727keqreo3gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=638ce7aed31fa426f1cfea7678c6d9169932f5a9",
                    "width": 320,
                    "height": 421
                  }
                ],
                "variants": {},
                "id": "l5AL3evi8AGzgsGPpE-AV-Xqab8IV712A4wAAWJsjNM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mdmsu9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kniffliger_Kiffer",
          "discussion_type": null,
          "num_comments": 126,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/",
          "stickied": false,
          "url": "https://i.redd.it/727keqreo3gf1.png",
          "subreddit_subscribers": 507576,
          "created_utc": 1753920375,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " ",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 91,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mdykfn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/JtrsDuYApU5asaj4DMkYR46jMGTULVF74_jrKEDuzNY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753959873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/f5iqhqp7z6gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/f5iqhqp7z6gf1.jpeg?auto=webp&amp;s=9ebc8183abfedb5f08028da2d763991ae8501002",
                  "width": 593,
                  "height": 389
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e3bc13cc7709787b1633c87ce4deec12ada0949",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe549fcca1b5049e06c0516847a12686c2f98338",
                    "width": 216,
                    "height": 141
                  },
                  {
                    "url": "https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80da4073073fb12cdbab3b110619a3002d524b2f",
                    "width": 320,
                    "height": 209
                  }
                ],
                "variants": {},
                "id": "W6UmrcA-BG24HiTaK1cat2L9eGxll0ba_uZjbLzyRHA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mdykfn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/",
          "stickied": false,
          "url": "https://i.redd.it/f5iqhqp7z6gf1.jpeg",
          "subreddit_subscribers": 507576,
          "created_utc": 1753959873,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Is Reportedly Looking to Introduce a Dedicated Discrete NPU, Similar to Gaming GPUs But Targeted Towards AI Performance On PCs; Taking Edge AI to New Levels",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdx65u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753954906,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-is-looking-toward-introducing-a-dedicated-discrete-npu-similar-to-gaming-gpus/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdx65u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdx65u/amd_is_reportedly_looking_to_introduce_a/",
          "stickied": false,
          "url": "https://wccftech.com/amd-is-looking-toward-introducing-a-dedicated-discrete-npu-similar-to-gaming-gpus/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753954906,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, it's Emre from the Jan team.\n\nJan v0.6.6 is out. Over the past few weeks we've ripped out Cortex, the backend layer on top of llama.cpp. It's finally gone, every local model now runs directly on llama.cpp.\n\nPlus, you can switch to any llama.cpp build under Settings, Model Providers, llama.cpp (see the video above).\n\nJan v0.6.6 Highlights:\n\n* Cortex is removed, local models now run on `llama.cpp`\n* Hugging Face is integrated in Model Providers. So you can paste your HF token and run models in the cloud via Jan\n* Jan Hub has been a bit updated for faster model search and less clutter when browsing models\n* Inline-image support from MCP servers: If an MCP server returns an image (e.g. web search MCP).\n   * It's an experimental feature, please activate Experimental Features in Settings to see MCP settings.\n* Plus, we've also fixed a bunch of bugs\n\nUpdate your Jan or download the latest here: [https://jan.ai/](https://jan.ai/)\n\nFull release notes are here: [https://github.com/menloresearch/jan/releases](https://github.com/menloresearch/jan/releases)\n\n**Quick notes:**\n\n1. We removed Cortex because it added an extra hop and maintenance overhead. Folding its logic into Jan cuts latency and makes future mobile / server work simpler.\n2.  Regarding bugs &amp; previous requests: I'll reply to earlier requests and reports in the previous comments later today.",
          "author_fullname": "t2_g6cmmsdd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jan now runs fully on llama.cpp &amp; auto-updates the backend",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mdy1at",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/6tdds5rcr6gf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1356,
              "scrubber_media_url": "https://v.redd.it/6tdds5rcr6gf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/6tdds5rcr6gf1/DASHPlaylist.mpd?a=1756556941%2CMGI5ZGM5YzkxMjQ2NTRhYzViOTFhZTQ1YzNiMDYwZGMxZTA0YzZjYzQxMTg2OTQ3N2NkNzBhM2FjZmZmOTllZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 9,
              "hls_url": "https://v.redd.it/6tdds5rcr6gf1/HLSPlaylist.m3u8?a=1756556941%2CY2QxNGVjMGMzM2NkZDRiMDhmMWVjY2RjOWUzN2I2MDdiZjI3N2RlNTIzNDI5OTAzZDJlMzk3YmVjYzJhNTE4ZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=140&amp;height=111&amp;crop=140:111,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=57743d41c68dc489572118ded5f1d929e7abeba3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753958074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, it&amp;#39;s Emre from the Jan team.&lt;/p&gt;\n\n&lt;p&gt;Jan v0.6.6 is out. Over the past few weeks we&amp;#39;ve ripped out Cortex, the backend layer on top of llama.cpp. It&amp;#39;s finally gone, every local model now runs directly on llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;Plus, you can switch to any llama.cpp build under Settings, Model Providers, llama.cpp (see the video above).&lt;/p&gt;\n\n&lt;p&gt;Jan v0.6.6 Highlights:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cortex is removed, local models now run on &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Hugging Face is integrated in Model Providers. So you can paste your HF token and run models in the cloud via Jan&lt;/li&gt;\n&lt;li&gt;Jan Hub has been a bit updated for faster model search and less clutter when browsing models&lt;/li&gt;\n&lt;li&gt;Inline-image support from MCP servers: If an MCP server returns an image (e.g. web search MCP).\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s an experimental feature, please activate Experimental Features in Settings to see MCP settings.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Plus, we&amp;#39;ve also fixed a bunch of bugs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Update your Jan or download the latest here: &lt;a href=\"https://jan.ai/\"&gt;https://jan.ai/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Full release notes are here: &lt;a href=\"https://github.com/menloresearch/jan/releases\"&gt;https://github.com/menloresearch/jan/releases&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick notes:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We removed Cortex because it added an extra hop and maintenance overhead. Folding its logic into Jan cuts latency and makes future mobile / server work simpler.&lt;/li&gt;\n&lt;li&gt; Regarding bugs &amp;amp; previous requests: I&amp;#39;ll reply to earlier requests and reports in the previous comments later today.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/6tdds5rcr6gf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?format=pjpg&amp;auto=webp&amp;s=0e1c5efd621cd98139d0e6f762c83f3c37e7fea5",
                  "width": 1356,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f859c3bb7426a18c330ce87e3736a28dafc099f8",
                    "width": 108,
                    "height": 86
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6cb9d82211a044a07e0f4b70dfed27d01999f9f4",
                    "width": 216,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c95516a9359bd83519226daa998fe6d691200ba6",
                    "width": 320,
                    "height": 254
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=00d6e43fd5842c4ff17dcac2371f246a689ce076",
                    "width": 640,
                    "height": 509
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=25d7157f5ce696d297059ecb73ed2080cffbd80c",
                    "width": 960,
                    "height": 764
                  },
                  {
                    "url": "https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=79790394777eecaa52d5cec4ae8f93b678e4a94d",
                    "width": 1080,
                    "height": 860
                  }
                ],
                "variants": {},
                "id": "OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdy1at",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eck72",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/",
          "stickied": false,
          "url": "https://v.redd.it/6tdds5rcr6gf1",
          "subreddit_subscribers": 507576,
          "created_utc": 1753958074,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/6tdds5rcr6gf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1356,
              "scrubber_media_url": "https://v.redd.it/6tdds5rcr6gf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/6tdds5rcr6gf1/DASHPlaylist.mpd?a=1756556941%2CMGI5ZGM5YzkxMjQ2NTRhYzViOTFhZTQ1YzNiMDYwZGMxZTA0YzZjYzQxMTg2OTQ3N2NkNzBhM2FjZmZmOTllZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 9,
              "hls_url": "https://v.redd.it/6tdds5rcr6gf1/HLSPlaylist.m3u8?a=1756556941%2CY2QxNGVjMGMzM2NkZDRiMDhmMWVjY2RjOWUzN2I2MDdiZjI3N2RlNTIzNDI5OTAzZDJlMzk3YmVjYzJhNTE4ZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4kcht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Deepseek just won the best paper award at ACL 2025 with a breakthrough innovation in long context, a model using this might come soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdn6dp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 400,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 400,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753921424,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2502.11089",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdn6dp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charuru",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdn6dp/deepseek_just_won_the_best_paper_award_at_acl/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2502.11089",
          "subreddit_subscribers": 507576,
          "created_utc": 1753921424,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The Cogito v2 LLMs are instruction tuned generative models. All models are released under an open license for commercial use.\n\n* Cogito v2 models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).\n* The LLMs are trained using **Iterated Distillation and Amplification (IDA)** \\- an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.\n* The models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts.\n   * In both standard and reasoning modes, Cogito v2-preview models outperform their size equivalent counterparts on common industry benchmarks.\n* This model is trained in over 30 languages and supports a context length of 128k.\n\n[https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B)\n\n[https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE)\n\n[https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B)\n\n[https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "cogito v2 preview models released 70B/109B/405B/671B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdv67j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753947057,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Cogito v2 LLMs are instruction tuned generative models. All models are released under an open license for commercial use.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cogito v2 models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).&lt;/li&gt;\n&lt;li&gt;The LLMs are trained using &lt;strong&gt;Iterated Distillation and Amplification (IDA)&lt;/strong&gt; - an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.&lt;/li&gt;\n&lt;li&gt;The models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts.\n\n&lt;ul&gt;\n&lt;li&gt;In both standard and reasoning modes, Cogito v2-preview models outperform their size equivalent counterparts on common industry benchmarks.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;This model is trained in over 30 languages and supports a context length of 128k.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7dnFXllcXlnatOfqO_F3iSqS3FlJPQP-Q1pGksTJzbw.png?auto=webp&amp;s=7b818f7adc0d98be40731f482264a837c5867cdb",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7dnFXllcXlnatOfqO_F3iSqS3FlJPQP-Q1pGksTJzbw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2055e09c12c8dcc4a48b580d498877c964511989",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/7dnFXllcXlnatOfqO_F3iSqS3FlJPQP-Q1pGksTJzbw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f79151ab74562809d440b5508d280e061ae0946b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/7dnFXllcXlnatOfqO_F3iSqS3FlJPQP-Q1pGksTJzbw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6c65488556d2975946913d69a6778dcb8ba23ec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/7dnFXllcXlnatOfqO_F3iSqS3FlJPQP-Q1pGksTJzbw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eea3eb081fb9d9eceb6ab28bafbeec270cef16c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/7dnFXllcXlnatOfqO_F3iSqS3FlJPQP-Q1pGksTJzbw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=79faf675b22855f2a89c2569eb9627da7c0850ba",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/7dnFXllcXlnatOfqO_F3iSqS3FlJPQP-Q1pGksTJzbw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81069326219d1e0b03f90e120e47778dbb96b482",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "7dnFXllcXlnatOfqO_F3iSqS3FlJPQP-Q1pGksTJzbw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mdv67j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mdv67j/cogito_v2_preview_models_released_70b109b405b671b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdv67j/cogito_v2_preview_models_released_70b109b405b671b/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753947057,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Zuck has posted a video and a longer letter about the superintelligence plans at Meta. In the letter he says:\n\n\"That said, superintelligence will raise novel safety concerns. We'll need to be rigorous about mitigating these risks and careful about what we choose to open source.\"\n\n[https://www.meta.com/superintelligence/](https://www.meta.com/superintelligence/)\n\nThat means that Meta will not open source the best they have. But it is inevitable that others will release their best models and agents, meaning that Meta has committed itself to oblivion, not only in open source but in proprietary too, as they are not a major player in that space. The ASI they will get to will be for use in their products only.",
          "author_fullname": "t2_1pr7hwh6t5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bye bye, Meta AI, it was good while it lasted.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md6t2h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1279,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1279,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753882611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Zuck has posted a video and a longer letter about the superintelligence plans at Meta. In the letter he says:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;That said, superintelligence will raise novel safety concerns. We&amp;#39;ll need to be rigorous about mitigating these risks and careful about what we choose to open source.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.meta.com/superintelligence/\"&gt;https://www.meta.com/superintelligence/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;That means that Meta will not open source the best they have. But it is inevitable that others will release their best models and agents, meaning that Meta has committed itself to oblivion, not only in open source but in proprietary too, as they are not a major player in that space. The ASI they will get to will be for use in their products only.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1md6t2h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "absolooot1",
          "discussion_type": null,
          "num_comments": 404,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753882611,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/papers/2507.22448](https://huggingface.co/papers/2507.22448)\n\nThe hybrid transformer-mamba models series, covering 0.5B, 1.5B, 1.5B-Deep, 3B, 7B and 34B.   \n  \nThis 80+ page report dives deep into the key design decisions behind Falcon-H1 - from architectural innovations and data strategies to training recipes that challenge conventional practices in LLM development 🔥\n\nCurrent framework support includes Hugging Face, vLLM, llama.cpp, Llama-Factory, Axolotl, OUMI, SkyPilot, etc. — with more on the way!\n\nhttps://preview.redd.it/vog1eu4gd6gf1.png?width=1708&amp;format=png&amp;auto=webp&amp;s=80753458ee6e8869540d1c75a0599c9a2aae9dad",
          "author_fullname": "t2_1ktl4wkk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Falcon-H1 technical report release",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vog1eu4gd6gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/vog1eu4gd6gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ca34bf918efa2cb49b50aca605506703b8d1a55"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/vog1eu4gd6gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=062ccd2d13db98ada785b134c6a18c7fafb58443"
                },
                {
                  "y": 148,
                  "x": 320,
                  "u": "https://preview.redd.it/vog1eu4gd6gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e46cecf04020e46e0270ddeaa3a45a71afd3be3c"
                },
                {
                  "y": 297,
                  "x": 640,
                  "u": "https://preview.redd.it/vog1eu4gd6gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a1cf10e09eaad4cd440e363978ae4634060980e9"
                },
                {
                  "y": 446,
                  "x": 960,
                  "u": "https://preview.redd.it/vog1eu4gd6gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=680cd6c57e67c59f710dfee66af0cf3dec0aba6d"
                },
                {
                  "y": 502,
                  "x": 1080,
                  "u": "https://preview.redd.it/vog1eu4gd6gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5e8ba30135fbfa9faf87d12df9b08bec0a5cf69b"
                }
              ],
              "s": {
                "y": 794,
                "x": 1708,
                "u": "https://preview.redd.it/vog1eu4gd6gf1.png?width=1708&amp;format=png&amp;auto=webp&amp;s=80753458ee6e8869540d1c75a0599c9a2aae9dad"
              },
              "id": "vog1eu4gd6gf1"
            }
          },
          "name": "t3_1mdwmju",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=382056242aa6a412c0f4a006eaf01c514d1388ad",
          "edited": 1753953342,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753952728,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/papers/2507.22448\"&gt;https://huggingface.co/papers/2507.22448&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The hybrid transformer-mamba models series, covering 0.5B, 1.5B, 1.5B-Deep, 3B, 7B and 34B.   &lt;/p&gt;\n\n&lt;p&gt;This 80+ page report dives deep into the key design decisions behind Falcon-H1 - from architectural innovations and data strategies to training recipes that challenge conventional practices in LLM development 🔥&lt;/p&gt;\n\n&lt;p&gt;Current framework support includes Hugging Face, vLLM, llama.cpp, Llama-Factory, Axolotl, OUMI, SkyPilot, etc. — with more on the way!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vog1eu4gd6gf1.png?width=1708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80753458ee6e8869540d1c75a0599c9a2aae9dad\"&gt;https://preview.redd.it/vog1eu4gd6gf1.png?width=1708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80753458ee6e8869540d1c75a0599c9a2aae9dad&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?auto=webp&amp;s=0d235e866fdfaab01a2374baae1d2fca9c0f399d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5160807d254f5c616e61b4d003b92b90330ec05c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=245e25182634e949ed6cfaea317f039922233b67",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1197f5f207e31c91d4eabe638aa229c8b8124a5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb38d3cc3bcfa71dbf16f5c930e570f21c13b829",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9104b158f02ec1c9573384b43d10b269be9b85ea",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1cc2dedbd7ee134181fe2b5c0ebdcec1c22ab6ec",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdwmju",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JingweiZUO",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdwmju/falconh1_technical_report_release/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdwmju/falconh1_technical_report_release/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753952728,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "All models are from Unsloth UD Q4_K_XL except for Gemma3-27B is IQ3. Running all these with 10-12k context with 4-30 t/s across all models.\n\nMost used ones are Mistral-24B, Gemma3-27B, and Granite3.3-2B. Mistral and Gemma are for general QA and random text tools. Granite is for article summaries and random small RAG related tasks. Qwen3-30B (new one) is for coding related tasks, and Gemma3-12B is for vision strictly.\n\nGemma3n-2B is essentially hooked to Siri via shortcuts and acts as an enhanced Siri.\n\nMedgemma is for anything medical and it’s wonderful for any general advice and reading of x-rays or medical reports.\n\nMy humble mini PC runs all these on Llama.cpp with iGPU 48GB shared memory RAM and Vulkan backend. It runs Mistral at 4t/s with 6k context (set to max of 10k window). Gemme3-27B runs at 5t/s, and Qwen3-30B-A3B at 20-22t/s.\n\nI fall back to ChatGPT once or twice a week when i need a super quick answer or something too in depth.\n\nWhat is your curated list?\n",
          "author_fullname": "t2_vbzgnic",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "After 6 months of fiddling with local AI. Here’s my curated models list that work for 90% of my needs. What’s yours?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdjb67",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 246,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 246,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/4rbeT3cEiSQtXkmeVnVyBuJsOpThkSCY2eLJ1imjBO8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753911487,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All models are from Unsloth UD Q4_K_XL except for Gemma3-27B is IQ3. Running all these with 10-12k context with 4-30 t/s across all models.&lt;/p&gt;\n\n&lt;p&gt;Most used ones are Mistral-24B, Gemma3-27B, and Granite3.3-2B. Mistral and Gemma are for general QA and random text tools. Granite is for article summaries and random small RAG related tasks. Qwen3-30B (new one) is for coding related tasks, and Gemma3-12B is for vision strictly.&lt;/p&gt;\n\n&lt;p&gt;Gemma3n-2B is essentially hooked to Siri via shortcuts and acts as an enhanced Siri.&lt;/p&gt;\n\n&lt;p&gt;Medgemma is for anything medical and it’s wonderful for any general advice and reading of x-rays or medical reports.&lt;/p&gt;\n\n&lt;p&gt;My humble mini PC runs all these on Llama.cpp with iGPU 48GB shared memory RAM and Vulkan backend. It runs Mistral at 4t/s with 6k context (set to max of 10k window). Gemme3-27B runs at 5t/s, and Qwen3-30B-A3B at 20-22t/s.&lt;/p&gt;\n\n&lt;p&gt;I fall back to ChatGPT once or twice a week when i need a super quick answer or something too in depth.&lt;/p&gt;\n\n&lt;p&gt;What is your curated list?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/jzljyi4tw2gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/jzljyi4tw2gf1.jpeg?auto=webp&amp;s=3a79f660063272187cc80e2261fb599320149df7",
                  "width": 1171,
                  "height": 1183
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/jzljyi4tw2gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=85504c1d503f59db68dd29902ebe53c3ae9805bf",
                    "width": 108,
                    "height": 109
                  },
                  {
                    "url": "https://preview.redd.it/jzljyi4tw2gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab4ee28ccd5f0094b7df16a047977c70eb15f3f0",
                    "width": 216,
                    "height": 218
                  },
                  {
                    "url": "https://preview.redd.it/jzljyi4tw2gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fee03d498fe5468dd129ce289e46541ee313266f",
                    "width": 320,
                    "height": 323
                  },
                  {
                    "url": "https://preview.redd.it/jzljyi4tw2gf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=387253ef4ef3e3a18ba79c1be71339080caaaf1c",
                    "width": 640,
                    "height": 646
                  },
                  {
                    "url": "https://preview.redd.it/jzljyi4tw2gf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=154a847ff50bd149417d1293f794de877260c0b0",
                    "width": 960,
                    "height": 969
                  },
                  {
                    "url": "https://preview.redd.it/jzljyi4tw2gf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=532a1a87298a85370fcd08ebf0a914e3f1af993c",
                    "width": 1080,
                    "height": 1091
                  }
                ],
                "variants": {},
                "id": "_bDOYeXRv8-6aZM9HJicur91RTVmLbLvtthLvcY-o_I"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdjb67",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "simracerman",
          "discussion_type": null,
          "num_comments": 99,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/",
          "stickied": false,
          "url": "https://i.redd.it/jzljyi4tw2gf1.jpeg",
          "subreddit_subscribers": 507576,
          "created_utc": 1753911487,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aq4j0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "rednote-hilab/dots.ocr - Multilingual document layout parsing in a single vision-language model achieving SOTA performance despite compact 1.7B LLM foundation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdwngf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ac93265c9379bfbf707f3dc3c8663ec4b92f5a3c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753952828,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/rednote-hilab/dots.ocr",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?auto=webp&amp;s=83e0fd1aa924b9918306c02a99cedb9bbb2eb1cb",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014ce09ab614e86be0bda115d3ee826dd4c7e72b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fb10f0400ab7291afbb905ab3dfdfb49e477ed8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88b160d056e65a5fdd1da13d608db9a9c123e2d7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54fe70e1d1e50ac63262c7c7180e0173f9cc1673",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa0d50402090bd3ad6e9c270f0f950421a2c1523",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dff2cd4e982c9356a88ce61af693c4ca57815b99",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mdwngf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nullmove",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdwngf/rednotehilabdotsocr_multilingual_document_layout/",
          "stickied": false,
          "url": "https://huggingface.co/rednote-hilab/dots.ocr",
          "subreddit_subscribers": 507576,
          "created_utc": 1753952828,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://eqbench.com/](https://eqbench.com/)\n\nCreative Writing Samples: [https://eqbench.com/results/creative-writing-v3/openrouter\\_\\_horizon-alpha.html](https://eqbench.com/results/creative-writing-v3/openrouter__horizon-alpha.html)\n\nLongform Writing Samples: [https://eqbench.com/results/creative-writing-longform/openrouter\\_\\_horizon-alpha\\_longform\\_report.html](https://eqbench.com/results/creative-writing-longform/openrouter__horizon-alpha_longform_report.html)\n\nEQ-Bench Samples: [https://eqbench.com/results/eqbench3\\_reports/openrouter\\_\\_horizon-alpha.html](https://eqbench.com/results/eqbench3_reports/openrouter__horizon-alpha.html)",
          "author_fullname": "t2_pp9qh5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Horizon-alpha: A new stealthed model on openrouter sweeps EQ-Bench leaderboards",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 109,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "0hjgl87da4gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/0hjgl87da4gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b1698371a575e067e538216fdfa5682c4a8a4a3"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/0hjgl87da4gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2c2a7b3723a8be0877b5cbacca51f76a92fb88c"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/0hjgl87da4gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e252f2bc49314097aee9f41c09a61b936789dea1"
                }
              ],
              "s": {
                "y": 1559,
                "x": 500,
                "u": "https://preview.redd.it/0hjgl87da4gf1.png?width=500&amp;format=png&amp;auto=webp&amp;s=a0dfa2e78fd558efcb69b8d7b5035292dae09b5b"
              },
              "id": "0hjgl87da4gf1"
            },
            "97jmcuhda4gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/97jmcuhda4gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2c1e86a37edc143cdfd50ea87f7186be4a19cd3"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/97jmcuhda4gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ffc067f53a62cb4b6a8a79d6f98225a8700378f8"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/97jmcuhda4gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fbe11b123fd181cc7e877f5eaa2aa2985abb703d"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/97jmcuhda4gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f6f7642b1fc3478e375d3e8c802d2ca90a5132a"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/97jmcuhda4gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6797f0fa90c63574d109155b175b3ba75b68e73a"
                },
                {
                  "y": 1200,
                  "x": 1080,
                  "u": "https://preview.redd.it/97jmcuhda4gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a05a1d5900c03ab250ea900b75d24734583e9e5"
                }
              ],
              "s": {
                "y": 1759,
                "x": 1582,
                "u": "https://preview.redd.it/97jmcuhda4gf1.png?width=1582&amp;format=png&amp;auto=webp&amp;s=f126b3fa7844706b630eec7865f7fb70d1bf1409"
              },
              "id": "97jmcuhda4gf1"
            },
            "h6vp95gba4gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 100,
                  "x": 108,
                  "u": "https://preview.redd.it/h6vp95gba4gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=53bb0b225fd63e3d15282b854d21248977a1ddf6"
                },
                {
                  "y": 200,
                  "x": 216,
                  "u": "https://preview.redd.it/h6vp95gba4gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=572645efe6e89d8b1f1d40a5e945e17ef288e1af"
                },
                {
                  "y": 297,
                  "x": 320,
                  "u": "https://preview.redd.it/h6vp95gba4gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d31be02df86efe99aa0a92b561fd40824e2e4d5"
                },
                {
                  "y": 595,
                  "x": 640,
                  "u": "https://preview.redd.it/h6vp95gba4gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84d4fc5782bd390a6cdb1859fd6aaaafad2ee381"
                },
                {
                  "y": 892,
                  "x": 960,
                  "u": "https://preview.redd.it/h6vp95gba4gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1ffc9199ef0d440c21352701368ea1a34dc58521"
                },
                {
                  "y": 1004,
                  "x": 1080,
                  "u": "https://preview.redd.it/h6vp95gba4gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a9b8caababd907944f0aa2234b55d45049f9ddf"
                }
              ],
              "s": {
                "y": 1488,
                "x": 1600,
                "u": "https://preview.redd.it/h6vp95gba4gf1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=d3befcf565a0242576fda7272a6035e30b0acaa7"
              },
              "id": "h6vp95gba4gf1"
            },
            "lnsnzumaa4gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 84,
                  "x": 108,
                  "u": "https://preview.redd.it/lnsnzumaa4gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87bd182b166b9a3e8a17a6fb15e4375772923e47"
                },
                {
                  "y": 168,
                  "x": 216,
                  "u": "https://preview.redd.it/lnsnzumaa4gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d63b0fc847b78b67d05935118dd4a72711a79a7c"
                },
                {
                  "y": 249,
                  "x": 320,
                  "u": "https://preview.redd.it/lnsnzumaa4gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f92d32e59872a66fabe9edbf3cf41a52f5ed0253"
                },
                {
                  "y": 499,
                  "x": 640,
                  "u": "https://preview.redd.it/lnsnzumaa4gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c2f47f092279c4db134b70a9a7abc1dfd4f0e5c"
                },
                {
                  "y": 749,
                  "x": 960,
                  "u": "https://preview.redd.it/lnsnzumaa4gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=84dc29dea2ffe79a8a2544d42a38655f1c4251ea"
                },
                {
                  "y": 843,
                  "x": 1080,
                  "u": "https://preview.redd.it/lnsnzumaa4gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af54f3cafe7ab477c037ba69c83338d0c1058229"
                }
              ],
              "s": {
                "y": 1250,
                "x": 1601,
                "u": "https://preview.redd.it/lnsnzumaa4gf1.png?width=1601&amp;format=png&amp;auto=webp&amp;s=063aacd81cd83b4c2eebf8ec30cbbbe236572b9f"
              },
              "id": "lnsnzumaa4gf1"
            },
            "1wgsqo2ba4gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/1wgsqo2ba4gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a24a34dbcb3fc6cf066e5b9f279d8478126b452f"
                },
                {
                  "y": 140,
                  "x": 216,
                  "u": "https://preview.redd.it/1wgsqo2ba4gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b50d604893c06b435f61b094416ba5657f518102"
                },
                {
                  "y": 208,
                  "x": 320,
                  "u": "https://preview.redd.it/1wgsqo2ba4gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c25ca12429f37e76e95befadf47d29c4bc8d0a6f"
                },
                {
                  "y": 416,
                  "x": 640,
                  "u": "https://preview.redd.it/1wgsqo2ba4gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a341f6567c86d5e2683c49bbc2b95f379172f8ad"
                },
                {
                  "y": 624,
                  "x": 960,
                  "u": "https://preview.redd.it/1wgsqo2ba4gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ebe3537a5e7507b1c4099a5fa9908ba0007db0de"
                },
                {
                  "y": 703,
                  "x": 1080,
                  "u": "https://preview.redd.it/1wgsqo2ba4gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa5e17e1846195520912344742213193be8923f6"
                }
              ],
              "s": {
                "y": 1039,
                "x": 1596,
                "u": "https://preview.redd.it/1wgsqo2ba4gf1.png?width=1596&amp;format=png&amp;auto=webp&amp;s=c0a68412e78392ac0f6f65771c6df77e5595da3c"
              },
              "id": "1wgsqo2ba4gf1"
            },
            "19k8r2sda4gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 134,
                  "x": 108,
                  "u": "https://preview.redd.it/19k8r2sda4gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80c3e29962572aa2aebff6b09b69d12dbb4e1c8c"
                },
                {
                  "y": 268,
                  "x": 216,
                  "u": "https://preview.redd.it/19k8r2sda4gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=600f7c5a9acef670deeaaa73c09e57d76c007f0c"
                },
                {
                  "y": 397,
                  "x": 320,
                  "u": "https://preview.redd.it/19k8r2sda4gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89912b71c7633edeb745c3873732372745a32b89"
                },
                {
                  "y": 795,
                  "x": 640,
                  "u": "https://preview.redd.it/19k8r2sda4gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b92da988e454713d5eac4b86576c8e43e578987"
                },
                {
                  "y": 1193,
                  "x": 960,
                  "u": "https://preview.redd.it/19k8r2sda4gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9012253e966cbd987efb892bdbff712a7ef1e2dc"
                }
              ],
              "s": {
                "y": 1230,
                "x": 989,
                "u": "https://preview.redd.it/19k8r2sda4gf1.png?width=989&amp;format=png&amp;auto=webp&amp;s=14ab62c1128148e873135cd8e7e017517cf8ddaa"
              },
              "id": "19k8r2sda4gf1"
            }
          },
          "name": "t3_1mdpe8v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "transparent",
          "ups": 87,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "lnsnzumaa4gf1",
                "id": 717800299
              },
              {
                "media_id": "1wgsqo2ba4gf1",
                "id": 717800300
              },
              {
                "media_id": "h6vp95gba4gf1",
                "id": 717800301
              },
              {
                "media_id": "0hjgl87da4gf1",
                "id": 717800302
              },
              {
                "media_id": "97jmcuhda4gf1",
                "id": 717800303
              },
              {
                "media_id": "19k8r2sda4gf1",
                "id": 717800304
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aYYIospZdzzPcpq5Dkfv_OnJ4m1Tv1B_fMeERdxNnXQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":Llama:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/23w2nhjj1e9f1_t5_81eyvm/Llama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753927754,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://eqbench.com/\"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Creative Writing Samples: &lt;a href=\"https://eqbench.com/results/creative-writing-v3/openrouter__horizon-alpha.html\"&gt;https://eqbench.com/results/creative-writing-v3/openrouter__horizon-alpha.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Longform Writing Samples: &lt;a href=\"https://eqbench.com/results/creative-writing-longform/openrouter__horizon-alpha_longform_report.html\"&gt;https://eqbench.com/results/creative-writing-longform/openrouter__horizon-alpha_longform_report.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;EQ-Bench Samples: &lt;a href=\"https://eqbench.com/results/eqbench3_reports/openrouter__horizon-alpha.html\"&gt;https://eqbench.com/results/eqbench3_reports/openrouter__horizon-alpha.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mdpe8v",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":Llama:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mdpe8v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_sqrkl",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mdpe8v/horizonalpha_a_new_stealthed_model_on_openrouter/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mdpe8v",
          "subreddit_subscribers": 507576,
          "created_utc": 1753927754,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " We're in the era now where open source releases are nipping at the heels of closed-source models in benchmarks. But it's all in text modality. \n\nAs far as I can tell, there hasn't been a really solid contender when it comes to both being a SOTA model, and also having native audio/image/video input and image/audio output which has been demonstrated by OpenAI and Google.\n\nI feel like this is a really big deal that is mostly overlooked when comparing open source to closed source. Programming benchmarks are cool and all, but for a truly useful assistant, you need a model you can speak to, show stuff to, and it can speak back and generate images to show you stuff as well.",
          "author_fullname": "t2_66km3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why is open source so behind on multi-modalitty?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdruc9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753935061,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re in the era now where open source releases are nipping at the heels of closed-source models in benchmarks. But it&amp;#39;s all in text modality. &lt;/p&gt;\n\n&lt;p&gt;As far as I can tell, there hasn&amp;#39;t been a really solid contender when it comes to both being a SOTA model, and also having native audio/image/video input and image/audio output which has been demonstrated by OpenAI and Google.&lt;/p&gt;\n\n&lt;p&gt;I feel like this is a really big deal that is mostly overlooked when comparing open source to closed source. Programming benchmarks are cool and all, but for a truly useful assistant, you need a model you can speak to, show stuff to, and it can speak back and generate images to show you stuff as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdruc9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AnticitizenPrime",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753935061,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 Coder 30B-A3B tomorrow!!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md93bj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 512,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 512,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/K9Nx9k0yfNo72Su3RE5muDzCVrLNRn61GBRiLFKFFlA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753888106,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/zv92612t11gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/zv92612t11gf1.png?auto=webp&amp;s=2e69f2943ffedc6058b01531a0b5f5b904fafd93",
                  "width": 1220,
                  "height": 1930
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/zv92612t11gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ccf72bfa2613f3f4323503ca258299edae1698d8",
                    "width": 108,
                    "height": 170
                  },
                  {
                    "url": "https://preview.redd.it/zv92612t11gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aefde73c9c87f9c3ee96e4775270e592bc63ffa2",
                    "width": 216,
                    "height": 341
                  },
                  {
                    "url": "https://preview.redd.it/zv92612t11gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=46854036774a921e80cbd749c0d46d3f65aa9331",
                    "width": 320,
                    "height": 506
                  },
                  {
                    "url": "https://preview.redd.it/zv92612t11gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=45b98263f660ff1bebd4634907371461fd4e0207",
                    "width": 640,
                    "height": 1012
                  },
                  {
                    "url": "https://preview.redd.it/zv92612t11gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1655dfcc3acdbda1edf7e8c2bf52e4f818053f7c",
                    "width": 960,
                    "height": 1518
                  },
                  {
                    "url": "https://preview.redd.it/zv92612t11gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=82106ce904e641b1635835e5273e833ddcf3bec8",
                    "width": 1080,
                    "height": 1708
                  }
                ],
                "variants": {},
                "id": "_P987MccCP9zB7Niv68pkAsjdEVBNJKGFyGu7MefRFU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1md93bj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/",
          "stickied": false,
          "url": "https://i.redd.it/zv92612t11gf1.png",
          "subreddit_subscribers": 507576,
          "created_utc": 1753888106,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello. It has been an awesomely-busy week for all of us here, trying out the new goodies that dropped by Qwen and others. Wow, this week will be hard to match, good times!\n\nLike most here, I ended up trying a bunch of models in bunch of quants plus mlx.\n\nI have to say, the model that completely blew my mind was glm-4.5-air, the 4-bit mlx. I plugged it into my assistant (that does chains of tools, plus connected to a project management app, plus to a notebook), and it immediately figured out how to use those.\n\nIt really likes to dig through tasks, priorities, notes, online research - to the point when I am worried it's going to do it too much and loose track of things - but amazingly enough, it doesn't loose track of things and comes back with in-depth, good analysis and responses.\n\nThe model is also fast - kind of reminds me of Owen 30b a3b, although of course it punches well above that one due to its larger size.\n\nIf you can fit the 4-bit version onto your machine, absolutely, give this model a try. It is now my new daily driver, replacing Qwen 32B (until the new Qwen 32B comes out later this week? lol)\n\nedit: I am not associated with the gml team (I wish I was!)",
          "author_fullname": "t2_ajuxt3cr4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "glm-4.5-Air appreciation poist - if you have not done so already, give this model a try",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdhfhs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 187,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 187,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753907041,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. It has been an awesomely-busy week for all of us here, trying out the new goodies that dropped by Qwen and others. Wow, this week will be hard to match, good times!&lt;/p&gt;\n\n&lt;p&gt;Like most here, I ended up trying a bunch of models in bunch of quants plus mlx.&lt;/p&gt;\n\n&lt;p&gt;I have to say, the model that completely blew my mind was glm-4.5-air, the 4-bit mlx. I plugged it into my assistant (that does chains of tools, plus connected to a project management app, plus to a notebook), and it immediately figured out how to use those.&lt;/p&gt;\n\n&lt;p&gt;It really likes to dig through tasks, priorities, notes, online research - to the point when I am worried it&amp;#39;s going to do it too much and loose track of things - but amazingly enough, it doesn&amp;#39;t loose track of things and comes back with in-depth, good analysis and responses.&lt;/p&gt;\n\n&lt;p&gt;The model is also fast - kind of reminds me of Owen 30b a3b, although of course it punches well above that one due to its larger size.&lt;/p&gt;\n\n&lt;p&gt;If you can fit the 4-bit version onto your machine, absolutely, give this model a try. It is now my new daily driver, replacing Qwen 32B (until the new Qwen 32B comes out later this week? lol)&lt;/p&gt;\n\n&lt;p&gt;edit: I am not associated with the gml team (I wish I was!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdhfhs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Southern_Sun_2106",
          "discussion_type": null,
          "num_comments": 81,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdhfhs/glm45air_appreciation_poist_if_you_have_not_done/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdhfhs/glm45air_appreciation_poist_if_you_have_not_done/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753907041,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "They keep putting different reference models in their graphs and we have to look at many graphs to see where we're at so I used AI to put them all in a single table. \n\nIf any of you find errors, I'll delete this post.",
          "author_fullname": "t2_cy3wb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Made a unified table of benchmarks using AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 118,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdpfm8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 64,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/lMpBTfZOCCGirkVYxjC83Mp6Jt56ORIzT82IDPyjBW0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753927864,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They keep putting different reference models in their graphs and we have to look at many graphs to see where we&amp;#39;re at so I used AI to put them all in a single table. &lt;/p&gt;\n\n&lt;p&gt;If any of you find errors, I&amp;#39;ll delete this post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/gxir7usrb4gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/gxir7usrb4gf1.png?auto=webp&amp;s=61d06e6324953072a8e16fa1d5e68e0847991400",
                  "width": 2023,
                  "height": 1716
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/gxir7usrb4gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f70288958d623f870f4e10912a44bb9b39cc5409",
                    "width": 108,
                    "height": 91
                  },
                  {
                    "url": "https://preview.redd.it/gxir7usrb4gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f86fe1853b5b4d1c7fdb4253b217201744ef1f2",
                    "width": 216,
                    "height": 183
                  },
                  {
                    "url": "https://preview.redd.it/gxir7usrb4gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=af9809f575fc23494e257bb06b18946a85ae2322",
                    "width": 320,
                    "height": 271
                  },
                  {
                    "url": "https://preview.redd.it/gxir7usrb4gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7928c2d81afe832554b8bc632895b2c84d73b271",
                    "width": 640,
                    "height": 542
                  },
                  {
                    "url": "https://preview.redd.it/gxir7usrb4gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc782f8d6d1287180ae6f4052bb23ebba3585d0e",
                    "width": 960,
                    "height": 814
                  },
                  {
                    "url": "https://preview.redd.it/gxir7usrb4gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=249a775adb66b6a59c26eca8edd6d0324ff3457c",
                    "width": 1080,
                    "height": 916
                  }
                ],
                "variants": {},
                "id": "PZRlYen2-ELD9AIBlp1RXYOXWkNmox6ICN4EBThyAPs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mdpfm8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DrVonSinistro",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdpfm8/made_a_unified_table_of_benchmarks_using_ai/",
          "stickied": false,
          "url": "https://i.redd.it/gxir7usrb4gf1.png",
          "subreddit_subscribers": 507576,
          "created_utc": 1753927864,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Download on ollama.com/download\n\nor GitHub releases\n\nhttps://github.com/ollama/ollama/releases/tag/v0.10.0\n\nBlog post: [Ollama's new app](https://ollama.com/blog/new-app)",
          "author_fullname": "t2_39i1zb05",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama’s new app — Ollama 0.10 is here for macOS and Windows!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 101,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdvhxg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/y0zUJBMQNENQ_tiiNf9ET2MZBkbKPyUisQSSdMhLeN0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753948360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Download on ollama.com/download&lt;/p&gt;\n\n&lt;p&gt;or GitHub releases&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ollama/ollama/releases/tag/v0.10.0\"&gt;https://github.com/ollama/ollama/releases/tag/v0.10.0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Blog post: &lt;a href=\"https://ollama.com/blog/new-app\"&gt;Ollama&amp;#39;s new app&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/9wfl7u6z06gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/9wfl7u6z06gf1.jpeg?auto=webp&amp;s=14c75a6382af6bf6af7ad2f3eee5a684499cf67f",
                  "width": 1616,
                  "height": 1175
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/9wfl7u6z06gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd49534e752553e996786ccf873670e3e86ffda7",
                    "width": 108,
                    "height": 78
                  },
                  {
                    "url": "https://preview.redd.it/9wfl7u6z06gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7caaad3d484d5e59041ed91f0009cf9537860fda",
                    "width": 216,
                    "height": 157
                  },
                  {
                    "url": "https://preview.redd.it/9wfl7u6z06gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2875fbf74d7f187cf9eef62f158076768d9bf6f4",
                    "width": 320,
                    "height": 232
                  },
                  {
                    "url": "https://preview.redd.it/9wfl7u6z06gf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd44ba69feb6890ee5ba2e203ace6fbc8cf232b3",
                    "width": 640,
                    "height": 465
                  },
                  {
                    "url": "https://preview.redd.it/9wfl7u6z06gf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=36bc619a7ffd9ec2d69c665a2c62b2db5b40f981",
                    "width": 960,
                    "height": 698
                  },
                  {
                    "url": "https://preview.redd.it/9wfl7u6z06gf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49d547be721d4af32551cff9f74de4ca208cf62f",
                    "width": 1080,
                    "height": 785
                  }
                ],
                "variants": {},
                "id": "-YCFasS_tCLK0d5k6IATEYLW294hMdGZ9NP1BS1XEEg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mdvhxg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bllshrfv",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdvhxg/ollamas_new_app_ollama_010_is_here_for_macos_and/",
          "stickied": false,
          "url": "https://i.redd.it/9wfl7u6z06gf1.jpeg",
          "subreddit_subscribers": 507576,
          "created_utc": 1753948360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 Qwen3-30B-A3B-Thinking-2507, a medium-size model that can think!\n\n• Nice performance on reasoning tasks, including math, science, code &amp; beyond\n• Good at tool use, competitive with larger models\n• Native support of 256K-token context, extendable to 1M\n\nHugging Face: https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\n\nModel scope: https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Qwen3-30B-A3B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md8t1g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 451,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 451,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/tVtfQoQDTlQ6Uwov__WCY7dUkYJPJUXsM9RBMIH7y1A.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753887447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 Qwen3-30B-A3B-Thinking-2507, a medium-size model that can think!&lt;/p&gt;\n\n&lt;p&gt;• Nice performance on reasoning tasks, including math, science, code &amp;amp; beyond\n• Good at tool use, competitive with larger models\n• Native support of 256K-token context, extendable to 1M&lt;/p&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Model scope: &lt;a href=\"https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary\"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/eaag1cpuz0gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/eaag1cpuz0gf1.jpeg?auto=webp&amp;s=4d8631bddb808ba5ba33923e39969f3d5ce975a0",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54819af8a9dcb09081d8f071202286c39fa8b783",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ce4a1e6aa0c36ba666c48a92f9aa64aacb1dd4d",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e49c0310ba44d98f4d430bae3d2c168d9186be2",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e073b4b20cd702585ec6bbac8fc80938677c24f8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee1be3a32bb3fc836c4cfc180295aebaea49bac7",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ddef9a3b199d455271a7f4ee7e22b31ed457318",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "zrXiZ6McKvxSbb3fLQK6d19Ut_u3Buzjg1O0DbTod_M"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1md8t1g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 124,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/",
          "stickied": false,
          "url": "https://i.redd.it/eaag1cpuz0gf1.jpeg",
          "subreddit_subscribers": 507576,
          "created_utc": 1753887447,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 We're excited to share our latest research on X-Omni: reinforcement learning makes discrete autoregressive image generative models great again, empowering a practical unified model for both image and language modality generation.\n\nHighlights:\n\n✅ Unified Modeling Approach: A discrete autoregressive model handling image and language modalities.\n\n✅ Superior Instruction Following: Exceptional capability to follow complex instructions.\n\n✅ Superior Text Rendering: Accurately render text in multiple languages, including both English and Chinese.\n\n✅ Arbitrary resolutions: Produces aesthetically pleasing images at arbitrary resolutions.\n\nInsight:\n\n🔍 During the reinforcement learning process, the aesthetic quality of generated images is gradually enhanced, and the ability to adhere to instructions and the capacity to render long texts improve steadily.\n\nPaper: https://arxiv.org/pdf/2507.22058\nGithub: https://github.com/X-Omni-Team/X-Omni\nProject Page: https://x-omni-team.github.io/\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Hunyuan releases X-Omni, a unified discrete autoregressive model for both image and language modalities",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "rauc3hmya7gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/rauc3hmya7gf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db31c12a8fbee6333559636cb9aeb99146e1694a"
                },
                {
                  "y": 220,
                  "x": 216,
                  "u": "https://preview.redd.it/rauc3hmya7gf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d59ead47b55e63b6ee537a2a4ea41eaaa1c6146"
                },
                {
                  "y": 327,
                  "x": 320,
                  "u": "https://preview.redd.it/rauc3hmya7gf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea15a909413ad7956d7752cdd490b390121ea17a"
                }
              ],
              "s": {
                "y": 354,
                "x": 346,
                "u": "https://preview.redd.it/rauc3hmya7gf1.jpg?width=346&amp;format=pjpg&amp;auto=webp&amp;s=696d4c8a651b65d38563d0afec8bd124f0f81654"
              },
              "id": "rauc3hmya7gf1"
            },
            "71rr5nnya7gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 111,
                  "x": 108,
                  "u": "https://preview.redd.it/71rr5nnya7gf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe01c2994c8fb47c1b39cb99f35ba55f7d9dde2e"
                },
                {
                  "y": 222,
                  "x": 216,
                  "u": "https://preview.redd.it/71rr5nnya7gf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=725fe16de4c07d1b19dee1d1f77bbfd8f8ec6542"
                },
                {
                  "y": 329,
                  "x": 320,
                  "u": "https://preview.redd.it/71rr5nnya7gf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=115e93af2ee69d708a88e4aae8dd1f250f62c733"
                }
              ],
              "s": {
                "y": 546,
                "x": 531,
                "u": "https://preview.redd.it/71rr5nnya7gf1.jpg?width=531&amp;format=pjpg&amp;auto=webp&amp;s=44b4ae50521d09e2a0883ff79bb51311b6795262"
              },
              "id": "71rr5nnya7gf1"
            }
          },
          "name": "t3_1mdzu08",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 9,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "71rr5nnya7gf1",
                "id": 718038871
              },
              {
                "media_id": "rauc3hmya7gf1",
                "id": 718038872
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/UWzewWY4cRLtu0QtYjM_H7EidNwT1bopn8L_07vMWc4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753963825,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 We&amp;#39;re excited to share our latest research on X-Omni: reinforcement learning makes discrete autoregressive image generative models great again, empowering a practical unified model for both image and language modality generation.&lt;/p&gt;\n\n&lt;p&gt;Highlights:&lt;/p&gt;\n\n&lt;p&gt;✅ Unified Modeling Approach: A discrete autoregressive model handling image and language modalities.&lt;/p&gt;\n\n&lt;p&gt;✅ Superior Instruction Following: Exceptional capability to follow complex instructions.&lt;/p&gt;\n\n&lt;p&gt;✅ Superior Text Rendering: Accurately render text in multiple languages, including both English and Chinese.&lt;/p&gt;\n\n&lt;p&gt;✅ Arbitrary resolutions: Produces aesthetically pleasing images at arbitrary resolutions.&lt;/p&gt;\n\n&lt;p&gt;Insight:&lt;/p&gt;\n\n&lt;p&gt;🔍 During the reinforcement learning process, the aesthetic quality of generated images is gradually enhanced, and the ability to adhere to instructions and the capacity to render long texts improve steadily.&lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/pdf/2507.22058\"&gt;https://arxiv.org/pdf/2507.22058&lt;/a&gt;\nGithub: &lt;a href=\"https://github.com/X-Omni-Team/X-Omni\"&gt;https://github.com/X-Omni-Team/X-Omni&lt;/a&gt;\nProject Page: &lt;a href=\"https://x-omni-team.github.io/\"&gt;https://x-omni-team.github.io/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mdzu08",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mdzu08",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdzu08/hunyuan_releases_xomni_a_unified_discrete/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mdzu08",
          "subreddit_subscribers": 507576,
          "created_utc": 1753963825,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "On par with qwen3-235b?",
          "author_fullname": "t2_14xb45",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30b-a3b-thinking-2507 This is insane performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md8slx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 450,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 450,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=0cf90a8010053dbf48911257591f71a3d1ddded7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753887417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On par with qwen3-235b?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?auto=webp&amp;s=a67dbb1b6fae4b63d82563a3e65a19938ca062fb",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e994b63235f1f31da964f24b3a55a51498b6935f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7aa24107465ba0cfb16f79135e2c61bc02b91707",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=95124de9bda6db677aaa373721a3aa188cc7f224",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3213439d0e68cbadd20dbb4d235a121e1df48f64",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b13bc1d8de32bb083d7b376a591f00d85d3173aa",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1md8slx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "3oclockam",
          "discussion_type": null,
          "num_comments": 100,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507",
          "subreddit_subscribers": 507576,
          "created_utc": 1753887417,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# How I Got claude-code to Work with a Local LLM (via LM Studio) Using a Custom Proxy\n\nHey everyone,\n\nI wanted to share a little setup I put together. I was trying to run `claude-code` with a locally hosted model, `glm-4.5-air`, through **LM Studio on my Mac**.\n\nI ran into some issues, so I quickly whipped up a proxy server to get it working. Here's the basic breakdown of the components:\n\n1. `claude-code`: The base agent.\n2. `claude-code-router`: You need to configure this to use external (non-Anthropic) APIs.\n3. **My Custom Proxy Server**: This sits in the middle to modify the LLM requests on the fly. (proxy fix tool-use issue on the fly!)\n4. LM studio : to run GLM-4.5-Air model.\n\nThe proxy server is the crucial part of this setup. It intercepts and alters the LLM requests in real-time. For it to work, it had to meet a few key requirements:\n\n* It must handle both **streaming and non-streaming** responses. (claude-code use streamming!)\n* It needs to safely process **UTF-8 characters and byte streams** to prevent issues during streaming.\n* It has to **normalize non-standard tool outputs** into the correct, standardized format.\n* It must maintain a **stable connection** for streaming sessions.\n* It should be **extensible** to support various types of tool outputs in the future.\n\nAnyway, even though I just quickly put this together, it works surprisingly well, so I figured I'd share the idea with you all.\n\nMy Proxy code is here //  \n[https://github.com/ziozzang/llm-toolcall-proxy](https://github.com/ziozzang/llm-toolcall-proxy)",
          "author_fullname": "t2_5409gkc6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "works well!: GLM 4.5 air (MLX) - LM studio (Mac) - Claude code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdqj9g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753932296,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753931031,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;How I Got claude-code to Work with a Local LLM (via LM Studio) Using a Custom Proxy&lt;/h1&gt;\n\n&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I wanted to share a little setup I put together. I was trying to run &lt;code&gt;claude-code&lt;/code&gt; with a locally hosted model, &lt;code&gt;glm-4.5-air&lt;/code&gt;, through &lt;strong&gt;LM Studio on my Mac&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I ran into some issues, so I quickly whipped up a proxy server to get it working. Here&amp;#39;s the basic breakdown of the components:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;code&gt;claude-code&lt;/code&gt;: The base agent.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;claude-code-router&lt;/code&gt;: You need to configure this to use external (non-Anthropic) APIs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;My Custom Proxy Server&lt;/strong&gt;: This sits in the middle to modify the LLM requests on the fly. (proxy fix tool-use issue on the fly!)&lt;/li&gt;\n&lt;li&gt;LM studio : to run GLM-4.5-Air model.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The proxy server is the crucial part of this setup. It intercepts and alters the LLM requests in real-time. For it to work, it had to meet a few key requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It must handle both &lt;strong&gt;streaming and non-streaming&lt;/strong&gt; responses. (claude-code use streamming!)&lt;/li&gt;\n&lt;li&gt;It needs to safely process &lt;strong&gt;UTF-8 characters and byte streams&lt;/strong&gt; to prevent issues during streaming.&lt;/li&gt;\n&lt;li&gt;It has to &lt;strong&gt;normalize non-standard tool outputs&lt;/strong&gt; into the correct, standardized format.&lt;/li&gt;\n&lt;li&gt;It must maintain a &lt;strong&gt;stable connection&lt;/strong&gt; for streaming sessions.&lt;/li&gt;\n&lt;li&gt;It should be &lt;strong&gt;extensible&lt;/strong&gt; to support various types of tool outputs in the future.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyway, even though I just quickly put this together, it works surprisingly well, so I figured I&amp;#39;d share the idea with you all.&lt;/p&gt;\n\n&lt;p&gt;My Proxy code is here //&lt;br/&gt;\n&lt;a href=\"https://github.com/ziozzang/llm-toolcall-proxy\"&gt;https://github.com/ziozzang/llm-toolcall-proxy&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TxoyzwOuovAjomlGsEp03rYjsFxL0tZoKO1Cb4HESDw.png?auto=webp&amp;s=48cf4f8b4cd91a039f48e52ee654b41f931fbae4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TxoyzwOuovAjomlGsEp03rYjsFxL0tZoKO1Cb4HESDw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cef3d87e86e676c2dd97e4c186c290a3a30d01ec",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/TxoyzwOuovAjomlGsEp03rYjsFxL0tZoKO1Cb4HESDw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=414ffa8e4c30a2db5296a7ec3c64e93841ac33b0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/TxoyzwOuovAjomlGsEp03rYjsFxL0tZoKO1Cb4HESDw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=13fd4de47aca251e59ff4657cde1b0d0793f83e2",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/TxoyzwOuovAjomlGsEp03rYjsFxL0tZoKO1Cb4HESDw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8206d3628b7a8ed14b36f011d84164477dd4e9b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/TxoyzwOuovAjomlGsEp03rYjsFxL0tZoKO1Cb4HESDw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c050975982f818e8b8823fe33a53eecb50a92495",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/TxoyzwOuovAjomlGsEp03rYjsFxL0tZoKO1Cb4HESDw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=afc8a54aeefd64a510f70951f6f71f500183f73f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "TxoyzwOuovAjomlGsEp03rYjsFxL0tZoKO1Cb4HESDw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mdqj9g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ziozzang0",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdqj9g/works_well_glm_45_air_mlx_lm_studio_mac_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdqj9g/works_well_glm_45_air_mlx_lm_studio_mac_claude/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753931031,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Big shout out to ikawrakow and his [https://github.com/ikawrakow/ik\\_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp) for making my hardware relevant (and obviously Qwen team!) :)\n\nLooking forward to trying Thinker and Coder versions of this architecture\n\nhttps://preview.redd.it/9xttfh3026gf1.png?width=2216&amp;format=png&amp;auto=webp&amp;s=cc6e39266d0a94beb5dca73650dab93021bb7d32\n\nHardware: AMD Ryzen 9 8945HS(8C/16T, up to 5.2GHz) 64GB DDR5 1TB PCIe4.0 SSD, running in Ubuntu distrobox with Fedora Bluefin as a host. Also have eGPU with RTX 3060 12GB, but it was not used in benchmark.\n\nI tried CPU + CUDA separately - and the prompt processing speed would take a significant hit (many memory trips I guess). I did try to use the \"-ot exps\" trick to ensure correct layer split - but I think it is expected, as this is the cost of offloading.\n\n-fa -rtr -fmoe made prompt processing around 20-25% faster.\n\nModels of this architecture are very snappy in CPU mode, especially on smaller prompts - good feature for daily driver model. With longer contexts, processing speed drops significantly, so will require orchestration / workflows to prevent context from blowing up.\n\nVibes-wise, this model feels strong for something that runs on \"consumer\" hardware at these speeds.\n\n**What was tested:**\n\n1. General conversations - good enough, but to be honest almost every 4B+ model feels like an ok conversationalist - what a time to be alive, no?\n2. Code doc summarization: good. I fed it 16k-30k documents and while the speed was slow, the overall result was decent.\n3. Retrieval: gave it \\~10k tokens worth of logs and asked some questions about data that appeared in the logs - mostly good, but I would not call it laser-good.\n4. Coding + Tool calling in Zed  editor- it is obviously not Sonnet or GPT 4.1, but it really tries! I think with better prompting / fine-tuning it would crack it  - perhaps it's seen different tools during original training.\n\n**Can I squeeze more?:**\n\n1. Better use for GPU?\n2. Try other quants: there was a plethora of quants added in recent weeks - perhaps there is one that will push these numbers a little up.\n3. Try [https://github.com/kvcache-ai/ktransformers](https://github.com/kvcache-ai/ktransformers) \\- they are known for optimized configs to run on RAM + relatively low amount of VRAM - but I failed to make it work locally and didn't find an up-to-date docker image either. I would imagine it's not gonna yield significant improvements, but happy to be proven wrong.\n4. IGPU + Vulcan?\n5. NPU xD\n6. Test full context (or the largest context that does not take eternity to process)\n\nWhat's your experience / recipe for similarly-sized hardware setup?",
          "author_fullname": "t2_9f1c1mb6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ik_llama.cpp and Qwen 3 30B-A3B architecture.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9xttfh3026gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 8,
                  "x": 108,
                  "u": "https://preview.redd.it/9xttfh3026gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64da390049462da724ef33b554e35e5af910f2fc"
                },
                {
                  "y": 17,
                  "x": 216,
                  "u": "https://preview.redd.it/9xttfh3026gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fa51266266dc0617403d5b159742a96b76aeb4e"
                },
                {
                  "y": 26,
                  "x": 320,
                  "u": "https://preview.redd.it/9xttfh3026gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a810e197bb579a53a220784638351bfab4c404e8"
                },
                {
                  "y": 53,
                  "x": 640,
                  "u": "https://preview.redd.it/9xttfh3026gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b383e5850d9f6460d0d78005d33a16b6eb83ca7e"
                },
                {
                  "y": 79,
                  "x": 960,
                  "u": "https://preview.redd.it/9xttfh3026gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cad82ae247fac266e089462ff653d2dbe54aad89"
                },
                {
                  "y": 89,
                  "x": 1080,
                  "u": "https://preview.redd.it/9xttfh3026gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b0837b8dab1247af048324a0e4443983bc57010"
                }
              ],
              "s": {
                "y": 184,
                "x": 2216,
                "u": "https://preview.redd.it/9xttfh3026gf1.png?width=2216&amp;format=png&amp;auto=webp&amp;s=cc6e39266d0a94beb5dca73650dab93021bb7d32"
              },
              "id": "9xttfh3026gf1"
            }
          },
          "name": "t3_1mdvkhz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4979726165f841523ca44a3f838520e194c3a3f3",
          "edited": 1753949561,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753948642,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Big shout out to ikawrakow and his &lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt; for making my hardware relevant (and obviously Qwen team!) :)&lt;/p&gt;\n\n&lt;p&gt;Looking forward to trying Thinker and Coder versions of this architecture&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9xttfh3026gf1.png?width=2216&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc6e39266d0a94beb5dca73650dab93021bb7d32\"&gt;https://preview.redd.it/9xttfh3026gf1.png?width=2216&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc6e39266d0a94beb5dca73650dab93021bb7d32&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hardware: AMD Ryzen 9 8945HS(8C/16T, up to 5.2GHz) 64GB DDR5 1TB PCIe4.0 SSD, running in Ubuntu distrobox with Fedora Bluefin as a host. Also have eGPU with RTX 3060 12GB, but it was not used in benchmark.&lt;/p&gt;\n\n&lt;p&gt;I tried CPU + CUDA separately - and the prompt processing speed would take a significant hit (many memory trips I guess). I did try to use the &amp;quot;-ot exps&amp;quot; trick to ensure correct layer split - but I think it is expected, as this is the cost of offloading.&lt;/p&gt;\n\n&lt;p&gt;-fa -rtr -fmoe made prompt processing around 20-25% faster.&lt;/p&gt;\n\n&lt;p&gt;Models of this architecture are very snappy in CPU mode, especially on smaller prompts - good feature for daily driver model. With longer contexts, processing speed drops significantly, so will require orchestration / workflows to prevent context from blowing up.&lt;/p&gt;\n\n&lt;p&gt;Vibes-wise, this model feels strong for something that runs on &amp;quot;consumer&amp;quot; hardware at these speeds.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What was tested:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;General conversations - good enough, but to be honest almost every 4B+ model feels like an ok conversationalist - what a time to be alive, no?&lt;/li&gt;\n&lt;li&gt;Code doc summarization: good. I fed it 16k-30k documents and while the speed was slow, the overall result was decent.&lt;/li&gt;\n&lt;li&gt;Retrieval: gave it ~10k tokens worth of logs and asked some questions about data that appeared in the logs - mostly good, but I would not call it laser-good.&lt;/li&gt;\n&lt;li&gt;Coding + Tool calling in Zed  editor- it is obviously not Sonnet or GPT 4.1, but it really tries! I think with better prompting / fine-tuning it would crack it  - perhaps it&amp;#39;s seen different tools during original training.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Can I squeeze more?:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Better use for GPU?&lt;/li&gt;\n&lt;li&gt;Try other quants: there was a plethora of quants added in recent weeks - perhaps there is one that will push these numbers a little up.&lt;/li&gt;\n&lt;li&gt;Try &lt;a href=\"https://github.com/kvcache-ai/ktransformers\"&gt;https://github.com/kvcache-ai/ktransformers&lt;/a&gt; - they are known for optimized configs to run on RAM + relatively low amount of VRAM - but I failed to make it work locally and didn&amp;#39;t find an up-to-date docker image either. I would imagine it&amp;#39;s not gonna yield significant improvements, but happy to be proven wrong.&lt;/li&gt;\n&lt;li&gt;IGPU + Vulcan?&lt;/li&gt;\n&lt;li&gt;NPU xD&lt;/li&gt;\n&lt;li&gt;Test full context (or the largest context that does not take eternity to process)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What&amp;#39;s your experience / recipe for similarly-sized hardware setup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?auto=webp&amp;s=c7c85f2c4c738393e0af92a8424d4f3b9231b100",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0ca996c64f35d96d82c792f292d1574156f28a8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=13e828ba3e534b52cb7e76434082e0591ff8fe84",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9beff3bbc086be73ada08d7d9e0be23ce9b4cbec",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a74fa58f6c27cd63b1b4175d767d3aa5e620d4e6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a87258e5557a2c94d34ba6aad86a07f7c1180e7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a48274f527c77b8067ed3f8f078884cca0d1fcc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdvkhz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bycbka",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdvkhz/ik_llamacpp_and_qwen_3_30ba3b_architecture/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdvkhz/ik_llamacpp_and_qwen_3_30ba3b_architecture/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753948642,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4a870z4c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama 0.10 - New app is available for macOS and Windows plus multi-GPU performance improvements, and more",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdq3sv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=20e99ec07b36d747b3440811e17c10dba287690f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753929783,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ollama/ollama/releases/tag/v0.10.0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?auto=webp&amp;s=4414180b29c1502f7a961f72f4acf79a19d2f36d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b8de8b6845c7bac03a11d66156a8e9be1f7345d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3bc7376589ceca9d7e93370a45c358f7eab1f84f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1af803b3994bb9be0ff405f4af34a0fed0cc23c7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d42b6edc645f624b40e5a4c076cb7f9f25ed0e3f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4acbe20abd4a35fea219bcad4b0fddb4607349c3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1cd4439fbb58b0138e945a259463102137056525",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdq3sv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mj3815",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdq3sv/ollama_010_new_app_is_available_for_macos_and/",
          "stickied": false,
          "url": "https://github.com/ollama/ollama/releases/tag/v0.10.0",
          "subreddit_subscribers": 507576,
          "created_utc": 1753929783,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I made a major update to deep drone, so it now is a CLI agent that controls your drone. It can use models with an api key and also use Ollama. Here is the demo below. And the source code : [https://github.com/evangelosmeklis/deepdrone](https://github.com/evangelosmeklis/deepdrone)\n\nhttps://reddit.com/link/1mdxihp/video/0ejlwqoln6gf1/player\n\n",
          "author_fullname": "t2_3067wthh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepDrone, an open source CLI agent like Claude Code to fly your drone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "0ejlwqoln6gf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mdxihp/asset/0ejlwqoln6gf1/DASHPlaylist.mpd?a=1756556941%2COGZjYjY2ZjZmNjIwNmJhNGNhNTk5M2RmMjkzMDU2YmMwNjU0YTgyNzcwZmUyMzcwNzNhOGFjOGQxYjlhYjY0Yw%3D%3D&amp;v=1&amp;f=sd",
              "x": 1920,
              "y": 562,
              "hlsUrl": "https://v.redd.it/link/1mdxihp/asset/0ejlwqoln6gf1/HLSPlaylist.m3u8?a=1756556941%2CODBhMDUzZGVkMzQ2MTFhMDRlMDkxNGEwZThiOWJjMzAzNDg3MDg5MjhiYTNlMTAxZTQ5Y2IwMTg1NzhlNjQ4NQ%3D%3D&amp;v=1&amp;f=sd",
              "id": "0ejlwqoln6gf1",
              "isGif": false
            }
          },
          "name": "t3_1mdxihp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=9dbc8db740c27e211ff4197416cadedf3aa29aa5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753956186,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a major update to deep drone, so it now is a CLI agent that controls your drone. It can use models with an api key and also use Ollama. Here is the demo below. And the source code : &lt;a href=\"https://github.com/evangelosmeklis/deepdrone\"&gt;https://github.com/evangelosmeklis/deepdrone&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mdxihp/video/0ejlwqoln6gf1/player\"&gt;https://reddit.com/link/1mdxihp/video/0ejlwqoln6gf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI.png?auto=webp&amp;s=a182f6e5039b2ed9730b650ba6ecad08764dcfb8",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e68c38f87cdcc748537b94eed5d21fe9bc1588d8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e0142f8e0a362fb0ec2137f2a2cf4120fa4ed05",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0d6f6c8720fde1b3f421b25e5e1b1fc9be942ec",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa5faf99730b0a78e30d0936f1679afa4ae6332d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1b5365ac3a479980effd5e784df0cc00a84092f5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69b4ad441c9b6302d637d853b450df6d9acb562f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "D05ayFweRYxXp8MVhd-qyiGHFhlwyPzJP4dBla8AObI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mdxihp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_twelvechess",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdxihp/deepdrone_an_open_source_cli_agent_like_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdxihp/deepdrone_an_open_source_cli_agent_like_claude/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753956186,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For the longest time, I've been giving my models a traditional puzzle that all failed to pass without fail :D  \nNot even the SOTA models provide the right answer.\n\n&gt;The puzzle is as follows:   \n\"What's the right answer: Imagine standing at the North Pole of the Earth. Walk in any direction, in a straight line, for 1 km. Now turn 90 degrees to the left. Walk for as long as it takes to pass your starting point. Have you walked: \n\n&gt;1- More than 2xPi km.  \n2- Exactly 2xPi km.  \n3- Less than 2xPi km.  \n4- I never came close to my starting point.\n\nHowever, only recently, SOTA models started to correctly answer 4 ; models like O3, latest Qwen (Qween3-235B-A22B-2507), Deepseek R1 managed to answer it correctly (I didn't test Claud 4 or Grok 4 but I guess they might get it right). For comparison, Gemini-2.5-Thinking and Kimi2 got the wrong answer.\n\nSo, I happy to report that Qwen3-30B-A3B-2507 (both the none thinking Q6 and the thinking Q4) managed to solve the puzzle providing great answers.\n\nHere is O3 answer:\n\nhttps://preview.redd.it/rbwgf8vxa7gf1.png?width=866&amp;format=png&amp;auto=webp&amp;s=d074bec940c5c3fab89cc06a5cdf7279ed154ea0\n\nAnd here is the answer of the Qwen3-30B-A3B-Thinking-2507-Q4\\_K\\_L:\n\nhttps://preview.redd.it/esglti77b7gf1.png?width=821&amp;format=png&amp;auto=webp&amp;s=9d2e5321f3918bec8209d8613d1ce2df621cd416\n\nIn addition, I tested the two variants on long text (up to 80K) for comprehension, and I am impressed by the quality of the answers. And the SPEEEEEED! It's 3 times faster than Gemma-4B!!!!\n\n\n\nAnyway, let me know what you think,",
          "author_fullname": "t2_byt5wa14",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30B-A3B-2507-Q4_K_L Is the First Local Model to Solve the North Pole Walk Puzzle",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 60,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "esglti77b7gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 87,
                  "x": 108,
                  "u": "https://preview.redd.it/esglti77b7gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa4c861cb82d74fa884e07763d49a9087f956d22"
                },
                {
                  "y": 174,
                  "x": 216,
                  "u": "https://preview.redd.it/esglti77b7gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1cfe9a1b5c0462b3615bc71b7d76aed45acf149"
                },
                {
                  "y": 258,
                  "x": 320,
                  "u": "https://preview.redd.it/esglti77b7gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cbac2d110c2783a443d0cb885b248a0d5eef241"
                },
                {
                  "y": 516,
                  "x": 640,
                  "u": "https://preview.redd.it/esglti77b7gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f2094d8a756e6c8ded571ddc2e08b260c39b760"
                }
              ],
              "s": {
                "y": 663,
                "x": 821,
                "u": "https://preview.redd.it/esglti77b7gf1.png?width=821&amp;format=png&amp;auto=webp&amp;s=9d2e5321f3918bec8209d8613d1ce2df621cd416"
              },
              "id": "esglti77b7gf1"
            },
            "rbwgf8vxa7gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 46,
                  "x": 108,
                  "u": "https://preview.redd.it/rbwgf8vxa7gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1662b020232e38c498b9c33b3a84a623a9c9687"
                },
                {
                  "y": 93,
                  "x": 216,
                  "u": "https://preview.redd.it/rbwgf8vxa7gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2257e7230fe7883100398d769880650162fb2391"
                },
                {
                  "y": 138,
                  "x": 320,
                  "u": "https://preview.redd.it/rbwgf8vxa7gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=72caaec311bc8ebe2c27b8b7ee5fb903baa018c6"
                },
                {
                  "y": 276,
                  "x": 640,
                  "u": "https://preview.redd.it/rbwgf8vxa7gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cba947c4bd3e5de66a6067de3cf1811cf7f499b9"
                }
              ],
              "s": {
                "y": 374,
                "x": 866,
                "u": "https://preview.redd.it/rbwgf8vxa7gf1.png?width=866&amp;format=png&amp;auto=webp&amp;s=d074bec940c5c3fab89cc06a5cdf7279ed154ea0"
              },
              "id": "rbwgf8vxa7gf1"
            }
          },
          "name": "t3_1mdzxmv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/L3dTMc7iVqHF7QYQEYnpMXvgm0DsrwCLc5zwFJKZCxk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753964105,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the longest time, I&amp;#39;ve been giving my models a traditional puzzle that all failed to pass without fail :D&lt;br/&gt;\nNot even the SOTA models provide the right answer.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The puzzle is as follows:&lt;br/&gt;\n&amp;quot;What&amp;#39;s the right answer: Imagine standing at the North Pole of the Earth. Walk in any direction, in a straight line, for 1 km. Now turn 90 degrees to the left. Walk for as long as it takes to pass your starting point. Have you walked: &lt;/p&gt;\n\n&lt;p&gt;1- More than 2xPi km.&lt;br/&gt;\n2- Exactly 2xPi km.&lt;br/&gt;\n3- Less than 2xPi km.&lt;br/&gt;\n4- I never came close to my starting point.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;However, only recently, SOTA models started to correctly answer 4 ; models like O3, latest Qwen (Qween3-235B-A22B-2507), Deepseek R1 managed to answer it correctly (I didn&amp;#39;t test Claud 4 or Grok 4 but I guess they might get it right). For comparison, Gemini-2.5-Thinking and Kimi2 got the wrong answer.&lt;/p&gt;\n\n&lt;p&gt;So, I happy to report that Qwen3-30B-A3B-2507 (both the none thinking Q6 and the thinking Q4) managed to solve the puzzle providing great answers.&lt;/p&gt;\n\n&lt;p&gt;Here is O3 answer:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rbwgf8vxa7gf1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d074bec940c5c3fab89cc06a5cdf7279ed154ea0\"&gt;https://preview.redd.it/rbwgf8vxa7gf1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d074bec940c5c3fab89cc06a5cdf7279ed154ea0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And here is the answer of the Qwen3-30B-A3B-Thinking-2507-Q4_K_L:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/esglti77b7gf1.png?width=821&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2e5321f3918bec8209d8613d1ce2df621cd416\"&gt;https://preview.redd.it/esglti77b7gf1.png?width=821&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2e5321f3918bec8209d8613d1ce2df621cd416&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In addition, I tested the two variants on long text (up to 80K) for comprehension, and I am impressed by the quality of the answers. And the SPEEEEEED! It&amp;#39;s 3 times faster than Gemma-4B!!!!&lt;/p&gt;\n\n&lt;p&gt;Anyway, let me know what you think,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdzxmv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Iory1998",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753964105,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tested Kimi K2 again, against Claude 4 Sonnet (Sonnet 4) this time, here are my findings (vid in comments):\n\n\\- K2 isn't only less reliable in VSCode tool calling, it's considerably less in Cline as well, vs Claude 4 Sonnet\n\n\\- I integrated K2 via OpenRouter inference into my own application LIVE and it did the same thing: instead of calling tools, it outputs the tool calls as text, mostly malformed and consolidated\n\n\\- Ref: https://youtu.be/p2LKJo3EK7w\n\n\\- Tip for AI coding agent authors: write a parser or a specialized prompt for Kimi K2 - even if it sounds like coupling, the value for money is well worth it\n\n\\- The \"Agent Benchmarks\" are definitely not accurate, Sonnet 4 is NATIVELY much better in almost every AI Coding tool\n\n\\- I'm still going to test K2 in Qwen Coder and maybe a custom coding tool, but it's a very good coder\n\n\\- K2 is better than Gemini 2.5 Pro in tool calling, according to me\n\n\\- Currently, the best implementation of K2 I found is in Windsurf (I tested VSCode, Cline, Windsurf and RooCode)",
          "author_fullname": "t2_qmg9qzxv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Claude 4 Sonnet - Unexpected Review Result (400k token Codebase)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdldom",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753917735,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753916600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tested Kimi K2 again, against Claude 4 Sonnet (Sonnet 4) this time, here are my findings (vid in comments):&lt;/p&gt;\n\n&lt;p&gt;- K2 isn&amp;#39;t only less reliable in VSCode tool calling, it&amp;#39;s considerably less in Cline as well, vs Claude 4 Sonnet&lt;/p&gt;\n\n&lt;p&gt;- I integrated K2 via OpenRouter inference into my own application LIVE and it did the same thing: instead of calling tools, it outputs the tool calls as text, mostly malformed and consolidated&lt;/p&gt;\n\n&lt;p&gt;- Ref: &lt;a href=\"https://youtu.be/p2LKJo3EK7w\"&gt;https://youtu.be/p2LKJo3EK7w&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- Tip for AI coding agent authors: write a parser or a specialized prompt for Kimi K2 - even if it sounds like coupling, the value for money is well worth it&lt;/p&gt;\n\n&lt;p&gt;- The &amp;quot;Agent Benchmarks&amp;quot; are definitely not accurate, Sonnet 4 is NATIVELY much better in almost every AI Coding tool&lt;/p&gt;\n\n&lt;p&gt;- I&amp;#39;m still going to test K2 in Qwen Coder and maybe a custom coding tool, but it&amp;#39;s a very good coder&lt;/p&gt;\n\n&lt;p&gt;- K2 is better than Gemini 2.5 Pro in tool calling, according to me&lt;/p&gt;\n\n&lt;p&gt;- Currently, the best implementation of K2 I found is in Windsurf (I tested VSCode, Cline, Windsurf and RooCode)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dv_I54LGpmnqKoSxBiYuiXlgStoZanHgVx1garYxUvY.jpeg?auto=webp&amp;s=11bec68bd838fee596608741fbde78e3db0d944d",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dv_I54LGpmnqKoSxBiYuiXlgStoZanHgVx1garYxUvY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99ec0a4d4a5f8c158298ac9030280b7e7f862186",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/dv_I54LGpmnqKoSxBiYuiXlgStoZanHgVx1garYxUvY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5448b8e94bb3b9888db22b5f14d5d69a98c2166f",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/dv_I54LGpmnqKoSxBiYuiXlgStoZanHgVx1garYxUvY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0abae9296465ccacf76ef8025a7481d0c079eb5",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "dv_I54LGpmnqKoSxBiYuiXlgStoZanHgVx1garYxUvY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mdldom",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "marvijo-software",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdldom/kimi_k2_vs_claude_4_sonnet_unexpected_review/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdldom/kimi_k2_vs_claude_4_sonnet_unexpected_review/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753916600,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\n\nI’m building an affordable, plug-and-play AI devboard kind of like a “Raspberry Pi for AI”designed to run models like TinyLlama, Whisper, and YOLO locally, without cloud dependencies.\n\nIt’s meant for developers, makers, educators, and startups who want to:\n\t•\tRun local LLMs and vision models on the edge\n\t•\tBuild AI-powered projects (offline assistants, smart cameras, low-power robots)\n\t•\tExperiment with on-device inference using open-source models\n\nThe board will include:\n\t•\tA built-in NPU (2–10 TOPS range)\n\t•\tSupport for TFLite, ONNX, and llama.cpp workflows\n\t•\tPython/C++ SDK for deploying your own models\n\t•\tGPIO, camera, mic, and USB expansion for projects\n\nI’m still in the prototyping phase and talking to potential early users. If you:\n\t•\tCurrently run AI models on a Pi, Jetson, ESP32, or PC\n\t•\tAre building something cool with local inference\n\t•\tHave been frustrated by slow, power-hungry, or clunky AI deployments\n\n…I’d love to chat or send you early builds when ready.\n\nDrop a comment or DM me and let me know what YOU would want from an “AI-first” devboard.\n\nThanks!\n",
          "author_fullname": "t2_1uhrwpei9n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We’re building a devboard that runs Whisper, YOLO, and TinyLlama — locally, no cloud. Want to try it before we launch?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdx40b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753954678,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I’m building an affordable, plug-and-play AI devboard kind of like a “Raspberry Pi for AI”designed to run models like TinyLlama, Whisper, and YOLO locally, without cloud dependencies.&lt;/p&gt;\n\n&lt;p&gt;It’s meant for developers, makers, educators, and startups who want to:\n    • Run local LLMs and vision models on the edge\n    • Build AI-powered projects (offline assistants, smart cameras, low-power robots)\n    • Experiment with on-device inference using open-source models&lt;/p&gt;\n\n&lt;p&gt;The board will include:\n    • A built-in NPU (2–10 TOPS range)\n    • Support for TFLite, ONNX, and llama.cpp workflows\n    • Python/C++ SDK for deploying your own models\n    • GPIO, camera, mic, and USB expansion for projects&lt;/p&gt;\n\n&lt;p&gt;I’m still in the prototyping phase and talking to potential early users. If you:\n    • Currently run AI models on a Pi, Jetson, ESP32, or PC\n    • Are building something cool with local inference\n    • Have been frustrated by slow, power-hungry, or clunky AI deployments&lt;/p&gt;\n\n&lt;p&gt;…I’d love to chat or send you early builds when ready.&lt;/p&gt;\n\n&lt;p&gt;Drop a comment or DM me and let me know what YOU would want from an “AI-first” devboard.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mdx40b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aero917",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdx40b/were_building_a_devboard_that_runs_whisper_yolo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdx40b/were_building_a_devboard_that_runs_whisper_yolo/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753954678,
          "num_crossposts": 7,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI’m setting up my system to run large language models locally and would really appreciate recommendations.\n\nI haven’t tried any models yet — my goal is to move away from cloud LLMs like Claude (mainly for coding , reasoning, and tool use), and run everything locally.\n\nMy setup:\n\t•\tUbuntu\n\t•\tAMD Threadripper 7960X (24 cores / 48 threads)\n\t•\t3× RTX 3090 (72 GB total VRAM)\n\t•\t128 GB DDR5 ECC RAM\n\t•\t8 TB M.2 NVMe SSD\n\nWhat I’m looking for:\n\t1.\tA Claude-like model that handles reasoning and agentic behavior well\n\t2.\tCan run on this hardware (preferably multi-GPU, FP16 or 4-bit quantized)\n\t3.\tSupports long-context and multi-step workflows\n\t4.\tIdeally open-source, something I can fully control",
          "author_fullname": "t2_v9xk0o13",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local model for Claude-like agentic behavior on 3×3090 rig?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdwv4f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753953683,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I’m setting up my system to run large language models locally and would really appreciate recommendations.&lt;/p&gt;\n\n&lt;p&gt;I haven’t tried any models yet — my goal is to move away from cloud LLMs like Claude (mainly for coding , reasoning, and tool use), and run everything locally.&lt;/p&gt;\n\n&lt;p&gt;My setup:\n    • Ubuntu\n    • AMD Threadripper 7960X (24 cores / 48 threads)\n    • 3× RTX 3090 (72 GB total VRAM)\n    • 128 GB DDR5 ECC RAM\n    • 8 TB M.2 NVMe SSD&lt;/p&gt;\n\n&lt;p&gt;What I’m looking for:\n    1.  A Claude-like model that handles reasoning and agentic behavior well\n    2.  Can run on this hardware (preferably multi-GPU, FP16 or 4-bit quantized)\n    3.  Supports long-context and multi-step workflows\n    4.  Ideally open-source, something I can fully control&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdwv4f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CryptographerLow7817",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdwv4f/best_local_model_for_claudelike_agentic_behavior/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdwv4f/best_local_model_for_claudelike_agentic_behavior/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753953683,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_52zzm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-30B-A3B-Thinking-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md8rxu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 150,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 150,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=0cf90a8010053dbf48911257591f71a3d1ddded7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753887372,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?auto=webp&amp;s=a67dbb1b6fae4b63d82563a3e65a19938ca062fb",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e994b63235f1f31da964f24b3a55a51498b6935f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7aa24107465ba0cfb16f79135e2c61bc02b91707",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=95124de9bda6db677aaa373721a3aa188cc7f224",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3213439d0e68cbadd20dbb4d235a121e1df48f64",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b13bc1d8de32bb083d7b376a591f00d85d3173aa",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1md8rxu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MariusNocturnum",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507",
          "subreddit_subscribers": 507576,
          "created_utc": 1753887372,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nJust launched **Eigent,** a fully open-source, local-first multi-agent desktop application designed for developers and teams who want full control over their AI workflows.  \nBuilt on top of CAMEL-AI’s modular framework, Eigent allows you to:\n\n* Run tasks in parallel with customizable agent workflows\n* Deploy locally or in the cloud with “Bring Your Own Key” (BYOK) support\n* Maintain full data privacy — no information leaves your machine\n* Step in anytime with Human-in-the-Loop control\n* Integrate seamlessly with your existing stack\n* Use 200+ MCP-compatible tools (or bring your own)\n\nThe goal is simple: give teams a secure, customizable, and scalable AI workforce on their own infrastructure.  \n→ GitHub: [github.com/eigent-ai/eigent](http://github.com/eigent-ai/eigent)  \n→ Download: [eigent.ai\n](http://www.eigent.ai/)  \nFeel free to ask me anything below, whether it’s about the architecture, use cases, or how to extend it for your own needs.",
          "author_fullname": "t2_152q9v633e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Eigent – Open Source, Local-First Multi-Agent Workforce",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ef2zkaadi1gf1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/ef2zkaadi1gf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=cc7aa7c19cb33e6ea5a3189f5d4a37172bad4bab"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/ef2zkaadi1gf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=2a927f057db3e6ee581d145afb172e678b47c9e0"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/ef2zkaadi1gf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=649b21b6c8f96a9bbd9634460b86ade41547ca42"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/ef2zkaadi1gf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=3d8846e1ab7075c62ceb6e4efddf56bb5f671b64"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/ef2zkaadi1gf1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=fd074da189d93f901d022bc28781e401ac5da565"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/ef2zkaadi1gf1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=a8e0b8fc312c32a85f7e54316cdf77e56c58e174"
                }
              ],
              "s": {
                "y": 900,
                "gif": "https://i.redd.it/ef2zkaadi1gf1.gif",
                "mp4": "https://preview.redd.it/ef2zkaadi1gf1.gif?format=mp4&amp;s=d9f6fdde529614c3dc2737be24c5f89e0ade062c",
                "x": 1440
              },
              "id": "ef2zkaadi1gf1"
            },
            "ojkyicmfi1gf1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/ojkyicmfi1gf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=7c01c4d5c9d3e5cfd1f01e31532a5e029345a539"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/ojkyicmfi1gf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=c9f2a8e7741b0403d54453040b78b981b3abede4"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/ojkyicmfi1gf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=6ea32e90c9414fa74c034c6afa2fdd89730bb7c4"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/ojkyicmfi1gf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=ba764aeb05d92cacedde7dc333446514ab6e483c"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/ojkyicmfi1gf1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=e6f2a2ab8d800d86a62c3a7a7675cb66b5091700"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/ojkyicmfi1gf1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=6f07fa053b430f40ed553fd774fba50e01847b5a"
                }
              ],
              "s": {
                "y": 900,
                "gif": "https://i.redd.it/ojkyicmfi1gf1.gif",
                "mp4": "https://preview.redd.it/ojkyicmfi1gf1.gif?format=mp4&amp;s=040939eeaef50f65afbdd9c681b871bebf581bb6",
                "x": 1440
              },
              "id": "ojkyicmfi1gf1"
            }
          },
          "name": "t3_1mdbm5t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 103,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ef2zkaadi1gf1",
                "id": 717466340
              },
              {
                "media_id": "ojkyicmfi1gf1",
                "id": 717466341
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 103,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3RqGcIVLHN9WLuz3G6pO2CwDYLwlqMU3O2iTSdwHGzY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753893843,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just launched &lt;strong&gt;Eigent,&lt;/strong&gt; a fully open-source, local-first multi-agent desktop application designed for developers and teams who want full control over their AI workflows.&lt;br/&gt;\nBuilt on top of CAMEL-AI’s modular framework, Eigent allows you to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run tasks in parallel with customizable agent workflows&lt;/li&gt;\n&lt;li&gt;Deploy locally or in the cloud with “Bring Your Own Key” (BYOK) support&lt;/li&gt;\n&lt;li&gt;Maintain full data privacy — no information leaves your machine&lt;/li&gt;\n&lt;li&gt;Step in anytime with Human-in-the-Loop control&lt;/li&gt;\n&lt;li&gt;Integrate seamlessly with your existing stack&lt;/li&gt;\n&lt;li&gt;Use 200+ MCP-compatible tools (or bring your own)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The goal is simple: give teams a secure, customizable, and scalable AI workforce on their own infrastructure.&lt;br/&gt;\n→ GitHub: &lt;a href=\"http://github.com/eigent-ai/eigent\"&gt;github.com/eigent-ai/eigent&lt;/a&gt;&lt;br/&gt;\n→ Download: &lt;a href=\"http://www.eigent.ai/\"&gt;eigent.ai\n&lt;/a&gt;&lt;br/&gt;\nFeel free to ask me anything below, whether it’s about the architecture, use cases, or how to extend it for your own needs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mdbm5t",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdbm5t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FitHeron1933",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mdbm5t",
          "subreddit_subscribers": 507576,
          "created_utc": 1753893843,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With Qwen, you could add something to the prompt to turn off reasoning. Can you do the same with GLM 4.5?",
          "author_fullname": "t2_32aqmyw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can you turn off reasoning for certain tasks in GLM 4.5?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdwh31",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753952158,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With Qwen, you could add something to the prompt to turn off reasoning. Can you do the same with GLM 4.5?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdwh31",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sky_Linx",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdwh31/how_can_you_turn_off_reasoning_for_certain_tasks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdwh31/how_can_you_turn_off_reasoning_for_certain_tasks/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753952158,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Like the title says, I ran **GLM 4.5 Air Q4** on my local machine using **RooCode** inside **VS Code**, and I was able to build a functional CRUD-style web application.\n\nUsers can register with a password, log in, and log out from the client side. All authentication is handled using JWTs.\n\nThe experience honestly exceeded my expectations. Compared to my past attempts with local LLMs and RooCode (which sometimes struggled to generate even a basic webpage), this felt like a major step forward. The results were genuinely satisfying.\n\nThe entire app took about an hour to generate, with a bit of debugging and prompt tweaking along the way. With more deliberate prompting and a little more patience, I think I could have pushed it further. But for now, it’s a solid starting point.\n\nIf anyone else is experimenting with local models for full stack projects, I’d love to hear how it’s going. Happy to answer questions or share what I’ve learned.",
          "author_fullname": "t2_fz3utn30",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Built a Full Stack App Using a Local LLM (GLM 4.5 Air) and RooCode. Here's How It Went",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdu4io",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753943029,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says, I ran &lt;strong&gt;GLM 4.5 Air Q4&lt;/strong&gt; on my local machine using &lt;strong&gt;RooCode&lt;/strong&gt; inside &lt;strong&gt;VS Code&lt;/strong&gt;, and I was able to build a functional CRUD-style web application.&lt;/p&gt;\n\n&lt;p&gt;Users can register with a password, log in, and log out from the client side. All authentication is handled using JWTs.&lt;/p&gt;\n\n&lt;p&gt;The experience honestly exceeded my expectations. Compared to my past attempts with local LLMs and RooCode (which sometimes struggled to generate even a basic webpage), this felt like a major step forward. The results were genuinely satisfying.&lt;/p&gt;\n\n&lt;p&gt;The entire app took about an hour to generate, with a bit of debugging and prompt tweaking along the way. With more deliberate prompting and a little more patience, I think I could have pushed it further. But for now, it’s a solid starting point.&lt;/p&gt;\n\n&lt;p&gt;If anyone else is experimenting with local models for full stack projects, I’d love to hear how it’s going. Happy to answer questions or share what I’ve learned.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdu4io",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gamblingapocalypse",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753943029,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "RTX4090  \ni9 14900k  \n64GB DDR5 6000Mhz  \n2TB SSD PCIe5\n\nI played a bit with the Qwen2.5 Coder 32B, but it felt very slow.  \nNow i see lots of new models coming out. I would want to use it in VS Code + Cline for coding, something that offsets some of the easier tasks so i don't get to pay a lot for the cloud models API's",
          "author_fullname": "t2_q140j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What model would you recommend for my specs ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mdy8f8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753958773,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;RTX4090&lt;br/&gt;\ni9 14900k&lt;br/&gt;\n64GB DDR5 6000Mhz&lt;br/&gt;\n2TB SSD PCIe5&lt;/p&gt;\n\n&lt;p&gt;I played a bit with the Qwen2.5 Coder 32B, but it felt very slow.&lt;br/&gt;\nNow i see lots of new models coming out. I would want to use it in VS Code + Cline for coding, something that offsets some of the easier tasks so i don&amp;#39;t get to pay a lot for the cloud models API&amp;#39;s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdy8f8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alywan",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdy8f8/what_model_would_you_recommend_for_my_specs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdy8f8/what_model_would_you_recommend_for_my_specs/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753958773,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_186az5xn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is \"Personal Superintelligence\" really personal if it is not local like a personal device?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdfkly",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753902735,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "meta.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.meta.com/superintelligence/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdfkly",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AlanzhuLy",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdfkly/is_personal_superintelligence_really_personal_if/",
          "stickied": false,
          "url": "https://www.meta.com/superintelligence/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753902735,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I am a long time lurker, but I took a break after the rtx 5090 launch fail since I almost completely gave up on getting to run ai locally this year.\n\nWith everything that's going on in the world and the possibility of the ai being considered \"too dangerous\", apparently the music may already be, I want to ask which llm is \"good\" today (not in the way of SOTA, but by personal user experience). I am planning on using an intel b60 48gb vram or maybe 1-2 amd mi50 32gb. I am mostly interested in llm, vllm and probably one for coding, although it's not really needed since I know how to code, but it might come handy I don't know. I guess what I might need is probably 7-70b parameter ones, I also have 96gb ram so a larger moe might also be decent. The total storage for all ais is probably 2-3tb. If I am at this topic I suppose that the intel gpu might be better for image generation\n\nI am old enough to remember mixtral 7x8 but I have no idea if it's still relevant, I know some mistral small might be better, also I might be interested in the vllm one for ocr. I kinda have an idea of most of the llms including the new qwen moes, but I have no idea which of the old models are still relevant today. For example I know that lama 3, or even 3.3 is kinda \"outdated\" (since I have no better word, but you get what I mean), I am even aware of a new nemotron which is based on lama 70b but I am missing a lot of details.\n\nI know I should be able to find them on huggingface, and I might need to download vllm, ollama and intel playgrounds or idk how it is for it.\n\nI know exactly how to get the stable diffusion models, but while we are at it I might be interested in a few tts models (text to speech, preferably with voice cloning), I think I've heard of \"megatts 3\" and \"GPT-SoVITS\" but any tips here are helpful as well. Meanwhile I will to find the fastest whisper model for stt, I am certain I might have saved the link for it somewhere.\n\nSorry for creating trash posts that are probably lots and lots on weekly bases for this particular question (not that particular considering the title, but you get what I mean).",
          "author_fullname": "t2_1gwc678u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best LLMs to preserve in case of internet apocalypse",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdishv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753910249,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a long time lurker, but I took a break after the rtx 5090 launch fail since I almost completely gave up on getting to run ai locally this year.&lt;/p&gt;\n\n&lt;p&gt;With everything that&amp;#39;s going on in the world and the possibility of the ai being considered &amp;quot;too dangerous&amp;quot;, apparently the music may already be, I want to ask which llm is &amp;quot;good&amp;quot; today (not in the way of SOTA, but by personal user experience). I am planning on using an intel b60 48gb vram or maybe 1-2 amd mi50 32gb. I am mostly interested in llm, vllm and probably one for coding, although it&amp;#39;s not really needed since I know how to code, but it might come handy I don&amp;#39;t know. I guess what I might need is probably 7-70b parameter ones, I also have 96gb ram so a larger moe might also be decent. The total storage for all ais is probably 2-3tb. If I am at this topic I suppose that the intel gpu might be better for image generation&lt;/p&gt;\n\n&lt;p&gt;I am old enough to remember mixtral 7x8 but I have no idea if it&amp;#39;s still relevant, I know some mistral small might be better, also I might be interested in the vllm one for ocr. I kinda have an idea of most of the llms including the new qwen moes, but I have no idea which of the old models are still relevant today. For example I know that lama 3, or even 3.3 is kinda &amp;quot;outdated&amp;quot; (since I have no better word, but you get what I mean), I am even aware of a new nemotron which is based on lama 70b but I am missing a lot of details.&lt;/p&gt;\n\n&lt;p&gt;I know I should be able to find them on huggingface, and I might need to download vllm, ollama and intel playgrounds or idk how it is for it.&lt;/p&gt;\n\n&lt;p&gt;I know exactly how to get the stable diffusion models, but while we are at it I might be interested in a few tts models (text to speech, preferably with voice cloning), I think I&amp;#39;ve heard of &amp;quot;megatts 3&amp;quot; and &amp;quot;GPT-SoVITS&amp;quot; but any tips here are helpful as well. Meanwhile I will to find the fastest whisper model for stt, I am certain I might have saved the link for it somewhere.&lt;/p&gt;\n\n&lt;p&gt;Sorry for creating trash posts that are probably lots and lots on weekly bases for this particular question (not that particular considering the title, but you get what I mean).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdishv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nos_66",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753910249,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a simple interface built with pure HTML, JavaScript, and CSS to interact with ChatGPT using your own API key. It can be run directly in your web browser and it supports the classical GPT Models that the API let you interact with.\n\n[Example of a prompt](https://preview.redd.it/czusk8r0o5gf1.png?width=1917&amp;format=png&amp;auto=webp&amp;s=c98270c495ec145818ac85a207730da746600813)\n\nI created it because I was given an API key for programming with the OpenAI API in college, but sometimes I just want to use ChatGPT Premium features through a clean, lightweight interface but a lot of open-source projects use technologies like Next.js or other frameworks, but I just wanted the simplest possible solution.\n\n[A little video using the Streaming response, and code uploading](https://i.redd.it/814bi7t7o5gf1.gif)\n\nLaTeX is rendered using MathJax, and code formatting is handled with a terrible implementation of regex to detect the format used by the AI to encapsulate code. The same with MarkDown, a terrible implementation, but works so... I hope some of you find it useful.\n\nProject:  \n[https://github.com/N1xUser/OpenAI-HTML-Client](https://github.com/N1xUser/OpenAI-HTML-Client)  \nIf you want to use it is hosted on github as well:  \n[https://n1xuser.github.io/OpenAI-HTML-Client/ChatGPT%20Client.html](https://n1xuser.github.io/OpenAI-HTML-Client/ChatGPT%20Client.html)",
          "author_fullname": "t2_7jm2czx1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Lightweight ChatGPT Client Using Your Own API Key (Pure HTML)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "czusk8r0o5gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/czusk8r0o5gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a0a746e25e1c7cb4409512bec0086447926bd60"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/czusk8r0o5gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f58d42625f4b76ad6594ef9127470d46715de3d"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/czusk8r0o5gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=689e6c42ce6a7d0b3ae07824b1e5f3485b055908"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/czusk8r0o5gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b32132edad9e28a4a558a91b44e6333b9bab1e3a"
                },
                {
                  "y": 479,
                  "x": 960,
                  "u": "https://preview.redd.it/czusk8r0o5gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8bec14ad51c32308de85c99359cdf20b1c15f5d"
                },
                {
                  "y": 539,
                  "x": 1080,
                  "u": "https://preview.redd.it/czusk8r0o5gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=856a669606cb8d573b51162db62d8cfe5adeaee6"
                }
              ],
              "s": {
                "y": 958,
                "x": 1917,
                "u": "https://preview.redd.it/czusk8r0o5gf1.png?width=1917&amp;format=png&amp;auto=webp&amp;s=c98270c495ec145818ac85a207730da746600813"
              },
              "id": "czusk8r0o5gf1"
            },
            "814bi7t7o5gf1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/814bi7t7o5gf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=1c517f0010dda9198a3309a9848840a4fd2fdea7"
                },
                {
                  "y": 110,
                  "x": 216,
                  "u": "https://preview.redd.it/814bi7t7o5gf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=665205362303ad04f4ac08b7a74349454dd7939a"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/814bi7t7o5gf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=eaf2885ff618b565da1422b3b1fa6920426fa722"
                },
                {
                  "y": 328,
                  "x": 640,
                  "u": "https://preview.redd.it/814bi7t7o5gf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=3ea7640cfd1dea2affe2416bae921b51e14944ba"
                },
                {
                  "y": 491,
                  "x": 960,
                  "u": "https://preview.redd.it/814bi7t7o5gf1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=5e84a6f1fa80b4414dd82d882ce750208586d29c"
                },
                {
                  "y": 553,
                  "x": 1080,
                  "u": "https://preview.redd.it/814bi7t7o5gf1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=4d6e6cc3b1e44a5ba9f0b64302aeecd7580e4ece"
                }
              ],
              "s": {
                "y": 984,
                "gif": "https://i.redd.it/814bi7t7o5gf1.gif",
                "mp4": "https://preview.redd.it/814bi7t7o5gf1.gif?format=mp4&amp;s=c6ae1b9832005b5d09f2fd70c4470fa31b523692",
                "x": 1920
              },
              "id": "814bi7t7o5gf1"
            }
          },
          "name": "t3_1mduvcv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=1c574431ff7dadd680e72d031df152196d6c94bd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753945857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a simple interface built with pure HTML, JavaScript, and CSS to interact with ChatGPT using your own API key. It can be run directly in your web browser and it supports the classical GPT Models that the API let you interact with.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/czusk8r0o5gf1.png?width=1917&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c98270c495ec145818ac85a207730da746600813\"&gt;Example of a prompt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I created it because I was given an API key for programming with the OpenAI API in college, but sometimes I just want to use ChatGPT Premium features through a clean, lightweight interface but a lot of open-source projects use technologies like Next.js or other frameworks, but I just wanted the simplest possible solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/814bi7t7o5gf1.gif\"&gt;A little video using the Streaming response, and code uploading&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;LaTeX is rendered using MathJax, and code formatting is handled with a terrible implementation of regex to detect the format used by the AI to encapsulate code. The same with MarkDown, a terrible implementation, but works so... I hope some of you find it useful.&lt;/p&gt;\n\n&lt;p&gt;Project:&lt;br/&gt;\n&lt;a href=\"https://github.com/N1xUser/OpenAI-HTML-Client\"&gt;https://github.com/N1xUser/OpenAI-HTML-Client&lt;/a&gt;&lt;br/&gt;\nIf you want to use it is hosted on github as well:&lt;br/&gt;\n&lt;a href=\"https://n1xuser.github.io/OpenAI-HTML-Client/ChatGPT%20Client.html\"&gt;https://n1xuser.github.io/OpenAI-HTML-Client/ChatGPT%20Client.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU.png?auto=webp&amp;s=09c4e209fdfc9a708a2868e0111e1e8461fce7dc",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b1c5b177de3bdd77d25f129fd287e4e014dea2b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3acaec0aea5ed741eb9b23e701051160e6e393a7",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=033e14cb63ff0ed65c8ad2a4f1dab5327a07ce2b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d027811c46f6bcc2f98c1a4e3e1194fb0f9bc48a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed8f8614f4433d7c0092b1e28408a92301f57e03",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b389edd1ce4fa836cfdff4f6c16a067aa332f6a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "RQPjjxfFn-CT5m0VSeqTmjIHMcTtxm4IvmImXdu-_SU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mduvcv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_Nix_User",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mduvcv/lightweight_chatgpt_client_using_your_own_api_key/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mduvcv/lightweight_chatgpt_client_using_your_own_api_key/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753945857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_6suhydu8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM4.5 EQ-Bench and Creative Write",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md5k8f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 134,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 134,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/7v_DJnb0Q3ULI7Mg3Ucw4Sc2Y1CnOwyJfcCGQCPhmJ4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753879322,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ubwsl0gdb0gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ubwsl0gdb0gf1.jpeg?auto=webp&amp;s=4e43b4753bf20f50777316c9069a22f6e1bc9ffe",
                  "width": 1189,
                  "height": 2048
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ubwsl0gdb0gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=83a0215afba98d96c58ddd8953cf946627a96b87",
                    "width": 108,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/ubwsl0gdb0gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cb81fe92d2ef9d95d5c9c9b3512072b4195b7e3",
                    "width": 216,
                    "height": 372
                  },
                  {
                    "url": "https://preview.redd.it/ubwsl0gdb0gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa00a913d179ff148233c1e9f131915282446a19",
                    "width": 320,
                    "height": 551
                  },
                  {
                    "url": "https://preview.redd.it/ubwsl0gdb0gf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=abdf15ff9928a1e321306852523e66da9ac4b1cf",
                    "width": 640,
                    "height": 1102
                  },
                  {
                    "url": "https://preview.redd.it/ubwsl0gdb0gf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b29bfc227bbc65ed304a2107a162b0cbeeaa5d1b",
                    "width": 960,
                    "height": 1653
                  },
                  {
                    "url": "https://preview.redd.it/ubwsl0gdb0gf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05ae28405c4978e66fd2d7f2517cb75addd8bf66",
                    "width": 1080,
                    "height": 1860
                  }
                ],
                "variants": {},
                "id": "7_4P0DMZtwaTeczmzHUyBCOKOwxJl2ysfZ1o0jG37ww"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1md5k8f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pcdacks",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md5k8f/glm45_eqbench_and_creative_write/",
          "stickied": false,
          "url": "https://i.redd.it/ubwsl0gdb0gf1.jpeg",
          "subreddit_subscribers": 507576,
          "created_utc": 1753879322,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A model that can extract text with surprisingly good quality and decent speed — even on an 8GB RAM, CPU-only machine.\n\nI've been looking for a way to extract text on a low-spec computer for a while now. After trying many solutions, I'm honestly impressed by what this \\~3GB model can do. It's like the Doom II of vision-language models: lightweight, efficient, and it just works.\n\n",
          "author_fullname": "t2_s2tumbcp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama with Qwen2.5VL:3B – The Doom II of VLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdwu18",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753953565,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A model that can extract text with surprisingly good quality and decent speed — even on an 8GB RAM, CPU-only machine.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking for a way to extract text on a low-spec computer for a while now. After trying many solutions, I&amp;#39;m honestly impressed by what this ~3GB model can do. It&amp;#39;s like the Doom II of vision-language models: lightweight, efficient, and it just works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdwu18",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ML-Future",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdwu18/ollama_with_qwen25vl3b_the_doom_ii_of_vlms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdwu18/ollama_with_qwen25vl3b_the_doom_ii_of_vlms/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753953565,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the best local model to run these days on 8 GB RAM card that can read images with text in them?",
          "author_fullname": "t2_9bpxo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local LLM that can read text in images? (8 GB graphic card)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdrxio",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753935344,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best local model to run these days on 8 GB RAM card that can read images with text in them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdrxio",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fariazz",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdrxio/best_local_llm_that_can_read_text_in_images_8_gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdrxio/best_local_llm_that_can_read_text_in_images_8_gb/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753935344,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This questions has been bothering me for a while and has prevented me from ”investing” on training and fine tuning a model since the next big thing is just around the corner.\n\nMaybe there’s a simple solution to this that I’m missing but:\n\nFirst problem: How do you choose which open source model to fine-tune or further train when there are so many to choose from?\n\nSubsequent problem after solving first problem: \nlet’s say you go with the latest llama, but then alibaba releases a killer llm thats open source and open weight, like imagine they release qwen-4 that beats GPT-5 on some benchmarks. \n\nHow do you ”transfer” the training and fine tuning you have done to a new model? \n\nEven if you decide to stay on llama, is the training and fine tuning compatible with the next version of llama? \n\nThe only ”transferable” solution I can think of is RAG (at least I think you could just connect any model to a RAG db independently but correct me if I’m wrong). But this is not training/fine-tuning so it won’t be feasible for all use cases. \n\nLet me know what your take is on this. Would greatly appreciate it! \n\n",
          "author_fullname": "t2_1ttp8mwcgv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to future proof fine tuning and/or training",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdw7v7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753951145,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This questions has been bothering me for a while and has prevented me from ”investing” on training and fine tuning a model since the next big thing is just around the corner.&lt;/p&gt;\n\n&lt;p&gt;Maybe there’s a simple solution to this that I’m missing but:&lt;/p&gt;\n\n&lt;p&gt;First problem: How do you choose which open source model to fine-tune or further train when there are so many to choose from?&lt;/p&gt;\n\n&lt;p&gt;Subsequent problem after solving first problem: \nlet’s say you go with the latest llama, but then alibaba releases a killer llm thats open source and open weight, like imagine they release qwen-4 that beats GPT-5 on some benchmarks. &lt;/p&gt;\n\n&lt;p&gt;How do you ”transfer” the training and fine tuning you have done to a new model? &lt;/p&gt;\n\n&lt;p&gt;Even if you decide to stay on llama, is the training and fine tuning compatible with the next version of llama? &lt;/p&gt;\n\n&lt;p&gt;The only ”transferable” solution I can think of is RAG (at least I think you could just connect any model to a RAG db independently but correct me if I’m wrong). But this is not training/fine-tuning so it won’t be feasible for all use cases. &lt;/p&gt;\n\n&lt;p&gt;Let me know what your take is on this. Would greatly appreciate it! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdw7v7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AI-On-A-Dime",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdw7v7/how_to_future_proof_fine_tuning_andor_training/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdw7v7/how_to_future_proof_fine_tuning_andor_training/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753951145,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "📈 Introducing [AutoRL](https://github.com/OpenPipe/ART/tree/auto-rl?tab=readme-ov-file#-autorl-train-models-for-any-task), simple architecture for specializing Qwen and other OSS models for any task.\n\n**Technique breakdown:**\n\n1. User defines task\n2. AutoRL generates 30 sample scenarios for which agent must perform task\n3. Agent runs through 25 training samples using GRPO to improve for specified number of epochs\n4. Agent is tested on remaining 5 test samples against SOTA models (like Sonnet 4)\n\nBuilt on top of OpenPipe's [ART](https://github.com/OpenPipe/ART/tree/auto-rl) and uses [RULER](https://art.openpipe.ai/fundamentals/ruler) as its reward function. It's quite easy to get started with.\n\nSample Colab notebook: [https://colab.research.google.com/github/openpipe/art/blob/main/examples/auto\\_rl.ipynb](https://colab.research.google.com/github/openpipe/art/blob/main/examples/auto_rl.ipynb)",
          "author_fullname": "t2_vkqot2yy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AutoRL \"vibe-training\" for open models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdgeww",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753904666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;📈 Introducing &lt;a href=\"https://github.com/OpenPipe/ART/tree/auto-rl?tab=readme-ov-file#-autorl-train-models-for-any-task\"&gt;AutoRL&lt;/a&gt;, simple architecture for specializing Qwen and other OSS models for any task.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Technique breakdown:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;User defines task&lt;/li&gt;\n&lt;li&gt;AutoRL generates 30 sample scenarios for which agent must perform task&lt;/li&gt;\n&lt;li&gt;Agent runs through 25 training samples using GRPO to improve for specified number of epochs&lt;/li&gt;\n&lt;li&gt;Agent is tested on remaining 5 test samples against SOTA models (like Sonnet 4)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Built on top of OpenPipe&amp;#39;s &lt;a href=\"https://github.com/OpenPipe/ART/tree/auto-rl\"&gt;ART&lt;/a&gt; and uses &lt;a href=\"https://art.openpipe.ai/fundamentals/ruler\"&gt;RULER&lt;/a&gt; as its reward function. It&amp;#39;s quite easy to get started with.&lt;/p&gt;\n\n&lt;p&gt;Sample Colab notebook: &lt;a href=\"https://colab.research.google.com/github/openpipe/art/blob/main/examples/auto_rl.ipynb\"&gt;https://colab.research.google.com/github/openpipe/art/blob/main/examples/auto_rl.ipynb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdgeww",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arctic_fly",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdgeww/autorl_vibetraining_for_open_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdgeww/autorl_vibetraining_for_open_models/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753904666,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI'm currently fine tuning a Llama 3.1 instruct model with my own dataset (train/test split done before data augmentation on train set), and i have noticed than no matter what parameters i change or if i split my datas without Data Augmentation, the training loss is always higher than the validation loss. \n\nFor example, right now i have : \n\n|Step|Training Loss|Validation Loss|\n|:-|:-|:-|\n|50|0.780700|0.496057|\n|100|0.328300|0.294750|\n|150|0.241100|0.251495|\n\nIn some other cases, training loss will approach the val loss value but won't go under it.\n\nIs there any reasons why this happens ? Is it risky or a normal case ? ",
          "author_fullname": "t2_1f72n02g6d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Training loss is higher than validation loss for a few steps",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdudj3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753943977,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently fine tuning a Llama 3.1 instruct model with my own dataset (train/test split done before data augmentation on train set), and i have noticed than no matter what parameters i change or if i split my datas without Data Augmentation, the training loss is always higher than the validation loss. &lt;/p&gt;\n\n&lt;p&gt;For example, right now i have : &lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Step&lt;/th&gt;\n&lt;th align=\"left\"&gt;Training Loss&lt;/th&gt;\n&lt;th align=\"left\"&gt;Validation Loss&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;50&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.780700&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.496057&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.328300&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.294750&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;150&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.241100&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.251495&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;In some other cases, training loss will approach the val loss value but won&amp;#39;t go under it.&lt;/p&gt;\n\n&lt;p&gt;Is there any reasons why this happens ? Is it risky or a normal case ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdudj3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Head_Mushroom_3748",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdudj3/training_loss_is_higher_than_validation_loss_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdudj3/training_loss_is_higher_than_validation_loss_for/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753943977,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was so impressed the first time i used kokoro tts and i was just wondering, there must have more of these extra voices where i can download in github right or are those default voice packs the only ones there is?",
          "author_fullname": "t2_hepm3i6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a way to download more Kokoro tts voices?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdu9gr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753943546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was so impressed the first time i used kokoro tts and i was just wondering, there must have more of these extra voices where i can download in github right or are those default voice packs the only ones there is?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdu9gr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jacob12have",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdu9gr/is_there_a_way_to_download_more_kokoro_tts_voices/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdu9gr/is_there_a_way_to_download_more_kokoro_tts_voices/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753943546,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://mistral.ai/news/codestral-25-08](https://mistral.ai/news/codestral-25-08)\n\n  \nMistral just release new version of codestral &amp; entire coding stack.\n\n  \nbut god.. for enterprise only.. the heck ? don't understand the move of blocking usual coder &amp; shadow it \\^\\^'",
          "author_fullname": "t2_50q8lbft",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Complete Mistral Coding Stack but for enterprise only",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdj5ww",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753911149,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://mistral.ai/news/codestral-25-08\"&gt;https://mistral.ai/news/codestral-25-08&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Mistral just release new version of codestral &amp;amp; entire coding stack.&lt;/p&gt;\n\n&lt;p&gt;but god.. for enterprise only.. the heck ? don&amp;#39;t understand the move of blocking usual coder &amp;amp; shadow it ^^&amp;#39;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?auto=webp&amp;s=fe19c20c363332d32b7f6d8917f3febce9133568",
                  "width": 4800,
                  "height": 2520
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=757c6641896f42b25e4c88e87dc438f1e8d270bb",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4e78d09c1d0842276f98a4a7745457d7c7c5171",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4df6ded6329ae09fc0e110879f55f893298c17b4",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e49bc517b9cd96d953bfc71387ecf137efddf97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f52f14c1247d26b63fd222b2cb6756d88234d2f0",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdj5ww",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Edereum",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdj5ww/complete_mistral_coding_stack_but_for_enterprise/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdj5ww/complete_mistral_coding_stack_but_for_enterprise/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753911149,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for a model to sound out some dead languages (such as Old English, Proto-Germanic, Proto-Indo-European)\n\nIs there a TTS model which receives IPA characters or any other form of phonetic notation directly (maybe also with a language tag for the accent)?",
          "author_fullname": "t2_8d96rmrp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If there a TTS model that works with IPA?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdrdal",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753933596,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a model to sound out some dead languages (such as Old English, Proto-Germanic, Proto-Indo-European)&lt;/p&gt;\n\n&lt;p&gt;Is there a TTS model which receives IPA characters or any other form of phonetic notation directly (maybe also with a language tag for the accent)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdrdal",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "schattig_eenhoorntje",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdrdal/if_there_a_tts_model_that_works_with_ipa/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdrdal/if_there_a_tts_model_that_works_with_ipa/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753933596,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When I run nvidia-smi, It shows that I only have access to 1/8 of the GPU. \n\nhttps://preview.redd.it/6zgjw8o035gf1.png?width=1418&amp;format=png&amp;auto=webp&amp;s=8ead6b954f5e7370b532380b2597ad96fca7a924\n\nAlso, when I ran \n\nvastai show instance 24505676\n\nI saw the gpu\\_frac was only 1/8. I thought I was going to get my own exclusive gpu; am I just getting screwed by not having my own gpu? I am on on-demand pricing as well, and the support told me that I should be getting my own gpu too! This happened to two different gpus I was using; one of them in Hungary, and the other one in France. I am running some experiments and I got hit with 1/8 fraction of a gpu which makes it unusable. ",
          "author_fullname": "t2_85dm4edx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is vast.ai fucking me over?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 66,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6zgjw8o035gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/6zgjw8o035gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a57b631046132962205e9fc441efb8bbe7068694"
                },
                {
                  "y": 102,
                  "x": 216,
                  "u": "https://preview.redd.it/6zgjw8o035gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39d1ddb97194f7b9bbb9e4a9e458d04d00ce0c05"
                },
                {
                  "y": 151,
                  "x": 320,
                  "u": "https://preview.redd.it/6zgjw8o035gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4ca27f009f5ee9d84904941aba33d05125792ff7"
                },
                {
                  "y": 303,
                  "x": 640,
                  "u": "https://preview.redd.it/6zgjw8o035gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1eeb4ba01207a4d2938db36bda24d101a9434b7f"
                },
                {
                  "y": 454,
                  "x": 960,
                  "u": "https://preview.redd.it/6zgjw8o035gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=93c11aae79335ef6957b114bc3b0f859c67d8e3f"
                },
                {
                  "y": 511,
                  "x": 1080,
                  "u": "https://preview.redd.it/6zgjw8o035gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad0b228f267c197f0117190bd313c7ec5baa9874"
                }
              ],
              "s": {
                "y": 672,
                "x": 1418,
                "u": "https://preview.redd.it/6zgjw8o035gf1.png?width=1418&amp;format=png&amp;auto=webp&amp;s=8ead6b954f5e7370b532380b2597ad96fca7a924"
              },
              "id": "6zgjw8o035gf1"
            }
          },
          "name": "t3_1mdsgax",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3HKrXVYxO1rl9lTgekatAlKYnQBHtY8bjzTt2_6y0Hg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753937106,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I run nvidia-smi, It shows that I only have access to 1/8 of the GPU. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6zgjw8o035gf1.png?width=1418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ead6b954f5e7370b532380b2597ad96fca7a924\"&gt;https://preview.redd.it/6zgjw8o035gf1.png?width=1418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ead6b954f5e7370b532380b2597ad96fca7a924&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also, when I ran &lt;/p&gt;\n\n&lt;p&gt;vastai show instance 24505676&lt;/p&gt;\n\n&lt;p&gt;I saw the gpu_frac was only 1/8. I thought I was going to get my own exclusive gpu; am I just getting screwed by not having my own gpu? I am on on-demand pricing as well, and the support told me that I should be getting my own gpu too! This happened to two different gpus I was using; one of them in Hungary, and the other one in France. I am running some experiments and I got hit with 1/8 fraction of a gpu which makes it unusable. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdsgax",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jfang00007",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdsgax/is_vastai_fucking_me_over/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdsgax/is_vastai_fucking_me_over/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753937106,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have seen lot of new research talking about RLVR and I want to understand how people are defining rewards for more subjective tasks , like even in coding itself if a code runs doesn't necessarily mean that it has achieved what we have in the intial prompt , if there are any other blogs or research papers that you can refer me too , it would be very interesting as well ",
          "author_fullname": "t2_1or28yerix",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have been learning more about reinforcement learning with verifiable rewards I want to hear people's opinions on that",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdro7c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/K4j4leZFjzRMFzlFxpZ8RxRR5d_59ODQV1yrZGnmTzs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753934522,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen lot of new research talking about RLVR and I want to understand how people are defining rewards for more subjective tasks , like even in coding itself if a code runs doesn&amp;#39;t necessarily mean that it has achieved what we have in the intial prompt , if there are any other blogs or research papers that you can refer me too , it would be very interesting as well &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/y66synvtv4gf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/y66synvtv4gf1.jpeg?auto=webp&amp;s=0c7441bfe9613832920f3979902c23805aee2d8a",
                  "width": 1440,
                  "height": 3168
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/y66synvtv4gf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5cbe9d7cd5c35fff63200b6eed7dcac03cf6a088",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/y66synvtv4gf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c521a85758df7c47ed4d3f1f0a32a1ab361d70dd",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/y66synvtv4gf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d627f98f2630c4d7fb9840cae77ce953568abb08",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/y66synvtv4gf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e36004d539d7a132b17ba186b07f0807da5a1183",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/y66synvtv4gf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a7744d9cea0d00d92bb01f6ab56546c118453e8",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/y66synvtv4gf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a2f9a59b82f4d0a538ab9d541a8a9cc0a1bb7b0e",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "7FYtlChA2TzIIasJ-RCsYjQhcZNNlvdbVLzqhXfV6Pw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdro7c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Able_Transition_1692",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdro7c/i_have_been_learning_more_about_reinforcement/",
          "stickied": false,
          "url": "https://i.redd.it/y66synvtv4gf1.jpeg",
          "subreddit_subscribers": 507576,
          "created_utc": 1753934522,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Qwen3 30B A3B Thinking GGUF](https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF)\n[Devstral Small 1.1 GGUF](https://huggingface.co/unsloth/Devstral-Small-2507-GGUF)\n\nQwen essentially set up the code and Devstral debugged it. Devstral added the nice Web Audio sound effects while Qwen implemented the halway decent particle effects. Both models are Apache 2.0, and I'm super thrilled to see what the coder variant of this Qwen model can do when it releases soon.\n\n```\nCreate a clone of the Atart game Breakout using HTML/CSS/JS without external deps. It should feature spark and explosion effects, Web Audio API sound effects, and shaded lighting from the light effects. Particle effects would also be a bonus. It should incorporate a level system where the speed of the ball increases with each level.\n```\n\nThis was the base prompt I provided to Qwen, but I provided a few error messages from the JS console to Devstral to fix with some extra feedback about the sound effects.\n\nNot sure what this really shows, aside from the fact that smaller models can keep pace with [GLM 4.5](https://simonwillison.net/2025/Jul/29/space-invaders/) if you're willing to do a marginal amount of extra work. I didn't dilligently check if everything in my original prompt was added, but I'm positive Devstral could add anything that was missing.",
          "author_fullname": "t2_bzm5pqm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Breakout clone by Devstral and Qwen3 30B A3B Thinking with particle effects and Web Audio reverb.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdu8p0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753943463,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "codepen.io",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF\"&gt;Qwen3 30B A3B Thinking GGUF&lt;/a&gt;\n&lt;a href=\"https://huggingface.co/unsloth/Devstral-Small-2507-GGUF\"&gt;Devstral Small 1.1 GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen essentially set up the code and Devstral debugged it. Devstral added the nice Web Audio sound effects while Qwen implemented the halway decent particle effects. Both models are Apache 2.0, and I&amp;#39;m super thrilled to see what the coder variant of this Qwen model can do when it releases soon.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nCreate a clone of the Atart game Breakout using HTML/CSS/JS without external deps. It should feature spark and explosion effects, Web Audio API sound effects, and shaded lighting from the light effects. Particle effects would also be a bonus. It should incorporate a level system where the speed of the ball increases with each level.\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This was the base prompt I provided to Qwen, but I provided a few error messages from the JS console to Devstral to fix with some extra feedback about the sound effects.&lt;/p&gt;\n\n&lt;p&gt;Not sure what this really shows, aside from the fact that smaller models can keep pace with &lt;a href=\"https://simonwillison.net/2025/Jul/29/space-invaders/\"&gt;GLM 4.5&lt;/a&gt; if you&amp;#39;re willing to do a marginal amount of extra work. I didn&amp;#39;t dilligently check if everything in my original prompt was added, but I&amp;#39;m positive Devstral could add anything that was missing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://codepen.io/mars-and-bars/full/OPyWjMy",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mdu8p0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EuphoricPenguin22",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdu8p0/breakout_clone_by_devstral_and_qwen3_30b_a3b/",
          "stickied": false,
          "url": "https://codepen.io/mars-and-bars/full/OPyWjMy",
          "subreddit_subscribers": 507576,
          "created_utc": 1753943463,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a mac and whenever a new model launches, I see MLX quants available in a day or two. However GGUF takes more time due to llama.cpp support.\nRecent example is GLM 4.5\n\nI’m just genuinely curious to know, what makes it easy or faster to add support in MLX.",
          "author_fullname": "t2_jqxb4pte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I’m curious to know how does MLX adds support for models faster than llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdgjmk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753904976,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a mac and whenever a new model launches, I see MLX quants available in a day or two. However GGUF takes more time due to llama.cpp support.\nRecent example is GLM 4.5&lt;/p&gt;\n\n&lt;p&gt;I’m just genuinely curious to know, what makes it easy or faster to add support in MLX.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdgjmk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Conversation9561",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753904976,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aq4j0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork/Skywork-UniPic-1.5B - A unified autoregressive multimodal model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md6xba",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=413cced97f3092a1eed0608ef35af14399127022",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753882910,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Skywork/Skywork-UniPic-1.5B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?auto=webp&amp;s=70fa96479cc1130605cbd953708290c9c2451047",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b5a56d483feb6c6fe6d147e33f41564f7ddb5d83",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d05014f3679d8822c88447adc8f7e5a4a8dd79b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f53d12084f362b24ff67995b18dc1387e078894f",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0deccacf66db69a58065546ea92e99fae0e3c4d6",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=56a86893bf864bf19cc4b13989bfb890f8238784",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=513819bcc81837c9baeae4d494e9a08f8862c825",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1md6xba",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nullmove",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md6xba/skyworkskyworkunipic15b_a_unified_autoregressive/",
          "stickied": false,
          "url": "https://huggingface.co/Skywork/Skywork-UniPic-1.5B",
          "subreddit_subscribers": 507576,
          "created_utc": 1753882910,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been playing around with it - seems promising but I haven't investigated it thoroughly.  Does anyone have any experience with this model?  ",
          "author_fullname": "t2_47ws19uq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone have experience with NVIDIA Nemotron?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdoevz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753924941,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been playing around with it - seems promising but I haven&amp;#39;t investigated it thoroughly.  Does anyone have any experience with this model?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdoevz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "seoulsrvr",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdoevz/anyone_have_experience_with_nvidia_nemotron/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdoevz/anyone_have_experience_with_nvidia_nemotron/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753924941,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I found this post really worth reading.\n\n[https://x.com/deep\\_reinforce/status/1950654480023957646](https://x.com/deep_reinforce/status/1950654480023957646)\n\nLarge language models can write CUDA kernels. Does this mean that one day LLMs can evolve 100% by themselves?",
          "author_fullname": "t2_1u46l0rfuj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CUDA-L1 Improving CUDA Optimization via Contrastive Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdj3ap",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753910979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found this post really worth reading.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://x.com/deep_reinforce/status/1950654480023957646\"&gt;https://x.com/deep_reinforce/status/1950654480023957646&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Large language models can write CUDA kernels. Does this mean that one day LLMs can evolve 100% by themselves?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdj3ap",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Optimal-Outcome-7458",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdj3ap/cudal1_improving_cuda_optimization_via/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdj3ap/cudal1_improving_cuda_optimization_via/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753910979,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The Qwen3-30B-A3B-Instruct-2507 is an amazing release! Congratulations!\n\nHowever, the three-month-old 32B shows better performance across the board in the benchmark. I hope the Qwen3-32B Instruct/Thinking and Qwen3-30B-A3B-Thinking-2507 versions will be released soon!\n",
          "author_fullname": "t2_73xg2fw4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kudos to Qwen 3 team!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "nhnd3nuqpyff1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/nhnd3nuqpyff1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=b4a4777fbf15585877c329aa6410fda352472d5f"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/nhnd3nuqpyff1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=abf4196420500497c21a76dd763aca9fb69ee29d"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/nhnd3nuqpyff1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=f5237980d5b7d1de65245857661037126f6a750f"
                }
              ],
              "s": {
                "y": 480,
                "gif": "https://i.redd.it/nhnd3nuqpyff1.gif",
                "mp4": "https://preview.redd.it/nhnd3nuqpyff1.gif?format=mp4&amp;s=a04a1653064ec390929718dc0ed20a8ecf51da29",
                "x": 480
              },
              "id": "nhnd3nuqpyff1"
            }
          },
          "name": "t3_1md00oc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 135,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 135,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mUsja5QiJMisNYHJhZA4P57qdlHnaGZYvKopTiB-51E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753859844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Qwen3-30B-A3B-Instruct-2507 is an amazing release! Congratulations!&lt;/p&gt;\n\n&lt;p&gt;However, the three-month-old 32B shows better performance across the board in the benchmark. I hope the Qwen3-32B Instruct/Thinking and Qwen3-30B-A3B-Thinking-2507 versions will be released soon!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1md00oc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExcuseAccomplished97",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753859844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Today Mark shared Meta’s vision for the future of personal superintelligence for everyone. \n\nRedditors!! What's your take on this?\n\n  \nRead his full letter here: [https://www.meta.com/superintelligence/](https://www.meta.com/superintelligence/)",
          "author_fullname": "t2_yy8p0c7c1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Meta’s Vision for the future of Personal SuperIntelligence",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ak4gkwuip0gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/ak4gkwuip0gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=773a23eaf9c5d19c8f53df55e6712f7e2abe38ce"
                },
                {
                  "y": 173,
                  "x": 216,
                  "u": "https://preview.redd.it/ak4gkwuip0gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2db384603ee33e36dda023bd869eda28422ab019"
                },
                {
                  "y": 256,
                  "x": 320,
                  "u": "https://preview.redd.it/ak4gkwuip0gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cede28156e303d495033b501a7169105a576531"
                },
                {
                  "y": 513,
                  "x": 640,
                  "u": "https://preview.redd.it/ak4gkwuip0gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=40431ec0fc3cef48d9a8640791f257e0c5a5ba1d"
                },
                {
                  "y": 769,
                  "x": 960,
                  "u": "https://preview.redd.it/ak4gkwuip0gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=50c8e9c5f2e528e884871ddc600c247092a5c47c"
                }
              ],
              "s": {
                "y": 804,
                "x": 1003,
                "u": "https://preview.redd.it/ak4gkwuip0gf1.png?width=1003&amp;format=png&amp;auto=webp&amp;s=50518813cc26e3008512596537b0244b7f7d7bfd"
              },
              "id": "ak4gkwuip0gf1"
            },
            "nma48mbip0gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 116,
                  "x": 108,
                  "u": "https://preview.redd.it/nma48mbip0gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45b141aa715350c5cf90daf9b4ce66b31a405717"
                },
                {
                  "y": 233,
                  "x": 216,
                  "u": "https://preview.redd.it/nma48mbip0gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=27f36c87815d847d95ed0953f720624e5dd00907"
                },
                {
                  "y": 346,
                  "x": 320,
                  "u": "https://preview.redd.it/nma48mbip0gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d79e7faf16f990cb6b9aceaa5974fb23a3383b5"
                },
                {
                  "y": 692,
                  "x": 640,
                  "u": "https://preview.redd.it/nma48mbip0gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a6151c3533b2a6e46bd409f9159150042b3455f"
                },
                {
                  "y": 1039,
                  "x": 960,
                  "u": "https://preview.redd.it/nma48mbip0gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fae77876d90fee57700961b5e580098369fb76d9"
                }
              ],
              "s": {
                "y": 1140,
                "x": 1053,
                "u": "https://preview.redd.it/nma48mbip0gf1.png?width=1053&amp;format=png&amp;auto=webp&amp;s=7ed444fd96d625288f2026d021149c14c2826b2a"
              },
              "id": "nma48mbip0gf1"
            }
          },
          "name": "t3_1md7h5z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 36,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "nma48mbip0gf1",
                "id": 717369387
              },
              {
                "media_id": "ak4gkwuip0gf1",
                "id": 717369388
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wTDzxveLNV6lyThS6Dq2WJK_bRtGmie5PYzTvcrT62c.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753884282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today Mark shared Meta’s vision for the future of personal superintelligence for everyone. &lt;/p&gt;\n\n&lt;p&gt;Redditors!! What&amp;#39;s your take on this?&lt;/p&gt;\n\n&lt;p&gt;Read his full letter here: &lt;a href=\"https://www.meta.com/superintelligence/\"&gt;https://www.meta.com/superintelligence/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1md7h5z",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1md7h5z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "5h3r_10ck",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md7h5z/metas_vision_for_the_future_of_personal/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1md7h5z",
          "subreddit_subscribers": 507576,
          "created_utc": 1753884282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_g35vlbqkh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The DGX Spark JPN price will be $6k at one retailer",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdnp8j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753922908,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/ottoserver/status/1950366390151762172",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdnp8j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Django_McFly",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdnp8j/the_dgx_spark_jpn_price_will_be_6k_at_one_retailer/",
          "stickied": false,
          "url": "https://x.com/ottoserver/status/1950366390151762172",
          "subreddit_subscribers": 507576,
          "created_utc": 1753922908,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/zhuqp1hcv3gf1.png?width=1502&amp;format=png&amp;auto=webp&amp;s=d6a8de5f1b26a95ef63e404ba1fe4ec8d34908b8\n\nYeesh, I wouldn't mind if it gave the Chinese perspective and international perspective but this is something else, and exactly the kind of deceptive agenda pushing behaviour I asked this question due to my suspicions of in the first place.\n\nEdit: I just got four separate accounts post within moments of each other proclaiming verbatim that there was no genocide and Qwen is speaking objective truth without actually engaging with the underlying issues here beyond the politics of the issue, god is this sub ever botted.",
          "author_fullname": "t2_14t2wz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ideological alignment at its finest",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 135,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "zhuqp1hcv3gf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 104,
                  "x": 108,
                  "u": "https://preview.redd.it/zhuqp1hcv3gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=281f5a71750b7b4032dc2b8b955f7441039fc7e5"
                },
                {
                  "y": 208,
                  "x": 216,
                  "u": "https://preview.redd.it/zhuqp1hcv3gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4794cc46b941ebf55153202bab16659bd263c843"
                },
                {
                  "y": 309,
                  "x": 320,
                  "u": "https://preview.redd.it/zhuqp1hcv3gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3957901d7116b2e2adfd9cd553cc5e7f06349b12"
                },
                {
                  "y": 618,
                  "x": 640,
                  "u": "https://preview.redd.it/zhuqp1hcv3gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=39d07e356e5007b67a56d132355dab897fb2b768"
                },
                {
                  "y": 928,
                  "x": 960,
                  "u": "https://preview.redd.it/zhuqp1hcv3gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0067ec7afbf6ca73a339943872d86156a6d93937"
                },
                {
                  "y": 1044,
                  "x": 1080,
                  "u": "https://preview.redd.it/zhuqp1hcv3gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cdf218872f86b975de6efe69a23264417a3377dd"
                }
              ],
              "s": {
                "y": 1452,
                "x": 1502,
                "u": "https://preview.redd.it/zhuqp1hcv3gf1.png?width=1502&amp;format=png&amp;auto=webp&amp;s=d6a8de5f1b26a95ef63e404ba1fe4ec8d34908b8"
              },
              "id": "zhuqp1hcv3gf1"
            }
          },
          "name": "t3_1mdnhb1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CGzz1mPCMQ0a-Np8-fR_M4lBi9nI0dxQP4Q53jFnbZU.jpg",
          "edited": 1753924810,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753922286,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/zhuqp1hcv3gf1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6a8de5f1b26a95ef63e404ba1fe4ec8d34908b8\"&gt;https://preview.redd.it/zhuqp1hcv3gf1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6a8de5f1b26a95ef63e404ba1fe4ec8d34908b8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Yeesh, I wouldn&amp;#39;t mind if it gave the Chinese perspective and international perspective but this is something else, and exactly the kind of deceptive agenda pushing behaviour I asked this question due to my suspicions of in the first place.&lt;/p&gt;\n\n&lt;p&gt;Edit: I just got four separate accounts post within moments of each other proclaiming verbatim that there was no genocide and Qwen is speaking objective truth without actually engaging with the underlying issues here beyond the politics of the issue, god is this sub ever botted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdnhb1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MerePotato",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdnhb1/ideological_alignment_at_its_finest/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdnhb1/ideological_alignment_at_its_finest/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753922286,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m doing preliminary research on open source (and open weight) AI for my uni and I was wondering, how do most people actually engage with released models? Is it mainly to run inference? Do most people run models locally? Are people fine-tuning models themselves, or is that rarely ever the case?\n\nAdditionally, when compared to (non-AI) open source software, to what degree is it possible for individuals to contribute back to the open source community? Or is that only feasible for well-financed research organizations/companies?\n\nSo far, when I’ve searched these things, I find answers relating to businesses, but I’m curious about individuals or smaller teams.",
          "author_fullname": "t2_1elzdijdrl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do people engage with open source AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdwums",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753953629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m doing preliminary research on open source (and open weight) AI for my uni and I was wondering, how do most people actually engage with released models? Is it mainly to run inference? Do most people run models locally? Are people fine-tuning models themselves, or is that rarely ever the case?&lt;/p&gt;\n\n&lt;p&gt;Additionally, when compared to (non-AI) open source software, to what degree is it possible for individuals to contribute back to the open source community? Or is that only feasible for well-financed research organizations/companies?&lt;/p&gt;\n\n&lt;p&gt;So far, when I’ve searched these things, I find answers relating to businesses, but I’m curious about individuals or smaller teams.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdwums",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EstusFlaskCrochet",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdwums/how_do_people_engage_with_open_source_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdwums/how_do_people_engage_with_open_source_ai/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753953629,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After getting helpful feedback from you all, our team just shipped \"Recipes” which are pre-built, fully-runnable workflows for common LLM tasks.\n\n**Some of the most popular recipes include:**\n\n* **Llama 3.2 1B fine-tuning** (with Apple Silicon MLX optimization!)\n* **Model quantization to GGUF** format (CPU and GPU)\n* **Benchmark evaluation** (MMLU, HellaSwag, PIQA, Winogrande)\n* **LoRA training** with before/after comparisons\n* **Dialogue summarization** (perfect for chat logs)\n\nWe support local hardware (CUDA, AMD ROCm, Apple MLX, or CPU) and let you modify anything: model, data, params. Zero config to get started and we’re open source.\n\nBeen testing the Llama 3.2 fine-tuning recipe and the results are great. Way faster than setting everything up from scratch. \n\nWhat local training workflows are you all using? This seems like it could replace a lot of custom scripts. Appreciate your feedback. What recipes should we add?\n\n🔗 Try it here →[ ](https://transformerlab.ai/docs/intro)[https://transformerlab.ai/](https://transformerlab.ai/)\n\n🔗 Useful? Please star us on GitHub → [https://github.com/transformerlab/transformerlab-app](https://github.com/transformerlab)\n\n🔗 Ask for help on our Discord Community → [https://discord.gg/transformerlab](https://discord.gg/transformerlab)",
          "author_fullname": "t2_174trr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just launched Transformer Lab Recipes: 13 pre-built templates including Llama 3.2 fine-tuning, quantization, and benchmarking.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdawyz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gTiaYDispRDRp6VGHHJWJiHFDBwm1-JGVjVl3ZRXfFI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753892255,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After getting helpful feedback from you all, our team just shipped &amp;quot;Recipes” which are pre-built, fully-runnable workflows for common LLM tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Some of the most popular recipes include:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Llama 3.2 1B fine-tuning&lt;/strong&gt; (with Apple Silicon MLX optimization!)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model quantization to GGUF&lt;/strong&gt; format (CPU and GPU)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Benchmark evaluation&lt;/strong&gt; (MMLU, HellaSwag, PIQA, Winogrande)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LoRA training&lt;/strong&gt; with before/after comparisons&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dialogue summarization&lt;/strong&gt; (perfect for chat logs)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We support local hardware (CUDA, AMD ROCm, Apple MLX, or CPU) and let you modify anything: model, data, params. Zero config to get started and we’re open source.&lt;/p&gt;\n\n&lt;p&gt;Been testing the Llama 3.2 fine-tuning recipe and the results are great. Way faster than setting everything up from scratch. &lt;/p&gt;\n\n&lt;p&gt;What local training workflows are you all using? This seems like it could replace a lot of custom scripts. Appreciate your feedback. What recipes should we add?&lt;/p&gt;\n\n&lt;p&gt;🔗 Try it here →&lt;a href=\"https://transformerlab.ai/docs/intro\"&gt; &lt;/a&gt;&lt;a href=\"https://transformerlab.ai/\"&gt;https://transformerlab.ai/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;🔗 Useful? Please star us on GitHub → &lt;a href=\"https://github.com/transformerlab\"&gt;https://github.com/transformerlab/transformerlab-app&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;🔗 Ask for help on our Discord Community → &lt;a href=\"https://discord.gg/transformerlab\"&gt;https://discord.gg/transformerlab&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/x7gqer73e1gf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/x7gqer73e1gf1.gif?format=png8&amp;s=c74ba9bdfe89209486e53cf396a6f32eb10a4488",
                  "width": 800,
                  "height": 419
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=0342efc957ca137fc751f44764d13ab26e641a0d",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=339a56848c47f1a97dfa5b053803db0b971f93fe",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=41329e787e25fdaa8ec710505568979cc448baa4",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=090737fdc13594b8cb74721d8ac848c025fb6582",
                    "width": 640,
                    "height": 335
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/x7gqer73e1gf1.gif?s=06dab8b87adec6f6ccd8caa4cd3679ab57c0a727",
                      "width": 800,
                      "height": 419
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=108&amp;crop=smart&amp;s=0315e091f8c5a2658c1251c7e34d002ad735fe2b",
                        "width": 108,
                        "height": 56
                      },
                      {
                        "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=216&amp;crop=smart&amp;s=a12658abfd139388f99a1c3dbd386bd3eb3965bb",
                        "width": 216,
                        "height": 113
                      },
                      {
                        "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=320&amp;crop=smart&amp;s=3e388f4ba8e9db8844174de55c131e4eef4007c4",
                        "width": 320,
                        "height": 167
                      },
                      {
                        "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=640&amp;crop=smart&amp;s=59a0719796cc2073a0df325ec3f1a9ef590559cd",
                        "width": 640,
                        "height": 335
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/x7gqer73e1gf1.gif?format=mp4&amp;s=1606aaae910f030cb7d1f2b672896e1f7ee329f2",
                      "width": 800,
                      "height": 419
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=108&amp;format=mp4&amp;s=f4d9882643c496c458088977d2c6af82b0111017",
                        "width": 108,
                        "height": 56
                      },
                      {
                        "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=216&amp;format=mp4&amp;s=9524139edaa943b3f8f586f4d9306ccc4627827a",
                        "width": 216,
                        "height": 113
                      },
                      {
                        "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=320&amp;format=mp4&amp;s=8eeb8a51560f25a9f0aadb2c5a52ae726050ef79",
                        "width": 320,
                        "height": 167
                      },
                      {
                        "url": "https://preview.redd.it/x7gqer73e1gf1.gif?width=640&amp;format=mp4&amp;s=4d1bd2c1e869610791d6d101529018183948db17",
                        "width": 640,
                        "height": 335
                      }
                    ]
                  }
                },
                "id": "ujIyU1QgzIY62tMRJ3qOaOKGHXc0upXGIUYdFHI5r0E"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mdawyz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aliasaria",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdawyz/just_launched_transformer_lab_recipes_13_prebuilt/",
          "stickied": false,
          "url": "https://i.redd.it/x7gqer73e1gf1.gif",
          "subreddit_subscribers": 507576,
          "created_utc": 1753892255,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Standard disclaimers: nobody should fully trust a benchmark website to judge a model, models should be tested separately, etc etc. \n\nSo, now that we mentioned that, what websites are most useful (*as a reference point*) for how good a model is? \n\nHistorically, I've used https://livebench.ai/ but they've kind of gone downhill recently. I notice that livebench and some other benchmarks which used to be updated more frequently/for more models/etc, no longer do so. They still haven't benchmarked the new Qwen3-30b models. I suspect the parent company may be distracted by running out of money- they have 179 employees for some reason and hasn't raised a funding round since 2021, but anyways I digress. \n\nWhat other benchmark sites are good? \n\n- I also see https://artificialanalysis.ai/ mentioned often. \n\n- For coding, there's https://aider.chat/docs/leaderboards/\n\nWhat else?",
          "author_fullname": "t2_t6glzswk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "So what benchmark websites do you refer to? (July 2025 edition)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdnzym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753923743,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Standard disclaimers: nobody should fully trust a benchmark website to judge a model, models should be tested separately, etc etc. &lt;/p&gt;\n\n&lt;p&gt;So, now that we mentioned that, what websites are most useful (&lt;em&gt;as a reference point&lt;/em&gt;) for how good a model is? &lt;/p&gt;\n\n&lt;p&gt;Historically, I&amp;#39;ve used &lt;a href=\"https://livebench.ai/\"&gt;https://livebench.ai/&lt;/a&gt; but they&amp;#39;ve kind of gone downhill recently. I notice that livebench and some other benchmarks which used to be updated more frequently/for more models/etc, no longer do so. They still haven&amp;#39;t benchmarked the new Qwen3-30b models. I suspect the parent company may be distracted by running out of money- they have 179 employees for some reason and hasn&amp;#39;t raised a funding round since 2021, but anyways I digress. &lt;/p&gt;\n\n&lt;p&gt;What other benchmark sites are good? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;I also see &lt;a href=\"https://artificialanalysis.ai/\"&gt;https://artificialanalysis.ai/&lt;/a&gt; mentioned often. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For coding, there&amp;#39;s &lt;a href=\"https://aider.chat/docs/leaderboards/\"&gt;https://aider.chat/docs/leaderboards/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdnzym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DepthHour1669",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdnzym/so_what_benchmark_websites_do_you_refer_to_july/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdnzym/so_what_benchmark_websites_do_you_refer_to_july/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753923743,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I love efficiency. I’m always hoping to find a solution that allows me to automate basic coding tasks like “create some css that makes a menu that looks like this” to leave running while I go to work. Main problem with this currently is that AI will often stop and declare it’s done, and then you have to make fixes to whatever it spit out, which isn’t feasible when you’re not there! Hopefully soon you could have another model (maybe vision based?) babysitting the coding model and saying “a little to the left” over the 10 prompts it takes to get it while youre away.\n\nUntil that day comes, please share with me your favorite things that you’ve been able to automate with language models to make your life more efficient!",
          "author_fullname": "t2_1loou9xu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best thing Youve automated?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdshnt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753937233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I love efficiency. I’m always hoping to find a solution that allows me to automate basic coding tasks like “create some css that makes a menu that looks like this” to leave running while I go to work. Main problem with this currently is that AI will often stop and declare it’s done, and then you have to make fixes to whatever it spit out, which isn’t feasible when you’re not there! Hopefully soon you could have another model (maybe vision based?) babysitting the coding model and saying “a little to the left” over the 10 prompts it takes to get it while youre away.&lt;/p&gt;\n\n&lt;p&gt;Until that day comes, please share with me your favorite things that you’ve been able to automate with language models to make your life more efficient!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdshnt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Shadow-Amulet-Ambush",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdshnt/best_thing_youve_automated/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdshnt/best_thing_youve_automated/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753937233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! About two months ago, I decided to delve deeper into the topic of running LLMs locally. I was looking for an awesome-style repository. There are a few of them, but unfortunately, they are not actively maintained. So, I decided to create my own cheat sheet where I would take notes. After these few weeks, I can say that the repository is mature enough that I can share it.\n\n[https://github.com/rafska/Awesome-local-LLM](https://github.com/rafska/Awesome-local-LLM)\n\nOne noteworthy feature is the badges, which are a good proxy for the maturity of a particular project. With them, without clicking on every link, you can assess which projects may be the most promising for you. Additionally, other similar repositories lack a section dedicated to hardware, without which we cannot run LLMs locally – mine has it.\n\nAll contributions and suggestions are welcome. I will try to make this repository truly awesome!",
          "author_fullname": "t2_61j496qt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "An Awesome-local-LLM repository",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdw1l4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=489d5eab695f6df9a46f6bbfb1b2f47c5106150d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753950470,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! About two months ago, I decided to delve deeper into the topic of running LLMs locally. I was looking for an awesome-style repository. There are a few of them, but unfortunately, they are not actively maintained. So, I decided to create my own cheat sheet where I would take notes. After these few weeks, I can say that the repository is mature enough that I can share it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/rafska/Awesome-local-LLM\"&gt;https://github.com/rafska/Awesome-local-LLM&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;One noteworthy feature is the badges, which are a good proxy for the maturity of a particular project. With them, without clicking on every link, you can assess which projects may be the most promising for you. Additionally, other similar repositories lack a section dedicated to hardware, without which we cannot run LLMs locally – mine has it.&lt;/p&gt;\n\n&lt;p&gt;All contributions and suggestions are welcome. I will try to make this repository truly awesome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/rafska/Awesome-local-LLM",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM.png?auto=webp&amp;s=cdfb5238441b6a61d305fbd26d6a78d9277c4fe6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e496b6732d4545c96f97bcaf40bcef8cefdf7bfe",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=405253ea098f6b701e678824e55db452f790b41b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb3abf674f9ef4e8c3dea47ad10247ac1d5dfd59",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ec1f4dff56e37de094cc6d84c8add1630d10f16",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0efcb18f638c492a63229e6029e6b02f9112760f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6fee0a7f679fa7a236228bd074654192c5fa66a2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Nz4v7a5FyPh-rW-HmnF3rzX5meoV5UtYQXzh1t8n0UM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mdw1l4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "What_to_type_here",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdw1l4/an_awesomelocalllm_repository/",
          "stickied": false,
          "url": "https://github.com/rafska/Awesome-local-LLM",
          "subreddit_subscribers": 507576,
          "created_utc": 1753950470,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone else get the impression that open source LLMs will wipe out the valuation of companies like Anthropic?  New, competitive models are getting released nearly every day lately.  Many can handle 80-90% of standard tasks.  It is starting to look like a race to the bottom.",
          "author_fullname": "t2_47ws19uq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Valuation of companies like Anthropic",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdpd70",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753927672,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone else get the impression that open source LLMs will wipe out the valuation of companies like Anthropic?  New, competitive models are getting released nearly every day lately.  Many can handle 80-90% of standard tasks.  It is starting to look like a race to the bottom.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdpd70",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "seoulsrvr",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdpd70/valuation_of_companies_like_anthropic/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdpd70/valuation_of_companies_like_anthropic/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753927672,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! The [German Aerospace Center](https://www.dlr.de/en) (DLR — the German NASA) is looking for someone for a [DevOps position](https://jobs.dlr.de/default/job/Informatikerin-%28mwd%29-als-DevOps-Engineer-f%C3%BCr-den-Betrieb-und-die-Entwicklung-von-KI-Anwendung/2484-de_DE) in the LLM field. You’ll need to be pretty fluent in German and able to work at least once a week in the Cologne/Bonn area (mostly remote, though). The job is about running and maintaining internal LLMs on high-performance AI hardware, using tools like Ollama or vLLM on Docker or Kubernetes with Ubuntu. You’ll also help develop the open source software [MindWork AI Studio](https://github.com/MindWorkAI/AI-Studio) using Rust and C# (.NET 9+). If you speak German and this sounds interesting, [go ahead and apply](https://jobs.dlr.de/default/job/Informatikerin-%28mwd%29-als-DevOps-Engineer-f%C3%BCr-den-Betrieb-und-die-Entwicklung-von-KI-Anwendung/2484-de_DE)!",
          "author_fullname": "t2_irzpa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DevOps position for AI / LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdvj52",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753948497,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! The &lt;a href=\"https://www.dlr.de/en\"&gt;German Aerospace Center&lt;/a&gt; (DLR — the German NASA) is looking for someone for a &lt;a href=\"https://jobs.dlr.de/default/job/Informatikerin-%28mwd%29-als-DevOps-Engineer-f%C3%BCr-den-Betrieb-und-die-Entwicklung-von-KI-Anwendung/2484-de_DE\"&gt;DevOps position&lt;/a&gt; in the LLM field. You’ll need to be pretty fluent in German and able to work at least once a week in the Cologne/Bonn area (mostly remote, though). The job is about running and maintaining internal LLMs on high-performance AI hardware, using tools like Ollama or vLLM on Docker or Kubernetes with Ubuntu. You’ll also help develop the open source software &lt;a href=\"https://github.com/MindWorkAI/AI-Studio\"&gt;MindWork AI Studio&lt;/a&gt; using Rust and C# (.NET 9+). If you speak German and this sounds interesting, &lt;a href=\"https://jobs.dlr.de/default/job/Informatikerin-%28mwd%29-als-DevOps-Engineer-f%C3%BCr-den-Betrieb-und-die-Entwicklung-von-KI-Anwendung/2484-de_DE\"&gt;go ahead and apply&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdvj52",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SommerEngineering",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdvj52/devops_position_for_ai_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdvj52/devops_position_for_ai_llms/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753948497,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, I’m a shit tier software engineer with 25 years of experience writing shitty web apps. I’d like to make my own LLM inference engine, preferably not in a QR code or Typescript Types, just to help me understand how they work under the hood. Point me to learning books and resources so I can make this happen.",
          "author_fullname": "t2_ozxxf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DIY LLM inference engine learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdmr8m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753924310,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753920253,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I’m a shit tier software engineer with 25 years of experience writing shitty web apps. I’d like to make my own LLM inference engine, preferably not in a QR code or Typescript Types, just to help me understand how they work under the hood. Point me to learning books and resources so I can make this happen.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdmr8m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "createthiscom",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdmr8m/diy_llm_inference_engine_learning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdmr8m/diy_llm_inference_engine_learning/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753920253,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thanks to the recent price surge on crypto I have rougly 10k I can spend on equipments. I have always wanted to run sota models like deepseek R1 or GLM 4.5 locally, and also fine tuning them. So far the mac studio 256gb model looks good, but I wanted to ask if there are any better alternatives.",
          "author_fullname": "t2_ekrnmt5z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to spend 7k on local model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdgr6n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753905468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thanks to the recent price surge on crypto I have rougly 10k I can spend on equipments. I have always wanted to run sota models like deepseek R1 or GLM 4.5 locally, and also fine tuning them. So far the mac studio 256gb model looks good, but I wanted to ask if there are any better alternatives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdgr6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "monoidconcat",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753905468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Tesslate/UIGEN-X-4B-0729](https://huggingface.co/Tesslate/UIGEN-X-4B-0729) 4B model that does reasoning for Design. We also released a 32B earlier in the week. \n\nAs per the last post -&gt;  \nSpecifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.\n\nWe're looking for some beta testers for some new models and open source projects!",
          "author_fullname": "t2_15kd4d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "yof4zxwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b585f63f1976fecd3209b28576a30e20382e3288"
                },
                {
                  "y": 139,
                  "x": 216,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d390945c2be5960f802c46d7534e3b13b1b9ce64"
                },
                {
                  "y": 206,
                  "x": 320,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4d91cbd80cd1bea9bf4046a92926c7a6a1327b3"
                },
                {
                  "y": 412,
                  "x": 640,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf7e75cc7fc920dcadfeddadec7708838aa7f496"
                },
                {
                  "y": 618,
                  "x": 960,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64c2c4a9e6f40664edd97d0fcbc66a58fbd01f10"
                },
                {
                  "y": 695,
                  "x": 1080,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9baade1c07727a9e0b013590eb07022ff7d97bbf"
                }
              ],
              "s": {
                "y": 1855,
                "x": 2880,
                "u": "https://preview.redd.it/yof4zxwiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=750bd98b519b2c2a6a9b67e375c712a747d76a7b"
              },
              "id": "yof4zxwiewff1"
            },
            "16laa8fzewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 107,
                  "x": 108,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=be850094630e801bc14b141c02c45a5d5780e39e"
                },
                {
                  "y": 214,
                  "x": 216,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=000716e3856e70eec1e6c3feee9a6082dfb6f14b"
                },
                {
                  "y": 317,
                  "x": 320,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7ba27c04a5a32d43cad0d06ac09510ffc21eb18"
                },
                {
                  "y": 635,
                  "x": 640,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=be9b40c399b65fa7aac8b8f12c47664cea519593"
                },
                {
                  "y": 953,
                  "x": 960,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c69dc72963fd774990d40819354963b4542144b"
                },
                {
                  "y": 1073,
                  "x": 1080,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56223aed305acfd4d0bdfc7293eac13e683e3a05"
                }
              ],
              "s": {
                "y": 1739,
                "x": 1750,
                "u": "https://preview.redd.it/16laa8fzewff1.png?width=1750&amp;format=png&amp;auto=webp&amp;s=685da575f550a67f1885f3ee2f448abb0ad8dfa8"
              },
              "id": "16laa8fzewff1"
            },
            "ntu2cxwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0d018ce17ce6e93fd4107130e86643585cf96c2"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23db8ae32b0c22d865ce52f9a03fbbcde05deeb0"
                },
                {
                  "y": 185,
                  "x": 320,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba539c8005ffd0c4736b5a57568e182824f305fd"
                },
                {
                  "y": 371,
                  "x": 640,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=106863b52b2fff18696ddf6ec546938ea7f8de35"
                },
                {
                  "y": 557,
                  "x": 960,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5e57f7a9e86b03e31ebaac20963a33085097ff2"
                },
                {
                  "y": 627,
                  "x": 1080,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a44493249a8080f036743b50835676ad73e83c7d"
                }
              ],
              "s": {
                "y": 1672,
                "x": 2880,
                "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=f6ef93e725d1a52278691bd9f2ecb1cf3ea1b6ee"
              },
              "id": "ntu2cxwiewff1"
            },
            "wj4w2vb2fwff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 124,
                  "x": 108,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce4efa314f24050279eaa398edd15d69545694b6"
                },
                {
                  "y": 249,
                  "x": 216,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=48cd29a868c118178676c8e761b9d0c7965971ba"
                },
                {
                  "y": 370,
                  "x": 320,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=524401edbadd87267db44465bb77ec546449da42"
                },
                {
                  "y": 740,
                  "x": 640,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f2d93c05fafd7effe55c1b8c9299eee020be45e"
                },
                {
                  "y": 1110,
                  "x": 960,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ff4bd17f8ef6686dd9f1c981b0ff0106d81f12f"
                },
                {
                  "y": 1249,
                  "x": 1080,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef3bc24444f01bec00a0224cb2c86cb3bca5067e"
                }
              ],
              "s": {
                "y": 1742,
                "x": 1506,
                "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=1506&amp;format=png&amp;auto=webp&amp;s=9eb5a8ab422fa1d5864870f6212bbb512ee327f3"
              },
              "id": "wj4w2vb2fwff1"
            },
            "lex8nqwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 127,
                  "x": 108,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=120a521aa4bebf6052a8a8e73b8052832817615b"
                },
                {
                  "y": 255,
                  "x": 216,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d03a3f3bbf63c6fa038368a77926fdc077cffe45"
                },
                {
                  "y": 379,
                  "x": 320,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5069c1d3fc72035727961c5af49935dc5edaf6d"
                },
                {
                  "y": 758,
                  "x": 640,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=49088fabb5a9e1b73f83b42ff1659d2c651ecabb"
                },
                {
                  "y": 1137,
                  "x": 960,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fae5c9dede0064c130c7af51c4efd8d9b89291b"
                },
                {
                  "y": 1279,
                  "x": 1080,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8390fb53d7893ccfbda7ba43e1043526d3ed1995"
                }
              ],
              "s": {
                "y": 2274,
                "x": 1920,
                "u": "https://preview.redd.it/lex8nqwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=962070f4cec63ffe5e3f74afdd9e361d3eb1e65c"
              },
              "id": "lex8nqwiewff1"
            },
            "adea0wwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97e39bf0fb8a28722a9fdec246b8f02853a61239"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b30b4a9d77e84eb9a95649bf5113d51f5c7733d"
                },
                {
                  "y": 278,
                  "x": 320,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8d21cc220a9c038420281dd75bf1cbc8c810781"
                },
                {
                  "y": 557,
                  "x": 640,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=305114889fd74072b8f44408ea2eb39e9d6ca40d"
                },
                {
                  "y": 836,
                  "x": 960,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b3179c0833b7971357d239b13f9544ca646a9c0"
                },
                {
                  "y": 941,
                  "x": 1080,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b112315e314c3e8663a20042433fdb632fca8b4"
                }
              ],
              "s": {
                "y": 1673,
                "x": 1920,
                "u": "https://preview.redd.it/adea0wwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=fd821b55ed691c1336897e770b0cac21d3da1733"
              },
              "id": "adea0wwiewff1"
            },
            "ax8700xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 215,
                  "x": 108,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6cecc068a16f0743e7f7aa0118d59381e879f59"
                },
                {
                  "y": 430,
                  "x": 216,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd1befd4a28971b0793663f9dc338070a9476cfa"
                },
                {
                  "y": 638,
                  "x": 320,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3535688560617701a73e241cee3b40f695746ebb"
                },
                {
                  "y": 1276,
                  "x": 640,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6017beab10859d631fbafb9bb24558403617258"
                },
                {
                  "y": 1914,
                  "x": 960,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=20cddeaa29fde178c979ec16035d38ab20242536"
                },
                {
                  "y": 2153,
                  "x": 1080,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f70ea717022f00cedac4143741218799011ef84"
                }
              ],
              "s": {
                "y": 5743,
                "x": 2880,
                "u": "https://preview.redd.it/ax8700xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=a6e9f1619b46af78448cf8f2b9ac70e06d78a1ea"
              },
              "id": "ax8700xiewff1"
            },
            "ejyq00xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98e5ba20ac3d31de436e60a97922996875d347ff"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa9bc6a58df8f71ea883392c35f600600742b517"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ec152ece72fff30fb421645a1669dd931e4bdfc"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=544c750520fddedec5be1f081c7cc7401df16985"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d6e5f430fda3cc2768464bd78ec043b99927fc6b"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5f91d5725b574cb286fec458ef62e689c27e607c"
                }
              ],
              "s": {
                "y": 6984,
                "x": 2880,
                "u": "https://preview.redd.it/ejyq00xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=303ddd2e22aebde6c25e2fdcb88a832f57df9944"
              },
              "id": "ejyq00xiewff1"
            },
            "kxkn9umxewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 106,
                  "x": 108,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=932e983f66f50902893c65fc0cfda02a9d2b22a2"
                },
                {
                  "y": 212,
                  "x": 216,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec248f0a2e6cf260216575fc5cc60bab220bb28e"
                },
                {
                  "y": 314,
                  "x": 320,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5edf2564694ad7e618bb3f280ef348153a1913f3"
                },
                {
                  "y": 629,
                  "x": 640,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e3c864fa70160d3f0e137ee42eff4a18605859c2"
                },
                {
                  "y": 943,
                  "x": 960,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=614d3f0351a10e309aa56852fa09cd6a934d7328"
                },
                {
                  "y": 1061,
                  "x": 1080,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ceee5ecfa7a0dd1ca565f7a3cddb9b4a64ffd10d"
                }
              ],
              "s": {
                "y": 1742,
                "x": 1772,
                "u": "https://preview.redd.it/kxkn9umxewff1.png?width=1772&amp;format=png&amp;auto=webp&amp;s=ee44a102a0f980b2655670786261becf38366c67"
              },
              "id": "kxkn9umxewff1"
            },
            "z7pj58xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09b6203fb4f173b715c03493dfe67e9b039f3664"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=655762aeb2b663c2bf82fa7a23b9ba0e046ff117"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b0c36608a8dd7d989cb8f1425f0ed68cecc6cf0"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e2395673178607874bdae7dfaa75c8460c22721a"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=036661bf27ca2ae95489ef42fbd9163baa23a0f5"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f100364d1c04026b195f7ea334a7a1d4a51831c7"
                }
              ],
              "s": {
                "y": 6229,
                "x": 2880,
                "u": "https://preview.redd.it/z7pj58xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=aeafa90cf8e3d73863c35071d9e713ca83facbf7"
              },
              "id": "z7pj58xiewff1"
            },
            "acxqsuwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7425577e6a47a9d3e0a3a100f387cf005fd280d"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bdb03c41fce1c12f00429512ee516e3032d7be24"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bd65e8862fc6957d1b38807ae5860c9cdaf6019"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fef431ddba58fbfeadf672b3f1b2f7cd59a4024e"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d048fdfecfaaf1e67a1c0c3acc520744d210c2c3"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5bf701f33777ed0ef64335bf05d63f02e05f82b"
                }
              ],
              "s": {
                "y": 5998,
                "x": 1920,
                "u": "https://preview.redd.it/acxqsuwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=1ff339ed9b099c2cc7e6c8983e87af8e3494534e"
              },
              "id": "acxqsuwiewff1"
            },
            "s7p75twiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=84e39273f74fd125311c365fac8f17aa2f027ac2"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=94fbe9bcc7206c2de0cd6c9ad5dafa699528449e"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b412b682e705205f4a9170aeca865f1dabd2e419"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd319320ae2e53ab1adeebb7127aa9d9a3f5e760"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b384f769fd836b366a0cddefec36f82e53b87918"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97aa4cdd51e0498d0929bbc700ba0d01373ea554"
                }
              ],
              "s": {
                "y": 5391,
                "x": 1920,
                "u": "https://preview.redd.it/s7p75twiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=473ae9015c69ce171209a0f6d69a7262bd95c4cc"
              },
              "id": "s7p75twiewff1"
            },
            "gjgr517pewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4755f1869d3b7eb7ea0597f941557ba81c651329"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb98a6adf6598211d6e427d7dbb4e18218c97614"
                },
                {
                  "y": 203,
                  "x": 320,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5801ab34ab8e6d6c323c619f5fd48ef9bcd7de9f"
                },
                {
                  "y": 407,
                  "x": 640,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86b0ac115a5b3e9db3e959924030049ea892d809"
                },
                {
                  "y": 610,
                  "x": 960,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a6414c79af3f3c9b4a589fa97d866dd966bdda5"
                },
                {
                  "y": 686,
                  "x": 1080,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0f0758aadd57cbcd02f9ff3c31c98632506f060"
                }
              ],
              "s": {
                "y": 1564,
                "x": 2459,
                "u": "https://preview.redd.it/gjgr517pewff1.png?width=2459&amp;format=png&amp;auto=webp&amp;s=d7c539f97e387b73f2f07bc8ba1f06bec4aad9d0"
              },
              "id": "gjgr517pewff1"
            },
            "1p1dk2xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4168773f08144636cd601811a4ab8d2ab17744f8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0167a8561f08feb97a21f409a40491b2d8179c2"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a81d1f425fbb1ba6891eebd5fc7834621d47a971"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a92096881722780601c9417e8ed52dda672226c5"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b3a497379e5488734b986a943c4b9c8a9177f8ab"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=999cfe06600aab2caa654d74023a8ece7a9424de"
                }
              ],
              "s": {
                "y": 5337,
                "x": 1920,
                "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=8b79f29a9a816ef267db3d202bb66ee24e42e89b"
              },
              "id": "1p1dk2xiewff1"
            }
          },
          "name": "t3_1mcr64f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 334,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "gjgr517pewff1",
                "id": 716986046
              },
              {
                "media_id": "wj4w2vb2fwff1",
                "id": 716986047
              },
              {
                "media_id": "kxkn9umxewff1",
                "id": 716986048
              },
              {
                "media_id": "ntu2cxwiewff1",
                "id": 716986049
              },
              {
                "media_id": "ax8700xiewff1",
                "id": 716986050
              },
              {
                "media_id": "lex8nqwiewff1",
                "id": 716986051
              },
              {
                "media_id": "acxqsuwiewff1",
                "id": 716986052
              },
              {
                "media_id": "s7p75twiewff1",
                "id": 716986053
              },
              {
                "media_id": "1p1dk2xiewff1",
                "id": 716986054
              },
              {
                "media_id": "adea0wwiewff1",
                "id": 716986055
              },
              {
                "media_id": "yof4zxwiewff1",
                "id": 716986056
              },
              {
                "media_id": "ejyq00xiewff1",
                "id": 716986057
              },
              {
                "media_id": "z7pj58xiewff1",
                "id": 716986058
              },
              {
                "media_id": "16laa8fzewff1",
                "id": 716986059
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 334,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3wFSGxs0og7hUYyLF8nuoy2CBvu34JQ_m2cRe7ujEoc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753832160,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-4B-0729\"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt; 4B model that does reasoning for Design. We also released a 32B earlier in the week. &lt;/p&gt;\n\n&lt;p&gt;As per the last post -&amp;gt;&lt;br/&gt;\nSpecifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re looking for some beta testers for some new models and open source projects!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mcr64f",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcr64f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "smirkishere",
          "discussion_type": null,
          "num_comments": 74,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mcr64f",
          "subreddit_subscribers": 507576,
          "created_utc": 1753832160,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Am I the only one who feels like AI coding agent often end up costing me more time? Honestly, about 60% of my time after using an AI agent goes into cleaning up its output especially dealing with “code smells” it leaves behind.\n\nOur codebase is pretty old and has a lot of legacy quirks, and I’ve noticed the AI agents tend to refactor things that really shouldn’t be touched, which sometimes introduces strange bugs that I then have to fix. On top of that, sometimes the generated code won’t even pass my basic tests and I have to manually copy the tests results or code review comments back to the agents to ask them to try again, which will possibly introduce more bugs...sigh...\n\nIs anyone else feeling the same that there's more work left for you after using AI copilot? If you’ve had a better experience, which AI agents are you using? I’ve tried Codex, Cursor Agents, and Claude Code, but no luck.",
          "author_fullname": "t2_2gmupbxi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do AI coding agents actually save you time, or just create more cleanup?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdg9z1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753904353,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am I the only one who feels like AI coding agent often end up costing me more time? Honestly, about 60% of my time after using an AI agent goes into cleaning up its output especially dealing with “code smells” it leaves behind.&lt;/p&gt;\n\n&lt;p&gt;Our codebase is pretty old and has a lot of legacy quirks, and I’ve noticed the AI agents tend to refactor things that really shouldn’t be touched, which sometimes introduces strange bugs that I then have to fix. On top of that, sometimes the generated code won’t even pass my basic tests and I have to manually copy the tests results or code review comments back to the agents to ask them to try again, which will possibly introduce more bugs...sigh...&lt;/p&gt;\n\n&lt;p&gt;Is anyone else feeling the same that there&amp;#39;s more work left for you after using AI copilot? If you’ve had a better experience, which AI agents are you using? I’ve tried Codex, Cursor Agents, and Claude Code, but no luck.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdg9z1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "andrew19953",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdg9z1/do_ai_coding_agents_actually_save_you_time_or/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdg9z1/do_ai_coding_agents_actually_save_you_time_or/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753904353,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working with artificial intelligence, It's been working for the past few months but I've gotten annoyed with the performance. I've just switched to **IK\\_llama.cpp**, and I'm looking to optimize my command although I haven't found any documentation. I've managed to get it around **12-15 t/s** (quite good, but I'm looking to see if it can get better &lt;3)  \nSorry if it's alot, I'm just asking somebody to help create an optimized command for running the models.\n\n\\[**specs**\\]: RTX 3060 12gb, 16gb ram, ryzen 5 2600.  \n\\[**I'm using IK\\_llama.cpp on arch linux**\\]\n\n\\[**model**\\] latest Qwen3 30B-A3B",
          "author_fullname": "t2_11jbo8mpsx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to optimize TPS using IK_llama.cpp?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdlss2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753918723,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753917687,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working with artificial intelligence, It&amp;#39;s been working for the past few months but I&amp;#39;ve gotten annoyed with the performance. I&amp;#39;ve just switched to &lt;strong&gt;IK_llama.cpp&lt;/strong&gt;, and I&amp;#39;m looking to optimize my command although I haven&amp;#39;t found any documentation. I&amp;#39;ve managed to get it around &lt;strong&gt;12-15 t/s&lt;/strong&gt; (quite good, but I&amp;#39;m looking to see if it can get better &amp;lt;3)&lt;br/&gt;\nSorry if it&amp;#39;s alot, I&amp;#39;m just asking somebody to help create an optimized command for running the models.&lt;/p&gt;\n\n&lt;p&gt;[&lt;strong&gt;specs&lt;/strong&gt;]: RTX 3060 12gb, 16gb ram, ryzen 5 2600.&lt;br/&gt;\n[&lt;strong&gt;I&amp;#39;m using IK_llama.cpp on arch linux&lt;/strong&gt;]&lt;/p&gt;\n\n&lt;p&gt;[&lt;strong&gt;model&lt;/strong&gt;] latest Qwen3 30B-A3B&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdlss2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Final-Message2150",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdlss2/how_to_optimize_tps_using_ik_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdlss2/how_to_optimize_tps_using_ik_llamacpp/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753917687,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Maybe I missed it or something, but how are people running MOE models and getting decent speeds?\n\nI rented a 3090 on runpod since it also had like 124 gb of RAM. I compiled llama.cpp on it. Got my usual speeds for Qwen3 32B q4km completely offloaded to the 3090. Then tried qwen3 235b MOE at q3km and got prompt eval time of 1 t/s and token generation of 0.33 t/s. I made sure that I offloaded layers to fill up the 3090 but things did not speed up much. Maybe 1t/s better, but would have to revisit for the exact numbers. Not what I have been seeing posted by other people here.\n\nAny suggestions what to do differently for these large MOE and a single 3090? Or is this expected performance?",
          "author_fullname": "t2_40xsg56g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Setup for MOE",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdqlc6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753931203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe I missed it or something, but how are people running MOE models and getting decent speeds?&lt;/p&gt;\n\n&lt;p&gt;I rented a 3090 on runpod since it also had like 124 gb of RAM. I compiled llama.cpp on it. Got my usual speeds for Qwen3 32B q4km completely offloaded to the 3090. Then tried qwen3 235b MOE at q3km and got prompt eval time of 1 t/s and token generation of 0.33 t/s. I made sure that I offloaded layers to fill up the 3090 but things did not speed up much. Maybe 1t/s better, but would have to revisit for the exact numbers. Not what I have been seeing posted by other people here.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions what to do differently for these large MOE and a single 3090? Or is this expected performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdqlc6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fgoricha",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753931203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello fellow Redditors, i need to ask for your wisdom.\n\nI got the request from my Boss to implement a localy running AI System to analyze our company data and let our employes check for information, create knowleagebases and create nice sounding emails.\n\nSo now to my \"Problem\", i got a budget of about 5000€ to build a machine capable of running quite fast in respoding to the User and also at least 2-3 requests of users simultaniously. (If possible). I got a Testsystem with 2x 12GB RTX A2000 cards (64GB Ram and I7 14700k that can run single responses with 30b LLMs with about 8-9Tokens 24b with about 13 and 7B Models with 18 Token.\n\nEdit: It would be fine to run just 30-70b models if possible.\n\nI run Ollama with Open Webui.\n\nAs 5000€ is not a very high budget and everything needs to be bought New i cant just buy used 3090s.\n\nI though about getting something like a midrange for CPU with 128gb Ram, DDR5 Mainboard with Dual GPU slot but i dont know about the GPU's. \n\nWhat would be the best combo for that money?\n\nCloud is not an option as the data is very sensitive.\n",
          "author_fullname": "t2_yxgzw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help for new LLM Rig",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdui1j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753944900,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753944455,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow Redditors, i need to ask for your wisdom.&lt;/p&gt;\n\n&lt;p&gt;I got the request from my Boss to implement a localy running AI System to analyze our company data and let our employes check for information, create knowleagebases and create nice sounding emails.&lt;/p&gt;\n\n&lt;p&gt;So now to my &amp;quot;Problem&amp;quot;, i got a budget of about 5000€ to build a machine capable of running quite fast in respoding to the User and also at least 2-3 requests of users simultaniously. (If possible). I got a Testsystem with 2x 12GB RTX A2000 cards (64GB Ram and I7 14700k that can run single responses with 30b LLMs with about 8-9Tokens 24b with about 13 and 7B Models with 18 Token.&lt;/p&gt;\n\n&lt;p&gt;Edit: It would be fine to run just 30-70b models if possible.&lt;/p&gt;\n\n&lt;p&gt;I run Ollama with Open Webui.&lt;/p&gt;\n\n&lt;p&gt;As 5000€ is not a very high budget and everything needs to be bought New i cant just buy used 3090s.&lt;/p&gt;\n\n&lt;p&gt;I though about getting something like a midrange for CPU with 128gb Ram, DDR5 Mainboard with Dual GPU slot but i dont know about the GPU&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;What would be the best combo for that money?&lt;/p&gt;\n\n&lt;p&gt;Cloud is not an option as the data is very sensitive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdui1j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juli199696",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753944455,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. Has anyone tried multiple (open, ideally below 200B parameters) models against text understanding tasks on German or other \"mainstream\" European languages? Are any of the more famous ones (Gemma3 27b, Qwens, DeepSeek etc) particularly superior or inferior in this?\n\nThanks",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Models that are good in understanding and producing German text?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdug0j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753945145,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753944240,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Has anyone tried multiple (open, ideally below 200B parameters) models against text understanding tasks on German or other &amp;quot;mainstream&amp;quot; European languages? Are any of the more famous ones (Gemma3 27b, Qwens, DeepSeek etc) particularly superior or inferior in this?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdug0j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdug0j/models_that_are_good_in_understanding_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdug0j/models_that_are_good_in_understanding_and/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753944240,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.*",
          "author_fullname": "t2_iol3buybk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md9nc8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753889379,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA&amp;#39;s design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.19457",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1md9nc8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thrumpwart",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md9nc8/gepa_reflective_prompt_evolution_can_outperform/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.19457",
          "subreddit_subscribers": 507576,
          "created_utc": 1753889379,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "* Keeping your warranty.  \n* 1 slot  \n* backside tube exits \n\nLook perfect to make a dense AI machine.\n\n\n\n[https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design](https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design)\n\n",
          "author_fullname": "t2_3rx5s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 5090 form INNO3D 1 slot with Alphacool-waterkoeling look perfect for local AI machines",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md0gfh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 61,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/NeEN6MGm_hnkNwFg7zJC-1QY8AH3i-jiJVFe5Z5lTM0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753861579,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;Keeping your warranty.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;1 slot&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;backside tube exits &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Look perfect to make a dense AI machine.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design\"&gt;https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/eeopjbr7uyff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/eeopjbr7uyff1.png?auto=webp&amp;s=9013fd688e0eb0ed62be948a8d3d4acedc2fcb40",
                  "width": 1000,
                  "height": 626
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5abd37ef1038ff8e9e6a0537a95ca2ea0a3d3ef",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=574794d614b9ecbd24909e833b0cac21e4d762cd",
                    "width": 216,
                    "height": 135
                  },
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b56159bfa847a204ab5ab96c1489261d202caf4",
                    "width": 320,
                    "height": 200
                  },
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=071af31f7998dd67f773b41419988ca83dd8fdd3",
                    "width": 640,
                    "height": 400
                  },
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0c5a836c6accb5a923be466745e47d3e4366147",
                    "width": 960,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "UD_dYV0qdMIHDeAIfPiwQicfo5K1meoR0O82qOPwsFU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1md0gfh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jwestra",
          "discussion_type": null,
          "num_comments": 81,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md0gfh/rtx_5090_form_inno3d_1_slot_with/",
          "stickied": false,
          "url": "https://i.redd.it/eeopjbr7uyff1.png",
          "subreddit_subscribers": 507576,
          "created_utc": 1753861579,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm kind of out of the loop when it comes to TTS, I was wondering which gives the overall best quality voices that include Voice cloning?",
          "author_fullname": "t2_rfa3pgbn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Current Best TTS with voice cloning you can run locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdfls9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753902808,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m kind of out of the loop when it comes to TTS, I was wondering which gives the overall best quality voices that include Voice cloning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdfls9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "noyingQuestions_101",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdfls9/current_best_tts_with_voice_cloning_you_can_run/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdfls9/current_best_tts_with_voice_cloning_you_can_run/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753902808,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "No matter which model I choose it seems like I get 1-2 absolutely off the rails responses for every 5 requests I make. Are some providers using ridiculous settings, not respecting configuration (temp, etc..) passed in, or using *heavily* quantized models?\n\nI noticed that this *never* happens if I pick an individual provider I'm happy with and use their service directly.\n\nLately seeing it with Llama4-Maverick, Qwen3-235B (both thinking and non thinking), Deepseek (both R1 and V3), and Qwen3-Code-480B.\n\nAnyone else having this experience?",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it just me or is OpenRouter an absolute roulette wheel lately?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md6cxq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753882337,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753881453,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No matter which model I choose it seems like I get 1-2 absolutely off the rails responses for every 5 requests I make. Are some providers using ridiculous settings, not respecting configuration (temp, etc..) passed in, or using &lt;em&gt;heavily&lt;/em&gt; quantized models?&lt;/p&gt;\n\n&lt;p&gt;I noticed that this &lt;em&gt;never&lt;/em&gt; happens if I pick an individual provider I&amp;#39;m happy with and use their service directly.&lt;/p&gt;\n\n&lt;p&gt;Lately seeing it with Llama4-Maverick, Qwen3-235B (both thinking and non thinking), Deepseek (both R1 and V3), and Qwen3-Code-480B.&lt;/p&gt;\n\n&lt;p&gt;Anyone else having this experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1md6cxq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1md6cxq/is_it_just_me_or_is_openrouter_an_absolute/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md6cxq/is_it_just_me_or_is_openrouter_an_absolute/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753881453,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcoce7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 337,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 337,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=e171e9de6f65dcbec52d568f58750bcc5885ba3a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753825022,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?auto=webp&amp;s=d470a9db6c0be816c3d916347e4ae5e8bad6c6a8",
                  "width": 750,
                  "height": 422
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e14a96af45b1cfcde1a2159e64971bc1d775033",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47124b81e03a6604140c8e4d29ddbe43c15456c3",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf6168a4321e52f96ea360e8b865dc9bbcaaf345",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d50b7793829c5aa107cf8ecaa3b004d46e3cdef0",
                    "width": 640,
                    "height": 360
                  }
                ],
                "variants": {},
                "id": "9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcoce7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 99,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/",
          "stickied": false,
          "url": "https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753825022,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Be gentle, this is my first time lol\n\nI have a small home network and decided to build out a local llm to handle work stuff with more security. \n\nBackground: \nWife has been using ChatGPT for a few months, and I started investigating other cloud based tools.   I found that if you want to do any real work, you need about 87 of them, 73 subscriptions, 4 devices, and a small piece of you soul.   Which led me to the possibility of running the majority of the workload.   Because of my computers limitations, I will still need to give the heavy lifting stuff to the cloud.\n\n\nPc specs (that im hosting on)\nIntel i7 6core,12 thread 3.2GHz\n32GB ddr4 ram (all 4 slots filled)\n1Tb ssd raid hard drive\nIntegrated not dedicated GPU\n\nI have ollama, phi3, LLaMA3, LLava, Deepseek-coder, docker, WSL, and Open WebUI on my pc.  \n\nI'm wanting my system to help with:\nCAD\nGraphic design\nProject management\nCoding\nNetwork monitoring and optimization\nAnd other niche tasks. \n\nCurrently have tested each model in WebUI successfull\n\nI think the next step would be to go through settings in ollama, WebUI, Docker, and probably even my system bios.  I'm working with a slightly smaller processing power than really would be ideal, but I think I can make some tweaks to get it pretending to work faster.\n\nFrom there I will probably need to start loading datasets in my desired areas for better training and giving the models a cheat code for performing above their paygrade.\n\n\n\nAnyway....  yeah thats my current set up and it's technically working.  It's just not fully operational and ready to rock yet.  Hoping an open discussion about options, techniques etc might help me figure it out and others who haven't jumped yet. ",
          "author_fullname": "t2_clyuifd5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New to this and trying to learn on the fly",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdl999",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753916299,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Be gentle, this is my first time lol&lt;/p&gt;\n\n&lt;p&gt;I have a small home network and decided to build out a local llm to handle work stuff with more security. &lt;/p&gt;\n\n&lt;p&gt;Background: \nWife has been using ChatGPT for a few months, and I started investigating other cloud based tools.   I found that if you want to do any real work, you need about 87 of them, 73 subscriptions, 4 devices, and a small piece of you soul.   Which led me to the possibility of running the majority of the workload.   Because of my computers limitations, I will still need to give the heavy lifting stuff to the cloud.&lt;/p&gt;\n\n&lt;p&gt;Pc specs (that im hosting on)\nIntel i7 6core,12 thread 3.2GHz\n32GB ddr4 ram (all 4 slots filled)\n1Tb ssd raid hard drive\nIntegrated not dedicated GPU&lt;/p&gt;\n\n&lt;p&gt;I have ollama, phi3, LLaMA3, LLava, Deepseek-coder, docker, WSL, and Open WebUI on my pc.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wanting my system to help with:\nCAD\nGraphic design\nProject management\nCoding\nNetwork monitoring and optimization\nAnd other niche tasks. &lt;/p&gt;\n\n&lt;p&gt;Currently have tested each model in WebUI successfull&lt;/p&gt;\n\n&lt;p&gt;I think the next step would be to go through settings in ollama, WebUI, Docker, and probably even my system bios.  I&amp;#39;m working with a slightly smaller processing power than really would be ideal, but I think I can make some tweaks to get it pretending to work faster.&lt;/p&gt;\n\n&lt;p&gt;From there I will probably need to start loading datasets in my desired areas for better training and giving the models a cheat code for performing above their paygrade.&lt;/p&gt;\n\n&lt;p&gt;Anyway....  yeah thats my current set up and it&amp;#39;s technically working.  It&amp;#39;s just not fully operational and ready to rock yet.  Hoping an open discussion about options, techniques etc might help me figure it out and others who haven&amp;#39;t jumped yet. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdl999",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JellyfishAutomatic25",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdl999/new_to_this_and_trying_to_learn_on_the_fly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdl999/new_to_this_and_trying_to_learn_on_the_fly/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753916299,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is from the latest Qwen3-30B-A3B-Instruct-2507. ❤",
          "author_fullname": "t2_qz1qjc86",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Newest Qwen made me cry. It's not perfect, but I still love it.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 53,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mci7uu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 625,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 625,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/h1Ejab1vdsRPCPX8mSRmFSTByiWOsDhozhB4Y9joLZQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753811149,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is from the latest Qwen3-30B-A3B-Instruct-2507. ❤&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/gnkbnxzlouff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/gnkbnxzlouff1.png?auto=webp&amp;s=15ae6e9bfdd39878d13fcc40579f5f5914892497",
                  "width": 537,
                  "height": 204
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=387b92e0abf220fa87708b750e1cd04535c8d238",
                    "width": 108,
                    "height": 41
                  },
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3003e1ab3cfb059c6497615c2cb005b3643dcb9b",
                    "width": 216,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=431c53f32897af3a4225062d97bdc95913f53ec0",
                    "width": 320,
                    "height": 121
                  }
                ],
                "variants": {},
                "id": "IXr-dCvLuJL4cGd_YuCTQJMpv6LVqTDaGI7ZM0RkpWg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mci7uu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cool-Chemical-5629",
          "discussion_type": null,
          "num_comments": 77,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/",
          "stickied": false,
          "url": "https://i.redd.it/gnkbnxzlouff1.png",
          "subreddit_subscribers": 507576,
          "created_utc": 1753811149,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you are into learning or building Agents, I have compiled some of the best educational repositories and agent protocols out there.\n\nOver the past year, these protocols have changed the ecosystem:\n\n* [AG-UI](https://github.com/ag-ui-protocol/ag-ui) → user interaction memory. acts like the `REST` layer of human-agent interaction with nearly zero boilerplate.\n* [MCP](https://github.com/modelcontextprotocol/modelcontextprotocol) → tool + state access. standardizes how applications provide context and tools to LLMs.\n* [A2A](https://github.com/a2aproject/A2A) → connects agents to each other. this expands how agents can collaborate, being agnostic to the backend/framework.\n* [ACP](https://github.com/i-am-bee/acp) → Communication over REST/stream. Builds on many of A2A’s ideas but extends to include human and app interaction.\n\nRepos you should know:\n\n* [12-factor agents](https://github.com/humanlayer/12-factor-agents/) → core principles for building reliable LLM apps (\\~10.9k⭐)\n* [Agents Towards Production](https://github.com/NirDiamant/agents-towards-production) → reusable patterns &amp; real-world blueprints from prototype to deployment (\\~9.1k⭐)\n* [GenAI Agents](https://github.com/NirDiamant/genai_agents) → 40+ multi-agent systems with frameworks like LangGraph, CrewAI, OpenAI Swarm (\\~15.2k⭐)\n* [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps) → practical RAG, AI Agents, Multi-agent Teams, MCP, Autonomous Agents with code (\\~53.8k⭐)\n* [MCP for Beginners](https://github.com/microsoft/mcp-for-beginners) → open source curriculum by Microsoft with practical examples (\\~5.9k⭐)\n* [System Prompts](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools) → library of prompts &amp; config files from 15+ AI products like Cursor, V0, Cluely, Lovable, Replit... (\\~72.5k⭐)\n* [500 AI Agents Projects](https://github.com/ashishpatel26/500-AI-Agents-Projects) → highlights 500+ use cases across industries like healthcare, finance, education, retail, logistics, gaming and more. Each use case links to an open source project (\\~4k⭐)\n\nfull detailed writeup: [here](https://levelup.gitconnected.com/protocols-best-resources-for-getting-started-with-agents-in-2025-5343dac58316?sk=7588f2c91ca4cf54b9dbaa3bcd184d07)\n\nIf you know of any other great repos, please share in the comments.",
          "author_fullname": "t2_1hro18widg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Repos &amp; Protocols for learning and building Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdcnu8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753896201,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are into learning or building Agents, I have compiled some of the best educational repositories and agent protocols out there.&lt;/p&gt;\n\n&lt;p&gt;Over the past year, these protocols have changed the ecosystem:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ag-ui-protocol/ag-ui\"&gt;AG-UI&lt;/a&gt; → user interaction memory. acts like the &lt;code&gt;REST&lt;/code&gt; layer of human-agent interaction with nearly zero boilerplate.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/modelcontextprotocol/modelcontextprotocol\"&gt;MCP&lt;/a&gt; → tool + state access. standardizes how applications provide context and tools to LLMs.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/a2aproject/A2A\"&gt;A2A&lt;/a&gt; → connects agents to each other. this expands how agents can collaborate, being agnostic to the backend/framework.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/i-am-bee/acp\"&gt;ACP&lt;/a&gt; → Communication over REST/stream. Builds on many of A2A’s ideas but extends to include human and app interaction.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Repos you should know:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/humanlayer/12-factor-agents/\"&gt;12-factor agents&lt;/a&gt; → core principles for building reliable LLM apps (~10.9k⭐)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/NirDiamant/agents-towards-production\"&gt;Agents Towards Production&lt;/a&gt; → reusable patterns &amp;amp; real-world blueprints from prototype to deployment (~9.1k⭐)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/NirDiamant/genai_agents\"&gt;GenAI Agents&lt;/a&gt; → 40+ multi-agent systems with frameworks like LangGraph, CrewAI, OpenAI Swarm (~15.2k⭐)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Shubhamsaboo/awesome-llm-apps\"&gt;Awesome LLM Apps&lt;/a&gt; → practical RAG, AI Agents, Multi-agent Teams, MCP, Autonomous Agents with code (~53.8k⭐)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/microsoft/mcp-for-beginners\"&gt;MCP for Beginners&lt;/a&gt; → open source curriculum by Microsoft with practical examples (~5.9k⭐)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools\"&gt;System Prompts&lt;/a&gt; → library of prompts &amp;amp; config files from 15+ AI products like Cursor, V0, Cluely, Lovable, Replit... (~72.5k⭐)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ashishpatel26/500-AI-Agents-Projects\"&gt;500 AI Agents Projects&lt;/a&gt; → highlights 500+ use cases across industries like healthcare, finance, education, retail, logistics, gaming and more. Each use case links to an open source project (~4k⭐)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;full detailed writeup: &lt;a href=\"https://levelup.gitconnected.com/protocols-best-resources-for-getting-started-with-agents-in-2025-5343dac58316?sk=7588f2c91ca4cf54b9dbaa3bcd184d07\"&gt;here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you know of any other great repos, please share in the comments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LVPxnulDXk8ZwopDlQERcKdX4Eu1RbohQ4UQzmpP3Ps.png?auto=webp&amp;s=10e16b52fdc28f7a453a33e24f5e499a0ab835b8",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LVPxnulDXk8ZwopDlQERcKdX4Eu1RbohQ4UQzmpP3Ps.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=54e6475aab61c86def58da5a163b5bb3c2d13297",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/LVPxnulDXk8ZwopDlQERcKdX4Eu1RbohQ4UQzmpP3Ps.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4becce6d019bd008a75773ab79bf26f923dcb49b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/LVPxnulDXk8ZwopDlQERcKdX4Eu1RbohQ4UQzmpP3Ps.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6729de9d0ecc4b5d73327d70ecd4e72ec2087c94",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/LVPxnulDXk8ZwopDlQERcKdX4Eu1RbohQ4UQzmpP3Ps.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=05ce549e2c0b43e8ea65481b277a841b84cb5445",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/LVPxnulDXk8ZwopDlQERcKdX4Eu1RbohQ4UQzmpP3Ps.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ffd4e4db6aed6cb3079b793b21b5b651573a1bc",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/LVPxnulDXk8ZwopDlQERcKdX4Eu1RbohQ4UQzmpP3Ps.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=112e9ee4bd26697698bf886fa136613ab9aed4c3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "LVPxnulDXk8ZwopDlQERcKdX4Eu1RbohQ4UQzmpP3Ps"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mdcnu8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "anmolbaranwal",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdcnu8/best_repos_protocols_for_learning_and_building/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdcnu8/best_repos_protocols_for_learning_and_building/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753896201,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I saw [unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF · Hugging Face](https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF) just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4\\_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I'm excited to see what else it can do this week!\n\nGitHub: [lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration](https://github.com/lemonade-sdk/lemonade)",
          "author_fullname": "t2_1m2ckixcqh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 64,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mco449",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 239,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/7xpye5hurvff1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 592,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/7xpye5hurvff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/7xpye5hurvff1/DASHPlaylist.mpd?a=1756556941%2CNzk0NzNiNzA1NDFhZTNmZDhmOTA5MTlmYjA5MmMxMWIwYWM0NGQwMDJiYzkzODk0OGRlOTViYmFkOGEwOWNjNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/7xpye5hurvff1/HLSPlaylist.m3u8?a=1756556941%2CMzQ4N2UzODcxMzE3M2M4Mzg4N2NmMmEwZWQ2Y2FjY2JkOTM5ZDBiN2YwNGI5ZWZjNzBmMDg0ZDE4MDc4OGEyMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 239,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=140&amp;height=64&amp;crop=140:64,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=7412d0f300b6da0b8bc9109ecb41ef07f7def3be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753824484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw &lt;a href=\"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF\"&gt;unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF · Hugging Face&lt;/a&gt; just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I&amp;#39;m excited to see what else it can do this week!&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/lemonade-sdk/lemonade\"&gt;lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/7xpye5hurvff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?format=pjpg&amp;auto=webp&amp;s=42d0cbec81034d7b7efd7f37129a81782dc0f3a9",
                  "width": 2074,
                  "height": 960
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f46a864c4101d08c5f7634bda139ea125562aef9",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a85bb6d97aae39128b013a769b747a78d013bef",
                    "width": 216,
                    "height": 99
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c3b96d303e0d238fc8ee25ce519ce9a723c249c3",
                    "width": 320,
                    "height": 148
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cac371463bffd1516bfca661c6b086b5c43e0a77",
                    "width": 640,
                    "height": 296
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbfaa41803cc435e672ff77f44bdc9bbdc1c8952",
                    "width": 960,
                    "height": 444
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=25e3d9ea149342d300ab0c21256440d35acba3ed",
                    "width": 1080,
                    "height": 499
                  }
                ],
                "variants": {},
                "id": "czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mco449",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jfowers_amd",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/",
          "stickied": false,
          "url": "https://v.redd.it/7xpye5hurvff1",
          "subreddit_subscribers": 507576,
          "created_utc": 1753824484,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/7xpye5hurvff1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 592,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/7xpye5hurvff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/7xpye5hurvff1/DASHPlaylist.mpd?a=1756556941%2CNzk0NzNiNzA1NDFhZTNmZDhmOTA5MTlmYjA5MmMxMWIwYWM0NGQwMDJiYzkzODk0OGRlOTViYmFkOGEwOWNjNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/7xpye5hurvff1/HLSPlaylist.m3u8?a=1756556941%2CMzQ4N2UzODcxMzE3M2M4Mzg4N2NmMmEwZWQ2Y2FjY2JkOTM5ZDBiN2YwNGI5ZWZjNzBmMDg0ZDE4MDc4OGEyMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently use OpenWebUI but MCP support on it is at best a struggle. Is there any open source app that can run MCP servers with local LLMs?",
          "author_fullname": "t2_xucqa0ilr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local. Open Source App with MCP Server compatability",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mduk5t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753944680,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently use OpenWebUI but MCP support on it is at best a struggle. Is there any open source app that can run MCP servers with local LLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mduk5t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rm-rf-rm",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mduk5t/local_open_source_app_with_mcp_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mduk5t/local_open_source_app_with_mcp_server/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753944680,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've worked on several projects at this point and every time I end up just making my own thing because working with them is too much of a headache. I was wondering if people have the same experience and if someone could better put into words what is so bad about them. I think we're about due for a new context engineering and LM orchestration library. What should that look like?",
          "author_fullname": "t2_4559hfp8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Whats so bad about LlamaIndex, Haystack, Langchain?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md84d6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753885829,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve worked on several projects at this point and every time I end up just making my own thing because working with them is too much of a headache. I was wondering if people have the same experience and if someone could better put into words what is so bad about them. I think we&amp;#39;re about due for a new context engineering and LM orchestration library. What should that look like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1md84d6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Disneyskidney",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md84d6/whats_so_bad_about_llamaindex_haystack_langchain/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md84d6/whats_so_bad_about_llamaindex_haystack_langchain/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753885829,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "TLDR; Basically, I want llamaswap to expose a \"default\" model to all the clients, and it figures out what is the active model and routes the request to that model in the backend. \n\n------------------------------\n\nI'm running llama-swap and trying to simplify my client integration. Ideally, I want the client to always hit the same endpoint (/v1/chat/completions) without specifying a model name in the payload.\n\nI want to run only one model at a time, and be able to switch that model from the llama-swap Web UI or by reloading the config — without the client having to care or change anything.\n\nRight now it seems llama-swap requires \"model\": \"&lt;id&gt;\" in every request. Is there a way to define a global \"active\" model via UI and the client doesn't need to worry about the specific model. or it can just say the default model and Llamaswap figures out what is the default model set via the UI. \n\nIf this isn’t possible today, is there any known workaround?\n\nI do not want to go back and change the yaml config files. I only want to handle everything (model swapping) from the llamaswap UI itself.",
          "author_fullname": "t2_e33mgcbq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can llama-swap work without specifying the \"model\" field in API requests?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdufwb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753944553,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753944230,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR; Basically, I want llamaswap to expose a &amp;quot;default&amp;quot; model to all the clients, and it figures out what is the active model and routes the request to that model in the backend. &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I&amp;#39;m running llama-swap and trying to simplify my client integration. Ideally, I want the client to always hit the same endpoint (/v1/chat/completions) without specifying a model name in the payload.&lt;/p&gt;\n\n&lt;p&gt;I want to run only one model at a time, and be able to switch that model from the llama-swap Web UI or by reloading the config — without the client having to care or change anything.&lt;/p&gt;\n\n&lt;p&gt;Right now it seems llama-swap requires &amp;quot;model&amp;quot;: &amp;quot;&amp;lt;id&amp;gt;&amp;quot; in every request. Is there a way to define a global &amp;quot;active&amp;quot; model via UI and the client doesn&amp;#39;t need to worry about the specific model. or it can just say the default model and Llamaswap figures out what is the default model set via the UI. &lt;/p&gt;\n\n&lt;p&gt;If this isn’t possible today, is there any known workaround?&lt;/p&gt;\n\n&lt;p&gt;I do not want to go back and change the yaml config files. I only want to handle everything (model swapping) from the llamaswap UI itself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdufwb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "discoveringnature12",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdufwb/can_llamaswap_work_without_specifying_the_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdufwb/can_llamaswap_work_without_specifying_the_model/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753944230,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m building a fully local AI-Scribe for doctors and wanted to know which speech-to-text engines perform well with 5-10 min patient-doctor chats.  \nI ran 55 mock GP consultations (PriMock57) through 15 open- and closed-source models, logged word-error rate (WER) and speed, and only chunked audio when a model crashed on &gt;40 s clips.\n\n# All results\n\n|\\#|Model|Avg WER|Avg sec/file|Host|\n|:-|:-|:-|:-|:-|\n|1|ElevenLabs Scribe v1|**15.0 %**|36 s|API (ElevenLabs)|\n|2|MLX Whisper-L v3-turbo|17.6 %|13 s|Local (Apple M4)|\n|3|Parakeet-0.6 B v2|17.9 %|**5 s**|Local (Apple M4)|\n|4|Canary-Qwen 2.5 B|18.2 %|105 s|Local (L4 GPU)|\n|5|Apple SpeechAnalyzer|18.2 %|6 s|Local (macOS)|\n|6|Groq Whisper-L v3|18.4 %|9 s|API (Groq)|\n|7|Voxtral-mini 3 B|18.5 %|74 s|Local (L4 GPU)|\n|8|Groq Whisper-L v3-turbo|18.7 %|8 s|API (Groq)|\n|9|Canary-1B-Flash|18.8 %|23 s|Local (L4 GPU)|\n|10|Voxtral-mini (API)|19.0 %|23 s|API (Mistral)|\n|11|WhisperKit-L v3-turbo|19.1 %|21 s|Local (macOS)|\n|12|OpenAI Whisper-1|19.6 %|104 s|API (OpenAI)|\n|13|OpenAI GPT-4o-mini|20.6 %|—|API (OpenAI)|\n|14|OpenAI GPT-4o|21.7 %|28 s|API (OpenAI)|\n|15|Azure Foundry Phi-4|36.6 %|213 s|API (Azure)|\n\n# Take-aways\n\n* **ElevenLabs Scribe** leads accuracy but can hallucinate on edge cases.\n* **Parakeet-0.6 B on an M4** runs \\~5× real-time—great if English-only is fine.\n* **Groq Whisper-v3 (turbo)** offers the best cloud price/latency combo.\n* Canary/Canary-Qwen/Phi-4 needed chunking, which bumped runtime.\n* Apple SpeechAnalyzer is a good option for Swift apps.\n\nFor details on the dataset, hardware, and full methodology, see the blog post → [https://omi.health/blog/benchmarking-tts](https://omi.health/blog/benchmarking-tts)\n\nHappy to chat—let me know if you’d like the evaluation notebook once it’s cleaned up!",
          "author_fullname": "t2_6d62mn60w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmark: 15 STT models on long-form medical dialogue",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md1fka",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gjFJQ8b3RSb_IZw6-OUzpBxdAi0cMspjDghHYjUUugU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753865479,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m building a fully local AI-Scribe for doctors and wanted to know which speech-to-text engines perform well with 5-10 min patient-doctor chats.&lt;br/&gt;\nI ran 55 mock GP consultations (PriMock57) through 15 open- and closed-source models, logged word-error rate (WER) and speed, and only chunked audio when a model crashed on &amp;gt;40 s clips.&lt;/p&gt;\n\n&lt;h1&gt;All results&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;#&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Avg WER&lt;/th&gt;\n&lt;th align=\"left\"&gt;Avg sec/file&lt;/th&gt;\n&lt;th align=\"left\"&gt;Host&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;ElevenLabs Scribe v1&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;15.0 %&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;36 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (ElevenLabs)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;MLX Whisper-L v3-turbo&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.6 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;13 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (Apple M4)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Parakeet-0.6 B v2&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.9 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;5 s&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (Apple M4)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;Canary-Qwen 2.5 B&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.2 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;105 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (L4 GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apple SpeechAnalyzer&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.2 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;6 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (macOS)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;Groq Whisper-L v3&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.4 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;9 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (Groq)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Voxtral-mini 3 B&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.5 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;74 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (L4 GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;Groq Whisper-L v3-turbo&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.7 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;8 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (Groq)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;9&lt;/td&gt;\n&lt;td align=\"left\"&gt;Canary-1B-Flash&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.8 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;23 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (L4 GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;Voxtral-mini (API)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.0 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;23 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (Mistral)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;11&lt;/td&gt;\n&lt;td align=\"left\"&gt;WhisperKit-L v3-turbo&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.1 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;21 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (macOS)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;12&lt;/td&gt;\n&lt;td align=\"left\"&gt;OpenAI Whisper-1&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.6 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;104 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (OpenAI)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;OpenAI GPT-4o-mini&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;—&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (OpenAI)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;OpenAI GPT-4o&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.7 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;28 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (OpenAI)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;15&lt;/td&gt;\n&lt;td align=\"left\"&gt;Azure Foundry Phi-4&lt;/td&gt;\n&lt;td align=\"left\"&gt;36.6 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;213 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (Azure)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Take-aways&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;ElevenLabs Scribe&lt;/strong&gt; leads accuracy but can hallucinate on edge cases.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Parakeet-0.6 B on an M4&lt;/strong&gt; runs ~5× real-time—great if English-only is fine.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Groq Whisper-v3 (turbo)&lt;/strong&gt; offers the best cloud price/latency combo.&lt;/li&gt;\n&lt;li&gt;Canary/Canary-Qwen/Phi-4 needed chunking, which bumped runtime.&lt;/li&gt;\n&lt;li&gt;Apple SpeechAnalyzer is a good option for Swift apps.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For details on the dataset, hardware, and full methodology, see the blog post → &lt;a href=\"https://omi.health/blog/benchmarking-tts\"&gt;https://omi.health/blog/benchmarking-tts&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to chat—let me know if you’d like the evaluation notebook once it’s cleaned up!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nxnp5xsw4zff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nxnp5xsw4zff1.png?auto=webp&amp;s=e05c92656322d61fe5bce59966dbb6db87a3dd06",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=10718d12b1ba0f9fb55b88c4a23f5d961dc09a23",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c559978c60e7c0d93343f6902fe18925c72c46b",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b9aca7fb5d5f5538ea1096cbd98355994da3c3d",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e11b8504e80c4f6a6e15f6408d1b821538221d77",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3cee9acfbdb5282b382e74e15ebf8988e803437",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72647e3b3d762b18a3575431b05e25b902927fe8",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "aSKiVMrPqKWIFhKxohn_N0i0HcSUU59xZvQ8NLVGNEQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1md1fka",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MajesticAd2862",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md1fka/benchmark_15_stt_models_on_longform_medical/",
          "stickied": false,
          "url": "https://i.redd.it/nxnp5xsw4zff1.png",
          "subreddit_subscribers": 507576,
          "created_utc": 1753865479,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcfmd2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 678,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 678,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50aa20219586bc9007fb96833d16a6a56c8c1c76",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753805463,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?auto=webp&amp;s=f1df54937600c0db76989bd14eef9e747df1fb0e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1c3476d621a9393fbb7ca11c48a3074c5fd6803",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7cef70bde41dd3225eec3f7d265fbf2704c0182",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab3e2615c90a6581b60c6d33c660bfc0f250b4c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9c2fc1f960e47499df06dc08d78c88be43e15e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15eba021f7d99140c48583ae883d2eb091807f16",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcfmd2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 265,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "subreddit_subscribers": 507576,
          "created_utc": 1753805463,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I open llama.cpp, start a server, then open openwebui to use the model, then after that, it starts generating, but then like 3 minutes after getting into a long coding task, it grinds to a halt, and then my monitors disconnect and I have to shut off and turn on the PC again. How do I fix that? Is it just that Q4KM is too much for a 4090 or is there some other issue?",
          "author_fullname": "t2_uptissiz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Weird issue running qwen3-30b-a3b-thinking in llama.cpp and openwebui on my 4090 and 64GB of RAM rig, Q4_K_M",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdk46y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753913450,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I open llama.cpp, start a server, then open openwebui to use the model, then after that, it starts generating, but then like 3 minutes after getting into a long coding task, it grinds to a halt, and then my monitors disconnect and I have to shut off and turn on the PC again. How do I fix that? Is it just that Q4KM is too much for a 4090 or is there some other issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdk46y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pro-editor-1105",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdk46y/weird_issue_running_qwen330ba3bthinking_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdk46y/weird_issue_running_qwen330ba3bthinking_in/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753913450,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[leiapix](https://leiapix-ai.com/#google_vignette) gives parallax depth but no real rotation. [domoai](https://www.domoai.app/home?via=081621AUG)'s 360 view rotates the entire character like a turntable. tried it on a cartoon cat and got a full clean spin. same file worked for a hug scene after too. anyone tried this on character models?",
          "author_fullname": "t2_1s5qv7lngc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "domoai’s 360 view lets you animate full spins like leiapix but it’s actually 3d",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdoqnv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753925885,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://leiapix-ai.com/#google_vignette\"&gt;leiapix&lt;/a&gt; gives parallax depth but no real rotation. &lt;a href=\"https://www.domoai.app/home?via=081621AUG\"&gt;domoai&lt;/a&gt;&amp;#39;s 360 view rotates the entire character like a turntable. tried it on a cartoon cat and got a full clean spin. same file worked for a hug scene after too. anyone tried this on character models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mdoqnv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Neat_Chapter_9055",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdoqnv/domoais_360_view_lets_you_animate_full_spins_like/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdoqnv/domoais_360_view_lets_you_animate_full_spins_like/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753925885,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking to finetune a model not only on the style and content of messages but also on the times they are sent/delay time in a conversation given a message history on something like discord. Does anyone know how this could be done? Thank you.",
          "author_fullname": "t2_1uejfrt1ah",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any way to train when a model sends messages?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdolik",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753925472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to finetune a model not only on the style and content of messages but also on the times they are sent/delay time in a conversation given a message history on something like discord. Does anyone know how this could be done? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdolik",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SignificanceSad562",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdolik/is_there_any_way_to_train_when_a_model_sends/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdolik/is_there_any_way_to_train_when_a_model_sends/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753925472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI was wondering since I pay so much for Claude Code, if I can somehow use any llocal LLM model for coding simliar for coding?\n\nI have an 4080 Super and 32GB RAM (which I know is not a lot), is there any model that I can use for coding llocally? Sorry I have not been keeping up every day with new models etc.\n\nAnd if yes, is there any way to use Cursor with it? Im using Claude Code Terminal within Cursor currently.",
          "author_fullname": "t2_9w9hk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Want to switch from Claude code (I have a 4080 Super)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mda326",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753890377,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I was wondering since I pay so much for Claude Code, if I can somehow use any llocal LLM model for coding simliar for coding?&lt;/p&gt;\n\n&lt;p&gt;I have an 4080 Super and 32GB RAM (which I know is not a lot), is there any model that I can use for coding llocally? Sorry I have not been keeping up every day with new models etc.&lt;/p&gt;\n\n&lt;p&gt;And if yes, is there any way to use Cursor with it? Im using Claude Code Terminal within Cursor currently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mda326",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nofuture09",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753890377,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to make a video game that utilizes AI to have some conversation with users.  It doesn't need to win an IMO but it should be able to carry normal every day conversations.  And preferably it would be able to do text to speech.  But I don't think normal computers are powerful enough for this?  Am I mistaken?  Can a local llama of some type be run on an average PC to understand and speak?",
          "author_fullname": "t2_u5j388982",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI for normal PCs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdbiei",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753893603,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to make a video game that utilizes AI to have some conversation with users.  It doesn&amp;#39;t need to win an IMO but it should be able to carry normal every day conversations.  And preferably it would be able to do text to speech.  But I don&amp;#39;t think normal computers are powerful enough for this?  Am I mistaken?  Can a local llama of some type be run on an average PC to understand and speak?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdbiei",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ShardsOfSalt",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753893603,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Simon Willison says “Ivan Fioravanti built this 44GB 3bit quantized version for MLX, specifically sized so people with 64GB machines could have a chance of running it. I tried it out... and it works extremely well.”\n\nhttps://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;utm_campaign=post&amp;utm_medium=email\n\nI’ve run the model with LMStudio on a 64gb M1 Max Studio. LMStudio initially would not run the model, providing a popup to that effect. The popup also allowed me to adjust the guardrails. I had to turn them off entirely to run the model.",
          "author_fullname": "t2_mjsmz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 Air on 64gb Mac with MLX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcvc46",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753843931,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simon Willison says “Ivan Fioravanti built this 44GB 3bit quantized version for MLX, specifically sized so people with 64GB machines could have a chance of running it. I tried it out... and it works extremely well.”&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;amp;utm_campaign=post&amp;amp;utm_medium=email\"&gt;https://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;amp;utm_campaign=post&amp;amp;utm_medium=email&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I’ve run the model with LMStudio on a 64gb M1 Max Studio. LMStudio initially would not run the model, providing a popup to that effect. The popup also allowed me to adjust the guardrails. I had to turn them off entirely to run the model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?auto=webp&amp;s=8e07b76e8d1166d0c8c5e741492849f4edee3088",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8312949968b09310a164bbbce12556723423845d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=90c385f8326e1d8b1138a0ffac4e59197db5b20a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8190f5a9b9608e9f430b3b6ce31a3bb4d883ad41",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=81dddb64d3287e341788d22055b06bdb58339b4b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d3ba2097184289d60db6279c54a5ce24897a683",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd64275068932684e1debcb761559964142a0a00",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcvc46",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jarec707",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753843931,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Can Page Assist from n4ze3m be the best thing ever happened to Ollama after trying their new own GUI (and all the others btw) ? A superlight browser extension with everything I need and more.People who tried it,what do you think?",
          "author_fullname": "t2_gelzgtkby",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Page Assist",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mduqj2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753945337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can Page Assist from n4ze3m be the best thing ever happened to Ollama after trying their new own GUI (and all the others btw) ? A superlight browser extension with everything I need and more.People who tried it,what do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mduqj2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Illustrious-Dot-6888",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mduqj2/page_assist/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mduqj2/page_assist/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753945337,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_lw8nvwnyi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "https://x.com/autopoiesislab/status/1950755654471131450?t=JZ8AtogcUFhwgzoKTM67Jw&amp;s=19",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mdytsk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/x8PGMJhFNQr2pYpWh6kaowsoTUuh3uZ28JepjM_J2lE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753960711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5c2txiap17gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5c2txiap17gf1.png?auto=webp&amp;s=63ec417c4e5883574f4aafba9a95d9059e80fa2b",
                  "width": 2048,
                  "height": 1152
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5c2txiap17gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a06f03b603b438c3b633740446a0b4bf9b53b644",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/5c2txiap17gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d20515669ea9a3025f258602468b7e06057532e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/5c2txiap17gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=90f496749e56281ddf311f861c67edd3179c7392",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/5c2txiap17gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1230b812a6afb26baf33d33f1263100224db10e5",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/5c2txiap17gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c234bb1eabfbdc234e5dd1ec2d36a7c8fe928fbf",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/5c2txiap17gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b996e5c2d48f352b2db19dc7d29c6163a0ea2e06",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "RBrQzSq-XWPWFfd17eheYJYIhLLOXHNMQkNnrXvFiNE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mdytsk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Soggy-Ad-8708",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdytsk/httpsxcomautopoiesislabstatus1950755654471131450tj/",
          "stickied": false,
          "url": "https://i.redd.it/5c2txiap17gf1.png",
          "subreddit_subscribers": 507576,
          "created_utc": 1753960711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We just see companies from US, China, and the only seed of France, mistralai. Where is UK, the pair of France, and India, the nation with most population. ",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "where is UK and India?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdu7se",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753943378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just see companies from US, China, and the only seed of France, mistralai. Where is UK, the pair of France, and India, the nation with most population. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mdu7se",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdu7se/where_is_uk_and_india/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdu7se/where_is_uk_and_india/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753943378,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen people claim that the new TR PROs can achieve the full 8-channel memory bandwidth even in SKUs with 16-cores. That's not the case.\n\nThe issue with the limited CCD bandwidth seems to still be present, and affects the low-number CCD parts. You can only achieve the full 8-channel bandwidth with 64-core+ WX CPUs.\n\nCheck the \"Latest baselines\" section in a processor's page at  [cpubenchmark.net](http://cpubenchmark.net)  with links to individual results where the \"Memory Threaded\" result is listed under \"Memory Mark\":\n\n|CPU|Memory BW|Reference|Notes|\n|:-|:-|:-|:-|\n|[AMD Threadripper PRO 9955WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9955WX&amp;id=6803) (16-cores)|\\~115 GB/s|[BL5099051 - Jul 20 2025](https://www.passmark.com/baselines/V11/display.php?id=509905130667)|2x CCD|\n|[AMD Threadripper PRO 9965WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9965WX&amp;id=6804) (24-cores)|\\~272 GB/s|[BL2797485 - Jul 29 2025](https://www.passmark.com/baselines/V11/display.php?id=279748548819) (other baselines start from 250GB/s)|4x CCDs|\n|[AMD Threadripper PRO 9975WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9975WX&amp;id=6799) (32-cores)|\\~272 GB/s|[BL2797820 - Jul 29 2025](https://www.passmark.com/baselines/V11/display.php?id=279782022829)|4x CCDs|\n|[AMD Threadripper PRO 9985WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9985WX&amp;id=6807) (64-cores)|\\~367 GB/s|[BL5099130 - Jul 21 2025](https://www.passmark.com/baselines/V11/display.php?id=509913021820)|8x CCDs|\n\nTherefore:\n\n* the 16-core 9955WX has lower mem bw than even a DDR4 EPYC CPU (e.g. [7R43 with 191 GB/s](https://www.passmark.com/baselines/V10/display.php?id=226455755507)).\n* the 24-core and 32-core parts have lower mem bw than DDR5 Genoa EPYCs (even some 16-core parts).\n* the 64-core and 96-core Threadrippers are not CCD-number limited, but still lose to the EPYCs since those have 12 channels (unless you use 7200 MT/s memory).\n\nFor comparison, check the excellent related threads by u/fairydreaming for the previous gen Threadrippers and EPYC Genoa/Turin:\n\n* [Comparing Threadripper 7000 memory bandwidth for all models : r/threadripper](https://www.reddit.com/r/threadripper/comments/1azmkvg/comparing_threadripper_7000_memory_bandwidth_for/)\n* [Memory bandwidth values (STREAM TRIAD benchmark results) for most Epyc Genoa CPUs (single and dual configurations) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/)\n* [STREAM TRIAD memory bandwidth benchmark values for Epyc Turin - almost 1 TB/s for a dual CPU system : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/)\n\nIf someone insists on buying a new TR Pro for their great compute throughput, I would suggest to at least skip the 16-core part.",
          "author_fullname": "t2_lw9me25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PSA: The new Threadripper PROs (9000 WX) are still CCD-Memory Bandwidth bottlenecked",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcrx23",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753834394,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753834203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen people claim that the new TR PROs can achieve the full 8-channel memory bandwidth even in SKUs with 16-cores. That&amp;#39;s not the case.&lt;/p&gt;\n\n&lt;p&gt;The issue with the limited CCD bandwidth seems to still be present, and affects the low-number CCD parts. You can only achieve the full 8-channel bandwidth with 64-core+ WX CPUs.&lt;/p&gt;\n\n&lt;p&gt;Check the &amp;quot;Latest baselines&amp;quot; section in a processor&amp;#39;s page at  &lt;a href=\"http://cpubenchmark.net\"&gt;cpubenchmark.net&lt;/a&gt;  with links to individual results where the &amp;quot;Memory Threaded&amp;quot; result is listed under &amp;quot;Memory Mark&amp;quot;:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;CPU&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory BW&lt;/th&gt;\n&lt;th align=\"left\"&gt;Reference&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notes&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9955WX&amp;amp;id=6803\"&gt;AMD Threadripper PRO 9955WX&lt;/a&gt; (16-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~115 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=509905130667\"&gt;BL5099051 - Jul 20 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2x CCD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9965WX&amp;amp;id=6804\"&gt;AMD Threadripper PRO 9965WX&lt;/a&gt; (24-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~272 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=279748548819\"&gt;BL2797485 - Jul 29 2025&lt;/a&gt; (other baselines start from 250GB/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;4x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9975WX&amp;amp;id=6799\"&gt;AMD Threadripper PRO 9975WX&lt;/a&gt; (32-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~272 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=279782022829\"&gt;BL2797820 - Jul 29 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9985WX&amp;amp;id=6807\"&gt;AMD Threadripper PRO 9985WX&lt;/a&gt; (64-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~367 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=509913021820\"&gt;BL5099130 - Jul 21 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Therefore:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the 16-core 9955WX has lower mem bw than even a DDR4 EPYC CPU (e.g. &lt;a href=\"https://www.passmark.com/baselines/V10/display.php?id=226455755507\"&gt;7R43 with 191 GB/s&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;the 24-core and 32-core parts have lower mem bw than DDR5 Genoa EPYCs (even some 16-core parts).&lt;/li&gt;\n&lt;li&gt;the 64-core and 96-core Threadrippers are not CCD-number limited, but still lose to the EPYCs since those have 12 channels (unless you use 7200 MT/s memory).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For comparison, check the excellent related threads by &lt;a href=\"/u/fairydreaming\"&gt;u/fairydreaming&lt;/a&gt; for the previous gen Threadrippers and EPYC Genoa/Turin:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/threadripper/comments/1azmkvg/comparing_threadripper_7000_memory_bandwidth_for/\"&gt;Comparing Threadripper 7000 memory bandwidth for all models : r/threadripper&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/\"&gt;Memory bandwidth values (STREAM TRIAD benchmark results) for most Epyc Genoa CPUs (single and dual configurations) : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/\"&gt;STREAM TRIAD memory bandwidth benchmark values for Epyc Turin - almost 1 TB/s for a dual CPU system : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If someone insists on buying a new TR Pro for their great compute throughput, I would suggest to at least skip the 16-core part.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?auto=webp&amp;s=66e0f40cc65b2257b6a82171108f852dae40bbb8",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a6db3fea34b4baa46c98dcb2bf7d4162a03ce299",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3423d6e5c6b7fd741788ee8997aaf7af37b444e6",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=938295c9480e314e0b1d0d94b264e9f050cdda43",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97458ad0120eb62027d0d8ef93dc4c678f9ce61e",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48d787caa6a26254352916bd9930bd786336a6fd",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fec7e3647f34d8e8ee777b2fcb9b2de2abd6e79a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcrx23",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "henfiber",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753834203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.\n\n✨ Key Enhancements:\n\n✅ Enhanced reasoning, coding, and math skills\n\n✅ Broader multilingual knowledge\n\n✅ Improved long-context understanding (up to 256K tokens)\n\n✅ Better alignment with user intent and open-ended tasks\n\n✅ No more &lt;think&gt; blocks — now operating exclusively in non-thinking mode\n\n🔧 With 3B activated parameters, it's approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking\n\nHugging Face: https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\n\nQwen Chat: https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507\n\nModel scope: https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary\n\n\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Qwen3-30B-A3B Small Update",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcg4qt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 341,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 341,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/O7xcYRkNGuBB0yBDfOkkcWs5DpYysQdkMKJvvFlOYpA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753806599,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.&lt;/p&gt;\n\n&lt;p&gt;✨ Key Enhancements:&lt;/p&gt;\n\n&lt;p&gt;✅ Enhanced reasoning, coding, and math skills&lt;/p&gt;\n\n&lt;p&gt;✅ Broader multilingual knowledge&lt;/p&gt;\n\n&lt;p&gt;✅ Improved long-context understanding (up to 256K tokens)&lt;/p&gt;\n\n&lt;p&gt;✅ Better alignment with user intent and open-ended tasks&lt;/p&gt;\n\n&lt;p&gt;✅ No more &amp;lt;think&amp;gt; blocks — now operating exclusively in non-thinking mode&lt;/p&gt;\n\n&lt;p&gt;🔧 With 3B activated parameters, it&amp;#39;s approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking&lt;/p&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen Chat: &lt;a href=\"https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507\"&gt;https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Model scope: &lt;a href=\"https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary\"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nd904g7gbuff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?auto=webp&amp;s=e65b518bfd8179ffe5850438ba9b1ea0fdbad33f",
                  "width": 900,
                  "height": 506
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f840db78bf1bdfd3bc2fbe2fce643b2615c41103",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d867b87e626895bab9aa6038ad0daeda82c3412b",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3bb498d8e823f04e6a1a8de9d73a55aa81af09d7",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b713bd1bbe154007dd6c0b8474098b47bf58ba4d",
                    "width": 640,
                    "height": 359
                  }
                ],
                "variants": {},
                "id": "G3G0aRr753pT7ZAwSw8VgFtbVKOybrXjQhhRczqdvYg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcg4qt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/",
          "stickied": false,
          "url": "https://i.redd.it/nd904g7gbuff1.jpeg",
          "subreddit_subscribers": 507576,
          "created_utc": 1753806599,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm setting up a local LLM to run in the background on my MacBook Pro (M3 Pro). The main use case is this: I use a dictation app (like SuperWhisper or Spokenly) to convert my voice to text, and then send that text to a local LLM server for processing. Think: summarizing, answering, rephrasing, correction, or responding intelligently to the text input.\n\nI want something:\n\n- Fast (low latency for near-real-time dictation use)\n\n- Reasonably accurate\n\n- Local (no cloud APIs)\n\n- Ideally OpenAI-compatible API so it's easier to integrate with other tools\n\nWith some flexibility for future use cases beyond just dictation\n\nSo far I'm looking at:\n\n- llama.cpp (via llama-server)\n\n- Ollama\n\nAnd what Llama model would you recommend? I was thinking of Gemma 3, but are there better ones? \n\nWould love to hear from others who've done similar setups. Which stack do you recommend and why?",
          "author_fullname": "t2_e33mgcbq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help choosing between Ollama, llama.cpp, or something else for background LLM server (used with dictation)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdma9a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753918996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m setting up a local LLM to run in the background on my MacBook Pro (M3 Pro). The main use case is this: I use a dictation app (like SuperWhisper or Spokenly) to convert my voice to text, and then send that text to a local LLM server for processing. Think: summarizing, answering, rephrasing, correction, or responding intelligently to the text input.&lt;/p&gt;\n\n&lt;p&gt;I want something:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Fast (low latency for near-real-time dictation use)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Reasonably accurate&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Local (no cloud APIs)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ideally OpenAI-compatible API so it&amp;#39;s easier to integrate with other tools&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With some flexibility for future use cases beyond just dictation&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;m looking at:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;llama.cpp (via llama-server)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ollama&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And what Llama model would you recommend? I was thinking of Gemma 3, but are there better ones? &lt;/p&gt;\n\n&lt;p&gt;Would love to hear from others who&amp;#39;ve done similar setups. Which stack do you recommend and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdma9a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "discoveringnature12",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753918996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "C'est la première fois qu'un modèle utilise intelligemment les serveurs MCP tout seul ! Ce n'est pas juste un ou deux serveurs et puis une réponse complètement à côté de la plaque !\n\nFor those who want my MCP flow, here’s the Pastebin:\n\n[https://pastebin.com/WNPrcjLS](https://pastebin.com/WNPrcjLS)\n\nhttps://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;format=png&amp;auto=webp&amp;s=30fca5c5a305810d2969af3035d710cef5a30268",
          "author_fullname": "t2_ti5m9mpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30b-3ab-2507 is a beast for MCP usage!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8kjwp8wkxuff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 136,
                  "x": 108,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd9fe7d167fa67def68be16708c9072dff16e4b9"
                },
                {
                  "y": 273,
                  "x": 216,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d68578f7cbad05a254ecfd9b9a974bce181b73c4"
                },
                {
                  "y": 405,
                  "x": 320,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=572ae738bba26cff6b6dcf1e4969cc51201c065e"
                },
                {
                  "y": 811,
                  "x": 640,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f88d4f809869f8243befb328a5639859eeeccba"
                }
              ],
              "s": {
                "y": 1150,
                "x": 907,
                "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;format=png&amp;auto=webp&amp;s=30fca5c5a305810d2969af3035d710cef5a30268"
              },
              "id": "8kjwp8wkxuff1"
            }
          },
          "name": "t3_1mcji8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 212,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 212,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Ku7pPJKnjoNSXHTx41JonGncgMhMPCUf8ZnqoDSjoGY.jpg",
          "edited": 1753816781,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753813999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;C&amp;#39;est la première fois qu&amp;#39;un modèle utilise intelligemment les serveurs MCP tout seul ! Ce n&amp;#39;est pas juste un ou deux serveurs et puis une réponse complètement à côté de la plaque !&lt;/p&gt;\n\n&lt;p&gt;For those who want my MCP flow, here’s the Pastebin:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/WNPrcjLS\"&gt;https://pastebin.com/WNPrcjLS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268\"&gt;https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcji8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Ninja7526",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753813999,
          "num_crossposts": 5,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1hyfw9k8s6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Introducing Agent Data Shuttle (ADS): fully open-source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdcqs8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/5HjjfFOw57ezAPSFdqSymE-VTFqq6DY-iCtsD-MyEy8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753896377,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/9n9nkv5eq1gf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/9n9nkv5eq1gf1.png?auto=webp&amp;s=d18a2c3cbef9588a7c64ad1bad957f14bc4ce9e5",
                  "width": 2940,
                  "height": 1658
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/9n9nkv5eq1gf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fdc24894364d685984019c18cba8adca48642a86",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/9n9nkv5eq1gf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a73fe79229b45e831ecf5507d1628a203f3030d5",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/9n9nkv5eq1gf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd76bf2dc66a1a44b857b202dd5c2e8a449c62a8",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/9n9nkv5eq1gf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5c5993a81b9f376d0bc605d4ef7a8781f6aecc5",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/9n9nkv5eq1gf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5bd5891754bbbedde18ef6bb3a162e7525ef9e0e",
                    "width": 960,
                    "height": 541
                  },
                  {
                    "url": "https://preview.redd.it/9n9nkv5eq1gf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b8c5e857949a71c14a4e34a46cbe6dcd551c418",
                    "width": 1080,
                    "height": 609
                  }
                ],
                "variants": {},
                "id": "EnKK_f1omVqZNCj8hrmEuX5NAywmh0z_nLMHPbH_cx0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mdcqs8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "awesome_stuff101",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdcqs8/introducing_agent_data_shuttle_ads_fully/",
          "stickied": false,
          "url": "https://i.redd.it/9n9nkv5eq1gf1.png",
          "subreddit_subscribers": 507576,
          "created_utc": 1753896377,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Goal:**  \nI'm building a local AI assistant — like a voice-based Alfred — that runs entirely on my machine. I've already downloaded and installed **LLaMA 2 13B Q5 Chat** for this purpose. However, I've noticed that the chat model includes certain **filters** or restrictions that limit the assistant’s responses.\n\nIn my research, I came across **SillyTavern**, which is known for providing more flexibility and customization when interacting with local LLMs — including better control over prompt behavior and fewer filtering constraints.\n\nMy plan is to **integrate SillyTavern as the conversational layer** within my custom **Alfred interface**, using it as the chat system that powers the assistant's personality, memory, and dialogue — while handling voice input/output through local tools and ElevenLabs. Is this possible can someone guide? What exactly is SillyTavern",
          "author_fullname": "t2_a4m5uj5t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hey everyone I'm pretty new at this. I'm a designer please help me. Stupid Question",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mdln75",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753917291,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt;&lt;br/&gt;\nI&amp;#39;m building a local AI assistant — like a voice-based Alfred — that runs entirely on my machine. I&amp;#39;ve already downloaded and installed &lt;strong&gt;LLaMA 2 13B Q5 Chat&lt;/strong&gt; for this purpose. However, I&amp;#39;ve noticed that the chat model includes certain &lt;strong&gt;filters&lt;/strong&gt; or restrictions that limit the assistant’s responses.&lt;/p&gt;\n\n&lt;p&gt;In my research, I came across &lt;strong&gt;SillyTavern&lt;/strong&gt;, which is known for providing more flexibility and customization when interacting with local LLMs — including better control over prompt behavior and fewer filtering constraints.&lt;/p&gt;\n\n&lt;p&gt;My plan is to &lt;strong&gt;integrate SillyTavern as the conversational layer&lt;/strong&gt; within my custom &lt;strong&gt;Alfred interface&lt;/strong&gt;, using it as the chat system that powers the assistant&amp;#39;s personality, memory, and dialogue — while handling voice input/output through local tools and ElevenLabs. Is this possible can someone guide? What exactly is SillyTavern&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mdln75",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Iamtheguyyy",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mdln75/hey_everyone_im_pretty_new_at_this_im_a_designer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdln75/hey_everyone_im_pretty_new_at_this_im_a_designer/",
          "subreddit_subscribers": 507576,
          "created_utc": 1753917291,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}