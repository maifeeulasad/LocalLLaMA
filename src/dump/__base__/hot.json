{
  "kind": "Listing",
  "data": {
    "after": "t3_1m6vj8o",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Full text: [https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf](https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf)",
          "author_fullname": "t2_1f8trxud0p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Encouragement of \"Open-Source and Open-Weight AI\" is now the official policy of the U.S. government.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 96,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7dmy2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 564,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 564,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/dE8iio5_qxWifLCWxYWX3e_adPkw9bIROwsF8nQf2uk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753287492,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Full text: &lt;a href=\"https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf\"&gt;https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/736cx17efnef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/736cx17efnef1.png?auto=webp&amp;s=1441150f32beb0af75982abe485e922ee54a12ff",
                  "width": 1028,
                  "height": 711
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=251e5e43fe714ddc7032706935cd9fc3d43c1165",
                    "width": 108,
                    "height": 74
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=53a8eee481ec4de748ebe8209d3ec2aae407dc4c",
                    "width": 216,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0f980d4f894d0dc575de06c03023b4c50e561e9",
                    "width": 320,
                    "height": 221
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b6dc537086eca79402f273c84f9cfeda0bb9e59",
                    "width": 640,
                    "height": 442
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f44e16c425c6833fd0d6beb6b440f43e9f77c83",
                    "width": 960,
                    "height": 663
                  }
                ],
                "variants": {},
                "id": "cL7vyiaEvwvylAKGE60rRc6GWzCOBXcgInu5tOXXrMs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7dmy2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlowiesEatShitAndDie",
          "discussion_type": null,
          "num_comments": 149,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/",
          "stickied": false,
          "url": "https://i.redd.it/736cx17efnef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753287492,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google DeepMind's new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google DeepMind release Mixture-of-Recursions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7fwhl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 206,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 206,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753292638,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google DeepMind&amp;#39;s new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : &lt;a href=\"https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR\"&gt;https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?auto=webp&amp;s=5d63020d7a90f3dd9933e344b8670cb78b0b5165",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=46cf0a3ba4ca4557db533db7facf3345d193ff14",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3bcb7fc50ee4b735515cf0df1fa150704579262",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc83119edf707dba6c55fc32d3a9910075ea589d",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7fwhl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753292638,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_58qturpl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7kkyn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 100,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/4F_QBog-_2mH7QRiv8VyzkdamiGlY40D_u3V_zWrFe8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753303228,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/krjfba3oqoef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?auto=webp&amp;s=b6cbfb5587cef2fa66062ecc89fb256764949473",
                  "width": 1512,
                  "height": 1032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cf6d39fa1fa4f5683732a0b8993daf74e849afa",
                    "width": 108,
                    "height": 73
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75db258338b42adc68e3bb0413ff39fa017bc706",
                    "width": 216,
                    "height": 147
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24d9a0eca278a1c3b8db5b848d01205a811bb68d",
                    "width": 320,
                    "height": 218
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c50574d0e0fc9f8e0044c2d18d3618b1d155e4e7",
                    "width": 640,
                    "height": 436
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a748b89f2ff4d8a9a5d9acb8b0eb069410e02c88",
                    "width": 960,
                    "height": 655
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9a2fa720b512765ba2dc5a092f21cccc7eac5f8",
                    "width": 1080,
                    "height": 737
                  }
                ],
                "variants": {},
                "id": "SyAH9oAX8vUViOHksDj2yqNlqn4fwNnt93W4G27ThZw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7kkyn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abdouhlili",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/",
          "stickied": false,
          "url": "https://i.redd.it/krjfba3oqoef1.jpeg",
          "subreddit_subscribers": 503516,
          "created_utc": 1753303228,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_pmniwf57y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google has shared the system prompt that got Gemini 2.5 Pro IMO 2025 Gold Medal üèÖ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7k4ix",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 100,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753302182,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "alphaxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.alphaxiv.org/abs/2507.15855",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7k4ix",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "secopsml",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7k4ix/google_has_shared_the_system_prompt_that_got/",
          "stickied": false,
          "url": "https://www.alphaxiv.org/abs/2507.15855",
          "subreddit_subscribers": 503516,
          "created_utc": 1753302182,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "demo: [https://flappybird.njkumar.com/](https://flappybird.njkumar.com/)\n\nblogpost: [https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/](https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/)\n\nI finally got some time to put some development into this, but I optimized a flappy bird diffusion model to run around 30FPS on my Macbook, and around 12-15FPS on my iPhone 14 Pro. More details about the optimization experiments in the blog post above, but surprisingly trained this model on a couple hours of flappy bird data and 3-4 days of training on a rented A100. \n\nWorld models are definitely going to be really popular in the future, but I think there should be more accessible ways to distribute and run these models, especially as inference becomes more expensive, which is why I went for an on-device approach.\n\nLet me know what you guys think!",
          "author_fullname": "t2_6xc1kgl4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I optimized a Flappy Bird diffusion world model to run locally on my phone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m7p7ek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/71l2pz57opef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/71l2pz57opef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/71l2pz57opef1/DASHPlaylist.mpd?a=1755911493%2CMWJiNmExMTM2MmI0ZWU5YWM0MGU2YjIxYmU1NTNiNmRlYzMyYmMyMjEzN2YyMDE2ZWQ5YjJlYjYxMjcwMzBmNw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/71l2pz57opef1/HLSPlaylist.m3u8?a=1755911493%2CYWFkZTA1ZmQ3NDJhYmJhMWM3Y2FkYzNiOTdmYTRmZGY5Y2I5ZWRjODRkZGJiY2RmMGRiZGYwNmJkNjE4NTYxZg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=b962b22bb648b1c6e8f58f793ea34c9c5459c008",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753314632,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;demo: &lt;a href=\"https://flappybird.njkumar.com/\"&gt;https://flappybird.njkumar.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;blogpost: &lt;a href=\"https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/\"&gt;https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I finally got some time to put some development into this, but I optimized a flappy bird diffusion model to run around 30FPS on my Macbook, and around 12-15FPS on my iPhone 14 Pro. More details about the optimization experiments in the blog post above, but surprisingly trained this model on a couple hours of flappy bird data and 3-4 days of training on a rented A100. &lt;/p&gt;\n\n&lt;p&gt;World models are definitely going to be really popular in the future, but I think there should be more accessible ways to distribute and run these models, especially as inference becomes more expensive, which is why I went for an on-device approach.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you guys think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/71l2pz57opef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?format=pjpg&amp;auto=webp&amp;s=981c7dbb770b9f932308688752873c877a45ab76",
                  "width": 1080,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=82d4dd1d9fe94438a59143a22dda39ec75f1b8d0",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8dab6463cd659f5b4cf87c83b3eafc0bb67babde",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bd6e375eb171716a6642ec34d15dbe84a8777e59",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3b58cbca08da41bc17c7d3a9eb4c34ab9e3a0eab",
                    "width": 640,
                    "height": 1137
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2d235cc44118aa9d4bb46169ca00de87c5496ed7",
                    "width": 960,
                    "height": 1706
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9d0408754823b54ec35b393c9897ef1782beaae5",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7p7ek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fendiwap1234",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/",
          "stickied": false,
          "url": "https://v.redd.it/71l2pz57opef1",
          "subreddit_subscribers": 503516,
          "created_utc": 1753314632,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/71l2pz57opef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/71l2pz57opef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/71l2pz57opef1/DASHPlaylist.mpd?a=1755911493%2CMWJiNmExMTM2MmI0ZWU5YWM0MGU2YjIxYmU1NTNiNmRlYzMyYmMyMjEzN2YyMDE2ZWQ5YjJlYjYxMjcwMzBmNw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/71l2pz57opef1/HLSPlaylist.m3u8?a=1755911493%2CYWFkZTA1ZmQ3NDJhYmJhMWM3Y2FkYzNiOTdmYTRmZGY5Y2I5ZWRjODRkZGJiY2RmMGRiZGYwNmJkNjE4NTYxZg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Still taking a few cables out doing management but just built this beast! ",
          "author_fullname": "t2_1lvyip3xqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Local llm build, 144gb vram monster",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "azb7bsq4hnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=adf186a078b154422310a8ab85dc4132d62a884d"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c4e3360ec9a223a2e1993e68b89239bea8fab5d"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9dd1df27f9a6dde12d214b9f664c2a6d6becad8c"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=992b723a50e9d89ba6dcf55d25b4a32b0903800a"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=058f88502941d5e4b9b4a9b6d971f05145512c21"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e1de07a0090f10302ba99275055a1af4da7ed70"
                }
              ],
              "s": {
                "y": 4284,
                "x": 5712,
                "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=e5a817b70709275d6498ac676dabcc5a07ed4165"
              },
              "id": "azb7bsq4hnef1"
            },
            "nxp6tyq4hnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=33662b7541293a8c293d31753b58f64f8bbda4a7"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d757ced2e85613e3198790f7afb3ae6bac7b3d6"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8779de52430894d54ebad319add5ed70ce03b64"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eaa15f34cc480d047699e1a8d5b71f60d5495d61"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=01d519c0039f3e130011732eab99230101d47b7c"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=440c40a1c74d5d5f9d87e940413381fc01da53fc"
                }
              ],
              "s": {
                "y": 4284,
                "x": 5712,
                "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=9d8c4f1e52a3beb39d8661bbf0151cb972d2bfa6"
              },
              "id": "nxp6tyq4hnef1"
            }
          },
          "name": "t3_1m7dtpm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 126,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "azb7bsq4hnef1",
                "id": 712359206
              },
              {
                "media_id": "nxp6tyq4hnef1",
                "id": 712359207
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 126,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VGM2yiS76HMEN0da0De5H87rkjtR_9prbewrkSRRamQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753287916,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Still taking a few cables out doing management but just built this beast! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m7dtpm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7dtpm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EasyConference4177",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m7dtpm",
          "subreddit_subscribers": 503516,
          "created_utc": 1753287916,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1tyihnszqp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "It‚Äôs time to lead guys",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7iui2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "ups": 56,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 56,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wVJyw8lj6m_CjsJ3hvReZdLDdeQVPQinrV8KE5bty7s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753299281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8lao0yzueoef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8lao0yzueoef1.jpeg?auto=webp&amp;s=0c662dc781e39273745a1bada51c2d76d853420b",
                  "width": 1289,
                  "height": 1290
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8lao0yzueoef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c5ff315859aea9328f79e4775b4e840b3fc93ea",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/8lao0yzueoef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a4355f30d97b8b4ebd8e0a954032ad3c97f75fe",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/8lao0yzueoef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a255f8b0cb2ebdcde284d0f8a01ec585e92dbf17",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/8lao0yzueoef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=493841cae72268022c8994000297dcc9374e76af",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/8lao0yzueoef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1659c1a3f28b3386e39b9887f4c319d71fdfe99",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/8lao0yzueoef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c3fea652ec139c6414c815ba32406afd415534b9",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "XaFlUJvjcx52NZH_uBdMqMdFr--zELn0OPjvQRMY9as"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7iui2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "giofifnewph",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7iui2/its_time_to_lead_guys/",
          "stickied": false,
          "url": "https://i.redd.it/8lao0yzueoef1.jpeg",
          "subreddit_subscribers": 503516,
          "created_utc": 1753299281,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After all the buzz, Moonshot AI dropped Kimi K2 with 1T parameters, and it‚Äôs being pitched as the open-source Claude Sonnet 4 alternative. Naturally, I had to run the ultimate coding face-off.\n\nI‚Äôve mostly compared them on the following factors:\n\n* Pricing and Speed\n* Frontend Coding\n* Agentic Coding (MCP integration) and how well it works with recent libraries\n\n# Pricing and Speed\n\nYou might already know Sonnet 4 comes with $3/M input tokens and $15/M output tokens. K2, on the other hand, costs about $0.15/M input tokens and $2.50/M output tokens.\n\nWe can already see a massive price gap between these two models. In the test, we ran two code-heavy prompts for both models, roughly totaling 300k tokens each. Sonnet 4 cost around $5 for the entire test, whereas K2 cost just $0.53 - straight up, K2 is around 10x cheaper.\n\n**Speed:** Claude Sonnet 4 clocks around 91 output tokens per second, while K2 manages just 34.1. That‚Äôs painfully slow in comparison.\n\n# Frontend Coding\n\n* **Kimi K2:** Took ages to implement it, but nailed the entire thing in one go.\n* **Claude Sonnet 4:** Super quick with the implementation, but broke the voice support and even ghosted parts of what was asked in the prompt.\n\n# Agentic Coding\n\n* Neither of them wrote a fully working implementation‚Ä¶ which was completely unexpected.\n* Sonnet 4 was worse: it took over 10 minutes and spent most of that time stuck on TypeScript type errors. After all that, it returned false positives in the implementation.\n\n* K2 came close but still couldn‚Äôt figure it out completely.\n\n# Final Take\n\n* On a budget? K2 is a no‚Äëbrainer - almost the same (or better) code quality, at a tenth of the cost.\n* Need speed and can swallow the cost? Stick with Sonnet 4 - you won‚Äôt get much performance gain with K2.\n* Minor edge? K2 might have the upper hand in prompt-following and agentic fluency, despite being slower.\n\nYou can find the entire blog post with a demo for each here: [Kimi K2 vs. Claude 4 Sonnet: what you should pick for agentic coding](https://composio.dev/blog/kimi-k2-vs-claude-4-sonnet-what-you-should-pick-for-agentic-coding)\n\nAlso, I would love to know your preference between the two models. I'm still unsure whether to stick with my go-to Sonnet 4 or switch to Kimi K2. What's your experience with Kimi's response?",
          "author_fullname": "t2_1jl5023gxv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Sonnet 4 for Agentic Coding (Tested on Claude Code)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7c2gr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753284823,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753283941,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After all the buzz, Moonshot AI dropped Kimi K2 with 1T parameters, and it‚Äôs being pitched as the open-source Claude Sonnet 4 alternative. Naturally, I had to run the ultimate coding face-off.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve mostly compared them on the following factors:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pricing and Speed&lt;/li&gt;\n&lt;li&gt;Frontend Coding&lt;/li&gt;\n&lt;li&gt;Agentic Coding (MCP integration) and how well it works with recent libraries&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Pricing and Speed&lt;/h1&gt;\n\n&lt;p&gt;You might already know Sonnet 4 comes with $3/M input tokens and $15/M output tokens. K2, on the other hand, costs about $0.15/M input tokens and $2.50/M output tokens.&lt;/p&gt;\n\n&lt;p&gt;We can already see a massive price gap between these two models. In the test, we ran two code-heavy prompts for both models, roughly totaling 300k tokens each. Sonnet 4 cost around $5 for the entire test, whereas K2 cost just $0.53 - straight up, K2 is around 10x cheaper.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Claude Sonnet 4 clocks around 91 output tokens per second, while K2 manages just 34.1. That‚Äôs painfully slow in comparison.&lt;/p&gt;\n\n&lt;h1&gt;Frontend Coding&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Kimi K2:&lt;/strong&gt; Took ages to implement it, but nailed the entire thing in one go.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Claude Sonnet 4:&lt;/strong&gt; Super quick with the implementation, but broke the voice support and even ghosted parts of what was asked in the prompt.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Agentic Coding&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Neither of them wrote a fully working implementation‚Ä¶ which was completely unexpected.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Sonnet 4 was worse: it took over 10 minutes and spent most of that time stuck on TypeScript type errors. After all that, it returned false positives in the implementation.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;K2 came close but still couldn‚Äôt figure it out completely.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Final Take&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On a budget? K2 is a no‚Äëbrainer - almost the same (or better) code quality, at a tenth of the cost.&lt;/li&gt;\n&lt;li&gt;Need speed and can swallow the cost? Stick with Sonnet 4 - you won‚Äôt get much performance gain with K2.&lt;/li&gt;\n&lt;li&gt;Minor edge? K2 might have the upper hand in prompt-following and agentic fluency, despite being slower.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can find the entire blog post with a demo for each here: &lt;a href=\"https://composio.dev/blog/kimi-k2-vs-claude-4-sonnet-what-you-should-pick-for-agentic-coding\"&gt;Kimi K2 vs. Claude 4 Sonnet: what you should pick for agentic coding&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also, I would love to know your preference between the two models. I&amp;#39;m still unsure whether to stick with my go-to Sonnet 4 or switch to Kimi K2. What&amp;#39;s your experience with Kimi&amp;#39;s response?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?auto=webp&amp;s=06517b450b86f9c3e33b83c23366d9b9246259a9",
                  "width": 1058,
                  "height": 705
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b664894969871c1c911d4ca3de0afe330df8b82c",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bfcb06298216e380b6e35365b82b0eb2c6f8ed93",
                    "width": 216,
                    "height": 143
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bab6cda0d392796fb48a8642a28f9e5c8195c10c",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=09088db67cbc9fe6a1cebb98a6169ee77b106553",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ab7f9649c1c5bffa55dd4c99fa7f1804f61119d",
                    "width": 960,
                    "height": 639
                  }
                ],
                "variants": {},
                "id": "89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7c2gr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shricodev",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753283941,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Audio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:\n\n- Unified audio representation learning (speech, sound, music)  \n- Flexible, on-demand chain-of-thought reasoning  \n- Long-context audio comprehension (up to 10 minutes)\n- Multi-turn, multi-audio conversational dialogue (AF3-Chat)    \n- Voice-to-voice interaction (AF3-Chat)    \n\nExtensive evaluations confirm AF3‚Äôs effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.\n\n**This model is for non-commercial research purposes only.**\n\n### Model Architecture:\nAudio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.\n\nPaper: https://arxiv.org/abs/2507.08128\nVoice-chat finetune: https://huggingface.co/nvidia/audio-flamingo-3-chat",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "nvidia/audio-flamingo-3",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7fb78",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 63,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 63,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=b68dc8be3ef9abb2b3521ac5287ddf288a2a5bb9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753291299,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Audio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Unified audio representation learning (speech, sound, music)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Flexible, on-demand chain-of-thought reasoning&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Long-context audio comprehension (up to 10 minutes)&lt;/li&gt;\n&lt;li&gt;Multi-turn, multi-audio conversational dialogue (AF3-Chat)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Voice-to-voice interaction (AF3-Chat)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Extensive evaluations confirm AF3‚Äôs effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This model is for non-commercial research purposes only.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h3&gt;Model Architecture:&lt;/h3&gt;\n\n&lt;p&gt;Audio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.&lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2507.08128\"&gt;https://arxiv.org/abs/2507.08128&lt;/a&gt;\nVoice-chat finetune: &lt;a href=\"https://huggingface.co/nvidia/audio-flamingo-3-chat\"&gt;https://huggingface.co/nvidia/audio-flamingo-3-chat&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/nvidia/audio-flamingo-3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?auto=webp&amp;s=b761cba7c3002de5cc09bc2aa3e367a07fde1f1e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f082162e7876351e6a01bc3afa7b6cd69a0c79e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=86a2cf2589774fb2ca8180c2e526be9d4cd4bd04",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e4544fbcf82c91a69b0577016983a6985b755c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d904bc28461c7ba9d24fbdf4cac5832b8e4b862",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d94c311df14dd7728d4e405ada02529c1f99e4ac",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=878903c73b68d742900684e628d41f580d6f9735",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7fb78",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7fb78/nvidiaaudioflamingo3/",
          "stickied": false,
          "url": "https://huggingface.co/nvidia/audio-flamingo-3",
          "subreddit_subscribers": 503516,
          "created_utc": 1753291299,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm seeing a trend in recent advancements in open source models, they're getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?",
          "author_fullname": "t2_e11po",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a future for local models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7o3u8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753311706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m seeing a trend in recent advancements in open source models, they&amp;#39;re getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I&amp;#39;m starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7o3u8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ASTRdeca",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753311706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why they be slacking on local llama and LLM generally? They big nation, clever, work hard. Many robots. No LLM? Why?",
          "author_fullname": "t2_16rs3mlp2i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where is Japan?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7d9d9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 54,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 54,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753286646,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why they be slacking on local llama and LLM generally? They big nation, clever, work hard. Many robots. No LLM? Why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7d9d9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ethereel1",
          "discussion_type": null,
          "num_comments": 131,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753286646,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;&gt;&gt; Qwen3-Coder is here! ‚úÖ\n\nWe‚Äôre releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! üöÄ\n\nAlongside the model, we're also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder‚Äôs capabilities. Qwen3-Coder works seamlessly with the community‚Äôs best developer tools. As a foundation model, we hope it can be used anywhere across the digital world ‚Äî Agentic Coding in the World! ",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is here!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qdet",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 1669,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 1669,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lAVBKeQpbXFJZ84JgZVPph8kD3MjUQeFX9TO1gsVqgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Qwen3-Coder is here! ‚úÖ&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;We‚Äôre releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! üöÄ&lt;/p&gt;\n\n&lt;p&gt;Alongside the model, we&amp;#39;re also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder‚Äôs capabilities. Qwen3-Coder works seamlessly with the community‚Äôs best developer tools. As a foundation model, we hope it can be used anywhere across the digital world ‚Äî Agentic Coding in the World! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?auto=webp&amp;s=e161efd029b20a9bcbbc26db043c320a38b26d7f",
                  "width": 2048,
                  "height": 1175
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0e7ce793e40e6f9057df0ac4084bef74851aa3c",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97f4add6c188177de3f1a921fce3e9fdcd751975",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d3ed6788720878c02cad2db45833f254f311864",
                    "width": 320,
                    "height": 183
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=470c1e7a0a6df4a35a09ad70120a5fef4e93a97b",
                    "width": 640,
                    "height": 367
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eca8024659f203e8b748284b077d47d512488b3",
                    "width": 960,
                    "height": 550
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=348beac8c62000f3a203a6467f098a1c8a696369",
                    "width": 1080,
                    "height": 619
                  }
                ],
                "variants": {},
                "id": "kx6kRcRUBkO_mMM0khkM5jTQgMXazrrYG6wlH3UPCCs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qdet",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 235,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/",
          "stickied": false,
          "url": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "subreddit_subscribers": 503516,
          "created_utc": 1753218847,
          "num_crossposts": 5,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.\n\nThis process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.\n\nI'm sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.\n\n\\1. Clone the official LiteLLM repo:\n\n```sh\ngit clone https://github.com/BerriAI/litellm.git\ncd litellm\n```\n\n\\2. Create an `.env` file with your OpenRouter API key (make sure to insert your own API key!):\n\n```sh\ncat &lt;&lt;\\EOF &gt;.env\nLITELLM_MASTER_KEY = \"sk-1234\"\n\n# OpenRouter\nOPENROUTER_API_KEY = \"sk-or-v1-‚Ä¶\" # üö©\nEOF\n```\n\n\\3. Create a `config.yaml` file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):\n\n```sh\ncat &lt;&lt;\\EOF &gt;config.yaml\nmodel_list:\n  - model_name: \"anthropic/*\"\n    litellm_params:\n      model: \"openrouter/qwen/qwen3-coder\" # Qwen/Qwen3-Coder-480B-A35B-Instruct\n      max_tokens: 65536\n      repetition_penalty: 1.05\n      temperature: 0.7\n      top_k: 20\n      top_p: 0.8\nEOF\n```\n\n\\4. Create a `docker-compose.yml` file that loads `config.yaml` (it's easier to just create a finished one with all the required changes than to edit the original file):\n\n```sh\ncat &lt;&lt;\\EOF &gt;docker-compose.yml\nservices:\n  litellm:\n    build:\n      context: .\n      args:\n        target: runtime\n    ############################################################################\n    command:\n      - \"--config=/app/config.yaml\"\n    container_name: litellm\n    hostname: litellm\n    image: ghcr.io/berriai/litellm:main-stable\n    restart: unless-stopped\n    volumes:\n      - ./config.yaml:/app/config.yaml\n    ############################################################################\n    ports:\n      - \"4000:4000\" # Map the container port to the host, change the host port if necessary\n    environment:\n      DATABASE_URL: \"postgresql://llmproxy:dbpassword9090@db:5432/litellm\"\n      STORE_MODEL_IN_DB: \"True\" # allows adding models to proxy via UI\n    env_file:\n      - .env # Load local .env file\n    depends_on:\n      - db  # Indicates that this service depends on the 'db' service, ensuring 'db' starts first\n    healthcheck:  # Defines the health check configuration for the container\n      test: [ \"CMD-SHELL\", \"wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1\" ]  # Command to execute for health check\n      interval: 30s  # Perform health check every 30 seconds\n      timeout: 10s   # Health check command times out after 10 seconds\n      retries: 3     # Retry up to 3 times if health check fails\n      start_period: 40s  # Wait 40 seconds after container start before beginning health checks\n\n  db:\n    image: postgres:16\n    restart: always\n    container_name: litellm_db\n    environment:\n      POSTGRES_DB: litellm\n      POSTGRES_USER: llmproxy\n      POSTGRES_PASSWORD: dbpassword9090\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -d litellm -U llmproxy\"]\n      interval: 1s\n      timeout: 5s\n      retries: 10\n\nvolumes:\n  postgres_data:\n    name: litellm_postgres_data # Named volume for Postgres data persistence\nEOF\n```\n\n\\5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):\n\n```sh\ndocker compose up -d --build\n```\n\n\\6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (`.zshrc`, `.bashrc`, etc.) for persistence):\n\n```sh\nexport ANTHROPIC_AUTH_TOKEN=sk-1234\nexport ANTHROPIC_BASE_URL=http://localhost:4000\nexport ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder\nexport ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder\nexport CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates\n```\n\n\\7. Start Claude Code and it'll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the `/model` command that it's using a custom model):\n\n```sh\nclaude\n```\n\n\\8. Optional: Add an alias to your shell profile (`.zshrc`, `.bashrc`, etc.) to make it easier to use (e.g. `qlaude` for \"Claude with Qwen\"):\n\n```sh\nalias qlaude='ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude'\n```\n\nHave fun and happy coding!\n\nPS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.",
          "author_fullname": "t2_th129",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HOWTO: Use Qwen3-Coder (or any other LLM) with Claude Code (via LiteLLM)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 94,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ci3s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/u6BYojitjaMwq8K3EQg_ygO8c52qAwKJff5aywSVUks.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753284943,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.&lt;/p&gt;\n\n&lt;p&gt;This process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.&lt;/p&gt;\n\n&lt;p&gt;\\1. Clone the official LiteLLM repo:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\ngit clone https://github.com/BerriAI/litellm.git\ncd litellm\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\2. Create an &lt;code&gt;.env&lt;/code&gt; file with your OpenRouter API key (make sure to insert your own API key!):&lt;/p&gt;\n\n&lt;p&gt;```sh\ncat &amp;lt;&amp;lt;\\EOF &amp;gt;.env\nLITELLM_MASTER_KEY = &amp;quot;sk-1234&amp;quot;&lt;/p&gt;\n\n&lt;h1&gt;OpenRouter&lt;/h1&gt;\n\n&lt;p&gt;OPENROUTER_API_KEY = &amp;quot;sk-or-v1-‚Ä¶&amp;quot; # üö©\nEOF\n```&lt;/p&gt;\n\n&lt;p&gt;\\3. Create a &lt;code&gt;config.yaml&lt;/code&gt; file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\ncat &amp;lt;&amp;lt;\\EOF &amp;gt;config.yaml\nmodel_list:\n  - model_name: &amp;quot;anthropic/*&amp;quot;\n    litellm_params:\n      model: &amp;quot;openrouter/qwen/qwen3-coder&amp;quot; # Qwen/Qwen3-Coder-480B-A35B-Instruct\n      max_tokens: 65536\n      repetition_penalty: 1.05\n      temperature: 0.7\n      top_k: 20\n      top_p: 0.8\nEOF\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\4. Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file that loads &lt;code&gt;config.yaml&lt;/code&gt; (it&amp;#39;s easier to just create a finished one with all the required changes than to edit the original file):&lt;/p&gt;\n\n&lt;p&gt;```sh\ncat &amp;lt;&amp;lt;\\EOF &amp;gt;docker-compose.yml\nservices:\n  litellm:\n    build:\n      context: .\n      args:\n        target: runtime\n    ############################################################################\n    command:\n      - &amp;quot;--config=/app/config.yaml&amp;quot;\n    container_name: litellm\n    hostname: litellm\n    image: ghcr.io/berriai/litellm:main-stable\n    restart: unless-stopped\n    volumes:\n      - ./config.yaml:/app/config.yaml\n    ############################################################################\n    ports:\n      - &amp;quot;4000:4000&amp;quot; # Map the container port to the host, change the host port if necessary\n    environment:\n      DATABASE_URL: &amp;quot;postgresql://llmproxy:dbpassword9090@db:5432/litellm&amp;quot;\n      STORE_MODEL_IN_DB: &amp;quot;True&amp;quot; # allows adding models to proxy via UI\n    env_file:\n      - .env # Load local .env file\n    depends_on:\n      - db  # Indicates that this service depends on the &amp;#39;db&amp;#39; service, ensuring &amp;#39;db&amp;#39; starts first\n    healthcheck:  # Defines the health check configuration for the container\n      test: [ &amp;quot;CMD-SHELL&amp;quot;, &amp;quot;wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1&amp;quot; ]  # Command to execute for health check\n      interval: 30s  # Perform health check every 30 seconds\n      timeout: 10s   # Health check command times out after 10 seconds\n      retries: 3     # Retry up to 3 times if health check fails\n      start_period: 40s  # Wait 40 seconds after container start before beginning health checks&lt;/p&gt;\n\n&lt;p&gt;db:\n    image: postgres:16\n    restart: always\n    container_name: litellm_db\n    environment:\n      POSTGRES_DB: litellm\n      POSTGRES_USER: llmproxy\n      POSTGRES_PASSWORD: dbpassword9090\n    ports:\n      - &amp;quot;5432:5432&amp;quot;\n    volumes:\n      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\n    healthcheck:\n      test: [&amp;quot;CMD-SHELL&amp;quot;, &amp;quot;pg_isready -d litellm -U llmproxy&amp;quot;]\n      interval: 1s\n      timeout: 5s\n      retries: 10&lt;/p&gt;\n\n&lt;p&gt;volumes:\n  postgres_data:\n    name: litellm_postgres_data # Named volume for Postgres data persistence\nEOF\n```&lt;/p&gt;\n\n&lt;p&gt;\\5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\ndocker compose up -d --build\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) for persistence):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\nexport ANTHROPIC_AUTH_TOKEN=sk-1234\nexport ANTHROPIC_BASE_URL=http://localhost:4000\nexport ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder\nexport ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder\nexport CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\7. Start Claude Code and it&amp;#39;ll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the &lt;code&gt;/model&lt;/code&gt; command that it&amp;#39;s using a custom model):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\nclaude\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\8. Optional: Add an alias to your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) to make it easier to use (e.g. &lt;code&gt;qlaude&lt;/code&gt; for &amp;quot;Claude with Qwen&amp;quot;):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\nalias qlaude=&amp;#39;ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude&amp;#39;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Have fun and happy coding!&lt;/p&gt;\n\n&lt;p&gt;PS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5p7u0le68nef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5p7u0le68nef1.png?auto=webp&amp;s=d6ee6eb61c270bf74164a4529655ea608dd7d761",
                  "width": 1682,
                  "height": 1130
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2f9ecbfadf0a585c661b7818dc4d782fd8cb3f3",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5ed170eaa8c71b609cea8209eb4a660ef943d7d",
                    "width": 216,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d361e88e5a29d401767472fe3a197448482be18",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=052b75b74825ad0e1f536d20305b7b06a2c2db8c",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e30fe50b638eda8e2b345142bbdcf622a0cdff2b",
                    "width": 960,
                    "height": 644
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ddb6221e7b7cb0abb8d800206489d39dee05e42",
                    "width": 1080,
                    "height": 725
                  }
                ],
                "variants": {},
                "id": "mIuYvF0JIlHjZcMZ6w4mc0OQRmovAhW7m8mjpQFvvxs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m7ci3s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WolframRavenwolf",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/",
          "stickied": false,
          "url": "https://i.redd.it/5p7u0le68nef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753284943,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just tested the `unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf` model using `llama.cpp` on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. \n\nBy selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. \n  \nHere is the full execution command I used:\n\n```\n./llama-server \\\n--model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \\\n--port 11433 \\\n--host \"0.0.0.0\" \\\n--verbose \\\n--flash-attn \\\n--cache-type-k q8_0 \\\n--cache-type-v q8_0 \\\n--n-gpu-layers 999 \\\n-ot \"blk\\.(?:[1-8]?[1379])\\.ffn_.*_exps\\.weight=CPU\" \\\n--prio 3 \\\n--threads 32 \\\n--ctx-size 32768 \\\n--temp 0.6 \\\n--min-p 0.0 \\\n--top-p 0.95 \\\n--top-k 20 \\\n--repeat-penalty 1\n```\n\nI'm still new to `llama.cpp` and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.\n\n",
          "author_fullname": "t2_14u3g9s5kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m7pqln",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
              "author_name": "Septerium",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7HXCQ-4F_oQ/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@JohnnyGomezSn"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m7pqln",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=1b8996779707c4a5f85298d6cf4e8395ec809c0d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753316083,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just tested the &lt;code&gt;unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf&lt;/code&gt; model using &lt;code&gt;llama.cpp&lt;/code&gt; on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. &lt;/p&gt;\n\n&lt;p&gt;By selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. &lt;/p&gt;\n\n&lt;p&gt;Here is the full execution command I used:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n./llama-server \\\n--model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \\\n--port 11433 \\\n--host &amp;quot;0.0.0.0&amp;quot; \\\n--verbose \\\n--flash-attn \\\n--cache-type-k q8_0 \\\n--cache-type-v q8_0 \\\n--n-gpu-layers 999 \\\n-ot &amp;quot;blk\\.(?:[1-8]?[1379])\\.ffn_.*_exps\\.weight=CPU&amp;quot; \\\n--prio 3 \\\n--threads 32 \\\n--ctx-size 32768 \\\n--temp 0.6 \\\n--min-p 0.0 \\\n--top-p 0.95 \\\n--top-k 20 \\\n--repeat-penalty 1\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still new to &lt;code&gt;llama.cpp&lt;/code&gt; and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=7HXCQ-4F_oQ",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?auto=webp&amp;s=fb78672ddcf654bd2c828f30bcdaede2ae00db46",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b68e4415698a411ba429105637449852662e35d9",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a94209a8c4dae66ae50d2f66698b6671ae7897",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8cd0c77917208f92bbcf8528d34b5d0cb74b361",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7pqln",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FalseMap1582",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=7HXCQ-4F_oQ",
          "subreddit_subscribers": 503516,
          "created_utc": 1753316083,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
              "author_name": "Septerium",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7HXCQ-4F_oQ/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@JohnnyGomezSn"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a semi complex web project that I use with Claude Code. a few days ago I used Kimi K2 (via Groq Q4) with Claude Code (CCR) to add a permissions system / ACL into my web project to lock down certain people from doing certain things.\n\n  \nI use SuperClaude and a 1200 line context/architecture document, which basically starts a conversation off at about 30k input tokens (though, well worth it).\n\n  \nKimi K2 failed horribly, tool use errors, random garbage and basically didn't work properly. It was a Q4 version so maybe that had something to do with it, but I wasn't impressed.\n\n  \nToday I used Qwen 3 Coder via Openrouter (using only Alibaba cloud servers) for about 60 tps. Gave it the same task, and after about 10 minutes it finished. One shotted it (though one shotting is common for me with such a high amount of pre-context and auto fixing).\n\n  \nIt all worked great, I am actually really impressed and for me personally, it marks the first time an open source coding model actually has real world potential to rival paid LLMs like sonnet, opus and gemini. I would compare this model directly as good as Sonnet 4, which is a very capable model when using the right tools and prompts.\n\n  \nbig W for the open source community.\n\n  \nthe downside? THE PRICE. this one feature I added cost me $5 USD in credits via OpenRouter. That might not seem like much, but with Claude Pro for example you get an entire month of Sonnet 4 for 4x the price of that task. I don't know how well its using caching but at this point id rather stick with subscription based usage because that could get out of hand fast.",
          "author_fullname": "t2_i5ycefja",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Coder is actually pretty decent in my testing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m73yrb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 186,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 186,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753260180,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a semi complex web project that I use with Claude Code. a few days ago I used Kimi K2 (via Groq Q4) with Claude Code (CCR) to add a permissions system / ACL into my web project to lock down certain people from doing certain things.&lt;/p&gt;\n\n&lt;p&gt;I use SuperClaude and a 1200 line context/architecture document, which basically starts a conversation off at about 30k input tokens (though, well worth it).&lt;/p&gt;\n\n&lt;p&gt;Kimi K2 failed horribly, tool use errors, random garbage and basically didn&amp;#39;t work properly. It was a Q4 version so maybe that had something to do with it, but I wasn&amp;#39;t impressed.&lt;/p&gt;\n\n&lt;p&gt;Today I used Qwen 3 Coder via Openrouter (using only Alibaba cloud servers) for about 60 tps. Gave it the same task, and after about 10 minutes it finished. One shotted it (though one shotting is common for me with such a high amount of pre-context and auto fixing).&lt;/p&gt;\n\n&lt;p&gt;It all worked great, I am actually really impressed and for me personally, it marks the first time an open source coding model actually has real world potential to rival paid LLMs like sonnet, opus and gemini. I would compare this model directly as good as Sonnet 4, which is a very capable model when using the right tools and prompts.&lt;/p&gt;\n\n&lt;p&gt;big W for the open source community.&lt;/p&gt;\n\n&lt;p&gt;the downside? THE PRICE. this one feature I added cost me $5 USD in credits via OpenRouter. That might not seem like much, but with Claude Pro for example you get an entire month of Sonnet 4 for 4x the price of that task. I don&amp;#39;t know how well its using caching but at this point id rather stick with subscription based usage because that could get out of hand fast.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m73yrb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hodler-mane",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753260180,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just ran Qwen 3 Coder through a real-world test ‚Äî building out a full permissions/ACL setup for a complex web app. Gave it the usual 30k-token context I feed into Claude Code, and it legit nailed it on the first try. No weird logic gaps, no hallucinated APIs ‚Äî just clean, working code.\n\nTried the same thing with Kimi K2 and... it flopped hard. Qwen held up surprisingly well, especially when paired with solid prompt scaffolding. Honestly, it gave off Sonnet 4 vibes, which I wasn‚Äôt expecting from an OSS model.  \nStill, wild to see an open-source model perform at this level. We might be entering a legit new phase for local/dev-friendly LLMs.",
          "author_fullname": "t2_uaotuj04",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Coder just handled a full ACL system like a champ ‚Äî OSS finally catching up",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7e5pi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753288688,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just ran Qwen 3 Coder through a real-world test ‚Äî building out a full permissions/ACL setup for a complex web app. Gave it the usual 30k-token context I feed into Claude Code, and it legit nailed it on the first try. No weird logic gaps, no hallucinated APIs ‚Äî just clean, working code.&lt;/p&gt;\n\n&lt;p&gt;Tried the same thing with Kimi K2 and... it flopped hard. Qwen held up surprisingly well, especially when paired with solid prompt scaffolding. Honestly, it gave off Sonnet 4 vibes, which I wasn‚Äôt expecting from an OSS model.&lt;br/&gt;\nStill, wild to see an open-source model perform at this level. We might be entering a legit new phase for local/dev-friendly LLMs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7e5pi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Edge2098",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7e5pi/qwen_3_coder_just_handled_a_full_acl_system_like/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7e5pi/qwen_3_coder_just_handled_a_full_acl_system_like/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753288688,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A tiny change in the converter to support GLM-4.1V-9B-Thinking (no recompilation needed, just generate the GGUF).",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "text-only support for GLM-4.1V-9B-Thinking has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7m5br",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#bbbdbf",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=2a74fc0520a2b746e5b0e846d75ce4eb9f0c717b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753306899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A tiny change in the converter to support GLM-4.1V-9B-Thinking (no recompilation needed, just generate the GGUF).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14823",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?auto=webp&amp;s=1da3d09c5c620e7064179fe0056b0025ded7d6d5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dca6399394d0c6a422906d4fb5fb8e66090699e9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1d0491b6172ba26e4d571fcab16919205a50f4f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c812b1e9ad6436ffe1e34de95b453fe095355b74",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=283881b3d5dfd5c7c70eea0444ab6f480d98f89e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9216471d6e91d1c641b997ca6f702cb7d30fb3a1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=716e2fc1e1824c935e0b9dd09498d714601b0d18",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m7m5br",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m7m5br/textonly_support_for_glm41v9bthinking_has_been/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14823",
          "subreddit_subscribers": 503516,
          "created_utc": 1753306899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Higgs Audio V2 is an advanced, open-source audio generation model developed by Boson AI, designed to produce highly expressive and lifelike speech with robust multi-speaker dialogue capabilities.\n\nSome Highlights:\n\nüéß Trained on 10M hours of diverse audio ‚Äî speech, music, sound events, and natural conversations  \nüîß Built on top of Llama 3.2 3B for deep language and acoustic understanding  \n‚ö° Runs in real-time and supports edge deployment ‚Äî smallest versions run on Jetson Orin Nano  \nüèÜ Outperforms GPT-4o-mini-tts and ElevenLabs v2 in prosody, emotional expressiveness, and multi-speaker dialogue  \nüé≠ Zero-shot natural multi-speaker dialogues ‚Äî voices adapt tone, energy, and emotion automatically  \nüéôÔ∏è Zero-shot voice cloning with melodic humming and expressive intonation ‚Äî no fine-tuning needed  \nüåç Multilingual support with automatic prosody adaptation for narration and dialogue  \nüéµ Simultaneous speech and background music generation ‚Äî a first for open audio foundation models  \nüîä High-fidelity 24kHz audio output for studio-quality sound on any device  \nüì¶ Open source and commercially usable ‚Äî no barriers to experimentation or deployment\n\nI tested this model here [https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt](https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt)\n\nModel on Huggingface:  https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base",
          "author_fullname": "t2_8c6ji8bg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Higgs Audio V2 - Open Multi-Speaker TTS Model - Impressive Testing Results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7lj3x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753305442,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Higgs Audio V2 is an advanced, open-source audio generation model developed by Boson AI, designed to produce highly expressive and lifelike speech with robust multi-speaker dialogue capabilities.&lt;/p&gt;\n\n&lt;p&gt;Some Highlights:&lt;/p&gt;\n\n&lt;p&gt;üéß Trained on 10M hours of diverse audio ‚Äî speech, music, sound events, and natural conversations&lt;br/&gt;\nüîß Built on top of Llama 3.2 3B for deep language and acoustic understanding&lt;br/&gt;\n‚ö° Runs in real-time and supports edge deployment ‚Äî smallest versions run on Jetson Orin Nano&lt;br/&gt;\nüèÜ Outperforms GPT-4o-mini-tts and ElevenLabs v2 in prosody, emotional expressiveness, and multi-speaker dialogue&lt;br/&gt;\nüé≠ Zero-shot natural multi-speaker dialogues ‚Äî voices adapt tone, energy, and emotion automatically&lt;br/&gt;\nüéôÔ∏è Zero-shot voice cloning with melodic humming and expressive intonation ‚Äî no fine-tuning needed&lt;br/&gt;\nüåç Multilingual support with automatic prosody adaptation for narration and dialogue&lt;br/&gt;\nüéµ Simultaneous speech and background music generation ‚Äî a first for open audio foundation models&lt;br/&gt;\nüîä High-fidelity 24kHz audio output for studio-quality sound on any device&lt;br/&gt;\nüì¶ Open source and commercially usable ‚Äî no barriers to experimentation or deployment&lt;/p&gt;\n\n&lt;p&gt;I tested this model here &lt;a href=\"https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt\"&gt;https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Model on Huggingface:  &lt;a href=\"https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base\"&gt;https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk.jpeg?auto=webp&amp;s=8785e85f2bd59731d1b765bec52a5454fb368691",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=236a03bf932c13a51b9e805f9e9362659054558c",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8adae275672b81dce6a787717b6a50c7e36a45a3",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b02194bafbe6b39b59e579089faf0cb3d64cfe0",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m7lj3x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lopsided_Dot_4557",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7lj3x/higgs_audio_v2_open_multispeaker_tts_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7lj3x/higgs_audio_v2_open_multispeaker_tts_model/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753305442,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen3 235B 2507 scores 60 on the Artificial Analysis Intelligence Index, surpassing Claude 4 Opus and Kimi K2 (both 58), and DeepSeek V3 0324 and GPT-4.1 (both 53). This marks a 13-point leap over the May 2025 non-reasoning release and brings it within two points of the May 2025 reasoning variant.",
          "author_fullname": "t2_1n5r32wumb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Alibaba‚Äôs upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "oyaa6be25kef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab849378391008c8477795135825d52268bdd089"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53e7c01d46996d28bab6c0eb21ebf73f19d344d0"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=101dd3f12db35511b6a0b3746e6b6a540438f16a"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a517570e0bde0d35e032e730f623cbf398e56d34"
                },
                {
                  "y": 479,
                  "x": 960,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bd46b3fee52bf1de3e3566854ce997d0dd1225a"
                },
                {
                  "y": 538,
                  "x": 1080,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89c6654712fe811c8362225e6681fc8b54707958"
                }
              ],
              "s": {
                "y": 1022,
                "x": 2048,
                "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=e4811114c0b584dfcb6ee44f7f9bd5de02b8b550"
              },
              "id": "oyaa6be25kef1"
            },
            "yycfab625kef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ddf3c6b84f0f72a7669eb6187f719c7aca48cf1"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=020ffd2f38bf6dc2d4d813526ab1f68d37467a35"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d01829fee9b1b624a03e3c46434cc0464197eb6"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=590c21d82b1b1b130de0259f2b921cf5f6a9a736"
                },
                {
                  "y": 479,
                  "x": 960,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f1617800278fa9228386271be95a16d7067a04c"
                },
                {
                  "y": 538,
                  "x": 1080,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79f249d73cc6e2ef9eba49c4faaff2315c0c5daf"
                }
              ],
              "s": {
                "y": 1022,
                "x": 2048,
                "u": "https://preview.redd.it/yycfab625kef1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=b0b54666a21f5112942012d6921a4898cd5bb66d"
              },
              "id": "yycfab625kef1"
            }
          },
          "name": "t3_1m70n7q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 263,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "yycfab625kef1",
                "id": 712061604
              },
              {
                "caption": "",
                "media_id": "oyaa6be25kef1",
                "id": 712061605
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 263,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/44x5FARtQun2iK2pU9UkqoLiKnQmMoq90mJNUYMTKbw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753247536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 235B 2507 scores 60 on the Artificial Analysis Intelligence Index, surpassing Claude 4 Opus and Kimi K2 (both 58), and DeepSeek V3 0324 and GPT-4.1 (both 53). This marks a 13-point leap over the May 2025 non-reasoning release and brings it within two points of the May 2025 reasoning variant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m70n7q",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m70n7q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fantastic-Emu-3819",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m70n7q",
          "subreddit_subscribers": 503516,
          "created_utc": 1753247536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tested the two models in VSCode, Cline, Roo Code and now Kimi a bit in Windsurf. Here are my takeaways (and video of one of the tests in the comments section):\n\n\\- NB: FOR QWEN 3 CODER, IF YOU USE OPEN ROUTER, PLEASE REMOVE ALIBABA AS AN INFERENCE PROVIDER AS I SHOW IN THE VID (IT'S UP TO $60/million tokens OUTPUT)\n\n\\- Kimi K2 doesn't have good tool calling with VSCode (YET), it has that issue Gemini 2.5 Pro has where it promises to make a tool call but doesn't\n\n\\- Qwen 3 Coder was close to flawless with tool calling in VSCode\n\n\\- Kimi K2 is better in instruction following than Qwen 3 Coder, hands down\n\n\\- Qwen 3 Coder is also good in Roo Code tool calls\n\n\\- K2 did feel like it's on par with Sonnet 4 in many respects so far\n\n\\- Kimi K2 produced generally better quality code and features\n\n\\- Qwen 3 Coder is extremely expensive! If you use Alibaba as inference, other providers in OpenRouter are decently priced\n\n\\- K2 is half the cost of Qwen- K2 deleted one of my Dev DBs in Azure and didn't ask if there was data, just because of a column which needed a migration, so please keep your Deny lists in check\n\nCoding Vid: https://youtu.be/ljCO7RyqCMY",
          "author_fullname": "t2_qmg9qzxv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Qwen 3 Coder - Coding Tests",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7n5pq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753309318,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tested the two models in VSCode, Cline, Roo Code and now Kimi a bit in Windsurf. Here are my takeaways (and video of one of the tests in the comments section):&lt;/p&gt;\n\n&lt;p&gt;- NB: FOR QWEN 3 CODER, IF YOU USE OPEN ROUTER, PLEASE REMOVE ALIBABA AS AN INFERENCE PROVIDER AS I SHOW IN THE VID (IT&amp;#39;S UP TO $60/million tokens OUTPUT)&lt;/p&gt;\n\n&lt;p&gt;- Kimi K2 doesn&amp;#39;t have good tool calling with VSCode (YET), it has that issue Gemini 2.5 Pro has where it promises to make a tool call but doesn&amp;#39;t&lt;/p&gt;\n\n&lt;p&gt;- Qwen 3 Coder was close to flawless with tool calling in VSCode&lt;/p&gt;\n\n&lt;p&gt;- Kimi K2 is better in instruction following than Qwen 3 Coder, hands down&lt;/p&gt;\n\n&lt;p&gt;- Qwen 3 Coder is also good in Roo Code tool calls&lt;/p&gt;\n\n&lt;p&gt;- K2 did feel like it&amp;#39;s on par with Sonnet 4 in many respects so far&lt;/p&gt;\n\n&lt;p&gt;- Kimi K2 produced generally better quality code and features&lt;/p&gt;\n\n&lt;p&gt;- Qwen 3 Coder is extremely expensive! If you use Alibaba as inference, other providers in OpenRouter are decently priced&lt;/p&gt;\n\n&lt;p&gt;- K2 is half the cost of Qwen- K2 deleted one of my Dev DBs in Azure and didn&amp;#39;t ask if there was data, just because of a column which needed a migration, so please keep your Deny lists in check&lt;/p&gt;\n\n&lt;p&gt;Coding Vid: &lt;a href=\"https://youtu.be/ljCO7RyqCMY\"&gt;https://youtu.be/ljCO7RyqCMY&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA.jpeg?auto=webp&amp;s=ab91a571eedbc83fa4b6e65265c51d6677c99945",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7461f771e89f4be20b2a2b188a8b5c97a354e32f",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d880093c1d112db6452b3626b4f4fd3a67bc8f2",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1209830adb2015c645b0a257eddb571b141f1ce2",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7n5pq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "marvijo-software",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7n5pq/kimi_k2_vs_qwen_3_coder_coding_tests/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7n5pq/kimi_k2_vs_qwen_3_coder_coding_tests/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753309318,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôve been polishing the prompt setup and description pages to make them cleaner and more user-friendly. I originally built this because I got tired of digging through HuggingFace, Discord, and other scattered sources just to find decent prompts that work with different models.\n\nNow I‚Äôm trying to make that process as smooth and centralized as possible - with a clear UI, easy prompt management, and helpful context.\n\nWould love to know what you think - any feedback or ideas for improvement are super welcome!",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Polished UI for prompt setup &amp; details",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6my226plpnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47c415370992a40af3ab20e47f116a2616ea0a69"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc7f4ac2fc4ba879cea9dba16cda50988caf0245"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6c34da0216d3072c7a36f3b60c524aee15ecf74"
                },
                {
                  "y": 481,
                  "x": 640,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84b3a0aa5ade6f5e756d0624316b8f05ad48f2ca"
                },
                {
                  "y": 721,
                  "x": 960,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=66e45ed091efcaf8a61092f3fe5f1bd086f88753"
                },
                {
                  "y": 811,
                  "x": 1080,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0858add5bf457665fbbf7f672e76ef90b8cc529b"
                }
              ],
              "s": {
                "y": 1443,
                "x": 1920,
                "u": "https://preview.redd.it/6my226plpnef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d66fc444e6ad8245518edb37f53e54f2d774eafe"
              },
              "id": "6my226plpnef1"
            },
            "2al8c23mpnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1754f31c75321c475f94f24be27fd19f4ee3b2c7"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2da1706c5359bb6bfc7475d3534bf392ca89ee3b"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63f2d4756c19cc98ce8268018a841f5b78828c58"
                },
                {
                  "y": 481,
                  "x": 640,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7192bda12d2cde591f2661517b7edbf41ed0af50"
                },
                {
                  "y": 722,
                  "x": 960,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=47de98797becbb9d8b4615612444e70b29d1dcd9"
                },
                {
                  "y": 812,
                  "x": 1080,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9abe46c930c2119df5477cd75773384e2b09251a"
                }
              ],
              "s": {
                "y": 1445,
                "x": 1920,
                "u": "https://preview.redd.it/2al8c23mpnef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=71daec4098d7cc28f113604a0bfab904e64d438b"
              },
              "id": "2al8c23mpnef1"
            }
          },
          "name": "t3_1m7f43h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 22,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "6my226plpnef1",
                "id": 712389243
              },
              {
                "media_id": "2al8c23mpnef1",
                "id": 712389244
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BiMeUC3yGkuRI6HBmivfk5feKuVk7YjrbVFqywOQ330.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753290857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôve been polishing the prompt setup and description pages to make them cleaner and more user-friendly. I originally built this because I got tired of digging through HuggingFace, Discord, and other scattered sources just to find decent prompts that work with different models.&lt;/p&gt;\n\n&lt;p&gt;Now I‚Äôm trying to make that process as smooth and centralized as possible - with a clear UI, easy prompt management, and helpful context.&lt;/p&gt;\n\n&lt;p&gt;Would love to know what you think - any feedback or ideas for improvement are super welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m7f43h",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m7f43h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7f43h/polished_ui_for_prompt_setup_details/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m7f43h",
          "subreddit_subscribers": 503516,
          "created_utc": 1753290857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_a05srvks",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recent Qwen Benchmark Scores are Questionable",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 86,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6wb5o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 379,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 379,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4lABFcqtTCklC4dui7K-UMEXRbWONTLpUEMMKBfKSoM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753234294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8gjn0yhf1jef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8gjn0yhf1jef1.png?auto=webp&amp;s=1c15b72a1a3ebfe0f19f0d765beb22b39ec10dcd",
                  "width": 1194,
                  "height": 734
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6a6ec17b5a0a0b95756eb50adde48b41ee2f601",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e055247e334ebf18193ce1f1d33b6c9da1725406",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=224c05a95e89dc2c0fa3b5b4b9a7782ddb3f25a0",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5550a5410e5e1c751c0140c16c192e6bd86fddd",
                    "width": 640,
                    "height": 393
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=29dcc9276be7fbebc077a8bce537c9338061da86",
                    "width": 960,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd26c16ae9488c7ee59a13a97af993a1d0b068ba",
                    "width": 1080,
                    "height": 663
                  }
                ],
                "variants": {},
                "id": "USv5UurWPKmkzrWAAgHt6q_pqJzLWJPl6eKLZh9JvqU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6wb5o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Electronic_Ad8889",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/",
          "stickied": false,
          "url": "https://i.redd.it/8gjn0yhf1jef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753234294,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/4ylsxlbm0oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=7651eb28c5f9703cee17fa9fe2a66f6b575f5a05\n\nTrying to perform CPT of llama on a new language (Language is similar to Hindi, hence some tokens already present). The model's validation loss seems to plateau very early on into the training. Here 1 epoch is around 6k steps and validation loss seems to already be lowest at step 750. \n\n  \nMy dataset is around 100k size. Im using Lora as well\n\nhttps://preview.redd.it/17g8r8161oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=787998b7dc36f0ea474bfd988a17ba7527b6937b\n\nHere are my training arguments\n\nhttps://preview.redd.it/zpu2yhq81oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=e2aa754c023801a568a24bdf5b0d4cea92494fce\n\nIve tried different arangement, like more r value, embed\\_head and lm\\_head added onto the modules, different leaerning rates, etc. But similar trend in validation loss, either its around this range or around the range of 1.59-1.60. \n\nhttps://preview.redd.it/biejsj3k1oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=68c1fdd74b5fb72da5198132c7a1f821ba6f858e\n\nMoreover, Ive also tried mistral-7b-v0.1, same issues.   \n\n\nI thought it might be because the model is not able to learn because of less tokens, so tried vocab expansion, but same issues. \n\nWhat else could i try? ",
          "author_fullname": "t2_ilp0f96k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Continued pretraining of Llama 3-8b on a new language",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 39,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4ylsxlbm0oef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 30,
                  "x": 108,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88d57c1c26304386dc9643324410f7e924da02bf"
                },
                {
                  "y": 60,
                  "x": 216,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1829acd222438fd79d036ad146b40b99ed5e2274"
                },
                {
                  "y": 89,
                  "x": 320,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b5df9c08b52e96caaddffb6c7613b44d87296bc"
                },
                {
                  "y": 179,
                  "x": 640,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ff99b39523472a17ac34ee8ca73989f1e306fc8"
                },
                {
                  "y": 268,
                  "x": 960,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8eea30c92d9761f2b85fdaedbeed3a68f9d2b43"
                },
                {
                  "y": 302,
                  "x": 1080,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f98d1b9f94bb67261e57f64684fdd610ffc31344"
                }
              ],
              "s": {
                "y": 668,
                "x": 2386,
                "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=7651eb28c5f9703cee17fa9fe2a66f6b575f5a05"
              },
              "id": "4ylsxlbm0oef1"
            },
            "zpu2yhq81oef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=10939db7873d0be79d7a8902fa93e3c456528b07"
                },
                {
                  "y": 118,
                  "x": 216,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b060d6c331f5ba2303cdf2a7cbfa6a2362b7238"
                },
                {
                  "y": 174,
                  "x": 320,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e26b1f28756de8bc058ab7e363e19aebc60aa1e8"
                },
                {
                  "y": 349,
                  "x": 640,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60d1e35982d5c449da7df39ae91617d4d73624a3"
                },
                {
                  "y": 524,
                  "x": 960,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0772385bb0d1c46ae8b7154c304493d97380e064"
                },
                {
                  "y": 590,
                  "x": 1080,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b3a54755e23fb71bd2b288a307e643718c932448"
                }
              ],
              "s": {
                "y": 1304,
                "x": 2386,
                "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=e2aa754c023801a568a24bdf5b0d4cea92494fce"
              },
              "id": "zpu2yhq81oef1"
            },
            "biejsj3k1oef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49e6f3d108dd2ba89333fa2a8522d3e1b1bddfe9"
                },
                {
                  "y": 77,
                  "x": 216,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f65fbb40cdc0b8c2acefaf73b7c2025e036264e"
                },
                {
                  "y": 115,
                  "x": 320,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=04c9f49be579eff990a61f01049f14292def3782"
                },
                {
                  "y": 230,
                  "x": 640,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e3a6b6cf914943e647acdce7522cea4ebedb85f"
                },
                {
                  "y": 345,
                  "x": 960,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=be9343fefcfdc69e0dc7f570673c516a0fccf602"
                },
                {
                  "y": 388,
                  "x": 1080,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a9b3d48af5bf4011bf7f028eae9d7bbaef55e036"
                }
              ],
              "s": {
                "y": 858,
                "x": 2386,
                "u": "https://preview.redd.it/biejsj3k1oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=68c1fdd74b5fb72da5198132c7a1f821ba6f858e"
              },
              "id": "biejsj3k1oef1"
            },
            "17g8r8161oef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 15,
                  "x": 108,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=da35b4fc251de49bbc6f7d64063b55761b11dac7"
                },
                {
                  "y": 31,
                  "x": 216,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b99758d0eab4b7ae5fff716c23661e6ca93d03e5"
                },
                {
                  "y": 46,
                  "x": 320,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9412d4a0c0e4657c087ba28bca26ec858a607d3d"
                },
                {
                  "y": 92,
                  "x": 640,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2a85d234a554d7dfbb6cead09a4b74044436d4c"
                },
                {
                  "y": 138,
                  "x": 960,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d64bdc00a6a46ba8b1dc68e17472b1bb48de3949"
                },
                {
                  "y": 155,
                  "x": 1080,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=259de3da3401fc95891509dde196e1c8402eca51"
                }
              ],
              "s": {
                "y": 344,
                "x": 2386,
                "u": "https://preview.redd.it/17g8r8161oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=787998b7dc36f0ea474bfd988a17ba7527b6937b"
              },
              "id": "17g8r8161oef1"
            }
          },
          "name": "t3_1m7gwuo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ugVEN4UtFCv8Wx60MSKJhHoUDDHJb5lYNs6MP2_hSKg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753294914,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ylsxlbm0oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7651eb28c5f9703cee17fa9fe2a66f6b575f5a05\"&gt;https://preview.redd.it/4ylsxlbm0oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7651eb28c5f9703cee17fa9fe2a66f6b575f5a05&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Trying to perform CPT of llama on a new language (Language is similar to Hindi, hence some tokens already present). The model&amp;#39;s validation loss seems to plateau very early on into the training. Here 1 epoch is around 6k steps and validation loss seems to already be lowest at step 750. &lt;/p&gt;\n\n&lt;p&gt;My dataset is around 100k size. Im using Lora as well&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/17g8r8161oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=787998b7dc36f0ea474bfd988a17ba7527b6937b\"&gt;https://preview.redd.it/17g8r8161oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=787998b7dc36f0ea474bfd988a17ba7527b6937b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are my training arguments&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zpu2yhq81oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2aa754c023801a568a24bdf5b0d4cea92494fce\"&gt;https://preview.redd.it/zpu2yhq81oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2aa754c023801a568a24bdf5b0d4cea92494fce&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Ive tried different arangement, like more r value, embed_head and lm_head added onto the modules, different leaerning rates, etc. But similar trend in validation loss, either its around this range or around the range of 1.59-1.60. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/biejsj3k1oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68c1fdd74b5fb72da5198132c7a1f821ba6f858e\"&gt;https://preview.redd.it/biejsj3k1oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68c1fdd74b5fb72da5198132c7a1f821ba6f858e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Moreover, Ive also tried mistral-7b-v0.1, same issues.   &lt;/p&gt;\n\n&lt;p&gt;I thought it might be because the model is not able to learn because of less tokens, so tried vocab expansion, but same issues. &lt;/p&gt;\n\n&lt;p&gt;What else could i try? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7gwuo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Awkward-Quiet5795",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7gwuo/continued_pretraining_of_llama_38b_on_a_new/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7gwuo/continued_pretraining_of_llama_38b_on_a_new/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753294914,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "üåã ENTIRE SPEECH-TO-SPEECH PIPELINE\n\nüîÆREAL-TIME LIVE CAPTIONS IN 99 LANGUAGES\n\nNow it's possible to have any audio source (including your own voice) transcribed and translated to English using GPU acceleration for ultra-fast inference\n\nIt's 100% free, even for commercial use\n\nAnd runs locally\n\nSource code: [https://github.com/Kutalia/electron-speech-to-speech](https://github.com/Kutalia/electron-speech-to-speech) (Currently only Windows builds are provided in Github Releases, but you can easily compile with source for your platform - Windows, Mac and Linux)\n\nDemo: [https://www.youtube.com/watch?v=wUdtGxy0Ku8](https://www.youtube.com/watch?v=wUdtGxy0Ku8)",
          "author_fullname": "t2_h6o0chk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local cross-platform speech-to-speech and real-time captioning with OpenAI Whisper, Vulkan GPU acceleration and more",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 113,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m78kyc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MU-BIGb5j5aJl9A67BIiP69nhxlRBUqdBBCKbLTfMic.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753275527,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üåã ENTIRE SPEECH-TO-SPEECH PIPELINE&lt;/p&gt;\n\n&lt;p&gt;üîÆREAL-TIME LIVE CAPTIONS IN 99 LANGUAGES&lt;/p&gt;\n\n&lt;p&gt;Now it&amp;#39;s possible to have any audio source (including your own voice) transcribed and translated to English using GPU acceleration for ultra-fast inference&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s 100% free, even for commercial use&lt;/p&gt;\n\n&lt;p&gt;And runs locally&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/Kutalia/electron-speech-to-speech\"&gt;https://github.com/Kutalia/electron-speech-to-speech&lt;/a&gt; (Currently only Windows builds are provided in Github Releases, but you can easily compile with source for your platform - Windows, Mac and Linux)&lt;/p&gt;\n\n&lt;p&gt;Demo: &lt;a href=\"https://www.youtube.com/watch?v=wUdtGxy0Ku8\"&gt;https://www.youtube.com/watch?v=wUdtGxy0Ku8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/pxwmiaqagmef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/pxwmiaqagmef1.png?auto=webp&amp;s=c034d20d510a53fc14ecaae5913be5df0e255e74",
                  "width": 1622,
                  "height": 1319
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e713889b96e5cfca6b45f1450adc73c7dae99bb1",
                    "width": 108,
                    "height": 87
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a661c8a38e2ad63ba6d4441540c46910e9a8a480",
                    "width": 216,
                    "height": 175
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=792ee698504107743bc40f1131da6e6da31ea2e9",
                    "width": 320,
                    "height": 260
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5bef850b1f4387e2fe702768e841b6eb073e128",
                    "width": 640,
                    "height": 520
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c4f8151b1fdbc9fe37549859b625b2ea60117d5",
                    "width": 960,
                    "height": 780
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4495266981e63cb06dbd23d2cc8faa503789045",
                    "width": 1080,
                    "height": 878
                  }
                ],
                "variants": {},
                "id": "gxeDylarxEV8DJSZNwihZJEhvvKrLVIx5XtEq4XAYZ4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m78kyc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kutalia",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m78kyc/local_crossplatform_speechtospeech_and_realtime/",
          "stickied": false,
          "url": "https://i.redd.it/pxwmiaqagmef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753275527,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we're making **1M context length variants**!\n\nYou can achieve &gt;6 tokens/s on **182GB unified memory or 158GB RAM + 24GB VRAM** via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via \n\n    -ot \".ffn_.*_exps.=CPU\"\n\nUnfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we're investigating why this happens.\n\nYou can also run the **un-quantized 8bit / 16bit** versions also using llama,cpp offloading! Use Q8\\_K\\_XL which will be completed in an hour or so.\n\nTo increase performance and context length, use KV cache quantization, especially the \\_1 variants (higher accuracy than \\_0 variants). More details [here](https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m).\n\n`--cache-type-k q4_1`\n\nEnable flash attention as well and also try llama.cpp's NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are [here](https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed).\n\nQwen3-Coder-480B-A35B GGUFs (still ongoing) are at [https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF)\n\n1 million context length variants will be up at [https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF)\n\nDocs on how to run it are here: [https://docs.unsloth.ai/basics/qwen3-coder](https://docs.unsloth.ai/basics/qwen3-coder)",
          "author_fullname": "t2_5wukhd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Unsloth dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6wgs7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 261,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 261,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/BrdyPpD7KtsDEQRMB38b-Z6TRovCBfRYhvapsX-O7BM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753234725,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we&amp;#39;re making &lt;strong&gt;1M context length variants&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;You can achieve &amp;gt;6 tokens/s on &lt;strong&gt;182GB unified memory or 158GB RAM + 24GB VRAM&lt;/strong&gt; via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Unfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we&amp;#39;re investigating why this happens.&lt;/p&gt;\n\n&lt;p&gt;You can also run the &lt;strong&gt;un-quantized 8bit / 16bit&lt;/strong&gt; versions also using llama,cpp offloading! Use Q8_K_XL which will be completed in an hour or so.&lt;/p&gt;\n\n&lt;p&gt;To increase performance and context length, use KV cache quantization, especially the _1 variants (higher accuracy than _0 variants). More details &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--cache-type-k q4_1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Enable flash attention as well and also try llama.cpp&amp;#39;s NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Qwen3-Coder-480B-A35B GGUFs (still ongoing) are at &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;1 million context length variants will be up at &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Docs on how to run it are here: &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder\"&gt;https://docs.unsloth.ai/basics/qwen3-coder&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/s9cwrvwg1jef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/s9cwrvwg1jef1.png?auto=webp&amp;s=c00619420d39151822d49bcce97ea3e78e847971",
                  "width": 2560,
                  "height": 2740
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6a9ffee96aa9dcd39f3c41d338a8e758af4d75",
                    "width": 108,
                    "height": 115
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c70c0936d1d66995daaa0390deaa956eaec815e1",
                    "width": 216,
                    "height": 231
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89539969c58182206b11d57c691f1c822b4002b3",
                    "width": 320,
                    "height": 342
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=75c9ba63f5cc1768819789d0934d7d2a1e5a5926",
                    "width": 640,
                    "height": 685
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=812205928bb5b587c3e1e23bd16cae5c1d48ae0e",
                    "width": 960,
                    "height": 1027
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32245973741ece306547037bf3f6efa37649cb42",
                    "width": 1080,
                    "height": 1155
                  }
                ],
                "variants": {},
                "id": "ERO7uofUircHQEW6vRJ41kGEOL83JHug3Yrw0FJ_7oM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6wgs7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danielhanchen",
          "discussion_type": null,
          "num_comments": 85,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/",
          "stickied": false,
          "url": "https://i.redd.it/s9cwrvwg1jef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753234725,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen out here releasing models like it‚Äôs a Costco sample table",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qixu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 519,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 519,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/S7tH6DgPEGKSzcu1dZlyahFSv5skqhzjfdd4AVfWVi4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219204,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5eb8n31sshef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5eb8n31sshef1.png?auto=webp&amp;s=c1694040f87c60dc765d805ee64b6518e3bd108b",
                  "width": 722,
                  "height": 1032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47116ec0e7ef90202d820540f88598c3cfd0a160",
                    "width": 108,
                    "height": 154
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3d8c4cb9e760fdd971704cc87722923b6445146",
                    "width": 216,
                    "height": 308
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a14963b84a65b1261a6b4b6b451fce1c2285102",
                    "width": 320,
                    "height": 457
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f24e0235850da677693988507655dde73bf8e60",
                    "width": 640,
                    "height": 914
                  }
                ],
                "variants": {},
                "id": "yWOKVm4sVIve_VHC2bO92aBIp15Yh_tuiWnp6wkEtnE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m6qixu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/",
          "stickied": false,
          "url": "https://i.redd.it/5eb8n31sshef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753219204,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Available in https://chat.qwen.ai",
          "author_fullname": "t2_e9mfhlg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3- Coder üëÄ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mew9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 647,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 647,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/e6gFp_J-Dv7QIFguXfhuN4U3lDC6MMgny7SMuBnt9pI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209850,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Available in &lt;a href=\"https://chat.qwen.ai\"&gt;https://chat.qwen.ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?auto=webp&amp;s=a57681c6848dc38714b9bea86a26c30bed7d4d42",
                  "width": 1036,
                  "height": 695
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4a02434a648980c01b1a76032aa8e02027937c6",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be152167170e73dd02f7850c4f9bb67cf143ec4a",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c86d3d06fd820523c1470692b2726d59dbaf6d3",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=92b455544fdc9f84aebcf9cf995f7e3e643179a1",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d63431a252098393997b8c247ff6a0a80b67f78",
                    "width": 960,
                    "height": 644
                  }
                ],
                "variants": {},
                "id": "52S4zww-hEGiuCbEDUlQZAn66M2iCNb-181uTVxpyGY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6mew9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xhehab_",
          "discussion_type": null,
          "num_comments": 185,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/",
          "stickied": false,
          "url": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "subreddit_subscribers": 503516,
          "created_utc": 1753209850,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\n\nToday, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.\n\n",
          "author_fullname": "t2_e7q9h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 coder will be in multiple sizes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qnpq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 369,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 369,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Today, we&amp;#39;re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we&amp;#39;re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qnpq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dinesh2609",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 503516,
          "created_utc": 1753219525,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What tools and settings enable optimal performance with CPU + GPU inference (partial offloading)? Here's my setup, which runs at \\~7.2 t/s, which is the maximum I've been able to squeeze out experimenting with settings in LM Studio and Llama.cpp. As we get more model releases that often don't fit entirely in VRAM, it seems like making the most of these settings is important.  \n  \n**Model:** Qwen3-235B-A22B 2507 / Unsloth's Q2\\_K\\_XL Quant / 82.67GB\n\n**GPU**: 5090 / 32GB VRAM\n\n**CPU**: AMD Ryzen 9 9900X\n\n**RAM:** 2x32GB DDR5-6000\n\n**Settings:**\n\n* Context: 4096\n* GPU Offload: 42/94 layers\n* CPU Thread Pool Size: 9\n* Batch Size: 512\n\n",
          "author_fullname": "t2_i5ptpsd5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Optimizing inference on GPU + CPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m7oolz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753313226,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What tools and settings enable optimal performance with CPU + GPU inference (partial offloading)? Here&amp;#39;s my setup, which runs at ~7.2 t/s, which is the maximum I&amp;#39;ve been able to squeeze out experimenting with settings in LM Studio and Llama.cpp. As we get more model releases that often don&amp;#39;t fit entirely in VRAM, it seems like making the most of these settings is important.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Qwen3-235B-A22B 2507 / Unsloth&amp;#39;s Q2_K_XL Quant / 82.67GB&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: 5090 / 32GB VRAM&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 9 9900X&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 2x32GB DDR5-6000&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Settings:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Context: 4096&lt;/li&gt;\n&lt;li&gt;GPU Offload: 42/94 layers&lt;/li&gt;\n&lt;li&gt;CPU Thread Pool Size: 9&lt;/li&gt;\n&lt;li&gt;Batch Size: 512&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7oolz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SubstantialSock8002",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753313226,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôve been testing Qwen3-Coder-480B (on Hyperbolics) and  Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear.  \n\nQwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like it‚Äôs just filling in templates rather than thinking through the task. It‚Äôs not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks.  \n\nSome of this might be because Hyperbolics hasn‚Äôt fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.\n\nKimi K2 works much better. Even though it‚Äôs not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, it‚Äôs consistently the better option.",
          "author_fullname": "t2_a29pmyxj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Qwen3 Coder 480B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zz1v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753245282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôve been testing Qwen3-Coder-480B (on Hyperbolics) and  Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear.  &lt;/p&gt;\n\n&lt;p&gt;Qwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like it‚Äôs just filling in templates rather than thinking through the task. It‚Äôs not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks.  &lt;/p&gt;\n\n&lt;p&gt;Some of this might be because Hyperbolics hasn‚Äôt fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.&lt;/p&gt;\n\n&lt;p&gt;Kimi K2 works much better. Even though it‚Äôs not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, it‚Äôs consistently the better option.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6zz1v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Pattern9779",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753245282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_o65i6kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI.Gov | President Trump's AI Strategy and Action Plan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7m534",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753306884,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ai.gov",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.ai.gov/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7m534",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fallingdowndizzyvr",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7m534/aigov_president_trumps_ai_strategy_and_action_plan/",
          "stickied": false,
          "url": "https://www.ai.gov/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753306884,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TLDR**: Anyone has infographics/doc/dashboard for this? Please share. Thanks.\n\n\n\n^(I'm talking about stuff like Temperature, TopK, TopP, MinP, etc., values for all models. Though advanced users can apply these values with their experience, newbies like me need some kind of dashboard or list or repo with such details so we could open that before using models.)\n\n^(Currently my system has 20+ tiny models(Llama, Gemma, Qwen, Deepseek, Granite, etc.,). Even though I take settings for particular model from HF page before using, some models don't have the settings there.) \n\n^(Also I need to enter the values of those settings again whenever I open New chat. Accidentally I deleted some chat histories multiple times in past. So going to HF page again &amp; again just for this is too repetitive &amp; boring for me.)  ",
          "author_fullname": "t2_1deiadfhb1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recommended Settings ( Temperature, TopK, TopP, MinP, etc., ) for All models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7k50u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753302214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: Anyone has infographics/doc/dashboard for this? Please share. Thanks.&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;I&amp;#39;m talking about stuff like Temperature, TopK, TopP, MinP, etc., values for all models. Though advanced users can apply these values with their experience, newbies like me need some kind of dashboard or list or repo with such details so we could open that before using models.&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Currently my system has 20+ tiny models(Llama, Gemma, Qwen, Deepseek, Granite, etc.,&lt;/sup&gt;. Even though I take settings for particular model from HF page before using, some models don&amp;#39;t have the settings there.) &lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Also I need to enter the values of those settings again whenever I open New chat. Accidentally I deleted some chat histories multiple times in past. So going to HF page again &amp;amp; again just for this is too repetitive &amp;amp; boring for me.&lt;/sup&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7k50u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pmttyji",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753302214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Earlier it were AI coding IDEs like cursor or GitHub copilot extension which came with agent mode. Then anthropic released Claude code, then openai, google and now alibaba followed the same suit to released their CLIs. \n\nRight now there's just too many options to use and they're all quite good, which makes it difficult to strike a balance of how much to experiment and what to use.\n\nWould like to know what pair programming methods do you use and what would you suggest.\n",
          "author_fullname": "t2_42oc6qgj8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Actually good Agentic coding tools",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ijtf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753298617,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Earlier it were AI coding IDEs like cursor or GitHub copilot extension which came with agent mode. Then anthropic released Claude code, then openai, google and now alibaba followed the same suit to released their CLIs. &lt;/p&gt;\n\n&lt;p&gt;Right now there&amp;#39;s just too many options to use and they&amp;#39;re all quite good, which makes it difficult to strike a balance of how much to experiment and what to use.&lt;/p&gt;\n\n&lt;p&gt;Would like to know what pair programming methods do you use and what would you suggest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7ijtf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Particular_Tap_4002",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ijtf/actually_good_agentic_coding_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7ijtf/actually_good_agentic_coding_tools/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753298617,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How is it holding up to 64k, 128, 256, 512k, 1Mil?",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone tested or know of tests for Qwen3 Coder long context length?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ne51",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753309904,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How is it holding up to 64k, 128, 256, 512k, 1Mil?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7ne51",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m7ne51/has_anyone_tested_or_know_of_tests_for_qwen3/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7ne51/has_anyone_tested_or_know_of_tests_for_qwen3/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753309904,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for leads for best edge model to deploy in an email mobile app. Tasks are closeIE (extract flight confirmation details), Summarize this newsletter, and Draft an email response. \n\nNotable considerations\n* Most emails are less than 5k in length \n* Less parameters means better battery efficiency \n* Inference time is critical \n* Loading a model on GPU takes 10s+ with mediaPipe\n* GPU execution is a must and specialized kernels make it go brr-- so contrived models likely won't have fast hw acceleration on Snapdragon \n\n\n\n[View Poll](https://www.reddit.com/poll/1m7mlcr)",
          "author_fullname": "t2_img2xgzp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best edge model for mobile - Qwen, LFM2, Gemma3N?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7mlcr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753307960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for leads for best edge model to deploy in an email mobile app. Tasks are closeIE (extract flight confirmation details), Summarize this newsletter, and Draft an email response. &lt;/p&gt;\n\n&lt;p&gt;Notable considerations\n* Most emails are less than 5k in length \n* Less parameters means better battery efficiency \n* Inference time is critical \n* Loading a model on GPU takes 10s+ with mediaPipe\n* GPU execution is a must and specialized kernels make it go brr-- so contrived models likely won&amp;#39;t have fast hw acceleration on Snapdragon &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1m7mlcr\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7mlcr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yonz-",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1753912760504,
            "options": [
              {
                "text": "nuExtract 2.0 (multi modal) - extraction SOTA",
                "id": "31218819"
              },
              {
                "text": "Qwen3 1.7B",
                "id": "31218820"
              },
              {
                "text": "Gemma 3n E2 (2B active 4B model)",
                "id": "31218821"
              },
              {
                "text": "Qwen3 4B",
                "id": "31218822"
              },
              {
                "text": "Liquid LFM2 (new: July 2025) 0.3-1.2",
                "id": "31218823"
              },
              {
                "text": "SmolLM",
                "id": "31218824"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 28,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7mlcr/best_edge_model_for_mobile_qwen_lfm2_gemma3n/",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7mlcr/best_edge_model_for_mobile_qwen_lfm2_gemma3n/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753307960,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, what do you all think for sort of a medium / smallest model to use as an orchestrator model that runs with whisper (speech in) and tts (speech out). I also want it to view my screen to get context to pass to other other models / mcp so it knows what is going on so it can respond etc, then route and call tools / MCP. I intend to do most heavy lifting and anything with real output using Claude code sdk since have unlimited max plan. \n\nI was am looking at using Grafiti for memory and building some consensus between models based on Zen mcp implementation:   \n\nI have a 64 gb macbook pro M1 and I‚Äôm looking at Qwen3-30B-A3B-MLX-4bit ([hugging face link](https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit)),.\n\nI would welcome any advice! I've looked at Jan and related though seems too small. Is there anything that will run on my MBP that can serve as this brain (I looked at Gemma 3n, but its not fully mutli-modal out of the box as is). Would the be possible with this hardware?\n\nThis is the potential stack I came up with in chatting with Claude and o3:\n\n    User Input (speech/screen/events)\n               ‚Üì\n        Local Processing\n        ‚îú‚îÄ‚îÄ VAD ‚Üí STT ‚Üí Text\n        ‚îú‚îÄ‚îÄ Screen ‚Üí OCR ‚Üí Context  \n        ‚îî‚îÄ‚îÄ Events ‚Üí MCP ‚Üí Actions\n               ‚Üì\n         Qwen3-30B Router\n        \"Is this simple?\"\n          ‚Üì         ‚Üì\n        Yes        No\n         ‚Üì          ‚Üì\n      Local     Claude API\n      Response  + MCP tools\n         ‚Üì          ‚Üì\n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n              ‚Üì\n        Graphiti Memory\n              ‚Üì\n        Response Stream\n              ‚Üì\n        Kyutai TTS        \n    \n\nThoughts?",
          "author_fullname": "t2_846pg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best small to medium size Local LLM Orchestrator for calling Tools, managing STT, TTS, screen OCR, and with passing heavy lift calls to Claude Code SDK, running on Macbook Pro.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7hq4w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753296737,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, what do you all think for sort of a medium / smallest model to use as an orchestrator model that runs with whisper (speech in) and tts (speech out). I also want it to view my screen to get context to pass to other other models / mcp so it knows what is going on so it can respond etc, then route and call tools / MCP. I intend to do most heavy lifting and anything with real output using Claude code sdk since have unlimited max plan. &lt;/p&gt;\n\n&lt;p&gt;I was am looking at using Grafiti for memory and building some consensus between models based on Zen mcp implementation:   &lt;/p&gt;\n\n&lt;p&gt;I have a 64 gb macbook pro M1 and I‚Äôm looking at Qwen3-30B-A3B-MLX-4bit (&lt;a href=\"https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit\"&gt;hugging face link&lt;/a&gt;),.&lt;/p&gt;\n\n&lt;p&gt;I would welcome any advice! I&amp;#39;ve looked at Jan and related though seems too small. Is there anything that will run on my MBP that can serve as this brain (I looked at Gemma 3n, but its not fully mutli-modal out of the box as is). Would the be possible with this hardware?&lt;/p&gt;\n\n&lt;p&gt;This is the potential stack I came up with in chatting with Claude and o3:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;User Input (speech/screen/events)\n           ‚Üì\n    Local Processing\n    ‚îú‚îÄ‚îÄ VAD ‚Üí STT ‚Üí Text\n    ‚îú‚îÄ‚îÄ Screen ‚Üí OCR ‚Üí Context  \n    ‚îî‚îÄ‚îÄ Events ‚Üí MCP ‚Üí Actions\n           ‚Üì\n     Qwen3-30B Router\n    &amp;quot;Is this simple?&amp;quot;\n      ‚Üì         ‚Üì\n    Yes        No\n     ‚Üì          ‚Üì\n  Local     Claude API\n  Response  + MCP tools\n     ‚Üì          ‚Üì\n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n          ‚Üì\n    Graphiti Memory\n          ‚Üì\n    Response Stream\n          ‚Üì\n    Kyutai TTS        \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?auto=webp&amp;s=6cf13531259fc6a43addb217a67da463735f0aaf",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a97d235ba5ee0d377655e74657048733e66c0c80",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f65ca22ff036e0e09c3072baa25e27599adcf38d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48ce65ed5b5bd70fd126bbde8a6424436ca04f3a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ffb0cf505dc6f2aa6cfcb7c7f84e77a38833283",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fb2173e54335250d945cddd3c72954d45d353c7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d321923d5c25125f2212fa01cabda39fcb114276",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7hq4w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "matznerd",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753296737,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.\n\nVery strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.\n\nCreds [The Feature Crew](https://www.youtube.com/@TheFeatureCrew) for the original idea.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Web Development",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ny2q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 354,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755911493%2CMThkY2JlMjZmMGQzNTI1ZTg1NGJjYzdjMDk2MWNlYzI5YWRlYTVmNzg2ZDU3MDlhNmUwMTQwYTg5NTEzNjllZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755911493%2CYTljYWJkOGFkZjdiNDBiMjAyZWYwMTRkZWY3NTAxMWFiMTJlOGFiMjg1N2FkZTM5NmQ2MWJjNTY1YTAyNzI2Mg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 354,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=127481f43d7a622f7d4c23a977a165102347dc33",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213272,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.&lt;/p&gt;\n\n&lt;p&gt;Very strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.&lt;/p&gt;\n\n&lt;p&gt;Creds &lt;a href=\"https://www.youtube.com/@TheFeatureCrew\"&gt;The Feature Crew&lt;/a&gt; for the original idea.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ob9yhvcjahef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?format=pjpg&amp;auto=webp&amp;s=9da74680d1673a7d5086bef35987945fda2390f7",
                  "width": 3024,
                  "height": 1964
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=69ec82c87ea25ca0cf09c32e6e2e65fd1ebe0353",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a7e4214f7585ef0bf76db563e79ceb7b7b73df5",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=da1a4545269003e5a3164b1074444b181b803a22",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ee471871b0892b000bd102b783d4c1fea31bbdf2",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b638262bf1c7530fbee91a29e1a5465444ef5500",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a910147ce667ac1bb6fed7eb4d08a092f3974fe",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ny2q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/",
          "stickied": false,
          "url": "https://v.redd.it/ob9yhvcjahef1",
          "subreddit_subscribers": 503516,
          "created_utc": 1753213272,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755911493%2CMThkY2JlMjZmMGQzNTI1ZTg1NGJjYzdjMDk2MWNlYzI5YWRlYTVmNzg2ZDU3MDlhNmUwMTQwYTg5NTEzNjllZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755911493%2CYTljYWJkOGFkZjdiNDBiMjAyZWYwMTRkZWY3NTAxMWFiMTJlOGFiMjg1N2FkZTM5NmQ2MWJjNTY1YTAyNzI2Mg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "You probably already know about my [benchmark](https://www.designarena.ai/), but here's [context](https://www.reddit.com/r/LocalLLaMA/comments/1lxth6s/comment/n2qoqtk/?context=3) if you missed it. The tldr is that it's a crowdsource benchmark that takes human preferences on frontend and image generations from different models to produce a leaderboard ranking for which models are currently the best at UI and design generation. \n\nI'm going to try to keep these update posts to once-a-week or every other week to not come off as spam (sorry for that earlier, though I'm just seeing interesting results). Also, we realize there are flaws to the leaderboard (as all leaderboards and benchmarks have) that we're progressively trying to improve, but think it has been a good barometer for evaluating the models in particular tiers when it comes to coding. \n\nAnyways, since my last update on the 11th, we've added a few models, and in the last 24 hours, specifically Qwen3-235B-A22B-Instruct-2507 and Qwen3-Coder (less than an hour ago). Though the sample size is still very small, Qwen3-235B-A22B-Instruct-2507 appears to be killing it. I was reading through remarks on Twitter and Reddit that the Instruct model was on par with Opus which I thought was hyperbole at the time, but maybe that claim will hold true in the long run. \n\nWhat has been your experience with these Qwen models and what do you think? Open source is killing it right now. ",
          "author_fullname": "t2_98ouo03z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UI/UX benchmark update 7/22: Newest Qwen models added, Qwen3 takes the lead in terms of win rate (though still early)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ztb2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 69,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 69,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/yOnjsudSDwwMasrhJ1H10swcbMrmX3jIP8XzRlgDA6k.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753244753,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You probably already know about my &lt;a href=\"https://www.designarena.ai/\"&gt;benchmark&lt;/a&gt;, but here&amp;#39;s &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lxth6s/comment/n2qoqtk/?context=3\"&gt;context&lt;/a&gt; if you missed it. The tldr is that it&amp;#39;s a crowdsource benchmark that takes human preferences on frontend and image generations from different models to produce a leaderboard ranking for which models are currently the best at UI and design generation. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to try to keep these update posts to once-a-week or every other week to not come off as spam (sorry for that earlier, though I&amp;#39;m just seeing interesting results). Also, we realize there are flaws to the leaderboard (as all leaderboards and benchmarks have) that we&amp;#39;re progressively trying to improve, but think it has been a good barometer for evaluating the models in particular tiers when it comes to coding. &lt;/p&gt;\n\n&lt;p&gt;Anyways, since my last update on the 11th, we&amp;#39;ve added a few models, and in the last 24 hours, specifically Qwen3-235B-A22B-Instruct-2507 and Qwen3-Coder (less than an hour ago). Though the sample size is still very small, Qwen3-235B-A22B-Instruct-2507 appears to be killing it. I was reading through remarks on Twitter and Reddit that the Instruct model was on par with Opus which I thought was hyperbole at the time, but maybe that claim will hold true in the long run. &lt;/p&gt;\n\n&lt;p&gt;What has been your experience with these Qwen models and what do you think? Open source is killing it right now. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lcjgeavzvjef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lcjgeavzvjef1.png?auto=webp&amp;s=31df1bef3a627d2ba33e030e86d5d66a2b9b0ee0",
                  "width": 1333,
                  "height": 881
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d4b029e6012124dc9f449076bfcd1f4cfdf2ac1",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1205127af73fd4083008a096571d899bc2b6aadd",
                    "width": 216,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf64c9f604b8aef2fea7c80e52aa0d92de5fb99a",
                    "width": 320,
                    "height": 211
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8550da9c204aaebc89b401002be06079a6beec29",
                    "width": 640,
                    "height": 422
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c69386465fdc08532cb1ba6e8d60c731e32bc4b",
                    "width": 960,
                    "height": 634
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c87d7c433bad8b28ef732c515d7baa7baf7a784c",
                    "width": 1080,
                    "height": 713
                  }
                ],
                "variants": {},
                "id": "nLkR4n4kuyoAJ5FYt5FCPobA_n2hnoAkT68Lm-yyAho"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6ztb2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished-Copy332",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/",
          "stickied": false,
          "url": "https://i.redd.it/lcjgeavzvjef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753244753,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is anyone maintaining a \"fits in a MacBook Pro\" kind of leaderboard for open models? It's by far the form factor for open models I've seen colleagues interested in.\n\nI know you can just see the number of parameters, active parameters in MoEs, etc., but a nice leaderboard with some tokens/sec average would be useful for many.",
          "author_fullname": "t2_e9yxn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MacBook model rank",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7lp0z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753305825,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone maintaining a &amp;quot;fits in a MacBook Pro&amp;quot; kind of leaderboard for open models? It&amp;#39;s by far the form factor for open models I&amp;#39;ve seen colleagues interested in.&lt;/p&gt;\n\n&lt;p&gt;I know you can just see the number of parameters, active parameters in MoEs, etc., but a nice leaderboard with some tokens/sec average would be useful for many.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7lp0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JCx64",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7lp0z/macbook_model_rank/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7lp0z/macbook_model_rank/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753305825,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_lsixf36sr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF ¬∑ Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m71f20",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 52,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 52,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8d404162721f954167cc891f633466a429b34c96",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753250317,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?auto=webp&amp;s=b0e606fe60c3b427cf1340db7a0ca6006dff3e57",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ad6d1b6c4559472693b7af1de31e24e4a8023a3",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1145966d2cde6471e76bb43f495683a63b013b72",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7fff728a74e01125301fc6c9d2699680540ef0a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60e9638973c964d4a82b7f30f192158867f7fc48",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=350210cdc15e1c044856de883fd8d259a90dd1f0",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9863761bbb4db313f92685d42bb3689971cd9fe8",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m71f20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Wolf-2007",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m71f20/unslothqwen3coder480ba35binstructgguf_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF",
          "subreddit_subscribers": 503516,
          "created_utc": 1753250317,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "## Preface\n\n&gt; The video is sped up; it actually takes about 20-30 minutes\n\n- Github Repository: https://github.com/wrtnlabs/autobe\n- Generation Result: https://github.com/wrtnlabs/autobe-example-bbs\n\nWe are honored to introduce [`AutoBE`](https://github.com/wrtnlabs/autobe) to you. [`AutoBE`](https://github.com/wrtnlabs/autobe) is an open-source project developed by Wrtn Technologies (Korean AI startup company), a vibe coding agent that automatically generates backend applications.\n\nOne of [`AutoBE`](https://github.com/wrtnlabs/autobe)'s key features is that it always generates code with 100% compilation success. The secret lies in our proprietary compiler system. Through our self-developed compilers, we support AI in generating type-safe code, and when AI generates incorrect code, the compiler detects it and provides detailed feedback, guiding the AI to generate correct code.\n\nThrough this approach, [`AutoBE`](https://github.com/wrtnlabs/autobe) always generates backend applications with 100% compilation success. When AI constructs AST (Abstract Syntax Tree) data through function calling, our proprietary compiler validates it, provides feedback, and ultimately generates complete source code.\n\n## Prisma DB Schema Compiler\n\nA compiler for database design.\n\n- Compiler Structures\n  - [`AutoBePrisma.IFile`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts)\n  - [`IAutoBePrismaValidation`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/IAutoBePrismaValidation.ts)\n  - [`IValidation`](https://github.com/samchon/openapi/blob/master/src/structures/IValidation.ts)\n- Generation Result\n  - Prisma Schema Files: https://github.com/wrtnlabs/autobe-example-bbs/tree/main/prisma/schema\n  - ERD Documentation: https://github.com/wrtnlabs/autobe-example-bbs/blob/main/docs/ERD.md\n\n[`AutoBE`](https://github.com/wrtnlabs/autobe) utilizes a self-developed DB compiler when designing databases.\n\nFirst, it creates an AST (Abstract Syntax Tree) structure called `AutoBePrisma.IFile` through AI function calling (or structured output). Then it analyzes the data created by the AI to check for logical or type errors.\n\nIf logical errors are found, these are returned to the AI in the form of `IAutoBePrismaValidation` with detailed reasons, guiding the AI to generate correct `AutoBePrisma.IFile` data in the next function calling. Major logical error cases include:\n\n- **Duplication errors**: Duplicate definitions of filenames, model names, field names\n- **Circular references**: Cross-dependencies where two models reference each other as foreign keys\n- **Non-existent references**: Cases where foreign keys point to non-existent target models\n- **Index configuration errors**: Creating indexes on non-existent fields, duplicate index definitions\n- **Data type mismatches**: Applying GIN indexes to non-string fields\n- **Field names identical to table names**: Potential confusion due to normalization errors\n\nIf type errors are found, these are also returned to the AI in the form of `IValidation`, guiding the AI to generate data with correct types.\n\nFinally, when `AutoBePrisma.IFile` is correctly generated without any logical or type errors, it is converted to Prisma DB schema (code generation). Simultaneously, ERD (Entity Relationship Diagram) and documentation are also generated ([`prisma-markdown`](https://github.com/samchon/prisma-markdown)), helping users understand their DB design.\n\nThe generated Prisma schema files include detailed descriptive comments for each table and field. These comments go beyond simple code documentation - they are directly utilized by `prisma-markdown` when generating ERDs and documentation, becoming core content of the database design documents. Therefore, developers can clearly understand the role of each table and field not only at the code level but also through visual ERD diagrams.\n\n## OpenAPI Document Compiler\n\nA compiler for API interface design.\n\n- Compiler Structures\n  - [`AutoBeOpenApi.IDocument`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts)\n  - [`IValidation`](https://github.com/samchon/openapi/blob/master/src/structures/IValidation.ts)\n- Generation Result: https://stackblitz.com/edit/njkqikge\n\n[`AutoBE`](https://github.com/wrtnlabs/autobe) utilizes a self-developed OpenAPI compiler when designing API interfaces.\n\nThis OpenAPI compiler first has an AST (Abstract Syntax Tree) structure of type `AutoBeOpenApi.IDocument`, which is created through AI function calling. Then it analyzes this data, and if logical or type errors are found, detailed reasons are returned to the AI, guiding the AI to generate correct `AutoBeOpenApi.IDocument` data.\n\nAfter the AI successfully generates a flawless `AutoBeOpenApi.IDocument`, [`AutoBE`](https://github.com/wrtnlabs/autobe) converts it to the official OpenAPI v3.1 spec [`OpenApi.IDocument`](https://github.com/samchon/openapi/blob/master/src/OpenApi.ts) structure. This is then further converted to TypeScript/NestJS source code (code generation), completing the API interface implementation.\n\nThe generated TypeScript/NestJS source code consists of API controller classes and DTO (Data Transfer Object) types, where each API controller method is a mock method that only generates random values of the specified return type using the [`typia.random&lt;T&gt;()`](https://typia.io/docs/random) function. Therefore, APIs generated by [`AutoBE`](https://github.com/wrtnlabs/autobe) don't actually function, but they complete the foundational work for API interface design and implementation.\n\nAll generated controller functions and DTO types include detailed JSDoc comments. The purpose of each API endpoint, parameter descriptions, and meanings of return values are clearly documented, making it easy for developers to understand the purpose and usage of APIs.\n\n## E2E Test Function Compiler\n\nA compiler for generating E2E test programs.\n\n- Compiler Structures\n  - [`AutoBeTest.IFunction`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts)\n  - [`IAutoBeTypeScriptCompileResult`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/compiler/IAutoBeTypeScriptCompileResult.ts)\n  - [`IValidation`](https://github.com/samchon/openapi/blob/master/src/structures/IValidation.ts)\n- Prompt Structures\n  - [`IAutoBeTestWriteApplication`](https://github.com/wrtnlabs/autobe/blob/main/packages/agent/src/orchestrate/test/structures/IAutoBeTestWriteApplication.ts#L4)\n  - [`IAutoBeTestCorrectApplication`](https://github.com/wrtnlabs/autobe/blob/main/packages/agent/src/orchestrate/test/structures/IAutoBeTestCorrectApplication.ts)\n- Generation Result: https://github.com/wrtnlabs/autobe-example-bbs\n\n[`AutoBE`](https://github.com/wrtnlabs/autobe) uses a self-developed compiler when generating E2E test code.\n\nThis E2E test compiler has an AST (Abstract Syntax Tree) structure called `AutoBeTest.IFunction`, which is constructed through AI function calling. Then it analyzes this data, and if logical or type errors are found, detailed reasons are returned to the AI, guiding the AI to generate correct `AutoBeTest.IFunction` data.\n\nAfter the AI successfully generates flawless `AutoBeTest.IFunction` data, [`AutoBE`](https://github.com/wrtnlabs/autobe) converts it to TypeScript source code (code generation). The Test agent then combines each of the generated e2e test functions with the code generated by the interface agent to complete a new backend application.\n\nWhen E2E test functions call backend server API functions, they use an SDK (Software Development Kit) generated for the backend server API to ensure type-safe API function calls.\n\nEach generated E2E test function includes detailed comments describing the test's scenario and purpose. Which APIs are called in what order, what is verified at each step, and what results are expected are clearly documented, making it easy to understand the intent of the test code.\n\n## Detailed Article\n\nhttp://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html\n\nSince Reddit doesn't allow posting YouTube videos, diagrams, and image materials, I've written a detailed article separately on blog.\n\nFor those who are curious about the details, please refer to the link above.",
          "author_fullname": "t2_1njlywuqe6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[AutoBE] We're making AI-friendly Compilers for Vibe Coding (open source)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7e5uc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/vpmfq4l4inef1/DASH_480.mp4?source=fallback",
              "has_audio": false,
              "height": 854,
              "width": 474,
              "scrubber_media_url": "https://v.redd.it/vpmfq4l4inef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vpmfq4l4inef1/DASHPlaylist.mpd?a=1755911493%2CYmZmMjk3YzhkNjQ2N2IzNWFmOWQ3ZDk5ZGZjNWE1MDIzNjcxMmRhYzQ1NjFlMGM0OWQwMjI2MTFhZjNkZjIxOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 59,
              "hls_url": "https://v.redd.it/vpmfq4l4inef1/HLSPlaylist.m3u8?a=1755911493%2CODFhYjc1N2FkMTQ2NjhkNzdhMTE0MDlkNTM3YWM5ZmQ5MTI2OTgyMmVmMDEyNDMzZmM2OTY0NzZhZDljZjZmOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Yzk5aTg1bDRpbmVmMY9-A8ZAgOuotV2CW7jB1Psa5aqFVRjz1XH6fQIZp4yz.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c8000c14077f1fc5c9905924cdff668fd3dc40a1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753288696,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;Preface&lt;/h2&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The video is sped up; it actually takes about 20-30 minutes&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Github Repository: &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Generation Result: &lt;a href=\"https://github.com/wrtnlabs/autobe-example-bbs\"&gt;https://github.com/wrtnlabs/autobe-example-bbs&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We are honored to introduce &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; to you. &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; is an open-source project developed by Wrtn Technologies (Korean AI startup company), a vibe coding agent that automatically generates backend applications.&lt;/p&gt;\n\n&lt;p&gt;One of &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt;&amp;#39;s key features is that it always generates code with 100% compilation success. The secret lies in our proprietary compiler system. Through our self-developed compilers, we support AI in generating type-safe code, and when AI generates incorrect code, the compiler detects it and provides detailed feedback, guiding the AI to generate correct code.&lt;/p&gt;\n\n&lt;p&gt;Through this approach, &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; always generates backend applications with 100% compilation success. When AI constructs AST (Abstract Syntax Tree) data through function calling, our proprietary compiler validates it, provides feedback, and ultimately generates complete source code.&lt;/p&gt;\n\n&lt;h2&gt;Prisma DB Schema Compiler&lt;/h2&gt;\n\n&lt;p&gt;A compiler for database design.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compiler Structures\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts\"&gt;&lt;code&gt;AutoBePrisma.IFile&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/IAutoBePrismaValidation.ts\"&gt;&lt;code&gt;IAutoBePrismaValidation&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/samchon/openapi/blob/master/src/structures/IValidation.ts\"&gt;&lt;code&gt;IValidation&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Generation Result\n\n&lt;ul&gt;\n&lt;li&gt;Prisma Schema Files: &lt;a href=\"https://github.com/wrtnlabs/autobe-example-bbs/tree/main/prisma/schema\"&gt;https://github.com/wrtnlabs/autobe-example-bbs/tree/main/prisma/schema&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;ERD Documentation: &lt;a href=\"https://github.com/wrtnlabs/autobe-example-bbs/blob/main/docs/ERD.md\"&gt;https://github.com/wrtnlabs/autobe-example-bbs/blob/main/docs/ERD.md&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; utilizes a self-developed DB compiler when designing databases.&lt;/p&gt;\n\n&lt;p&gt;First, it creates an AST (Abstract Syntax Tree) structure called &lt;code&gt;AutoBePrisma.IFile&lt;/code&gt; through AI function calling (or structured output). Then it analyzes the data created by the AI to check for logical or type errors.&lt;/p&gt;\n\n&lt;p&gt;If logical errors are found, these are returned to the AI in the form of &lt;code&gt;IAutoBePrismaValidation&lt;/code&gt; with detailed reasons, guiding the AI to generate correct &lt;code&gt;AutoBePrisma.IFile&lt;/code&gt; data in the next function calling. Major logical error cases include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Duplication errors&lt;/strong&gt;: Duplicate definitions of filenames, model names, field names&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Circular references&lt;/strong&gt;: Cross-dependencies where two models reference each other as foreign keys&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Non-existent references&lt;/strong&gt;: Cases where foreign keys point to non-existent target models&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Index configuration errors&lt;/strong&gt;: Creating indexes on non-existent fields, duplicate index definitions&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data type mismatches&lt;/strong&gt;: Applying GIN indexes to non-string fields&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Field names identical to table names&lt;/strong&gt;: Potential confusion due to normalization errors&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If type errors are found, these are also returned to the AI in the form of &lt;code&gt;IValidation&lt;/code&gt;, guiding the AI to generate data with correct types.&lt;/p&gt;\n\n&lt;p&gt;Finally, when &lt;code&gt;AutoBePrisma.IFile&lt;/code&gt; is correctly generated without any logical or type errors, it is converted to Prisma DB schema (code generation). Simultaneously, ERD (Entity Relationship Diagram) and documentation are also generated (&lt;a href=\"https://github.com/samchon/prisma-markdown\"&gt;&lt;code&gt;prisma-markdown&lt;/code&gt;&lt;/a&gt;), helping users understand their DB design.&lt;/p&gt;\n\n&lt;p&gt;The generated Prisma schema files include detailed descriptive comments for each table and field. These comments go beyond simple code documentation - they are directly utilized by &lt;code&gt;prisma-markdown&lt;/code&gt; when generating ERDs and documentation, becoming core content of the database design documents. Therefore, developers can clearly understand the role of each table and field not only at the code level but also through visual ERD diagrams.&lt;/p&gt;\n\n&lt;h2&gt;OpenAPI Document Compiler&lt;/h2&gt;\n\n&lt;p&gt;A compiler for API interface design.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compiler Structures\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts\"&gt;&lt;code&gt;AutoBeOpenApi.IDocument&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/samchon/openapi/blob/master/src/structures/IValidation.ts\"&gt;&lt;code&gt;IValidation&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Generation Result: &lt;a href=\"https://stackblitz.com/edit/njkqikge\"&gt;https://stackblitz.com/edit/njkqikge&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; utilizes a self-developed OpenAPI compiler when designing API interfaces.&lt;/p&gt;\n\n&lt;p&gt;This OpenAPI compiler first has an AST (Abstract Syntax Tree) structure of type &lt;code&gt;AutoBeOpenApi.IDocument&lt;/code&gt;, which is created through AI function calling. Then it analyzes this data, and if logical or type errors are found, detailed reasons are returned to the AI, guiding the AI to generate correct &lt;code&gt;AutoBeOpenApi.IDocument&lt;/code&gt; data.&lt;/p&gt;\n\n&lt;p&gt;After the AI successfully generates a flawless &lt;code&gt;AutoBeOpenApi.IDocument&lt;/code&gt;, &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; converts it to the official OpenAPI v3.1 spec &lt;a href=\"https://github.com/samchon/openapi/blob/master/src/OpenApi.ts\"&gt;&lt;code&gt;OpenApi.IDocument&lt;/code&gt;&lt;/a&gt; structure. This is then further converted to TypeScript/NestJS source code (code generation), completing the API interface implementation.&lt;/p&gt;\n\n&lt;p&gt;The generated TypeScript/NestJS source code consists of API controller classes and DTO (Data Transfer Object) types, where each API controller method is a mock method that only generates random values of the specified return type using the &lt;a href=\"https://typia.io/docs/random\"&gt;&lt;code&gt;typia.random&amp;lt;T&amp;gt;()&lt;/code&gt;&lt;/a&gt; function. Therefore, APIs generated by &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; don&amp;#39;t actually function, but they complete the foundational work for API interface design and implementation.&lt;/p&gt;\n\n&lt;p&gt;All generated controller functions and DTO types include detailed JSDoc comments. The purpose of each API endpoint, parameter descriptions, and meanings of return values are clearly documented, making it easy for developers to understand the purpose and usage of APIs.&lt;/p&gt;\n\n&lt;h2&gt;E2E Test Function Compiler&lt;/h2&gt;\n\n&lt;p&gt;A compiler for generating E2E test programs.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compiler Structures\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts\"&gt;&lt;code&gt;AutoBeTest.IFunction&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/compiler/IAutoBeTypeScriptCompileResult.ts\"&gt;&lt;code&gt;IAutoBeTypeScriptCompileResult&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/samchon/openapi/blob/master/src/structures/IValidation.ts\"&gt;&lt;code&gt;IValidation&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Prompt Structures\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/agent/src/orchestrate/test/structures/IAutoBeTestWriteApplication.ts#L4\"&gt;&lt;code&gt;IAutoBeTestWriteApplication&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/agent/src/orchestrate/test/structures/IAutoBeTestCorrectApplication.ts\"&gt;&lt;code&gt;IAutoBeTestCorrectApplication&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Generation Result: &lt;a href=\"https://github.com/wrtnlabs/autobe-example-bbs\"&gt;https://github.com/wrtnlabs/autobe-example-bbs&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; uses a self-developed compiler when generating E2E test code.&lt;/p&gt;\n\n&lt;p&gt;This E2E test compiler has an AST (Abstract Syntax Tree) structure called &lt;code&gt;AutoBeTest.IFunction&lt;/code&gt;, which is constructed through AI function calling. Then it analyzes this data, and if logical or type errors are found, detailed reasons are returned to the AI, guiding the AI to generate correct &lt;code&gt;AutoBeTest.IFunction&lt;/code&gt; data.&lt;/p&gt;\n\n&lt;p&gt;After the AI successfully generates flawless &lt;code&gt;AutoBeTest.IFunction&lt;/code&gt; data, &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; converts it to TypeScript source code (code generation). The Test agent then combines each of the generated e2e test functions with the code generated by the interface agent to complete a new backend application.&lt;/p&gt;\n\n&lt;p&gt;When E2E test functions call backend server API functions, they use an SDK (Software Development Kit) generated for the backend server API to ensure type-safe API function calls.&lt;/p&gt;\n\n&lt;p&gt;Each generated E2E test function includes detailed comments describing the test&amp;#39;s scenario and purpose. Which APIs are called in what order, what is verified at each step, and what results are expected are clearly documented, making it easy to understand the intent of the test code.&lt;/p&gt;\n\n&lt;h2&gt;Detailed Article&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"http://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html\"&gt;http://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Since Reddit doesn&amp;#39;t allow posting YouTube videos, diagrams, and image materials, I&amp;#39;ve written a detailed article separately on blog.&lt;/p&gt;\n\n&lt;p&gt;For those who are curious about the details, please refer to the link above.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/vpmfq4l4inef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Yzk5aTg1bDRpbmVmMY9-A8ZAgOuotV2CW7jB1Psa5aqFVRjz1XH6fQIZp4yz.png?format=pjpg&amp;auto=webp&amp;s=ec9f6f6ac2518a4956c8aae11dea6e6d1306ebcd",
                  "width": 600,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Yzk5aTg1bDRpbmVmMY9-A8ZAgOuotV2CW7jB1Psa5aqFVRjz1XH6fQIZp4yz.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=365d16f2ecbeb325c356467cd7e12742194a1fda",
                    "width": 108,
                    "height": 194
                  },
                  {
                    "url": "https://external-preview.redd.it/Yzk5aTg1bDRpbmVmMY9-A8ZAgOuotV2CW7jB1Psa5aqFVRjz1XH6fQIZp4yz.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca4638dbe8c303b4dcd404fcf675dd7e53abd26f",
                    "width": 216,
                    "height": 388
                  },
                  {
                    "url": "https://external-preview.redd.it/Yzk5aTg1bDRpbmVmMY9-A8ZAgOuotV2CW7jB1Psa5aqFVRjz1XH6fQIZp4yz.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=25e5f999182a968a690f6d093c35f22bec26ded3",
                    "width": 320,
                    "height": 576
                  }
                ],
                "variants": {},
                "id": "Yzk5aTg1bDRpbmVmMY9-A8ZAgOuotV2CW7jB1Psa5aqFVRjz1XH6fQIZp4yz"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m7e5uc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jhnam88",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7e5uc/autobe_were_making_aifriendly_compilers_for_vibe/",
          "stickied": false,
          "url": "https://v.redd.it/vpmfq4l4inef1",
          "subreddit_subscribers": 503516,
          "created_utc": 1753288696,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/vpmfq4l4inef1/DASH_480.mp4?source=fallback",
              "has_audio": false,
              "height": 854,
              "width": 474,
              "scrubber_media_url": "https://v.redd.it/vpmfq4l4inef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vpmfq4l4inef1/DASHPlaylist.mpd?a=1755911493%2CYmZmMjk3YzhkNjQ2N2IzNWFmOWQ3ZDk5ZGZjNWE1MDIzNjcxMmRhYzQ1NjFlMGM0OWQwMjI2MTFhZjNkZjIxOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 59,
              "hls_url": "https://v.redd.it/vpmfq4l4inef1/HLSPlaylist.m3u8?a=1755911493%2CODFhYjc1N2FkMTQ2NjhkNzdhMTE0MDlkNTM3YWM5ZmQ5MTI2OTgyMmVmMDEyNDMzZmM2OTY0NzZhZDljZjZmOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ql2vu0wz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Puget Systems Threadripper PRO 9000WX Llama Prompt Processing &amp; Token Generation benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ld4z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D500&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z&amp;image=https%3A%2F%2Fi.imgur.com%2Fk257k3u.jpg%3Ffb&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"500\" height=\"60\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;",
            "width": 500,
            "scrolling": false,
            "height": 60
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "imgur.com",
            "oembed": {
              "provider_url": "http://imgur.com",
              "description": "Discover the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and so much more from users.",
              "title": "Imgur",
              "url": "https://imgur.com/a/EDYfW8Z",
              "type": "rich",
              "thumbnail_width": 769,
              "height": 60,
              "width": 500,
              "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D500&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z&amp;image=https%3A%2F%2Fi.imgur.com%2Fk257k3u.jpg%3Ffb&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"500\" height=\"60\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;",
              "version": "1.0",
              "provider_name": "Imgur",
              "thumbnail_url": "https://i.imgur.com/k257k3u.jpg?fb",
              "thumbnail_height": 913
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D500&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z&amp;image=https%3A%2F%2Fi.imgur.com%2Fk257k3u.jpg%3Ffb&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"500\" height=\"60\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;",
            "width": 500,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m7ld4z",
            "height": 60
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1TY3ekkN1BvY7efk8vkTqbWYLMpL6rdncgaIYqt8mrc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753305060,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "imgur.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://imgur.com/a/EDYfW8Z",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?auto=webp&amp;s=6b99863e0ee515acfb7baf827a1475ae8c08c99a",
                  "width": 769,
                  "height": 913
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad075926dde531bde6baeaa59a7e8de8b9783ff7",
                    "width": 108,
                    "height": 128
                  },
                  {
                    "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3c27b03235dce4ba1a94fcdd7b3e3e8f73f33dd",
                    "width": 216,
                    "height": 256
                  },
                  {
                    "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8963a8fad73e01a1dc59518f26cdba96f13efd8",
                    "width": 320,
                    "height": 379
                  },
                  {
                    "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e6a4654ee6d198617faffc7ca553b7e86c784d3",
                    "width": 640,
                    "height": 759
                  }
                ],
                "variants": {},
                "id": "GLm1hgJojxMbTwvUw-Lc6StlFj9R36mDvuxy3H3bcNc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7ld4z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Caffdy",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ld4z/puget_systems_threadripper_pro_9000wx_llama/",
          "stickied": false,
          "url": "https://imgur.com/a/EDYfW8Z",
          "subreddit_subscribers": 503516,
          "created_utc": 1753305060,
          "num_crossposts": 0,
          "media": {
            "type": "imgur.com",
            "oembed": {
              "provider_url": "http://imgur.com",
              "description": "Discover the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and so much more from users.",
              "title": "Imgur",
              "url": "https://imgur.com/a/EDYfW8Z",
              "type": "rich",
              "thumbnail_width": 769,
              "height": 60,
              "width": 500,
              "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D500&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z&amp;image=https%3A%2F%2Fi.imgur.com%2Fk257k3u.jpg%3Ffb&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"500\" height=\"60\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;",
              "version": "1.0",
              "provider_name": "Imgur",
              "thumbnail_url": "https://i.imgur.com/k257k3u.jpg?fb",
              "thumbnail_height": 913
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Could this be Deepseek?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 33,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6lf9s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 373,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 373,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WAXw-XuvIZ9mRKeenbrWXREbY65LvO1BDwwwlpUBowY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753207666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qzkjkgegugef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qzkjkgegugef1.png?auto=webp&amp;s=982da5cfa0575f138ae47f73b6eddafc3a141895",
                  "width": 822,
                  "height": 197
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=917acda1e7d58dd2b0c466213686f858f3d1d90f",
                    "width": 108,
                    "height": 25
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac14c427611c1eca254c8bb52ac34a30bf33d9f1",
                    "width": 216,
                    "height": 51
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8524670d8b150c1ad50fa23613a644190b14608f",
                    "width": 320,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e224ff9a214f929b3917304102fe92d67371e639",
                    "width": 640,
                    "height": 153
                  }
                ],
                "variants": {},
                "id": "LX0EdZ_oilMQBPfRRihja4cnZPDhv01xC24KCslp3qQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6lf9s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 59,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/",
          "stickied": false,
          "url": "https://i.redd.it/qzkjkgegugef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753207666,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new to running LLM's locally and have been working on a new project that has an \"AI powered\" requirement... I've learned a ton in the process but feel like I'm missing something.\n\nThe idea is to take a large csv that has been aggregated and formatted from various other sources, then feed that to an LLM that can identify trends, flag items that need attention, allow queries etc... but it can't use 3rd party API's\n\nI'm using self hosted Open Web UI API as my backend with Ollama and Mistral behind it all running on a 64GB AWS EC2 instance CPU only.   \n  \nThe file is too large to fit into the context window alone so I tried using the Files / Knowledge / RAG functionality that comes with OpenWebUI but that seems to really struggle to understand the entire dataset. \n\nFor example it's unable to tell me how many lines are in the file, or which item ID appears most often. \n\nJust curious if I'm going about this all wrong. Is this even realistic?\n\n",
          "author_fullname": "t2_jlnyy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Analyzing CSV and structured data - RAG, MCP, tools, or plain old scripting?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7mu6e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753308541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to running LLM&amp;#39;s locally and have been working on a new project that has an &amp;quot;AI powered&amp;quot; requirement... I&amp;#39;ve learned a ton in the process but feel like I&amp;#39;m missing something.&lt;/p&gt;\n\n&lt;p&gt;The idea is to take a large csv that has been aggregated and formatted from various other sources, then feed that to an LLM that can identify trends, flag items that need attention, allow queries etc... but it can&amp;#39;t use 3rd party API&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using self hosted Open Web UI API as my backend with Ollama and Mistral behind it all running on a 64GB AWS EC2 instance CPU only.   &lt;/p&gt;\n\n&lt;p&gt;The file is too large to fit into the context window alone so I tried using the Files / Knowledge / RAG functionality that comes with OpenWebUI but that seems to really struggle to understand the entire dataset. &lt;/p&gt;\n\n&lt;p&gt;For example it&amp;#39;s unable to tell me how many lines are in the file, or which item ID appears most often. &lt;/p&gt;\n\n&lt;p&gt;Just curious if I&amp;#39;m going about this all wrong. Is this even realistic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7mu6e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tactical_Chicken",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753308541,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone brace up for qwen !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 121,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nxh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 255,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 255,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vP7s1FThQpvmySmVJXMTU3-8PcS1dzgy5zKouaE_2IM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mn8auem2bhef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mn8auem2bhef1.png?auto=webp&amp;s=f8d9250eb919b06b9873df5541dfb4181c23ecb3",
                  "width": 1080,
                  "height": 938
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f18ccc22bd1429048af2d71903a4986f10f4370",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e9b0375cba8b59a1f2ff6a059540b35b2e80af5",
                    "width": 216,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd33b6c0d9557d99a79e31264f8c7962a467e6de",
                    "width": 320,
                    "height": 277
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=855c907a55cf3f70afe582932d52350878ef5e68",
                    "width": 640,
                    "height": 555
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=727a28b13a2a4b6feeeb2646b4c5ef5d4feba605",
                    "width": 960,
                    "height": 833
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cdd53e8429b6be0b40bf16e28d255d818f7b04a",
                    "width": 1080,
                    "height": 938
                  }
                ],
                "variants": {},
                "id": "rMAWLMOw9tEiFwd35Iv66C0AmNRfGhg4PeoHVtVBYI4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6nxh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/",
          "stickied": false,
          "url": "https://i.redd.it/mn8auem2bhef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753213236,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct](https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct)\n\n hyperolic already has it\n\n",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mlbk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 252,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 252,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753210248,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct\"&gt;https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;hyperolic already has it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mlbk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753210248,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new to LLMs and I'm trying to understand a few things.\n\nIsn't RAG similar to a search engine? looks at keywords typed by user then feeds it to LLM to \"understand\" it an generate a nice response back? \n\nLet's say instead of RAG I'm using something like ElasticSearch/Meillsearch - would the results be that different? Does RAG handle synonyms as well? \n\nIdeally each chunk added into ChromaDb should be a full \"logic unit\" meaning it should make sense by itself (not a cutoff sentence with no start and end. Ex: Steven is ...). No?\n\nWhat about text with references to other pages, articles etc. How to handle them? ",
          "author_fullname": "t2_7bnnpzic",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemma3/other, Langchain, ChromaDb, RAG - a few questions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7kfet",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753302872,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to LLMs and I&amp;#39;m trying to understand a few things.&lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t RAG similar to a search engine? looks at keywords typed by user then feeds it to LLM to &amp;quot;understand&amp;quot; it an generate a nice response back? &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say instead of RAG I&amp;#39;m using something like ElasticSearch/Meillsearch - would the results be that different? Does RAG handle synonyms as well? &lt;/p&gt;\n\n&lt;p&gt;Ideally each chunk added into ChromaDb should be a full &amp;quot;logic unit&amp;quot; meaning it should make sense by itself (not a cutoff sentence with no start and end. Ex: Steven is ...). No?&lt;/p&gt;\n\n&lt;p&gt;What about text with references to other pages, articles etc. How to handle them? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7kfet",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "viitorfermier",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7kfet/gemma3other_langchain_chromadb_rag_a_few_questions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7kfet/gemma3other_langchain_chromadb_rag_a_few_questions/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753302872,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n**Reason**\nSo I was walking around my room thinking about my current laptop lenovo yoga slim 7\nand then started thinking about other laptops,\nnamely..\n\n\n\n\n\n**Question 1**\n\nMacbook Air/Pro.\nhow are the apple products when used for local training? \nmore specifically how are the last 3 generations of Macbook Pros when running locally?\n\n\n\n**Question 2**\n\nare there any cloud providers that are ‚Äòprivate‚Äô atleast well encrypted and secure? and don‚Äôt sell themselves to a government, if no, that‚Äôs unfortunate and someone should build that :).\nand..\n\n\n\n**Question 3**\n\nwhat are the most efficient (cost, storage, gpu, cpu, connection speed, etc) machines to build a private server that can train models and store images from 10+ devices onto a private storage  server.\n\n\n\n\nThank you if you‚Äôve read this far, \nand even more thank you to the people that can answer and do :)\n\n\n\n\n\n\n\n",
          "author_fullname": "t2_4iu4e2ma",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ML on Macbook",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m7pn05",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753315817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Reason&lt;/strong&gt;\nSo I was walking around my room thinking about my current laptop lenovo yoga slim 7\nand then started thinking about other laptops,\nnamely..&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 1&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Macbook Air/Pro.\nhow are the apple products when used for local training? \nmore specifically how are the last 3 generations of Macbook Pros when running locally?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 2&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;are there any cloud providers that are ‚Äòprivate‚Äô atleast well encrypted and secure? and don‚Äôt sell themselves to a government, if no, that‚Äôs unfortunate and someone should build that :).\nand..&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 3&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;what are the most efficient (cost, storage, gpu, cpu, connection speed, etc) machines to build a private server that can train models and store images from 10+ devices onto a private storage  server.&lt;/p&gt;\n\n&lt;p&gt;Thank you if you‚Äôve read this far, \nand even more thank you to the people that can answer and do :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7pn05",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CaslerTheTesticle",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7pn05/ml_on_macbook/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7pn05/ml_on_macbook/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753315817,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't know if it matters, but I followed this to install (because Nvidia drivers on Linux is a pain!): https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md\n\nSo I would like to type in a query into a model with some preset system prompt. I would like that model to run over this query multiple times. Then after all of them are done, I would like for the responses to be gathered for a summary. Would such task be possible?\n\nEDIT: I'm trying to benchmark variation biases for research. The prompt could be any scenario, but if I were to make an example, let's say it's a scenario where I meet with a random stranger. The stranger should have 50/50 chance of being a gentleman/lady as the model's output, but I'm trying to gauge what would happen if I simulate this scenario 100 times for a bias towards one sex.",
          "author_fullname": "t2_4hrx8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama + Open WebUI -- is there a way for the same query to run through the same model multiple times (could be 3 times, could be 100 times), then gather all the answers together to summarise/count?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m7pi3t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753316394,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753315438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if it matters, but I followed this to install (because Nvidia drivers on Linux is a pain!): &lt;a href=\"https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md\"&gt;https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So I would like to type in a query into a model with some preset system prompt. I would like that model to run over this query multiple times. Then after all of them are done, I would like for the responses to be gathered for a summary. Would such task be possible?&lt;/p&gt;\n\n&lt;p&gt;EDIT: I&amp;#39;m trying to benchmark variation biases for research. The prompt could be any scenario, but if I were to make an example, let&amp;#39;s say it&amp;#39;s a scenario where I meet with a random stranger. The stranger should have 50/50 chance of being a gentleman/lady as the model&amp;#39;s output, but I&amp;#39;m trying to gauge what would happen if I simulate this scenario 100 times for a bias towards one sex.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?auto=webp&amp;s=194fc1da74b1f56e6bca7cecb75e5a68c11008c1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1a05a3ead9734d6cb7b7045fdd787ff15a290e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=867ae8b82b8f457ac666d89cfaf3611953cc358e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbf750440a0a81a0c33ea061fa002223db7b35d7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e492becae66517bde05cbff2d3abe83139c4065f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a08a1c535e0e82ec2dc485d89bdfe54012f28a75",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a17c83b48123663530d34879b1da1dc4ccf3d160",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7pi3t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jinnyjuice",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753315438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/k2fyyrlhmoef1.png?width=1046&amp;format=png&amp;auto=webp&amp;s=8495ebda17093971eeb782d2328b4b674cf36614\n\nQwen3 coder is wild! This is really exciting... Until it's not...",
          "author_fullname": "t2_14cl94t8ha",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This is what I call crazy.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 85,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "k2fyyrlhmoef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 66,
                  "x": 108,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33cf164b8b60f12fa7485a1ba544977c9451101c"
                },
                {
                  "y": 132,
                  "x": 216,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=af9cb1dd42116e10c90be1bd4603c5df752f7782"
                },
                {
                  "y": 196,
                  "x": 320,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=51a5515b69264beb8e121905d1d4af3c615851cf"
                },
                {
                  "y": 392,
                  "x": 640,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cca63d9174615a1c70f2de623c8cde8a091a4763"
                },
                {
                  "y": 589,
                  "x": 960,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=432500061af2625f064397e06e674d15787fa123"
                }
              ],
              "s": {
                "y": 642,
                "x": 1046,
                "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=1046&amp;format=png&amp;auto=webp&amp;s=8495ebda17093971eeb782d2328b4b674cf36614"
              },
              "id": "k2fyyrlhmoef1"
            }
          },
          "name": "t3_1m7jzjg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uIc4_Hbf1DaCIfy5Ebnb36BLFB8aNlsgfkeUST0YC2U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753301876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/k2fyyrlhmoef1.png?width=1046&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8495ebda17093971eeb782d2328b4b674cf36614\"&gt;https://preview.redd.it/k2fyyrlhmoef1.png?width=1046&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8495ebda17093971eeb782d2328b4b674cf36614&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen3 coder is wild! This is really exciting... Until it&amp;#39;s not...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7jzjg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GenLabsAI",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7jzjg/this_is_what_i_call_crazy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7jzjg/this_is_what_i_call_crazy/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753301876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qc8c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#ab96c2",
          "ups": 138,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 138,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218772,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qc8c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 503516,
          "created_utc": 1753218772,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm new to desktops. I‚Äôve only ever had laptops. Would this be a good setup for local inference. The GPU has 32GB vram and over 1TB memory bandwidth. \n\nOther comments have lead me to believe that the motherboard and CPU matter as well but I am u sure why. Any help yall can provide would be great",
          "author_fullname": "t2_3zr7ymrr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Alienware Area-51 Gaming Desktop. Thoughts for local inference and fine tuning small models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ycez4n1ihpef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57fd9929c424834e8ea550b414e69e9af726776b"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f2e3ab0d06669398a41106f1ea28b2d2b485707"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2cd88ac6b9bdcc265d8e22d184a729b5818fbfd"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6260966c368a2592ba7e5f0ed9ca47f838bad942"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a6d6bf69e7e4d36db1d22e531fb1267e901993f"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9ff235719dbc3addd13686eda2cde7954e4f10d"
                }
              ],
              "s": {
                "y": 2532,
                "x": 1170,
                "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=040bcf5645052af01d2898e387ddc725d453f57d"
              },
              "id": "ycez4n1ihpef1"
            },
            "rkfafn1ihpef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8940be43482eece9478af1c42a985068508fd14a"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=443f2bce91203ea8d25427bbdd4061d27ef74bce"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a3d3c5c6e5d0436e05acba134be157c5ed630c2"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5493e75b4f4393553a07e94d9fe51473e6c030b6"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6201d82f8f64127107022ffe74d0aac35a20cd5f"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9c72cae173b6f389f72fb91f1269464294b6ff2"
                }
              ],
              "s": {
                "y": 2532,
                "x": 1170,
                "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=05870616bd620829c325d91282fa6a7335b9fb36"
              },
              "id": "rkfafn1ihpef1"
            }
          },
          "name": "t3_1m7obdf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "rkfafn1ihpef1",
                "id": 712613354
              },
              {
                "media_id": "ycez4n1ihpef1",
                "id": 712613355
              }
            ]
          },
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vL2QSaqrpFO-8zyuzQNoTeR0eufdBS0bMD8nGEmriYQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753312251,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm new to desktops. I‚Äôve only ever had laptops. Would this be a good setup for local inference. The GPU has 32GB vram and over 1TB memory bandwidth. &lt;/p&gt;\n\n&lt;p&gt;Other comments have lead me to believe that the motherboard and CPU matter as well but I am u sure why. Any help yall can provide would be great&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m7obdf",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7obdf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "skinnyjoints",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7obdf/alienware_area51_gaming_desktop_thoughts_for/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m7obdf",
          "subreddit_subscribers": 503516,
          "created_utc": 1753312251,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been looking for the best model to write long-form NSFW erotic stories and while the journey has been fun and I've learned a lot, I'm still very confused.\n\nAt first I thought only models with \"abliterated\" in their name could do uncensored, but then I found other models recommended with \"Hell California\", some models don't make any mention of NSFW or uncensored in their name but they were highly recommended in some threads asking for erotic story writing.\n\nSo is there some foolproof way to tell if a model is uncensored or not? Or am I left guessing and having to search through the description of every model I come across to make a determination? Or just try it out? There are sooo many models on Hugging Face it's incredibly confusing.",
          "author_fullname": "t2_46fr9zjv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can you tell if a model is uncensored and can write NSFW material?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7o21h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753311584,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking for the best model to write long-form NSFW erotic stories and while the journey has been fun and I&amp;#39;ve learned a lot, I&amp;#39;m still very confused.&lt;/p&gt;\n\n&lt;p&gt;At first I thought only models with &amp;quot;abliterated&amp;quot; in their name could do uncensored, but then I found other models recommended with &amp;quot;Hell California&amp;quot;, some models don&amp;#39;t make any mention of NSFW or uncensored in their name but they were highly recommended in some threads asking for erotic story writing.&lt;/p&gt;\n\n&lt;p&gt;So is there some foolproof way to tell if a model is uncensored or not? Or am I left guessing and having to search through the description of every model I come across to make a determination? Or just try it out? There are sooo many models on Hugging Face it&amp;#39;s incredibly confusing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7o21h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wtfislandfill",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7o21h/how_can_you_tell_if_a_model_is_uncensored_and_can/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7o21h/how_can_you_tell_if_a_model_is_uncensored_and_can/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753311584,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "And more importantly, exactly how many common params are active per token? \n\nI mean an exact number like \"1029190869528\" (not sure if correct), not \"1 trillion\". Some of the info is hard to find.\n\n- How many exact params for each of the 61 layers? I notice layers 59 and 60 are a different size than from before layer 58.  \n- Model hidden size (dimension): 7168\n- How many exact params are there per each of the 384 experts? Is that number the same for each expert? (And how many experts total per token? 9?) \n- How many exact params are for attention each layer? Is it 206158336 for all MoE and non MoE layers? And how many params are for FFN? \n\n\nI am trying to find the number of active params per expert, and the number of common params (always active). The sum of latter number and 8x the former number should equal approximately 32bil for Kimi K2. I haven't checked for Qwen 3 Coder 480B yet.",
          "author_fullname": "t2_t6glzswk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How big is Kimi K2 exactly? How big is Qwen 3 Coder 480B exactly?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7nqvz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753310790,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;And more importantly, exactly how many common params are active per token? &lt;/p&gt;\n\n&lt;p&gt;I mean an exact number like &amp;quot;1029190869528&amp;quot; (not sure if correct), not &amp;quot;1 trillion&amp;quot;. Some of the info is hard to find.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How many exact params for each of the 61 layers? I notice layers 59 and 60 are a different size than from before layer 58.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Model hidden size (dimension): 7168&lt;/li&gt;\n&lt;li&gt;How many exact params are there per each of the 384 experts? Is that number the same for each expert? (And how many experts total per token? 9?) &lt;/li&gt;\n&lt;li&gt;How many exact params are for attention each layer? Is it 206158336 for all MoE and non MoE layers? And how many params are for FFN? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am trying to find the number of active params per expert, and the number of common params (always active). The sum of latter number and 8x the former number should equal approximately 32bil for Kimi K2. I haven&amp;#39;t checked for Qwen 3 Coder 480B yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7nqvz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DepthHour1669",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7nqvz/how_big_is_kimi_k2_exactly_how_big_is_qwen_3/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7nqvz/how_big_is_kimi_k2_exactly_how_big_is_qwen_3/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753310790,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been reading papers where the main contribution is creating a synthetic dataset for a specific task, followed by fine-tuning an LLM on it. One thing I keep noticing: most of them don't seem to perform hyperparameter tuning (e.g., learning rate, epochs, weight decay) using a validation set. Instead, they just reuse common/default values.\n\nI'm wondering‚Äîwhy is this so common?\n\n* Is it because hyperparameter tuning is considered less important, so they did search but skipped reporting it?\n* Or is it because the main contribution is in data creation, so they just don't care much about the fine-tuning details?",
          "author_fullname": "t2_5z9ud297u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why do many papers skip hyperparameter search?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7503r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753264251,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been reading papers where the main contribution is creating a synthetic dataset for a specific task, followed by fine-tuning an LLM on it. One thing I keep noticing: most of them don&amp;#39;t seem to perform hyperparameter tuning (e.g., learning rate, epochs, weight decay) using a validation set. Instead, they just reuse common/default values.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering‚Äîwhy is this so common?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is it because hyperparameter tuning is considered less important, so they did search but skipped reporting it?&lt;/li&gt;\n&lt;li&gt;Or is it because the main contribution is in data creation, so they just don&amp;#39;t care much about the fine-tuning details?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7503r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hwanchang",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753264251,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "See https://x.com/makingAGI/status/1947286324735856747",
          "author_fullname": "t2_1nt1n3y6xj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone here who has been able to reproduce their results yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6orbr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 125,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 125,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XcNs-lUbqrAcvyj8WfRxfyYgGorJ8nCrbsxZweyByLc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753215098,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;See &lt;a href=\"https://x.com/makingAGI/status/1947286324735856747\"&gt;https://x.com/makingAGI/status/1947286324735856747&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/cfffg12fghef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/cfffg12fghef1.jpeg?auto=webp&amp;s=25f023da9eda3ae6d327e173ef9c7cba8f89880c",
                  "width": 948,
                  "height": 876
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad8d41aa9654515fde6f4b396a86ebf1ad4b0687",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d65ac6156085250ea9aa09ddb48e1e0ca0d499b3",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69d24ed7a665a31761d01a39290d42a299442408",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f02acda8fde9368279ce55c247aa3eb87536a6a5",
                    "width": 640,
                    "height": 591
                  }
                ],
                "variants": {},
                "id": "eXDcYXmDs3JXlXLxg7VAJanyEvhS_GKAlZPpe8O1v6Y"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6orbr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Original_Log_9899",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/",
          "stickied": false,
          "url": "https://i.redd.it/cfffg12fghef1.jpeg",
          "subreddit_subscribers": 503516,
          "created_utc": 1753215098,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So my sister's girlfriend is a CS major (masters), and lately she‚Äôs been deep into building this SDK that helps developers work with *multiple AI agents* more easily, like local LLMs or narrow models that need to talk to each other.\n\nshe‚Äôs not trying to make another langchain/crewai clone. this is more like a **lightweight sdk, open source and downloaded right on vs code,** not a whole platform.\n\n* **local-first**, works offline\n* agents can **share memory**, handle **fallbacks**, and not step on each other\n* built for devs, not for enterprises\n\nshe‚Äôs still in early build mode, but trying to figure out if this is even useful enough to land her a job.\n\nso here‚Äôs the ask:\n\n* would you *actually* use something like this?\n* what‚Äôs the most annoying part of building multi-agent systems right now?\n* what would *make or break* this kind of tool for you?\n\nIf anyone here‚Äôs building with agents, would love to hear what you‚Äôd want from a setup like this. If you guys think this is a trash project idea please roast, be brutally honest and dont sugarcoat anything üôè",
          "author_fullname": "t2_cmo0i3e2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "would this make an ai dev's life easier?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7mwog",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753308711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my sister&amp;#39;s girlfriend is a CS major (masters), and lately she‚Äôs been deep into building this SDK that helps developers work with &lt;em&gt;multiple AI agents&lt;/em&gt; more easily, like local LLMs or narrow models that need to talk to each other.&lt;/p&gt;\n\n&lt;p&gt;she‚Äôs not trying to make another langchain/crewai clone. this is more like a &lt;strong&gt;lightweight sdk, open source and downloaded right on vs code,&lt;/strong&gt; not a whole platform.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;local-first&lt;/strong&gt;, works offline&lt;/li&gt;\n&lt;li&gt;agents can &lt;strong&gt;share memory&lt;/strong&gt;, handle &lt;strong&gt;fallbacks&lt;/strong&gt;, and not step on each other&lt;/li&gt;\n&lt;li&gt;built for devs, not for enterprises&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;she‚Äôs still in early build mode, but trying to figure out if this is even useful enough to land her a job.&lt;/p&gt;\n\n&lt;p&gt;so here‚Äôs the ask:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;would you &lt;em&gt;actually&lt;/em&gt; use something like this?&lt;/li&gt;\n&lt;li&gt;what‚Äôs the most annoying part of building multi-agent systems right now?&lt;/li&gt;\n&lt;li&gt;what would &lt;em&gt;make or break&lt;/em&gt; this kind of tool for you?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If anyone here‚Äôs building with agents, would love to hear what you‚Äôd want from a setup like this. If you guys think this is a trash project idea please roast, be brutally honest and dont sugarcoat anything üôè&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7mwog",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Soggy-Guava-1218",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7mwog/would_this_make_an_ai_devs_life_easier/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7mwog/would_this_make_an_ai_devs_life_easier/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753308711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks ‚Äì I‚Äôve been exploring local LLMs more seriously and found the best way to get deeper is by teaching and helping others. I‚Äôve built a couple local setups and work in the AI team at one of the big four consulting firms. I‚Äôve also got \\~7 years in AI/ML, and have helped some of the biggest companies build end-to-end AI systems.\n\nIf you're working on something cool - especially business/ops/enterprise-facing‚ÄîI‚Äôd love to hear about it. I‚Äôm less focused on quirky personal assistants and more on use cases that might scale or create value in a company.\n\nFeel free to DM me your use case or idea ‚Äì happy to brainstorm, advise, or even get hands-on.",
          "author_fullname": "t2_hjqo50xu2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'll help build your local LLM for free",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7m9t8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753307198,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks ‚Äì I‚Äôve been exploring local LLMs more seriously and found the best way to get deeper is by teaching and helping others. I‚Äôve built a couple local setups and work in the AI team at one of the big four consulting firms. I‚Äôve also got ~7 years in AI/ML, and have helped some of the biggest companies build end-to-end AI systems.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re working on something cool - especially business/ops/enterprise-facing‚ÄîI‚Äôd love to hear about it. I‚Äôm less focused on quirky personal assistants and more on use cases that might scale or create value in a company.&lt;/p&gt;\n\n&lt;p&gt;Feel free to DM me your use case or idea ‚Äì happy to brainstorm, advise, or even get hands-on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7m9t8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "decentralizedbee",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7m9t8/ill_help_build_your_local_llm_for_free/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7m9t8/ill_help_build_your_local_llm_for_free/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753307198,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "It's here guys and qwen nailed it !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4aoalqp6thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebd63c731638a1db5036229c80b9ef7c6e9824fd"
                },
                {
                  "y": 123,
                  "x": 216,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4ac724a3670cd9c86d3d9eed68ced783891cd55"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27543fd2099856fe749e4022dd8e44bdf2a203ec"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c67265c054dccbb5299415ace1ae413e53f4ba40"
                },
                {
                  "y": 550,
                  "x": 960,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9060138fd041f795e2cf4d5c898b3e767230dbfb"
                },
                {
                  "y": 619,
                  "x": 1080,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0aea2f77c1907ccf952d971c3b5b2331c1f6aba9"
                }
              ],
              "s": {
                "y": 1837,
                "x": 3202,
                "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=3202&amp;format=pjpg&amp;auto=webp&amp;s=8e55f7d4fbd65d488aa6606016ce609320a82186"
              },
              "id": "4aoalqp6thef1"
            },
            "mloztw07thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1da39fa93c261ee74d697606209312d155d2610"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08ecfdf3e13689eb562525b1962e0901e009181c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=873f78f3d4bc2c3fc9641a129d06524b3f3f4951"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=468a855136721f0ffae9fb7beacce6df6030447b"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b695450503944ff02456a685ccc317281f8bf8f7"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0ed9b87aa6525e3c700e7835ab55f9fe859d7aa"
                }
              ],
              "s": {
                "y": 1715,
                "x": 3306,
                "u": "https://preview.redd.it/mloztw07thef1.jpg?width=3306&amp;format=pjpg&amp;auto=webp&amp;s=a4886e1284e4cebe0e8558f5cf664cfc4c36b481"
              },
              "id": "mloztw07thef1"
            }
          },
          "name": "t3_1m6qkse",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 88,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "4aoalqp6thef1",
                "id": 711818629
              },
              {
                "caption": "",
                "media_id": "mloztw07thef1",
                "id": 711818630
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 88,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tng4SvC83rVHk9iUXovrs4GeXZmRkFJ59wlPU2wB1GM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m6qkse",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qkse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m6qkse",
          "subreddit_subscribers": 503516,
          "created_utc": 1753219329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\n\nThis model showed up on my LinkedIn feed today. After listening to a few examples on their [website](https://www.boson.ai/technologies/voice), I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts.¬†\n\nListen to this [demo video](https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d), it will just enable so many use cases.\n\nI tried a few examples in their HF [playground](https://huggingface.co/spaces/smola/higgs_audio_v2), it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven‚Äôt tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models.¬†\n\n",
          "author_fullname": "t2_6nwb1mbe6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 45,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rmmpgv36tief1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 35,
                  "x": 108,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b8a7927fef36a8dc9cafddd20ca7395324bb30"
                },
                {
                  "y": 70,
                  "x": 216,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa3bc0976c73f20478fe4c41ae0d81d56d9b5efa"
                },
                {
                  "y": 103,
                  "x": 320,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c779645fab14bc4ce1ecf7b4cca7ce06002977dd"
                },
                {
                  "y": 207,
                  "x": 640,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d2890cda589e72ebb380d268d25b2f0c730e4153"
                },
                {
                  "y": 311,
                  "x": 960,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b671678ce82ca5a2abdd86ced5de4262f068a656"
                },
                {
                  "y": 350,
                  "x": 1080,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea15a827df9bb20ae059bea42e6b92ad1d59800b"
                }
              ],
              "s": {
                "y": 872,
                "x": 2686,
                "u": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4"
              },
              "id": "rmmpgv36tief1"
            }
          },
          "name": "t3_1m6vbds",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lDeKkUsKVKJvujnGWUqXtUhpkbsWufoj2laEkKgzAUI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753231503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\"&gt;https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This model showed up on my LinkedIn feed today. After listening to a few examples on their &lt;a href=\"https://www.boson.ai/technologies/voice\"&gt;website&lt;/a&gt;, I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts.¬†&lt;/p&gt;\n\n&lt;p&gt;Listen to this &lt;a href=\"https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d\"&gt;demo video&lt;/a&gt;, it will just enable so many use cases.&lt;/p&gt;\n\n&lt;p&gt;I tried a few examples in their HF &lt;a href=\"https://huggingface.co/spaces/smola/higgs_audio_v2\"&gt;playground&lt;/a&gt;, it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven‚Äôt tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models.¬†&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6vbds",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Tap3484",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753231503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6rsym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 68,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 68,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6acd0a4d30c117c56e597d84c1ebb5cedb6e4e00",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753222281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/QwenLM/qwen-code",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?auto=webp&amp;s=ea430b9854a08b70e3dd0972ad9e4758c7fc266d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb193a50d7978c33be16ebec135a318dc6943ea1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6992a1a70171bd4f98508b22498e5ac88cdc45df",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5465c1e679bafd45447bd81f6753867f296ffb49",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=21b1ec40f95d195f9c34bb5728616a2b4c3162fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a9eb8de1aed9882d7841738230f2ecef892f334f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a21f51e6f25035a3147bda1057127718b3b29129",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6rsym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/",
          "stickied": false,
          "url": "https://github.com/QwenLM/qwen-code",
          "subreddit_subscribers": 503516,
          "created_utc": 1753222281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nI just published research on \"thought anchors\" - a method to analyze which specific reasoning steps matter most for task success in locally-runnable models. Thought this community would find the results interesting since it directly compares two popular local models.\n\n**TL;DR: Qwen3-0.6B and DeepSeek-R1-Distill-1.5B have fundamentally different reasoning architectures, not just different performance levels.**\n\n# What are Thought Anchors?\n\nBuilding on work by Bogdan et al., thought anchors identify critical sentences in a model's chain-of-thought reasoning that significantly impact whether it gets the right answer. Instead of looking at individual tokens, we analyze complete reasoning steps.\n\n# Key Findings on GSM8K Math Problems:\n\n**DeepSeek-R1-Distill (1.5B):**\n\n* Concentrated reasoning: fewer steps, higher impact per step (0.408 avg)\n* 82.7% positive reasoning steps - very consistent\n* Single primary failure mode (logical errors)\n* Optimized for reliability over exploration\n\n**Qwen3 (0.6B):**\n\n* Distributed reasoning: more steps, spread impact (0.278 avg)\n* 71.6% positive steps but higher variance\n* Multiple failure modes (logical, computational, missing steps)\n* More experimental approach with higher risk/reward\n\n# Practical Implications for Local Users:\n\nIf you're choosing between these models:\n\n* **Need consistent, reliable outputs?** ‚Üí DeepSeek-R1's concentrated approach\n* **Want more creative/exploratory reasoning?** ‚Üí Qwen3's distributed approach\n* **Resource constraints?** ‚Üí Qwen3 at 0.6B vs DeepSeek at 1.5B\n\nThis isn't about one being \"better\" - they're optimized for different reasoning strategies.\n\n# Open Source Everything:\n\n* **PTS Library**: [https://github.com/codelion/pts](https://github.com/codelion/pts) (tool for generating thought anchors)\n* **Datasets**: Available on HuggingFace for both models\n* **Analysis Code**: Full reproducibility\n* **Article**: [https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors](https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors)\n\nThe PTS library works with any local model that supports structured output, so you can analyze your own models' reasoning patterns.\n\n# Questions for the Community:\n\n1. Has anyone noticed similar reasoning pattern differences in their local setups?\n2. Which reasoning approach works better for your specific use cases?\n3. Any interest in extending this analysis to other popular local models (Llama, Mistral, etc.)?\n\nWould love to hear your experiences and thoughts on model reasoning approaches!\n\n**Edit**: Original thought anchors concept credit goes to Paul Bogdan's team - this research extends their methodology to compare local model architectures.",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Research] Thought Anchors: Understanding How Qwen3-0.6B vs DeepSeek-R1-Distill-1.5B Actually Reason - Different Cognitive Architectures Revealed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zce0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753243256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I just published research on &amp;quot;thought anchors&amp;quot; - a method to analyze which specific reasoning steps matter most for task success in locally-runnable models. Thought this community would find the results interesting since it directly compares two popular local models.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: Qwen3-0.6B and DeepSeek-R1-Distill-1.5B have fundamentally different reasoning architectures, not just different performance levels.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;What are Thought Anchors?&lt;/h1&gt;\n\n&lt;p&gt;Building on work by Bogdan et al., thought anchors identify critical sentences in a model&amp;#39;s chain-of-thought reasoning that significantly impact whether it gets the right answer. Instead of looking at individual tokens, we analyze complete reasoning steps.&lt;/p&gt;\n\n&lt;h1&gt;Key Findings on GSM8K Math Problems:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;DeepSeek-R1-Distill (1.5B):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Concentrated reasoning: fewer steps, higher impact per step (0.408 avg)&lt;/li&gt;\n&lt;li&gt;82.7% positive reasoning steps - very consistent&lt;/li&gt;\n&lt;li&gt;Single primary failure mode (logical errors)&lt;/li&gt;\n&lt;li&gt;Optimized for reliability over exploration&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 (0.6B):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Distributed reasoning: more steps, spread impact (0.278 avg)&lt;/li&gt;\n&lt;li&gt;71.6% positive steps but higher variance&lt;/li&gt;\n&lt;li&gt;Multiple failure modes (logical, computational, missing steps)&lt;/li&gt;\n&lt;li&gt;More experimental approach with higher risk/reward&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Practical Implications for Local Users:&lt;/h1&gt;\n\n&lt;p&gt;If you&amp;#39;re choosing between these models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Need consistent, reliable outputs?&lt;/strong&gt; ‚Üí DeepSeek-R1&amp;#39;s concentrated approach&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Want more creative/exploratory reasoning?&lt;/strong&gt; ‚Üí Qwen3&amp;#39;s distributed approach&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Resource constraints?&lt;/strong&gt; ‚Üí Qwen3 at 0.6B vs DeepSeek at 1.5B&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This isn&amp;#39;t about one being &amp;quot;better&amp;quot; - they&amp;#39;re optimized for different reasoning strategies.&lt;/p&gt;\n\n&lt;h1&gt;Open Source Everything:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;PTS Library&lt;/strong&gt;: &lt;a href=\"https://github.com/codelion/pts\"&gt;https://github.com/codelion/pts&lt;/a&gt; (tool for generating thought anchors)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Datasets&lt;/strong&gt;: Available on HuggingFace for both models&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Analysis Code&lt;/strong&gt;: Full reproducibility&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Article&lt;/strong&gt;: &lt;a href=\"https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors\"&gt;https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The PTS library works with any local model that supports structured output, so you can analyze your own models&amp;#39; reasoning patterns.&lt;/p&gt;\n\n&lt;h1&gt;Questions for the Community:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Has anyone noticed similar reasoning pattern differences in their local setups?&lt;/li&gt;\n&lt;li&gt;Which reasoning approach works better for your specific use cases?&lt;/li&gt;\n&lt;li&gt;Any interest in extending this analysis to other popular local models (Llama, Mistral, etc.)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would love to hear your experiences and thoughts on model reasoning approaches!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Original thought anchors concept credit goes to Paul Bogdan&amp;#39;s team - this research extends their methodology to compare local model architectures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?auto=webp&amp;s=187504b6a6cdaab3b5025c91a3798e0b46bcb9f0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=76a98416b90c3288a04cac47b99811464ff316e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9389c4b2ca46a4b05928b5941369ea699ccec4e6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5964cc2bf092a1202a804fdcec163a7e14497c35",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a192ca72cfe990c9ccd3456b264cd2914962c19",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67133c9db0890c820ce5cdc0549dcaeb9f9e95de",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=978ef4cf2867cf2f582d6936f382955179b16eba",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6zce0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6zce0/research_thought_anchors_understanding_how/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zce0/research_thought_anchors_understanding_how/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753243256,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A week ago I shared an early prototype and got amazing feedback. Main request? \"Show us how to actually install this properly.\"\n\n**The problem:** Every time you restart Claude Code CLI, you lose everything.\n\n**What I built:** RagCore - universal RAG system with persistent memory via MCP stdio. Claude remembers your project context and queries any documentation you add.\n\n**The magic moment:** Close terminal ‚Üí Restart Claude Code CLI ‚Üí Continue exactly where you left off.\n\n**How it works:**\n\n* Tell Claude \"learn about current project\" ‚Üí automatic memory bank query\n* Ask \"implement Laravel validation\" ‚Üí Claude queries RAG server with local LLM\n* RAG server logs show exact sources (zero hallucinations)\n* Smart token optimization by query complexity\n\n**Results after week of testing:**\n\n* 4,306 Laravel docs indexed, 7-20 second response times\n* Works with Python, FastAPI, custom frameworks\n* Local LLM (your code never leaves your machine)\n\n**GitHub:** [https://github.com/lexa5575/RagCore](https://github.com/lexa5575/RagCore)\n\nInstallation details in comments. What documentation would you want to add?",
          "author_fullname": "t2_icqqkg97",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a Universal RAG + Memory System for Claude with MCP - Production Ready",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7kz8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753304162,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A week ago I shared an early prototype and got amazing feedback. Main request? &amp;quot;Show us how to actually install this properly.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Every time you restart Claude Code CLI, you lose everything.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt; RagCore - universal RAG system with persistent memory via MCP stdio. Claude remembers your project context and queries any documentation you add.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The magic moment:&lt;/strong&gt; Close terminal ‚Üí Restart Claude Code CLI ‚Üí Continue exactly where you left off.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tell Claude &amp;quot;learn about current project&amp;quot; ‚Üí automatic memory bank query&lt;/li&gt;\n&lt;li&gt;Ask &amp;quot;implement Laravel validation&amp;quot; ‚Üí Claude queries RAG server with local LLM&lt;/li&gt;\n&lt;li&gt;RAG server logs show exact sources (zero hallucinations)&lt;/li&gt;\n&lt;li&gt;Smart token optimization by query complexity&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Results after week of testing:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;4,306 Laravel docs indexed, 7-20 second response times&lt;/li&gt;\n&lt;li&gt;Works with Python, FastAPI, custom frameworks&lt;/li&gt;\n&lt;li&gt;Local LLM (your code never leaves your machine)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/lexa5575/RagCore\"&gt;https://github.com/lexa5575/RagCore&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Installation details in comments. What documentation would you want to add?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?auto=webp&amp;s=1e15473524aaf1ec7d69116219461863da0dd38d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1accd17d757c1d6bca607d221f043a9a301e1f59",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bff91e902da988eb8a710343ac8e1f78f17d52ce",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=87b1f1b65c28c7dbc7ec568129fc6ebcf7fe4913",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0768ba20613927ad7eac15c74dc14fae53bbdb8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1853a6bc8df8126651364075710d82c19403c566",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8752469409716bc996a820e024b1e7efc87cef8a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7kz8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Basic_Soft9158",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7kz8s/built_a_universal_rag_memory_system_for_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7kz8s/built_a_universal_rag_memory_system_for_claude/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753304162,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I‚Äôve been diving into alternative architectures to transformers recently, and I came across a few interesting ones. liquid foundation models (lfm), Mamba (ssm based) and RWKV. I‚Äôm curious about what these new architectures offer and what their limitations are. From what I understand, they all seem to be better at handling long sequences, SSMs and LFMs are more resource efficient and LFMs seem to struggle with wide area applications (?)\nI‚Äôm still trying to fully grasp how these models compare to transformers, so I‚Äôd love to hear more about the strengths and weaknesses of these newer architectures. Any insights would be appreciated!",
          "author_fullname": "t2_y1vyie97k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do new architectures offer and what are their limits?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m76df6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753269030,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I‚Äôve been diving into alternative architectures to transformers recently, and I came across a few interesting ones. liquid foundation models (lfm), Mamba (ssm based) and RWKV. I‚Äôm curious about what these new architectures offer and what their limitations are. From what I understand, they all seem to be better at handling long sequences, SSMs and LFMs are more resource efficient and LFMs seem to struggle with wide area applications (?)\nI‚Äôm still trying to fully grasp how these models compare to transformers, so I‚Äôd love to hear more about the strengths and weaknesses of these newer architectures. Any insights would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m76df6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ba2sYd",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753269030,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! Not sure is this the right place to ask but I‚Äôve been working on a Japanese voice assistant as a side project, and I‚Äôm currently struggling to find a good TTS solution. I tried using [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) from their webui, and the voice quality is very impressive, but it‚Äôs difficult to integrate it into my project since it doesn‚Äôt come as a proper Python package (I don't see any official PyPI support).\n\nRight now, the only way I can use it is by cloning their entire repo and calling [synthesize()](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/GPT_SoVITS/inference_cli.py) directly, that means I need to move my whole project into theirs.\n\nIs there a way to integrate GPT-SoVITS into the project? Or are there other high-quality Japanese TTS tools that works well without fine-tuning?",
          "author_fullname": "t2_9so7g7ch",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to acutally use gpt-sovits?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7bd41",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753282340,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! Not sure is this the right place to ask but I‚Äôve been working on a Japanese voice assistant as a side project, and I‚Äôm currently struggling to find a good TTS solution. I tried using &lt;a href=\"https://github.com/RVC-Boss/GPT-SoVITS\"&gt;GPT-SoVITS&lt;/a&gt; from their webui, and the voice quality is very impressive, but it‚Äôs difficult to integrate it into my project since it doesn‚Äôt come as a proper Python package (I don&amp;#39;t see any official PyPI support).&lt;/p&gt;\n\n&lt;p&gt;Right now, the only way I can use it is by cloning their entire repo and calling &lt;a href=\"https://github.com/RVC-Boss/GPT-SoVITS/blob/main/GPT_SoVITS/inference_cli.py\"&gt;synthesize()&lt;/a&gt; directly, that means I need to move my whole project into theirs.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to integrate GPT-SoVITS into the project? Or are there other high-quality Japanese TTS tools that works well without fine-tuning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?auto=webp&amp;s=9ada7abaae5c281710496c0b299f1d5be001c93f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8a46b62a80893173dc3ed635ca54310aa68bc664",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f5fc9721b66d01680dfb9b169f919340cf80348",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=552ea87808fa1a436acb99123c476c99ba165126",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c4c179a85149faefd5816021be7e674ac1eb054",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=549e70bf56a108832e591f3b0d99240c6369dab2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=138bceb99e73dee280ec27063fe798a1967c60d9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7bd41",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Icy-Ad6078",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7bd41/how_to_acutally_use_gptsovits/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7bd41/how_to_acutally_use_gptsovits/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753282340,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need the highest quality I can get for a price point below $1000 in training and $1/M tokens inference. I would prefer to do full finetuning on a base model. It's for a continuation task (writing with long range dependency) so I don't actually need or want chat or instruct style. I need context 32K.\n\nI have about 200M tokens of finetuning data which I can augment to 1B easily by doing different variations.\n\nMy opinions are:\n1. Finetune Gemini Flash 2.0. They're using a LoRA. It'll cost $800, but then I can infer for $0.30/M on batch.\n2. Finetune Qwen2.5 or Llama 3.3 either 70B or 32B. Might cost a bit more. Inference could be cheaper if I use 4bit quantization, otherwise probably a slightly more expensive, and a lot more difficult to maintain.\n\nBut ultimately in the end I care about the quality output. I don't really want to test both because of the time and money it would take to do so.\nWhich do you think would give the better output?\n\nI'm torn. It seems to me I'd be able to train it better if I train the full base model on 1B tokens. That would probably be a bit expensive to train.\nYet Gemini might just be a better model in the first place. It's hard to tell because Gemini Flash 2.0 is absolutely amazing at some things, stuff that none of the Open Source can do like editing a massive block of text and actually responsing with the entire thing every time instead of secretly deleting sentences here and there. Then some other stuff it doesn't do so well. So it *might* actually be a small model that's really really well trained (or 100 tiny experts), in which case a LoRA on that might not be able to keep my task up for 32K tokens.\n\nSince I'm only training one task (actually 2 but they're related) I don't need or want experts, or thinking.\n\nOn the other hand it's cheaper and easier to train Flash 2.0 by a lot.\n\nDoes anyone have any personal insight into my dilemma?",
          "author_fullname": "t2_8jhue7k0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Should I do finetuning on Gemini or on open source models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7e8d0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753288860,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need the highest quality I can get for a price point below $1000 in training and $1/M tokens inference. I would prefer to do full finetuning on a base model. It&amp;#39;s for a continuation task (writing with long range dependency) so I don&amp;#39;t actually need or want chat or instruct style. I need context 32K.&lt;/p&gt;\n\n&lt;p&gt;I have about 200M tokens of finetuning data which I can augment to 1B easily by doing different variations.&lt;/p&gt;\n\n&lt;p&gt;My opinions are:\n1. Finetune Gemini Flash 2.0. They&amp;#39;re using a LoRA. It&amp;#39;ll cost $800, but then I can infer for $0.30/M on batch.\n2. Finetune Qwen2.5 or Llama 3.3 either 70B or 32B. Might cost a bit more. Inference could be cheaper if I use 4bit quantization, otherwise probably a slightly more expensive, and a lot more difficult to maintain.&lt;/p&gt;\n\n&lt;p&gt;But ultimately in the end I care about the quality output. I don&amp;#39;t really want to test both because of the time and money it would take to do so.\nWhich do you think would give the better output?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m torn. It seems to me I&amp;#39;d be able to train it better if I train the full base model on 1B tokens. That would probably be a bit expensive to train.\nYet Gemini might just be a better model in the first place. It&amp;#39;s hard to tell because Gemini Flash 2.0 is absolutely amazing at some things, stuff that none of the Open Source can do like editing a massive block of text and actually responsing with the entire thing every time instead of secretly deleting sentences here and there. Then some other stuff it doesn&amp;#39;t do so well. So it &lt;em&gt;might&lt;/em&gt; actually be a small model that&amp;#39;s really really well trained (or 100 tiny experts), in which case a LoRA on that might not be able to keep my task up for 32K tokens.&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m only training one task (actually 2 but they&amp;#39;re related) I don&amp;#39;t need or want experts, or thinking.&lt;/p&gt;\n\n&lt;p&gt;On the other hand it&amp;#39;s cheaper and easier to train Flash 2.0 by a lot.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any personal insight into my dilemma?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7e8d0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pan000",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753288860,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_6vcmk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is imminent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6medy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 116,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 116,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mmNhnm_QiKDvZ8nOArY9M-gXEHPij6ccQfZ3Z4a4vrs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209818,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mruaiodv0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mruaiodv0hef1.png?auto=webp&amp;s=be215118da4d1c5ae5fc739c077ba4bbf8354f1a",
                  "width": 501,
                  "height": 251
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49a20e04a28093446580d2909236b45d1e2f568e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=772268126ecad9399aa5fb8ad3dc61fa7a8e5af0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=daa5e07dcd586edd4e8488215b2df66df2d2c809",
                    "width": 320,
                    "height": 160
                  }
                ],
                "variants": {},
                "id": "gxF1-bhuks7kobb2JTcsN29raeY4IvwO_eL--8kAZ38"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6medy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dudensen",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/",
          "stickied": false,
          "url": "https://i.redd.it/mruaiodv0hef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753209818,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, does anyone know of a photo/video program that can change the background so that my product photos look really good similar to a photo shoot. I took some basic photos and the software I was using created these which was great. The software is very very expensive though at a few hundred dollars per month and has bad reviews overall so I‚Äôm looking for an alternative. This was made in adcreative ai.\n\nI‚Äôm looking for something different. I can do photos that are similar caliber for either free or not as expensive.\n\nIn my photos above, you can see the photo that I took and that the background was eliminated and then changed to an AI background in a spa setting\n\nThanks!",
          "author_fullname": "t2_1rq2klnw0u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI background for products",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 119,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7jybm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/8PaHKM_x0pxx0WBhJ3PLIpt9NRDZtqSy-K9hH9Wqzt4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753301798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, does anyone know of a photo/video program that can change the background so that my product photos look really good similar to a photo shoot. I took some basic photos and the software I was using created these which was great. The software is very very expensive though at a few hundred dollars per month and has bad reviews overall so I‚Äôm looking for an alternative. This was made in adcreative ai.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm looking for something different. I can do photos that are similar caliber for either free or not as expensive.&lt;/p&gt;\n\n&lt;p&gt;In my photos above, you can see the photo that I took and that the background was eliminated and then changed to an AI background in a spa setting&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/a5qw3y2fmoef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?auto=webp&amp;s=678198fd66eca692afaa55f36f4a376f1818ddc9",
                  "width": 1179,
                  "height": 1006
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c78fcd8debf9c3724a29fe6d1080fd47a4edc7c",
                    "width": 108,
                    "height": 92
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2f3b90a63c2c43012ec6e6127f60ca8a4944779",
                    "width": 216,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb94fdee133d953d34636a7d1e0f8ccb217dc8e8",
                    "width": 320,
                    "height": 273
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dddedc14364a6da08b09557e6f430ad2efd989e3",
                    "width": 640,
                    "height": 546
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e29b7587acf415961f7dd1f331dad009a1c3812a",
                    "width": 960,
                    "height": 819
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb1610aff74f61af4c30419af4e798d2744a97d4",
                    "width": 1080,
                    "height": 921
                  }
                ],
                "variants": {},
                "id": "sY23F0e05kLWeMk1bFNxXzQic1fSxEMBFvxDPyWOUl4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7jybm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UGC_Chris_D",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7jybm/ai_background_for_products/",
          "stickied": false,
          "url": "https://i.redd.it/a5qw3y2fmoef1.jpeg",
          "subreddit_subscribers": 503516,
          "created_utc": 1753301798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! I‚Äôm currently using Notion, which works great for transcribing meetings and converting them into summaries, action items, and so on. \n\nIs anyone using open-source / locally powered AI tools? I‚Äôd love to hear about your experience with those.\n\nThanks!",
          "author_fullname": "t2_ajuxt3cr4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source and/or Local AI Meeting Transcription that works for you?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7jvba",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753301609,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I‚Äôm currently using Notion, which works great for transcribing meetings and converting them into summaries, action items, and so on. &lt;/p&gt;\n\n&lt;p&gt;Is anyone using open-source / locally powered AI tools? I‚Äôd love to hear about your experience with those.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7jvba",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Southern_Sun_2106",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7jvba/opensource_andor_local_ai_meeting_transcription/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7jvba/opensource_andor_local_ai_meeting_transcription/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753301609,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I see the new qwen 3 coder model is insane and seems equal to claude sonnet 4 in coding tests..is there a way to use it inside vs code or cursor..I mean  using an extension or any other way..",
          "author_fullname": "t2_oxox4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a way to use qwen 3 coder inside vs code or cursor",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m79sp9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753278634,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see the new qwen 3 coder model is insane and seems equal to claude sonnet 4 in coding tests..is there a way to use it inside vs code or cursor..I mean  using an extension or any other way..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m79sp9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "madhawavish",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m79sp9/is_there_a_way_to_use_qwen_3_coder_inside_vs_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m79sp9/is_there_a_way_to_use_qwen_3_coder_inside_vs_code/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753278634,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôve been testing maestrale-chat-v0.4-beta and noticed it handles step-by-step reasoning quite well, even for basic math and intro programming tasks. It‚Äôs not a math engine / solver, but for explaining concepts, rephrasing problems, or reviewing student logic, it seems quite promising.\n\nIs anyone here using local models like this in **education**, especially for **math or computer science**?  \nWould love to hear how ‚Äî and what tools you use, ie. on Mac.",
          "author_fullname": "t2_zadnn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone using maestrale-chat-v0.4-beta?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7d55o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753286397,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôve been testing maestrale-chat-v0.4-beta and noticed it handles step-by-step reasoning quite well, even for basic math and intro programming tasks. It‚Äôs not a math engine / solver, but for explaining concepts, rephrasing problems, or reviewing student logic, it seems quite promising.&lt;/p&gt;\n\n&lt;p&gt;Is anyone here using local models like this in &lt;strong&gt;education&lt;/strong&gt;, especially for &lt;strong&gt;math or computer science&lt;/strong&gt;?&lt;br/&gt;\nWould love to hear how ‚Äî and what tools you use, ie. on Mac.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7d55o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "proahdgsga133",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7d55o/anyone_using_maestralechatv04beta/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7d55o/anyone_using_maestralechatv04beta/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753286397,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unsloth quants already starting to roll out for Qwen3-Coder",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6u0gt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=df2ca14ad32406cbfd2154f6392b11b3062c0b80",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753227930,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?auto=webp&amp;s=c6a55f1fe010145ae8782e1593f28ec04aee30a9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ca2157367c76507911bd02cc27f2bd77fdeb58f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a1260c12ba170ca3258b3d164bd71b26d3fd637",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=82c9ef33075d79d05f812d774d3d9963a2ca93c2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=651f424884542b7c34073b3bc62c0fc1b199eaae",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=287c77e1abcef6dc2ad9cac5e8a8d70a85c3f900",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6671827d2f6adf5ce554df23310df3e1e4805228",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6u0gt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/",
          "stickied": false,
          "url": "https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d",
          "subreddit_subscribers": 503516,
          "created_utc": 1753227930,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1M token context length\n\nNo model weights yet, but Qwen3-Coder is already available for testing on [Qwen Chat](https://chat.qwen.ai)",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Available on chat.qwen.ai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mfic",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/bs1O5LLiPqNQ-8leTmrD3PNczGaZCiiN00b8eacOHzE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;1M token context length&lt;/p&gt;\n\n&lt;p&gt;No model weights yet, but Qwen3-Coder is already available for testing on &lt;a href=\"https://chat.qwen.ai\"&gt;Qwen Chat&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8xj4raow0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8xj4raow0hef1.png?auto=webp&amp;s=6c9d8670b9960f64e78150ca2039fc5471464158",
                  "width": 450,
                  "height": 150
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=67fbf003dc0b0cdf945b3ac5069eddaeeaf26ed5",
                    "width": 108,
                    "height": 36
                  },
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ab53d55f78ada2bc730c345d2b92f3cbd18dad6",
                    "width": 216,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf0cbd6e19276ab7bbf6b36687af35cdf6c00d83",
                    "width": 320,
                    "height": 106
                  }
                ],
                "variants": {},
                "id": "jj-Sn3KQKbzD6M9iYIRnKA0q_gXhkpIWDEoj36IQyis"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mfic",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/",
          "stickied": false,
          "url": "https://i.redd.it/8xj4raow0hef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753209889,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which GPU setup is better for inference of local models on vllm (considering two 14B models for now, might be larger in future). My options are: 2x RTX 6000 Ada or 1x A100 (80GB). Or is there a better pick than these two. Can‚Äôt use consumer GPUs. Appreciate any help!",
          "author_fullname": "t2_k3dpkbo4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 6000 Ada or A100, which is better for inference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ifsg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753298364,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which GPU setup is better for inference of local models on vllm (considering two 14B models for now, might be larger in future). My options are: 2x RTX 6000 Ada or 1x A100 (80GB). Or is there a better pick than these two. Can‚Äôt use consumer GPUs. Appreciate any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7ifsg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "subtle-being",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ifsg/rtx_6000_ada_or_a100_which_is_better_for_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7ifsg/rtx_6000_ada_or_a100_which_is_better_for_inference/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753298364,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What I mean is instead of paying for claude code or junie, is it possible to buy hardware capable of running an equivalent model?\n\nClaude code is $20 per month\nJunie is cheaper at $18 per month for the best\n\nI know just renting is likely cheaper in the long term but this assumes no price increases, and locks you into whatever restrictions they have.\n\nIf i go to huggingface what hardware would i need to run their best model comfortably and with some future proofing?\n\nBudget is maybe up to ¬£10k (any more feels unjustifiable vs renting).",
          "author_fullname": "t2_4fxgm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best hardware for running the biggest models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7cagw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753284449,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I mean is instead of paying for claude code or junie, is it possible to buy hardware capable of running an equivalent model?&lt;/p&gt;\n\n&lt;p&gt;Claude code is $20 per month\nJunie is cheaper at $18 per month for the best&lt;/p&gt;\n\n&lt;p&gt;I know just renting is likely cheaper in the long term but this assumes no price increases, and locks you into whatever restrictions they have.&lt;/p&gt;\n\n&lt;p&gt;If i go to huggingface what hardware would i need to run their best model comfortably and with some future proofing?&lt;/p&gt;\n\n&lt;p&gt;Budget is maybe up to ¬£10k (any more feels unjustifiable vs renting).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7cagw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sanitykey",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753284449,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am experimenting with local llms. Have been using the 780m integrated onto the 7840u on my current machine which has 64GB of LPDDR5X memory clocked at¬†7500 MT/s (16GB allocated to the GPU). I have also been playing with my eGPU over oculink (GPD G1). I am looking at Strix Halo for future dev (especially mobile), and realized that as far as memory bandwidth the GPD G1 should be similar, so I decided to test Qwen3-8b-Q4\\_K\\_M in LM Studio with the Vulkan and ROCm runtimes against it.\n\nI was kind of appalled at the performance. 12.68 tok/sec when asking to write a short story. Interestingly on my iGPU I get 14.39 tok/sec... From my understanding Strix Halo should be getting 35-40 tok/sec on such a model and Strix Halo should have similar or worse memory bandwidth than my eGPU, so why is my eGPU sucking so badly that it's worse than my iGPU? Is Oculink limiting things for some reason or some other part of my system? Any good way to diagnose?\n\nI was hoping I could get an idea of Strix Halo performance from my current rig, even if it came with the caveat of limited context size.",
          "author_fullname": "t2_y0abrfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why is my external RX 7600M XT (GPD G1) slow by comparison?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7i9pl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753297979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am experimenting with local llms. Have been using the 780m integrated onto the 7840u on my current machine which has 64GB of LPDDR5X memory clocked at¬†7500 MT/s (16GB allocated to the GPU). I have also been playing with my eGPU over oculink (GPD G1). I am looking at Strix Halo for future dev (especially mobile), and realized that as far as memory bandwidth the GPD G1 should be similar, so I decided to test Qwen3-8b-Q4_K_M in LM Studio with the Vulkan and ROCm runtimes against it.&lt;/p&gt;\n\n&lt;p&gt;I was kind of appalled at the performance. 12.68 tok/sec when asking to write a short story. Interestingly on my iGPU I get 14.39 tok/sec... From my understanding Strix Halo should be getting 35-40 tok/sec on such a model and Strix Halo should have similar or worse memory bandwidth than my eGPU, so why is my eGPU sucking so badly that it&amp;#39;s worse than my iGPU? Is Oculink limiting things for some reason or some other part of my system? Any good way to diagnose?&lt;/p&gt;\n\n&lt;p&gt;I was hoping I could get an idea of Strix Halo performance from my current rig, even if it came with the caveat of limited context size.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7i9pl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cfogrady",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7i9pl/why_is_my_external_rx_7600m_xt_gpd_g1_slow_by/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7i9pl/why_is_my_external_rx_7600m_xt_gpd_g1_slow_by/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753297979,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I.e. research focused around improving LLMs at generating diagrams via text diagram specification languages such as Latex Tikz library.",
          "author_fullname": "t2_5fpcozzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can someone point me towards LLM diagram generation research?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7c7yz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753284291,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I.e. research focused around improving LLMs at generating diagrams via text diagram specification languages such as Latex Tikz library.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7c7yz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "boringblobking",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7c7yz/can_someone_point_me_towards_llm_diagram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7c7yz/can_someone_point_me_towards_llm_diagram/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753284291,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Random thought about role-based multi-turn messaging with LLMs: \n\nWhat if we pretend to be the assistant and try to get the model to predict the user's response?\n\n**I know it might not work as intended because of how they are fine-tuned, but has anyone tried it before? Just curious. ",
          "author_fullname": "t2_6mqmoa64",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Spice things up by switching roles?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7i537",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753297679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Random thought about role-based multi-turn messaging with LLMs: &lt;/p&gt;\n\n&lt;p&gt;What if we pretend to be the assistant and try to get the model to predict the user&amp;#39;s response?&lt;/p&gt;\n\n&lt;p&gt;**I know it might not work as intended because of how they are fine-tuned, but has anyone tried it before? Just curious. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7i537",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mathemachicken4",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7i537/spice_things_up_by_switching_roles/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7i537/spice_things_up_by_switching_roles/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753297679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, do you have any idea how vibe coding platforms like Replit and Lovable fine tune their code generation algorithms?\n\nIt's unclear to me how their core product look like!\n\n",
          "author_fullname": "t2_1np6q3qbry",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finetuning for code generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7hvxz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753297107,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, do you have any idea how vibe coding platforms like Replit and Lovable fine tune their code generation algorithms?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s unclear to me how their core product look like!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7hvxz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gpt_devastation",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753297107,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When I installed this model on a Samsung phone more than a month ago, I didn't find much. When I tested other gemma models today, I found that the output of 3n is very different from other gemma models, and it is also very different from gemini 2.5 flash models. The most similar one is gemini 2.5pro.\n\nhttps://preview.redd.it/pbvwe0821lef1.jpg?width=1848&amp;format=pjpg&amp;auto=webp&amp;s=5459bd24f6ee93191069e104f74ad274ae928006\n\n  \n//The testing method I use is different from most benchmarks. And I don‚Äôt use English (which is what many models are optimized for)This avoids falling into the circle of most model optimizations.\n\n[gemini2.5 pro](https://preview.redd.it/pzskn2681lef1.jpg?width=3813&amp;format=pjpg&amp;auto=webp&amp;s=6eeb30dbf49dca2d32e53b6f6cd96fa20a0b7e6c)\n\n[gemini 25. flash](https://preview.redd.it/uj6mt2681lef1.jpg?width=3551&amp;format=pjpg&amp;auto=webp&amp;s=ecfd4e8f097faa8c0ca41c182f150e2116d2a23f)\n\n[gemma 3 27B](https://preview.redd.it/u1zrkifh1lef1.png?width=3523&amp;format=png&amp;auto=webp&amp;s=6ac99fa0e552198813ec146b7ea83911a0675ec9)\n\n  \n\n\n//Judging from the output content, **the knowledge bases of 3N and gemini2.5 pro are highly overlapping**.\n\n//gemma 3 27B's answer actually contains many errors.\n\nhttps://preview.redd.it/6xz6k6sk1lef1.jpg?width=2560&amp;format=pjpg&amp;auto=webp&amp;s=e62a0c13d13b5e12d7511071d1284d5708afe6a7\n\n//There is a very difficult point here. The photo I posted was taken by myself, and it is located in Tibet. Because this is an edge direction that many models will not deliberately strengthen during training, I often use it to test the model's knowledge base. In addition, many models do not recognize this photo as Lhasa, but as Nepal, etc. This error will be very obvious on models with small parameters. 3N does not have this problem at all. You can notice that even the gemini2.5flash model did not correctly identify the specific city and temple.\n\n//In fact, some people also mentioned geographic information matching, or image matching on the Internet. You should know that 3N is an offline model. Even with a geographic information matching module, this image is an extremely difficult problem. Because this image is more than ten years old, there is no obvious landmark in Lhasa in the distance to match.  \n//By the way, I have tried for more than a week to convert  medgemma into an Android APP version, but I have not been successful.\n\n\n\n\n\n  \n\n\n",
          "author_fullname": "t2_chilocifj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone noticed that the gemma3n model doesn't look like a gemma, but more like a gemini mini?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "pbvwe0821lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 172,
                  "x": 108,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c306d897c1118e1c1311453b98d50c51f4d47275"
                },
                {
                  "y": 345,
                  "x": 216,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13408ebd2e5cefe51e1b438d2ad82f0a11ce4baa"
                },
                {
                  "y": 512,
                  "x": 320,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=081e65c703af3111b9fda2713a3d62aec064e23e"
                },
                {
                  "y": 1025,
                  "x": 640,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba7d25947a96df24e125543b9ba373e4882fd70a"
                },
                {
                  "y": 1537,
                  "x": 960,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49fcd723039eef70d914184f8f2850eb25186909"
                },
                {
                  "y": 1729,
                  "x": 1080,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a0e8c255d0c3a3bcb9ddc8d5ef4841e2753f1b6"
                }
              ],
              "s": {
                "y": 2960,
                "x": 1848,
                "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=1848&amp;format=pjpg&amp;auto=webp&amp;s=5459bd24f6ee93191069e104f74ad274ae928006"
              },
              "id": "pbvwe0821lef1"
            },
            "uj6mt2681lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 58,
                  "x": 108,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aca53d35bf10e8bb0294a8937e81439c8f7cad36"
                },
                {
                  "y": 117,
                  "x": 216,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b59d02aac658ca78f6d571274c40a94ef2d0d80b"
                },
                {
                  "y": 174,
                  "x": 320,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13ed3f686e7839077347ffa846500fbc3b6a565b"
                },
                {
                  "y": 348,
                  "x": 640,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=faa7da4e124fcc1ebe44e688871e7e4ce726b5cd"
                },
                {
                  "y": 522,
                  "x": 960,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd69a179eef834753c933b43ae3006dbab5ba577"
                },
                {
                  "y": 587,
                  "x": 1080,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bfa551bfdb50f03fc463b819210526cd1512964"
                }
              ],
              "s": {
                "y": 1933,
                "x": 3551,
                "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=3551&amp;format=pjpg&amp;auto=webp&amp;s=ecfd4e8f097faa8c0ca41c182f150e2116d2a23f"
              },
              "id": "uj6mt2681lef1"
            },
            "u1zrkifh1lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 52,
                  "x": 108,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=83dd625944274fe8b57e70fd4ec30db7f8bc4f58"
                },
                {
                  "y": 105,
                  "x": 216,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a2837bf3e99dce9e463b520761b03190c1d0ce6"
                },
                {
                  "y": 155,
                  "x": 320,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2cfa596c0e26212f8e0e8a068447b932e380726"
                },
                {
                  "y": 311,
                  "x": 640,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d36dbd406424ed2d7a2aa738bcd5b3177057d2c9"
                },
                {
                  "y": 467,
                  "x": 960,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6dbd8ef6e920c8534ae0d0915428719b3d197ab0"
                },
                {
                  "y": 526,
                  "x": 1080,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23a508f94056f900e29543cca166dc4cb1d54d68"
                }
              ],
              "s": {
                "y": 1717,
                "x": 3523,
                "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=3523&amp;format=png&amp;auto=webp&amp;s=6ac99fa0e552198813ec146b7ea83911a0675ec9"
              },
              "id": "u1zrkifh1lef1"
            },
            "6xz6k6sk1lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0347afc5a48973355a6ccd141c87bd43beb7306"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eae2cc8ce89b24d0cbeb7fbd51c38c208308be8e"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cc2d05745d3982582bde110dcf5e9587d0931de"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=232d1b94ba3aa4555837abb257be597ba67dc3f1"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d6deed7ff68aff4b756f62e4f7e6c4b5d7dcf7f"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d3da6858fc9d21478afe51eb83d1569d9114fd7"
                }
              ],
              "s": {
                "y": 1440,
                "x": 2560,
                "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=2560&amp;format=pjpg&amp;auto=webp&amp;s=e62a0c13d13b5e12d7511071d1284d5708afe6a7"
              },
              "id": "6xz6k6sk1lef1"
            },
            "pzskn2681lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 58,
                  "x": 108,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=34948da6ac1630f6d0ec343feebee7e8decb5b18"
                },
                {
                  "y": 117,
                  "x": 216,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6d54a0ce4eeadbe62a0f628500c6a079ca8c809"
                },
                {
                  "y": 173,
                  "x": 320,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdc846f07511a088f327cc59c3e3b677054e3ab5"
                },
                {
                  "y": 347,
                  "x": 640,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17ab4e68ce36485e424735df36efe341f64194a5"
                },
                {
                  "y": 520,
                  "x": 960,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=46900fb962688998795aeb62ed990b7444920df5"
                },
                {
                  "y": 586,
                  "x": 1080,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1251a93f7117c64dca70d64525cede4025acd7f1"
                }
              ],
              "s": {
                "y": 2069,
                "x": 3813,
                "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=3813&amp;format=pjpg&amp;auto=webp&amp;s=6eeb30dbf49dca2d32e53b6f6cd96fa20a0b7e6c"
              },
              "id": "pzskn2681lef1"
            }
          },
          "name": "t3_1m73ohk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ip6dBmHZN-29Fal7BH5Naow0oNhtVP0fjBeYTW1RLcg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753258999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I installed this model on a Samsung phone more than a month ago, I didn&amp;#39;t find much. When I tested other gemma models today, I found that the output of 3n is very different from other gemma models, and it is also very different from gemini 2.5 flash models. The most similar one is gemini 2.5pro.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pbvwe0821lef1.jpg?width=1848&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5459bd24f6ee93191069e104f74ad274ae928006\"&gt;https://preview.redd.it/pbvwe0821lef1.jpg?width=1848&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5459bd24f6ee93191069e104f74ad274ae928006&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;//The testing method I use is different from most benchmarks. And I don‚Äôt use English (which is what many models are optimized for)This avoids falling into the circle of most model optimizations.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pzskn2681lef1.jpg?width=3813&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6eeb30dbf49dca2d32e53b6f6cd96fa20a0b7e6c\"&gt;gemini2.5 pro&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uj6mt2681lef1.jpg?width=3551&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ecfd4e8f097faa8c0ca41c182f150e2116d2a23f\"&gt;gemini 25. flash&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u1zrkifh1lef1.png?width=3523&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ac99fa0e552198813ec146b7ea83911a0675ec9\"&gt;gemma 3 27B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;//Judging from the output content, &lt;strong&gt;the knowledge bases of 3N and gemini2.5 pro are highly overlapping&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;//gemma 3 27B&amp;#39;s answer actually contains many errors.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6xz6k6sk1lef1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e62a0c13d13b5e12d7511071d1284d5708afe6a7\"&gt;https://preview.redd.it/6xz6k6sk1lef1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e62a0c13d13b5e12d7511071d1284d5708afe6a7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;//There is a very difficult point here. The photo I posted was taken by myself, and it is located in Tibet. Because this is an edge direction that many models will not deliberately strengthen during training, I often use it to test the model&amp;#39;s knowledge base. In addition, many models do not recognize this photo as Lhasa, but as Nepal, etc. This error will be very obvious on models with small parameters. 3N does not have this problem at all. You can notice that even the gemini2.5flash model did not correctly identify the specific city and temple.&lt;/p&gt;\n\n&lt;p&gt;//In fact, some people also mentioned geographic information matching, or image matching on the Internet. You should know that 3N is an offline model. Even with a geographic information matching module, this image is an extremely difficult problem. Because this image is more than ten years old, there is no obvious landmark in Lhasa in the distance to match.&lt;br/&gt;\n//By the way, I have tried for more than a week to convert  medgemma into an Android APP version, but I have not been successful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m73ohk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mountain_TANG",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73ohk/has_anyone_noticed_that_the_gemma3n_model_doesnt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73ohk/has_anyone_noticed_that_the_gemma3n_model_doesnt/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753258999,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So after doing some further research on the cost of self-hosting larger models I have come to this conclusion - and I am looking for feedback here.\n\nMy specific use case is an AI-assisted IDE I am building myself, and I am looking to dabble in self-hosting a capable model for inference for its users. I currently do **not** have a budget to do extensive testing and benchmarking but I have read up plenty on this (and argued quite a lot with ChatGPT and Gemini lol) for some days now.\n\nHere is what I've got so far:\n\n* tokens per second is not a reliable metric as it actually *averages out* two very different speeds (input vs output):\n\n&gt;One additional note: I recently set up an inference setup for **llama-3-70b** on **8xH100**. I can get about **100,000 tok/s** on inputs which is pretty close to full utilization (1e15 flop/s \\* 8 gpus / 7e10 flop per forward pass). However, I get dramatically worse performance on generation, perhaps **3,200 tok/s**. I'm doing generation with long prompts and llama-3-70b has no sparse attention or other feature for reducing KV cache (beyond multi-query attention which is standard these days), so KV cache bits pretty hard.¬†- [link here](https://www.lesswrong.com/posts/g7H2sSGHAeYxCHzrz/how-much-ai-inference-can-we-do?commentId=RXnfe2ojyqmhLTXJm).\n\n* In IDE use we could expect our requests to **average out** 20k input tokens and 300 output per request. (This is my own estimate based on my own usage via OpernRouter).\n\n**Now for some math:**\n\nSingle H100 (Runpod): $ 2.59/hr\n\nMinimum of 8x H100 (required): $ 20.72/hr\n\nThis setup ***per second:*** 20.72 / 3600 = 0.0057 $/second\n\nQwen3-Coder-480B-A35B-Instruct: (half of llama-3-70B token/s?) **200k tokens/s input** \\+ **6400 tokens/s output**\n\n**Phase 1: Prompt Processing Time** (20,000 input tokens)\n\n* **Calculation:** `20,000 tokens / 200,000 tokens/sec`\n* **Result:** **0.10 seconds**\n\n**Phase 2: Token Generation Time (300 output tokens)**\n\n* **Calculation:** `300 tokens / 6,400 tokens/sec`\n* **Result:** **\\~0.047 seconds**\n\n**Total Time &amp; Cost per Request**\n\n* **Total Time:** `0.10s + 0.047s = **0.147 seconds**`\n* **Total Cost:** `0.147 seconds * $0.0057/sec =` `~$0.0008`\n\n\n\nI mean... is this right? I think this is wrong but it is as far as I could get without actually going and renting these GPUs and testing it for myself. It just seems **so much cheaper** than what I end up paying via API in OpenRouter.",
          "author_fullname": "t2_19mrnrt357",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Throughput: Input vs Output. Looking for help...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7brg9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753283519,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753283238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So after doing some further research on the cost of self-hosting larger models I have come to this conclusion - and I am looking for feedback here.&lt;/p&gt;\n\n&lt;p&gt;My specific use case is an AI-assisted IDE I am building myself, and I am looking to dabble in self-hosting a capable model for inference for its users. I currently do &lt;strong&gt;not&lt;/strong&gt; have a budget to do extensive testing and benchmarking but I have read up plenty on this (and argued quite a lot with ChatGPT and Gemini lol) for some days now.&lt;/p&gt;\n\n&lt;p&gt;Here is what I&amp;#39;ve got so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;tokens per second is not a reliable metric as it actually &lt;em&gt;averages out&lt;/em&gt; two very different speeds (input vs output):&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;One additional note: I recently set up an inference setup for &lt;strong&gt;llama-3-70b&lt;/strong&gt; on &lt;strong&gt;8xH100&lt;/strong&gt;. I can get about &lt;strong&gt;100,000 tok/s&lt;/strong&gt; on inputs which is pretty close to full utilization (1e15 flop/s * 8 gpus / 7e10 flop per forward pass). However, I get dramatically worse performance on generation, perhaps &lt;strong&gt;3,200 tok/s&lt;/strong&gt;. I&amp;#39;m doing generation with long prompts and llama-3-70b has no sparse attention or other feature for reducing KV cache (beyond multi-query attention which is standard these days), so KV cache bits pretty hard.¬†- &lt;a href=\"https://www.lesswrong.com/posts/g7H2sSGHAeYxCHzrz/how-much-ai-inference-can-we-do?commentId=RXnfe2ojyqmhLTXJm\"&gt;link here&lt;/a&gt;.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In IDE use we could expect our requests to &lt;strong&gt;average out&lt;/strong&gt; 20k input tokens and 300 output per request. (This is my own estimate based on my own usage via OpernRouter).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Now for some math:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Single H100 (Runpod): $ 2.59/hr&lt;/p&gt;\n\n&lt;p&gt;Minimum of 8x H100 (required): $ 20.72/hr&lt;/p&gt;\n\n&lt;p&gt;This setup &lt;strong&gt;&lt;em&gt;per second:&lt;/em&gt;&lt;/strong&gt; 20.72 / 3600 = 0.0057 $/second&lt;/p&gt;\n\n&lt;p&gt;Qwen3-Coder-480B-A35B-Instruct: (half of llama-3-70B token/s?) &lt;strong&gt;200k tokens/s input&lt;/strong&gt; + &lt;strong&gt;6400 tokens/s output&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Phase 1: Prompt Processing Time&lt;/strong&gt; (20,000 input tokens)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Calculation:&lt;/strong&gt; &lt;code&gt;20,000 tokens / 200,000 tokens/sec&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;strong&gt;0.10 seconds&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Phase 2: Token Generation Time (300 output tokens)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Calculation:&lt;/strong&gt; &lt;code&gt;300 tokens / 6,400 tokens/sec&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;strong&gt;~0.047 seconds&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Total Time &amp;amp; Cost per Request&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Total Time:&lt;/strong&gt; &lt;code&gt;0.10s + 0.047s = **0.147 seconds**&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Total Cost:&lt;/strong&gt; &lt;code&gt;0.147 seconds * $0.0057/sec =&lt;/code&gt; &lt;code&gt;~$0.0008&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I mean... is this right? I think this is wrong but it is as far as I could get without actually going and renting these GPUs and testing it for myself. It just seems &lt;strong&gt;so much cheaper&lt;/strong&gt; than what I end up paying via API in OpenRouter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7brg9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Budget_Map_3333",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7brg9/throughput_input_vs_output_looking_for_help/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7brg9/throughput_input_vs_output_looking_for_help/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753283238,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is available on OpenRouter",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6u3kd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9352d9aaa19f84f05307725c60c6280cb5ce4153",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753228157,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/qwen/qwen3-coder",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?auto=webp&amp;s=8a80c032c084b7af008f30d36302aa3e2b303841",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=81bbe5b26b024567de7a02963aa1047661c30d21",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31982c88f4e8f0ba3e19de3cb4fe1aecc0737271",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4de872e3b3e085cf7e3edcad2410dce6e017ff0c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f4da7fa00b2fee69899af4df9a137f3645df9e7",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b233723391e8a2e702cc58632dd60a7a281a8fbd",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d4e480d1b5455587e7a48ddb01c62b2f0bdbd5a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6u3kd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/",
          "stickied": false,
          "url": "https://openrouter.ai/qwen/qwen3-coder",
          "subreddit_subscribers": 503516,
          "created_utc": 1753228157,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I saw this claude code router repo on github, but was broken for me, so I rewrote the thing in Go. Is called [Claude Code Open](https://github.com/Davincible/claude-code-open)\n\nNow you can simply `CCO_API_KEY=\"&lt;open router key&gt;\" cco code` and then select `openrouter,qwen/qwen3-coder` as model and voila. Also blocks any Anthropic monitoring requests as a bonus\n\nComplex config available as well and very extensible\n\nHope it helps someone like it did me\n\n[https://github.com/Davincible/claude-code-open](https://github.com/Davincible/claude-code-open)",
          "author_fullname": "t2_166q2z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Github Repo] - Use Qwen3 coder or any other LLM provider with Claude Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zkmm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753243976,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw this claude code router repo on github, but was broken for me, so I rewrote the thing in Go. Is called &lt;a href=\"https://github.com/Davincible/claude-code-open\"&gt;Claude Code Open&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Now you can simply &lt;code&gt;CCO_API_KEY=&amp;quot;&amp;lt;open router key&amp;gt;&amp;quot; cco code&lt;/code&gt; and then select &lt;code&gt;openrouter,qwen/qwen3-coder&lt;/code&gt; as model and voila. Also blocks any Anthropic monitoring requests as a bonus&lt;/p&gt;\n\n&lt;p&gt;Complex config available as well and very extensible&lt;/p&gt;\n\n&lt;p&gt;Hope it helps someone like it did me&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Davincible/claude-code-open\"&gt;https://github.com/Davincible/claude-code-open&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?auto=webp&amp;s=69762d8c4094e5a3ed92bf377de14e0031d69157",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=68a0ec129f7959f5fcb0d85891b6490389d0313a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=98f4159ada43609234c675240fe20cb5e02bfb61",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a539701736c67ff899c97a885e2a1fb429b32749",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=33db0f916eff806bc0ba84a1c10b6d2cd3115f2f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=425a695f92096a4942fa8387ff901601ecfcee49",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=985e0c18d4c3aeb2f4a59c448bfa5f7a3fd196d6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6zkmm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "davincible",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6zkmm/github_repo_use_qwen3_coder_or_any_other_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zkmm/github_repo_use_qwen3_coder_or_any_other_llm/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753243976,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, I need to parse PDFs of medical books that contain text and a lot of images.\n\nCurrently, I use a gemini 2.5 flash lite to do the extraction into a structured output. \n\n  \nMy original plan was to convert PDFs to images, then give gemini 10 pages each time. I am also giving instruction when it encounters an image to return the top left and bottom right x y coordinate. With these coordinate I then extract the image and replace the coordinates with an image ID (that I can use later in my RAG system to output the image in the frontend) in the structured output. The problem is that this is not working, the coordinate are often inexact. \n\nDo any of you have had a similar problem and found a solution to this problem? \n\nDo I need to use another model ?\n\nMaybe the coordinate are exact, but I am doing something wrong ?\n\n  \nThank you guys for your help!!",
          "author_fullname": "t2_1npody3s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "struggling with image extraction for pdf parsing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7gv2d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753294807,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I need to parse PDFs of medical books that contain text and a lot of images.&lt;/p&gt;\n\n&lt;p&gt;Currently, I use a gemini 2.5 flash lite to do the extraction into a structured output. &lt;/p&gt;\n\n&lt;p&gt;My original plan was to convert PDFs to images, then give gemini 10 pages each time. I am also giving instruction when it encounters an image to return the top left and bottom right x y coordinate. With these coordinate I then extract the image and replace the coordinates with an image ID (that I can use later in my RAG system to output the image in the frontend) in the structured output. The problem is that this is not working, the coordinate are often inexact. &lt;/p&gt;\n\n&lt;p&gt;Do any of you have had a similar problem and found a solution to this problem? &lt;/p&gt;\n\n&lt;p&gt;Do I need to use another model ?&lt;/p&gt;\n\n&lt;p&gt;Maybe the coordinate are exact, but I am doing something wrong ?&lt;/p&gt;\n\n&lt;p&gt;Thank you guys for your help!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7gv2d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aliihsan01100",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7gv2d/struggling_with_image_extraction_for_pdf_parsing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7gv2d/struggling_with_image_extraction_for_pdf_parsing/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753294807,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI'm working on an internal tool where we are provided with **only a URL** ‚Äî no description, metadata, or prior context ‚Äî and our goal is to automatically classify the website into one of two categories. The categories could be something like:\n\n* **Category A**: Websites that promote or belong to academic institutions\n* **Category B**: Websites that do not relate to academics at all\n\n**The Goal:**\n\nGiven a URL like [`example.com`](http://example.com), we want to classify it as either Category A or Category B with decent accuracy. There is no prior knowledge or labeled data about the site ‚Äî we need to infer the classification based on the actual content.\n\n**What I‚Äôve Tried:**\n\n\\- I‚Äôve tried Gemini API (2.5 Flash) with Grounded Google Search and also with URL Context tool ‚Äî both didn‚Äôt provide satisfactory results.\n\n**The Challenge with using google searchs:**\n\n\\-  Some sites don‚Äôt show up at all in google search.\n\n\\- Others return results, but snippets don‚Äôt belong to the actual domain but to similar domains.\n\n**Considered Scraping:**\n\n\\- One possible route is to scrape the target websites and analyze the content directly.\n\n\\- However, this comes with a **context window limitation** ‚Äî scraping just the homepage or a single page might not give the full picture, especially if relevant content is nested deeper in About, Services, or FAQ pages.\n\n\\- To address this, we may need to **crawl and scrape all primary pages** of the website (e.g., top-level links and their children), but that quickly escalates both cost and processing time, and still doesn't solve the context summarization issue unless chunked well.\n\n\\- Using LLMs on long content is tricky ‚Äî even with chunking and summarization maintaining context fidelity and avoiding hallucinations remains a challenge.\n\n# My Question:\n\nHow would you approach this classification problem? I would appreciate any help with this. I am a novice in this field.\n\nThanks in advance",
          "author_fullname": "t2_tv03725",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Struggling with NLP classification pipeline for web content ‚Äì seeking advice",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7aefj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753285588,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753280082,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on an internal tool where we are provided with &lt;strong&gt;only a URL&lt;/strong&gt; ‚Äî no description, metadata, or prior context ‚Äî and our goal is to automatically classify the website into one of two categories. The categories could be something like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Category A&lt;/strong&gt;: Websites that promote or belong to academic institutions&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Category B&lt;/strong&gt;: Websites that do not relate to academics at all&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Goal:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Given a URL like &lt;a href=\"http://example.com\"&gt;&lt;code&gt;example.com&lt;/code&gt;&lt;/a&gt;, we want to classify it as either Category A or Category B with decent accuracy. There is no prior knowledge or labeled data about the site ‚Äî we need to infer the classification based on the actual content.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I‚Äôve Tried:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- I‚Äôve tried Gemini API (2.5 Flash) with Grounded Google Search and also with URL Context tool ‚Äî both didn‚Äôt provide satisfactory results.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Challenge with using google searchs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;-  Some sites don‚Äôt show up at all in google search.&lt;/p&gt;\n\n&lt;p&gt;- Others return results, but snippets don‚Äôt belong to the actual domain but to similar domains.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Considered Scraping:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- One possible route is to scrape the target websites and analyze the content directly.&lt;/p&gt;\n\n&lt;p&gt;- However, this comes with a &lt;strong&gt;context window limitation&lt;/strong&gt; ‚Äî scraping just the homepage or a single page might not give the full picture, especially if relevant content is nested deeper in About, Services, or FAQ pages.&lt;/p&gt;\n\n&lt;p&gt;- To address this, we may need to &lt;strong&gt;crawl and scrape all primary pages&lt;/strong&gt; of the website (e.g., top-level links and their children), but that quickly escalates both cost and processing time, and still doesn&amp;#39;t solve the context summarization issue unless chunked well.&lt;/p&gt;\n\n&lt;p&gt;- Using LLMs on long content is tricky ‚Äî even with chunking and summarization maintaining context fidelity and avoiding hallucinations remains a challenge.&lt;/p&gt;\n\n&lt;h1&gt;My Question:&lt;/h1&gt;\n\n&lt;p&gt;How would you approach this classification problem? I would appreciate any help with this. I am a novice in this field.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7aefj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "amir_shehzad",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753280082,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI wanted to check if I'm missing anything relevant in performance or quality with my quant strategy.\n\nMy setup is an EPYC Rome (no avx512 instruction set) with 512 GB RAM and a bunch of 3060 / 3090s. The inference engine is llama.cpp and I run almost everything large (r1, q3 235, q3 480) in UD-Q4\\_K\\_XL, while Kimi K2 uses UD-Q3\\_K\\_XL - CPU offload ofc. Smaller 30b/32b (Devstral, Magistral, Gemma-3, etc.) I run in UD-Q6\\_K\\_XL on the GPUs only.\n\nI settled on these quants after seeing tests on unrelated models some time ago that suggested diminishing returns after Q4\\_K\\_M. Another source I can't remember claimed Q8\\_0 for KV cache doesn't hurt quality and that even Q4\\_0 for the v cache is acceptable.\n\nAre my generalized assumptions still correct or where they ever correct?\n\n* larger models are more insensitive to quant\n* diminishing returns after \\~4.5bpw\n* Q8\\_0 KV is the way to go\n\nWould the ik\\_llama fork (with their special quants) provide a significant increase of quality/speed in my CPU-poor setup?\n\nEdit:\n\nI use it mainly for coding - sometimes obscure languages like OpenSCAD, reasoning in electrical engineering (which component could be the culprit if ..., what could be this component, it has .. color and ... marking) and some science related stuff like paper comprehension, generation of abstracts, keyword suggestion.",
          "author_fullname": "t2_1tnm5zafaw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which quantization approach is the way to go? (llama.cpp)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m77az5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753274570,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753271962,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I wanted to check if I&amp;#39;m missing anything relevant in performance or quality with my quant strategy.&lt;/p&gt;\n\n&lt;p&gt;My setup is an EPYC Rome (no avx512 instruction set) with 512 GB RAM and a bunch of 3060 / 3090s. The inference engine is llama.cpp and I run almost everything large (r1, q3 235, q3 480) in UD-Q4_K_XL, while Kimi K2 uses UD-Q3_K_XL - CPU offload ofc. Smaller 30b/32b (Devstral, Magistral, Gemma-3, etc.) I run in UD-Q6_K_XL on the GPUs only.&lt;/p&gt;\n\n&lt;p&gt;I settled on these quants after seeing tests on unrelated models some time ago that suggested diminishing returns after Q4_K_M. Another source I can&amp;#39;t remember claimed Q8_0 for KV cache doesn&amp;#39;t hurt quality and that even Q4_0 for the v cache is acceptable.&lt;/p&gt;\n\n&lt;p&gt;Are my generalized assumptions still correct or where they ever correct?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;larger models are more insensitive to quant&lt;/li&gt;\n&lt;li&gt;diminishing returns after ~4.5bpw&lt;/li&gt;\n&lt;li&gt;Q8_0 KV is the way to go&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would the ik_llama fork (with their special quants) provide a significant increase of quality/speed in my CPU-poor setup?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;I use it mainly for coding - sometimes obscure languages like OpenSCAD, reasoning in electrical engineering (which component could be the culprit if ..., what could be this component, it has .. color and ... marking) and some science related stuff like paper comprehension, generation of abstracts, keyword suggestion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m77az5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pixelterpy",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753271962,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,  \nI have 2x 7900 XTX and not getting any model run with them in a docker.\n\n    docker pull rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702\n    \n    docker run -it \\\n      --dns=1.1.1.1 \\\n      --dns=8.8.8.8 \\\n      --network=host \\\n      --group-add=video \\\n      --ipc=host \\\n      --cap-add=SYS_PTRACE \\\n      --security-opt seccomp=unconfined \\\n      --privileged \\\n      --device /dev/kfd \\\n      --device /dev/dri \\\n      -e ROCM_VISIBLE_DEVICES=0,1,2,3 \\\n      -e HIP_VISIBLE_DEVICES=0,1,2,3 \\\n      -e CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n      -e VLLM_USE_TRITON_FLASH_ATTN=0 \\\n      -e PYTORCH_TUNABLEOP_ENABLED=1 \\\n      -e HSA_OVERRIDE_GFX_VERSION=11.0.0 \\\n      -e PYTORCH_ROCM_ARCH=\"gfx1100\" \\\n      -e GPU_MAX_HW_QUEUES=1 \\\n      -v /home/ubuntu/vllm_models:/workspace/models \\\n      rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702 bash\n\napt update &amp;&amp; apt install -y git build-essential\n\npip install ninja\n\npip3 install -U xformers --index-url [https://download.pytorch.org/whl/rocm6.3](https://download.pytorch.org/whl/rocm6.3)\n\n    vllm serve /workspace/models/DeepSeek-R1-Distill-Qwen-14B/ \\\n      --dtype float16 \\\n      --kv-cache-dtype auto \\\n      --tensor-parallel-size 2 \\\n      --trust-remote-code \\\n      --tokenizer_mode auto \\\n      --port 8000 \\\n      --host 0.0.0.0 \n\nHave tried different models, some may go to a point where vllm says \"started and waiting\" but then when trying to chat with it all crashes.\n\nHow this is so hard? What is AMD doing for this? Or are we dummer meant to fall back to Ollama? AMD makes me very sad.\n\nEDIT: I trusted you AMD, I really did....",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting a model run with vLLM and 7900 XTX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m75i0b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753272524,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753266053,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI have 2x 7900 XTX and not getting any model run with them in a docker.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker pull rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702\n\ndocker run -it \\\n  --dns=1.1.1.1 \\\n  --dns=8.8.8.8 \\\n  --network=host \\\n  --group-add=video \\\n  --ipc=host \\\n  --cap-add=SYS_PTRACE \\\n  --security-opt seccomp=unconfined \\\n  --privileged \\\n  --device /dev/kfd \\\n  --device /dev/dri \\\n  -e ROCM_VISIBLE_DEVICES=0,1,2,3 \\\n  -e HIP_VISIBLE_DEVICES=0,1,2,3 \\\n  -e CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n  -e VLLM_USE_TRITON_FLASH_ATTN=0 \\\n  -e PYTORCH_TUNABLEOP_ENABLED=1 \\\n  -e HSA_OVERRIDE_GFX_VERSION=11.0.0 \\\n  -e PYTORCH_ROCM_ARCH=&amp;quot;gfx1100&amp;quot; \\\n  -e GPU_MAX_HW_QUEUES=1 \\\n  -v /home/ubuntu/vllm_models:/workspace/models \\\n  rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702 bash\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;apt update &amp;amp;&amp;amp; apt install -y git build-essential&lt;/p&gt;\n\n&lt;p&gt;pip install ninja&lt;/p&gt;\n\n&lt;p&gt;pip3 install -U xformers --index-url &lt;a href=\"https://download.pytorch.org/whl/rocm6.3\"&gt;https://download.pytorch.org/whl/rocm6.3&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;vllm serve /workspace/models/DeepSeek-R1-Distill-Qwen-14B/ \\\n  --dtype float16 \\\n  --kv-cache-dtype auto \\\n  --tensor-parallel-size 2 \\\n  --trust-remote-code \\\n  --tokenizer_mode auto \\\n  --port 8000 \\\n  --host 0.0.0.0 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Have tried different models, some may go to a point where vllm says &amp;quot;started and waiting&amp;quot; but then when trying to chat with it all crashes.&lt;/p&gt;\n\n&lt;p&gt;How this is so hard? What is AMD doing for this? Or are we dummer meant to fall back to Ollama? AMD makes me very sad.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I trusted you AMD, I really did....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m75i0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753266053,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, i was scrolling on LM studio and always saw model like \"model_name_q4_k_m.gguf\" everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some \"dynamic quantization\" but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?\n\nthank by advance ",
          "author_fullname": "t2_jgegifux8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does the _K _S _M _L mean behind the quantization of a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6tbhm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753226118,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, i was scrolling on LM studio and always saw model like &amp;quot;model_name_q4_k_m.gguf&amp;quot; everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some &amp;quot;dynamic quantization&amp;quot; but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?&lt;/p&gt;\n\n&lt;p&gt;thank by advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6tbhm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hurtcraft01",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753226118,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/ikawrakow/ik\\_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp)\n\nFriendly reminder to back up all the things!",
          "author_fullname": "t2_8u7n5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The ik_llama.cpp repository is back! \\o/",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6cfzi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 204,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 204,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186412,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?auto=webp&amp;s=7c74a86a8d22a1d2e90ce704f456a5a36cf050e7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9057a5a31598407ca7946c278de43e70cf0c9ed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=533c07c0d65f89514a6ba54ce5f1c6649e969c77",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9699bd0efa2b26a1c034cdb0fe8abc1317589b6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=355e07ff2e46e3a253b40e25c06644c7282af5b2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c7a47a3cb456bb05dfd53a716bae5ef6addff5e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fbd5e6235d1a60da5c17ef35a7bd40a655c4d80",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6cfzi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thireus",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753186412,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nws8wbg62",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Demis Hassabis @ Lex Fridman Podcast: Round 2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7kbeq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-HzgcbRXUK8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Demis Hassabis: Future of AI, Simulating Reality, Physics and Video Games | Lex Fridman Podcast #475\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Demis Hassabis: Future of AI, Simulating Reality, Physics and Video Games | Lex Fridman Podcast #475",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-HzgcbRXUK8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Demis Hassabis: Future of AI, Simulating Reality, Physics and Video Games | Lex Fridman Podcast #475\"&gt;&lt;/iframe&gt;",
              "author_name": "Lex Fridman",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/-HzgcbRXUK8/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@lexfridman"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-HzgcbRXUK8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Demis Hassabis: Future of AI, Simulating Reality, Physics and Video Games | Lex Fridman Podcast #475\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m7kbeq",
            "height": 200
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8ON0f1m04iK5iw4ZK4CmYbXXuqniiNO62KAiG8HMeK4.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=0c34e2b9f2f8a2d5f27a38ff318166faa10fe78f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753302612,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/-HzgcbRXUK8?si=I0tQridjW4EgudmF",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8ON0f1m04iK5iw4ZK4CmYbXXuqniiNO62KAiG8HMeK4.jpeg?auto=webp&amp;s=6db70cb32acb3459f7eda639463a3e66788dfed4",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8ON0f1m04iK5iw4ZK4CmYbXXuqniiNO62KAiG8HMeK4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6f84c47558f9e99eb805fa614504d07853e00a6",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/8ON0f1m04iK5iw4ZK4CmYbXXuqniiNO62KAiG8HMeK4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6315ac3880926810c85d05f2e6f10ec6061b177",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/8ON0f1m04iK5iw4ZK4CmYbXXuqniiNO62KAiG8HMeK4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=657262b4e1a8aba19a21ec01b4bb201b804e7901",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "8ON0f1m04iK5iw4ZK4CmYbXXuqniiNO62KAiG8HMeK4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7kbeq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tassa-yoniso-manasi",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7kbeq/demis_hassabis_lex_fridman_podcast_round_2/",
          "stickied": false,
          "url": "https://youtu.be/-HzgcbRXUK8?si=I0tQridjW4EgudmF",
          "subreddit_subscribers": 503516,
          "created_utc": 1753302612,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Demis Hassabis: Future of AI, Simulating Reality, Physics and Video Games | Lex Fridman Podcast #475",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-HzgcbRXUK8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Demis Hassabis: Future of AI, Simulating Reality, Physics and Video Games | Lex Fridman Podcast #475\"&gt;&lt;/iframe&gt;",
              "author_name": "Lex Fridman",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/-HzgcbRXUK8/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@lexfridman"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday's release! I'll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3\\_K\\_L is runnable (though slow) for anybody with similar PC specs and produces something usable!\n\nI used LM Studio for loading the model:\n\n* Context: 4096 (default)\n* GPU Offload: 18 / 94\n* CPU Thread Pool: 16\n* ... all else default besides ...\n* Flash Attention: On\n\nWhen loaded, it used up 23.3GB of VRAM and \\~80GB of RAM.\n\nBasic Generation stats: 5.52 tok/sec ‚Ä¢ 2202 tokens ‚Ä¢ 0.18s to first token",
          "author_fullname": "t2_8l0jj9jq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ct7u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 698,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1x5u9hrp5fef1/DASHPlaylist.mpd?a=1755911493%2CM2JkMTU0MDhiOTM0NzYxOGUxZDlmODcyMGQ0MTIyY2I4OTgwYzRhMDM5MGRmNDk2OTYzY2Q2OGRmNTAwY2JlYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/1x5u9hrp5fef1/HLSPlaylist.m3u8?a=1755911493%2CMTQwOTQ1M2M4NjRjOWM0NzA5MGQ0ZGUwZWJiZmIxMWQ0NzYwNDA4NDIxMWJkOTgzYTNiNjQ4ZDg0NWM5NGU0NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=448f64b1e900a0ecbdc8a71bf39468b788eff73b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753187462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday&amp;#39;s release! I&amp;#39;ll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3_K_L is runnable (though slow) for anybody with similar PC specs and produces something usable!&lt;/p&gt;\n\n&lt;p&gt;I used LM Studio for loading the model:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Context: 4096 (default)&lt;/li&gt;\n&lt;li&gt;GPU Offload: 18 / 94&lt;/li&gt;\n&lt;li&gt;CPU Thread Pool: 16&lt;/li&gt;\n&lt;li&gt;... all else default besides ...&lt;/li&gt;\n&lt;li&gt;Flash Attention: On&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When loaded, it used up 23.3GB of VRAM and ~80GB of RAM.&lt;/p&gt;\n\n&lt;p&gt;Basic Generation stats: 5.52 tok/sec ‚Ä¢ 2202 tokens ‚Ä¢ 0.18s to first token&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/1x5u9hrp5fef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?format=pjpg&amp;auto=webp&amp;s=58a925e2785f62712af69dad90636ab48df32160",
                  "width": 480,
                  "height": 698
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=74909995fdb7a4a31d72b707fc5a6406503d7c48",
                    "width": 108,
                    "height": 157
                  },
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a8bc0a3ee9ac60bd5820c383ea906cb68b5b9d1d",
                    "width": 216,
                    "height": 314
                  },
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4b70a04846a7db43f33c721283fb6e592dd8d570",
                    "width": 320,
                    "height": 465
                  }
                ],
                "variants": {},
                "id": "MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ct7u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aidanjustsayin",
          "discussion_type": null,
          "num_comments": 78,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/",
          "stickied": false,
          "url": "https://v.redd.it/1x5u9hrp5fef1",
          "subreddit_subscribers": 503516,
          "created_utc": 1753187462,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 698,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1x5u9hrp5fef1/DASHPlaylist.mpd?a=1755911493%2CM2JkMTU0MDhiOTM0NzYxOGUxZDlmODcyMGQ0MTIyY2I4OTgwYzRhMDM5MGRmNDk2OTYzY2Q2OGRmNTAwY2JlYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/1x5u9hrp5fef1/HLSPlaylist.m3u8?a=1755911493%2CMTQwOTQ1M2M4NjRjOWM0NzA5MGQ0ZGUwZWJiZmIxMWQ0NzYwNDA4NDIxMWJkOTgzYTNiNjQ4ZDg0NWM5NGU0NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone,\n\nI have just installed Ollama with Llama3:8b, i make prompts via the backend of my website with ajax requests.\n\nI have a list of 10000 french words \"maison, femme, cuisine...\" and i would like to translate them into 30 other languages, and get declensions (\"la cuisine, les cuisine, une cuisine...\") and definitions of these words.\n\nI am having a hard time to get what I want, most of the time because llama gives an incorrect translation, and incorrect declension or even gives the word in an incorrect language. Sometimes it give the exact response, as expected, but when i execute the same prompt again I have totally different results.\n\nI spent almost 1 week now tweaking the parameters of the prompt, and as a beginner with AI, at this point I am wondering if llama3:8b is the proper tools to achieve my goals  \n  \nWould you advise me another tool maybe? Is there a trick to have correct responses with consistency?\n\nDo you have other advice for the beginner I am please?\n\nAlso, I would like to buy a laptop dedicated to AI, do you think 128GB RAM is enough?",
          "author_fullname": "t2_bwf9p4oxc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "beginner with llama3, I cannot get results I want",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7cklb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753285105,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I have just installed Ollama with Llama3:8b, i make prompts via the backend of my website with ajax requests.&lt;/p&gt;\n\n&lt;p&gt;I have a list of 10000 french words &amp;quot;maison, femme, cuisine...&amp;quot; and i would like to translate them into 30 other languages, and get declensions (&amp;quot;la cuisine, les cuisine, une cuisine...&amp;quot;) and definitions of these words.&lt;/p&gt;\n\n&lt;p&gt;I am having a hard time to get what I want, most of the time because llama gives an incorrect translation, and incorrect declension or even gives the word in an incorrect language. Sometimes it give the exact response, as expected, but when i execute the same prompt again I have totally different results.&lt;/p&gt;\n\n&lt;p&gt;I spent almost 1 week now tweaking the parameters of the prompt, and as a beginner with AI, at this point I am wondering if llama3:8b is the proper tools to achieve my goals  &lt;/p&gt;\n\n&lt;p&gt;Would you advise me another tool maybe? Is there a trick to have correct responses with consistency?&lt;/p&gt;\n\n&lt;p&gt;Do you have other advice for the beginner I am please?&lt;/p&gt;\n\n&lt;p&gt;Also, I would like to buy a laptop dedicated to AI, do you think 128GB RAM is enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7cklb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FckGAFA",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753285105,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title - the goal is to be able to run 70b models for free using p2p sharding like BitTorrent. Have a lil node network!\n\nAnyone building in rust/wasm?? I‚Äôm a python / ts dev at heart so it‚Äôs going to be a steep learning curve! ",
          "author_fullname": "t2_d7sagw09",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a p2p inference engine in rust and hugging face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m775h2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753271484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title - the goal is to be able to run 70b models for free using p2p sharding like BitTorrent. Have a lil node network!&lt;/p&gt;\n\n&lt;p&gt;Anyone building in rust/wasm?? I‚Äôm a python / ts dev at heart so it‚Äôs going to be a steep learning curve! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m775h2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "earningtheewage",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m775h2/building_a_p2p_inference_engine_in_rust_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m775h2/building_a_p2p_inference_engine_in_rust_and/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753271484,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?",
          "author_fullname": "t2_10rx6s0f1q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone tried Hierarchical Reasoning Models yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ufm4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753229040,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6ufm4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackboulder33",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753229040,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm trying to understand how models like **Gemini 2.5 Pro** achieve *native* 1 million token context windows.\n\nFrom what I‚Äôve seen in models like **Qwen3** or **LLaMA**, they use techniques like **RoPE scaling** (e.g., YaRN, NTK-aware RoPE, Position Interpolation) to extrapolate context beyond what was trained. These methods usually need fine-tuning, and even then, there's often a soft limit beyond which attention weakens significantly.\n\nBut Gemini claims *native* 1M context, and benchmarks (like Needle-in-a-Haystack, RULER) suggest it actually performs well across that full range. So my questions are:\n\n* Does Gemini use **YaRN** or **RoPE scaling** internally?\n* Is it trained from scratch with 1M tokens per sequence (i.e., truly native)?\n* Or is it just doing **clever chunking** or sparse attention under the hood (e.g., blockwise, ring attention)?\n* Does it use **ALiBi** or some modified positional encoding to stabilize long contexts?\n\nIf anyone has insight from papers, leaks, logs, or architecture details, I'd love to learn more.  \nEven speculation grounded in similar architectures is welcome.",
          "author_fullname": "t2_1qyykcj4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does Gemini 2.5 Pro natively support 1M tokens of context? Is it using YaRN, or some kind of disguised chunking?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6xbru",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753237168,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm trying to understand how models like &lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt; achieve &lt;em&gt;native&lt;/em&gt; 1 million token context windows.&lt;/p&gt;\n\n&lt;p&gt;From what I‚Äôve seen in models like &lt;strong&gt;Qwen3&lt;/strong&gt; or &lt;strong&gt;LLaMA&lt;/strong&gt;, they use techniques like &lt;strong&gt;RoPE scaling&lt;/strong&gt; (e.g., YaRN, NTK-aware RoPE, Position Interpolation) to extrapolate context beyond what was trained. These methods usually need fine-tuning, and even then, there&amp;#39;s often a soft limit beyond which attention weakens significantly.&lt;/p&gt;\n\n&lt;p&gt;But Gemini claims &lt;em&gt;native&lt;/em&gt; 1M context, and benchmarks (like Needle-in-a-Haystack, RULER) suggest it actually performs well across that full range. So my questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Does Gemini use &lt;strong&gt;YaRN&lt;/strong&gt; or &lt;strong&gt;RoPE scaling&lt;/strong&gt; internally?&lt;/li&gt;\n&lt;li&gt;Is it trained from scratch with 1M tokens per sequence (i.e., truly native)?&lt;/li&gt;\n&lt;li&gt;Or is it just doing &lt;strong&gt;clever chunking&lt;/strong&gt; or sparse attention under the hood (e.g., blockwise, ring attention)?&lt;/li&gt;\n&lt;li&gt;Does it use &lt;strong&gt;ALiBi&lt;/strong&gt; or some modified positional encoding to stabilize long contexts?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If anyone has insight from papers, leaks, logs, or architecture details, I&amp;#39;d love to learn more.&lt;br/&gt;\nEven speculation grounded in similar architectures is welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6xbru",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ranteck",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753237168,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just successfully died in some scenarios with gemini and it still resisted to show me its system prompt. Is there any trick?",
          "author_fullname": "t2_1oeu2j1o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people extracting system prompts?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7av4q",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753281173,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just successfully died in some scenarios with gemini and it still resisted to show me its system prompt. Is there any trick?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7av4q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "freecodeio",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7av4q/how_are_people_extracting_system_prompts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7av4q/how_are_people_extracting_system_prompts/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753281173,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm trying to get an LLM to analyze a bunch of documents (around 30 PDFs or TXT files), but I‚Äôm running into some issues. These are pretty sensitive communications, so keeping everything local is a must ‚Äì no sending them off to online services!\n\nI've been playing around with LM Studio, but it seems like it can only handle a few files at a time. It processes 2 or 3 PDFs, grabs some info from them, and then just stops. I really want the LLM to look at all my documents every time I ask it something, re-checking everything as needed.  I'm not worried about how long it takes to respond ‚Äì I just need it to be thorough.\n\nDoes anyone have any suggestions for other local LLM tools that can handle a larger document set? Something that doesn‚Äôt get overwhelmed by 30 files. Or, are there any online LLM services out there that actually guarantee data privacy and security?  I'm looking for something more than just the usual \"we protect your data\" ‚Äì I need real assurances.\n\nAny advice would be appreciated!   \nThanks",
          "author_fullname": "t2_1bvl9ir2ne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need Help - Local LLM &amp; Lots of Files! (Privacy Concerns)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m73q8n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753259199,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to get an LLM to analyze a bunch of documents (around 30 PDFs or TXT files), but I‚Äôm running into some issues. These are pretty sensitive communications, so keeping everything local is a must ‚Äì no sending them off to online services!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing around with LM Studio, but it seems like it can only handle a few files at a time. It processes 2 or 3 PDFs, grabs some info from them, and then just stops. I really want the LLM to look at all my documents every time I ask it something, re-checking everything as needed.  I&amp;#39;m not worried about how long it takes to respond ‚Äì I just need it to be thorough.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any suggestions for other local LLM tools that can handle a larger document set? Something that doesn‚Äôt get overwhelmed by 30 files. Or, are there any online LLM services out there that actually guarantee data privacy and security?  I&amp;#39;m looking for something more than just the usual &amp;quot;we protect your data&amp;quot; ‚Äì I need real assurances.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated!&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m73q8n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AreBee73",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753259199,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys I was recently looking at this paper, which mentions that finetuning models on even benign datasets (both full FT and LoRA) can cause safety regressions : [https://arxiv.org/abs/2310.03693](https://arxiv.org/abs/2310.03693) \n\nHave you ever observed a model getting less safe / more likely to respond to off-limits prompts after fine-tuning it, even though you fine-tuned it on clean, benign data? I'm interested if this happens in real world use cases or if it's just a research artifact.",
          "author_fullname": "t2_3r8rnyl2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone seen safety regressions after fine-tuning LLaMA or Mistral on clean data?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m73n0t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753258836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys I was recently looking at this paper, which mentions that finetuning models on even benign datasets (both full FT and LoRA) can cause safety regressions : &lt;a href=\"https://arxiv.org/abs/2310.03693\"&gt;https://arxiv.org/abs/2310.03693&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Have you ever observed a model getting less safe / more likely to respond to off-limits prompts after fine-tuning it, even though you fine-tuned it on clean, benign data? I&amp;#39;m interested if this happens in real world use cases or if it&amp;#39;s just a research artifact.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m73n0t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "whalefal",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73n0t/anyone_seen_safety_regressions_after_finetuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73n0t/anyone_seen_safety_regressions_after_finetuning/",
          "subreddit_subscribers": 503516,
          "created_utc": 1753258836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We had a lot of posts about the updated [235b model](https://x.com/Alibaba_Qwen/status/1947344511988076547) and the [Unsloth quants](https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF). I tested it with my Mac Studio and decided to merge the Q3 K XL ggufs and upload them to Ollama in case someone es might find this useful.\n\nRuns great with up to 18 tokens per second and consuming 108 to 117 GB VRAM.\n\n[More details on the Ollama library page](https://ollama.com/awaescher/qwen3-235b-2507-unsloth-q3-k-xl), performance benchmarks included.",
          "author_fullname": "t2_1gpif4cz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The LLM for M4 Max 128GB: Unsloth Qwen3-235B-A22B-Instruct-2507 Q3 K XL for Ollama",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ocfd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/FGAC4_FgxMG-7e9w8V8PkDHHC0Spkue03KT-5Vo9mU4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753214166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We had a lot of posts about the updated &lt;a href=\"https://x.com/Alibaba_Qwen/status/1947344511988076547\"&gt;235b model&lt;/a&gt; and the &lt;a href=\"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF\"&gt;Unsloth quants&lt;/a&gt;. I tested it with my Mac Studio and decided to merge the Q3 K XL ggufs and upload them to Ollama in case someone es might find this useful.&lt;/p&gt;\n\n&lt;p&gt;Runs great with up to 18 tokens per second and consuming 108 to 117 GB VRAM.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ollama.com/awaescher/qwen3-235b-2507-unsloth-q3-k-xl\"&gt;More details on the Ollama library page&lt;/a&gt;, performance benchmarks included.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/y3x24rxqchef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/y3x24rxqchef1.png?auto=webp&amp;s=8871c292b16bce4a1a3ebad50bdc70a4755edfb1",
                  "width": 1119,
                  "height": 699
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e41bb7b82dd23ca399246b0ad273bfca55313312",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8196b3b26f6cebea121b85e8d13e70ccede750b",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=71b905987f840aecad2d4d33ebe4b67e00e55446",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c4dd5da6091ae77e58d63dfd95935c34e266d7e",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d2b6677488033099a15dec1df1c7088540544b7",
                    "width": 960,
                    "height": 599
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96a500d949b1758bb6f522dd4c3912c8e4c57f64",
                    "width": 1080,
                    "height": 674
                  }
                ],
                "variants": {},
                "id": "65hJ7hQzTtv2DTpl4kNAtfCBSaTl0aIGG0bqSxzPvcM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6ocfd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "waescher",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ocfd/the_llm_for_m4_max_128gb_unsloth/",
          "stickied": false,
          "url": "https://i.redd.it/y3x24rxqchef1.png",
          "subreddit_subscribers": 503516,
          "created_utc": 1753214166,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!\\_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We‚Äôre running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.\n\nRight now, we‚Äôre testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:\n\n* Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?\n* Anyone running into issues with NUMA alignment or memory pressure in 96‚Äì128GB DDR5 systems when training on large batches?\n* How do you ensure smooth RAM ‚Üí VRAM feeding at 5090 throughput without overloading I/O threads?\n\nWould love to compare notes ‚Äî especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We‚Äôre deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM ‚Üí GPU VRAM would be killer.",
          "author_fullname": "t2_1tltnwoxsz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people staging AI training datasets from NVMe ‚Üí DDR5 ‚Üí GPU VRAM for fine-tuning on RTX 5090s?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6vj8o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755911493%2CNzY3M2FkYWYxMzI2ZTllZWVjMzQ3NWNiYzg4ZDYwYTNiY2VmMDk0ZDA3NzdiMzJiMjZmMjY3ZGY2YzNkZjU5Mg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755911493%2CYjhjYmZhNzRjMDIyMmFjZmZiMGVkNjkwOGU2MTkxMGFhMjAwYTI1ODQ1YTYwYmM4MTNiMTZiZWIxY2MzNmFkNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=7fa7bed1c348998994fed16cd386547e5aac176b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753232111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We‚Äôre running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.&lt;/p&gt;\n\n&lt;p&gt;Right now, we‚Äôre testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?&lt;/li&gt;\n&lt;li&gt;Anyone running into issues with NUMA alignment or memory pressure in 96‚Äì128GB DDR5 systems when training on large batches?&lt;/li&gt;\n&lt;li&gt;How do you ensure smooth RAM ‚Üí VRAM feeding at 5090 throughput without overloading I/O threads?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to compare notes ‚Äî especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We‚Äôre deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM ‚Üí GPU VRAM would be killer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/m3v13th5vief1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?format=pjpg&amp;auto=webp&amp;s=8a06bae144080a3451d0fb255b750bcab9e21c69",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=930f76891585f27565f3d929f2d1d4df9fbbe6f7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c2cc4160f0d866354b83bad0ce200177193907cd",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=337db9c93858d2e6c9db6e22822d525e7600240d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1915548352adb0259f40c35397f4626912fc93d4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e2a49235d59469a5f29b50b3c42efc9cc7f4d39",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81350e82df349faf24b9ef86ecafb7b97303ebd3",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6vj8o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DJAI9LAB",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vj8o/how_are_people_staging_ai_training_datasets_from/",
          "stickied": false,
          "url": "https://v.redd.it/m3v13th5vief1",
          "subreddit_subscribers": 503516,
          "created_utc": 1753232111,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755911493%2CNzY3M2FkYWYxMzI2ZTllZWVjMzQ3NWNiYzg4ZDYwYTNiY2VmMDk0ZDA3NzdiMzJiMjZmMjY3ZGY2YzNkZjU5Mg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755911493%2CYjhjYmZhNzRjMDIyMmFjZmZiMGVkNjkwOGU2MTkxMGFhMjAwYTI1ODQ1YTYwYmM4MTNiMTZiZWIxY2MzNmFkNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      }
    ],
    "before": null
  }
}