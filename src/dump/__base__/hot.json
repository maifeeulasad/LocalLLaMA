{
  "kind": "Listing",
  "data": {
    "after": "t3_1lsq2m3",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was working on a [research](https://www.designarena.ai/) project (note that the votes and data is completely free and open, so not profiting off this, but just showing research as context) where users write a prompt, and then vote on content generated (e.g. websites, games, 3D visualizations) from 4 randomly generated models each. Note that when [voting](https://www.designarena.ai/vote), model names are hidden, so people don't immediately know which models generated what. \n\nFrom the data collected so far, Llama 4 Maverick is 19th and Llama 4 Scout is 23rd. On the other extreme, Claude and Deepseek are taking up most of the spots in the top 10 while Mistral and Grok have been surprising dark horses. \n\nAnything surprise you here? What models have you noticed been the best for UI/UX and frontend development? ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 57,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4dvcg5t2vcbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 44,
                  "x": 108,
                  "u": "https://preview.redd.it/4dvcg5t2vcbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=194c2718bcea83cd83000270555d9c7146124fd9"
                },
                {
                  "y": 89,
                  "x": 216,
                  "u": "https://preview.redd.it/4dvcg5t2vcbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=842167269b6c29f1d38ee31b7646452f9c11ca4d"
                },
                {
                  "y": 131,
                  "x": 320,
                  "u": "https://preview.redd.it/4dvcg5t2vcbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f50e3c44b76f102fae7967436a8882f6ae1d85cf"
                },
                {
                  "y": 263,
                  "x": 640,
                  "u": "https://preview.redd.it/4dvcg5t2vcbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27d34b01b0e5f02bb43cba8e41399df55825391b"
                },
                {
                  "y": 395,
                  "x": 960,
                  "u": "https://preview.redd.it/4dvcg5t2vcbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d79294115167b767b1cc31f2b9343848ca354c2f"
                },
                {
                  "y": 445,
                  "x": 1080,
                  "u": "https://preview.redd.it/4dvcg5t2vcbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=458cf1e010ac4847c708c30134c102db2713138f"
                }
              ],
              "s": {
                "y": 1128,
                "x": 2736,
                "u": "https://preview.redd.it/4dvcg5t2vcbf1.png?width=2736&amp;format=png&amp;auto=webp&amp;s=a0c60a48447d75fb57f45e2bd2cc032539ec0090"
              },
              "id": "4dvcg5t2vcbf1"
            },
            "n0u7huiovcbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/n0u7huiovcbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b75bbe5b0686af8ed41a6a491dc9bc2d52d32184"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/n0u7huiovcbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=73b3be1559e3ef7c3f4603ee2bf4dff475a19b55"
                },
                {
                  "y": 185,
                  "x": 320,
                  "u": "https://preview.redd.it/n0u7huiovcbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=40ae9695bc1269a2e04a386ce0025f37e3c887ee"
                },
                {
                  "y": 371,
                  "x": 640,
                  "u": "https://preview.redd.it/n0u7huiovcbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=12e3d5bce8a991befed7f1e9f4898963499ada4a"
                },
                {
                  "y": 556,
                  "x": 960,
                  "u": "https://preview.redd.it/n0u7huiovcbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e6825321e4af272d097398c834cb7de67676a80"
                },
                {
                  "y": 626,
                  "x": 1080,
                  "u": "https://preview.redd.it/n0u7huiovcbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eaf0268d4271c43fda1a54cd5bcdf1a0940a2b63"
                }
              ],
              "s": {
                "y": 1542,
                "x": 2658,
                "u": "https://preview.redd.it/n0u7huiovcbf1.png?width=2658&amp;format=png&amp;auto=webp&amp;s=301b623986fdee81705deb44c59fa48553c3801f"
              },
              "id": "n0u7huiovcbf1"
            }
          },
          "name": "t3_1lthtbn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 121,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4dvcg5t2vcbf1",
                "id": 700330534
              },
              {
                "media_id": "n0u7huiovcbf1",
                "id": 700330535
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 121,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YH48KR3uSeLFmipFEDX0ai8FZ4TwQmmccEKSaaUx3Fk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751852148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was working on a &lt;a href=\"https://www.designarena.ai/\"&gt;research&lt;/a&gt; project (note that the votes and data is completely free and open, so not profiting off this, but just showing research as context) where users write a prompt, and then vote on content generated (e.g. websites, games, 3D visualizations) from 4 randomly generated models each. Note that when &lt;a href=\"https://www.designarena.ai/vote\"&gt;voting&lt;/a&gt;, model names are hidden, so people don&amp;#39;t immediately know which models generated what. &lt;/p&gt;\n\n&lt;p&gt;From the data collected so far, Llama 4 Maverick is 19th and Llama 4 Scout is 23rd. On the other extreme, Claude and Deepseek are taking up most of the spots in the top 10 while Mistral and Grok have been surprising dark horses. &lt;/p&gt;\n\n&lt;p&gt;Anything surprise you here? What models have you noticed been the best for UI/UX and frontend development? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lthtbn",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lthtbn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 73,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lthtbn",
          "subreddit_subscribers": 495395,
          "created_utc": 1751852148,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "***TLDR***: VSCode + RooCode + LM Studio + Devstral + snowflake-arctic-embed2 + docs-mcp-server. A fast, cost-free, self-hosted AI coding assistant setup supports lesser-used languages and minimizes hallucinations on less powerful hardware.\n\n**Long Post:**\n\nHello everyone, sharing my findings on trying to find a self-hosted agentic AI coding assistant that:\n\n1. Responds reasonably well on a variety of hardware.\n2. Doesn’t hallucinate outdated syntax.\n3. Costs $0 (except electricity).\n4. Understands less common languages, e.g., KQL, Flutter, etc.\n\nAfter experimenting with several setups, here’s the combo I found that actually works.  \nPlease forgive any mistakes and feel free to let me know of any improvements you are aware of.\n\n**Hardware**  \nTested on a Ryzen 5700 + RTX 3080 (10GB VRAM), 48GB RAM.  \nShould work on both low, and high-end setups, your mileage may vary.\n\n**The Stack**\n\n`VSCode +(with) RooCode +(connected to) LM Studio +(running both) Devstral +(and) snowflake-arctic-embed2 +(supported by) docs-mcp-server`\n\n\\---\n\n**Edit 1:** Setup Process for users saying this is too complicated\n\n1. Install `VSCode` then get `RooCode` Extension\n2. Install `LMStudio` and pull `snowflake-arctic-embed2`  embeddings model, as well as `Devstral` large language model which suits your computer.\n3. Install `Docker` or `NodeJS`, depending on which config you prefer *(recommend Docker)*\n4. Include `docs-mcp-server` in your RooCode MCP configuration *(see json below)*\n\n**Edit 2**: I had been [misinformed](https://docs.useanything.com/setup/embedder-configuration/local/lmstudio) that running embeddings and LLM together via LM Studio is not possible, it certainly is! I have updated this guide to remove Ollama altogether and only use LM Studio.\n\nLM Studio made it slightly confusing because you cannot load embeddings model from \"Chat\" tab, you must navigate to \"Power User\" tab.\n\n\\---\n\n**VSCode + RooCode**  \nRooCode is a VS Code extension that enables agentic coding and has MCP support.\n\nVS Code: [https://code.visualstudio.com/download](https://code.visualstudio.com/download)  \nAlternative - VSCodium: [https://github.com/VSCodium/vscodium/releases](https://github.com/VSCodium/vscodium/releases) \\- No telemetry\n\nRooCode: [https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline)\n\nAlternative to this setup is Zed Editor: [https://zed.dev/download](https://zed.dev/download)\n\n( Zed is nice, but you cannot yet pass problems as context. Released only for MacOS and Linux, coming soon for windows. Unofficial windows nightly here: [github.com/send-me-a-ticket/zedforwindows](https://github.com/send-me-a-ticket/zedforwindows) )\n\n**LM Studio**  \n[https://lmstudio.ai/download](https://lmstudio.ai/download)\n\n* Nice UI with real-time logs\n* GPU offloading is too simple. Changing AI model parameters is a breeze. You can achieve same effect in ollama by creating custom models with changed num\\_gpu and num\\_ctx parameters\n* Good (better?) OpenAI-compatible API\n\n**Devstral (Unsloth finetune)**  \nSolid coding model with good tool usage.\n\nI use `devstral-small-2505@iq2_m`, which fully fits within 10GB VRAM. token context 32768.  \nOther variants &amp; parameters may work depending on your hardware.\n\n**snowflake-arctic-embed2**  \nTiny embeddings model used with docs-mcp-server. Feel free to substitute for any better ones.  \nI use `text-embedding-snowflake-arctic-embed-l-v2.0`\n\n**Docker**  \n[https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)  \nRecommend Docker use instead of NPX, for security and ease of use.\n\nPortainer is my recommended extension for ease of use:  \n[https://hub.docker.com/extensions/portainer/portainer-docker-extension](https://hub.docker.com/extensions/portainer/portainer-docker-extension)\n\n**docs-mcp-server**  \n[https://github.com/arabold/docs-mcp-server](https://github.com/arabold/docs-mcp-server)\n\nThis is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the *correct* syntax for *your* version of language implementation, and avoid hallucinations.\n\nYou *should* also be able to run `localhost:6281` to open web UI for the `docs-mcp-server`, however web UI doesn't seem to be working for me, which I can ignore because AI is managing that anyway.\n\nYou can implement this MCP server as following -\n\n*Docker version (needs Docker Installed)*\n\n    {\n      \"mcpServers\": {\n        \"docs-mcp-server\": {\n          \"command\": \"docker\",\n          \"args\": [\n            \"run\",\n            \"-i\",\n            \"--rm\",\n            \"-p\",\n            \"6280:6280\",\n            \"-p\",\n            \"6281:6281\",\n            \"-e\",\n            \"OPENAI_API_KEY\",\n            \"-e\",\n            \"OPENAI_API_BASE\",\n            \"-e\",\n            \"DOCS_MCP_EMBEDDING_MODEL\",\n            \"-v\",\n            \"docs-mcp-data:/data\",\n            \"ghcr.io/arabold/docs-mcp-server:latest\"\n          ],\n          \"env\": {\n            \"OPENAI_API_KEY\": \"ollama\",\n            \"OPENAI_API_BASE\": \"http://host.docker.internal:11434/v1\",\n            \"DOCS_MCP_EMBEDDING_MODEL\": \"snowflake-arctic-embed2\"\n          }\n        }\n      }\n    }\n\n*NPX version (needs NodeJS installed)*\n\n    {\n      \"mcpServers\": {\n        \"docs-mcp-server\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"@arabold/docs-mcp-server@latest\"\n          ],\n          \"env\": {\n            \"OPENAI_API_KEY\": \"ollama\",\n            \"OPENAI_API_BASE\": \"http://host.docker.internal:11434/v1\",\n            \"DOCS_MCP_EMBEDDING_MODEL\": \"snowflake-arctic-embed2\"\n          }\n        }\n      }\n    }\n\n**Adding documentation for your language**\n\nAsk AI to use the `scrape_docs`  tool with:\n\n* **url** (link to the documentation),\n* **library** (name of the documentation/programming language),\n* **version** (version of the documentation)\n\nyou can also provide (optional):\n\n* **maxPages** (maximum number of pages to scrape, default is 1000).\n* **maxDepth** (maximum navigation depth, default is 3).\n* **scope** (crawling boundary, which can be 'subpages', 'hostname', or 'domain', default is 'subpages').\n* **followRedirects** (whether to follow HTTP 3xx redirects, default is true).\n\nYou can ask AI to use **search\\_docs** tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.\n\nThis stack isn’t limited to coding, Devstral handles logical, non-coding tasks well too.  \nThe MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.\n\n**Thanks for reading... If you have used and/or improved on this, I’d love to hear about it..!**",
          "author_fullname": "t2_toqw74vmo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Self-hosted AI coding that just works",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt4y1z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 431,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 431,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751879600,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751818168,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;TLDR&lt;/em&gt;&lt;/strong&gt;: VSCode + RooCode + LM Studio + Devstral + snowflake-arctic-embed2 + docs-mcp-server. A fast, cost-free, self-hosted AI coding assistant setup supports lesser-used languages and minimizes hallucinations on less powerful hardware.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Long Post:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hello everyone, sharing my findings on trying to find a self-hosted agentic AI coding assistant that:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Responds reasonably well on a variety of hardware.&lt;/li&gt;\n&lt;li&gt;Doesn’t hallucinate outdated syntax.&lt;/li&gt;\n&lt;li&gt;Costs $0 (except electricity).&lt;/li&gt;\n&lt;li&gt;Understands less common languages, e.g., KQL, Flutter, etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;After experimenting with several setups, here’s the combo I found that actually works.&lt;br/&gt;\nPlease forgive any mistakes and feel free to let me know of any improvements you are aware of.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br/&gt;\nTested on a Ryzen 5700 + RTX 3080 (10GB VRAM), 48GB RAM.&lt;br/&gt;\nShould work on both low, and high-end setups, your mileage may vary.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Stack&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;VSCode +(with) RooCode +(connected to) LM Studio +(running both) Devstral +(and) snowflake-arctic-embed2 +(supported by) docs-mcp-server&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt; Setup Process for users saying this is too complicated&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Install &lt;code&gt;VSCode&lt;/code&gt; then get &lt;code&gt;RooCode&lt;/code&gt; Extension&lt;/li&gt;\n&lt;li&gt;Install &lt;code&gt;LMStudio&lt;/code&gt; and pull &lt;code&gt;snowflake-arctic-embed2&lt;/code&gt;  embeddings model, as well as &lt;code&gt;Devstral&lt;/code&gt; large language model which suits your computer.&lt;/li&gt;\n&lt;li&gt;Install &lt;code&gt;Docker&lt;/code&gt; or &lt;code&gt;NodeJS&lt;/code&gt;, depending on which config you prefer &lt;em&gt;(recommend Docker)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Include &lt;code&gt;docs-mcp-server&lt;/code&gt; in your RooCode MCP configuration &lt;em&gt;(see json below)&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit 2&lt;/strong&gt;: I had been &lt;a href=\"https://docs.useanything.com/setup/embedder-configuration/local/lmstudio\"&gt;misinformed&lt;/a&gt; that running embeddings and LLM together via LM Studio is not possible, it certainly is! I have updated this guide to remove Ollama altogether and only use LM Studio.&lt;/p&gt;\n\n&lt;p&gt;LM Studio made it slightly confusing because you cannot load embeddings model from &amp;quot;Chat&amp;quot; tab, you must navigate to &amp;quot;Power User&amp;quot; tab.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;VSCode + RooCode&lt;/strong&gt;&lt;br/&gt;\nRooCode is a VS Code extension that enables agentic coding and has MCP support.&lt;/p&gt;\n\n&lt;p&gt;VS Code: &lt;a href=\"https://code.visualstudio.com/download\"&gt;https://code.visualstudio.com/download&lt;/a&gt;&lt;br/&gt;\nAlternative - VSCodium: &lt;a href=\"https://github.com/VSCodium/vscodium/releases\"&gt;https://github.com/VSCodium/vscodium/releases&lt;/a&gt; - No telemetry&lt;/p&gt;\n\n&lt;p&gt;RooCode: &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline\"&gt;https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Alternative to this setup is Zed Editor: &lt;a href=\"https://zed.dev/download\"&gt;https://zed.dev/download&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;( Zed is nice, but you cannot yet pass problems as context. Released only for MacOS and Linux, coming soon for windows. Unofficial windows nightly here: &lt;a href=\"https://github.com/send-me-a-ticket/zedforwindows\"&gt;github.com/send-me-a-ticket/zedforwindows&lt;/a&gt; )&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://lmstudio.ai/download\"&gt;https://lmstudio.ai/download&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Nice UI with real-time logs&lt;/li&gt;\n&lt;li&gt;GPU offloading is too simple. Changing AI model parameters is a breeze. You can achieve same effect in ollama by creating custom models with changed num_gpu and num_ctx parameters&lt;/li&gt;\n&lt;li&gt;Good (better?) OpenAI-compatible API&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Devstral (Unsloth finetune)&lt;/strong&gt;&lt;br/&gt;\nSolid coding model with good tool usage.&lt;/p&gt;\n\n&lt;p&gt;I use &lt;code&gt;devstral-small-2505@iq2_m&lt;/code&gt;, which fully fits within 10GB VRAM. token context 32768.&lt;br/&gt;\nOther variants &amp;amp; parameters may work depending on your hardware.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;snowflake-arctic-embed2&lt;/strong&gt;&lt;br/&gt;\nTiny embeddings model used with docs-mcp-server. Feel free to substitute for any better ones.&lt;br/&gt;\nI use &lt;code&gt;text-embedding-snowflake-arctic-embed-l-v2.0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://www.docker.com/products/docker-desktop/\"&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;br/&gt;\nRecommend Docker use instead of NPX, for security and ease of use.&lt;/p&gt;\n\n&lt;p&gt;Portainer is my recommended extension for ease of use:&lt;br/&gt;\n&lt;a href=\"https://hub.docker.com/extensions/portainer/portainer-docker-extension\"&gt;https://hub.docker.com/extensions/portainer/portainer-docker-extension&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://github.com/arabold/docs-mcp-server\"&gt;https://github.com/arabold/docs-mcp-server&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the &lt;em&gt;correct&lt;/em&gt; syntax for &lt;em&gt;your&lt;/em&gt; version of language implementation, and avoid hallucinations.&lt;/p&gt;\n\n&lt;p&gt;You &lt;em&gt;should&lt;/em&gt; also be able to run &lt;code&gt;localhost:6281&lt;/code&gt; to open web UI for the &lt;code&gt;docs-mcp-server&lt;/code&gt;, however web UI doesn&amp;#39;t seem to be working for me, which I can ignore because AI is managing that anyway.&lt;/p&gt;\n\n&lt;p&gt;You can implement this MCP server as following -&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Docker version (needs Docker Installed)&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;docs-mcp-server&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;docker&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;run&amp;quot;,\n        &amp;quot;-i&amp;quot;,\n        &amp;quot;--rm&amp;quot;,\n        &amp;quot;-p&amp;quot;,\n        &amp;quot;6280:6280&amp;quot;,\n        &amp;quot;-p&amp;quot;,\n        &amp;quot;6281:6281&amp;quot;,\n        &amp;quot;-e&amp;quot;,\n        &amp;quot;OPENAI_API_KEY&amp;quot;,\n        &amp;quot;-e&amp;quot;,\n        &amp;quot;OPENAI_API_BASE&amp;quot;,\n        &amp;quot;-e&amp;quot;,\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;,\n        &amp;quot;-v&amp;quot;,\n        &amp;quot;docs-mcp-data:/data&amp;quot;,\n        &amp;quot;ghcr.io/arabold/docs-mcp-server:latest&amp;quot;\n      ],\n      &amp;quot;env&amp;quot;: {\n        &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;,\n        &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:11434/v1&amp;quot;,\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;snowflake-arctic-embed2&amp;quot;\n      }\n    }\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;NPX version (needs NodeJS installed)&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;docs-mcp-server&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;@arabold/docs-mcp-server@latest&amp;quot;\n      ],\n      &amp;quot;env&amp;quot;: {\n        &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;,\n        &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:11434/v1&amp;quot;,\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;snowflake-arctic-embed2&amp;quot;\n      }\n    }\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Adding documentation for your language&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Ask AI to use the &lt;code&gt;scrape_docs&lt;/code&gt;  tool with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;url&lt;/strong&gt; (link to the documentation),&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;library&lt;/strong&gt; (name of the documentation/programming language),&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;version&lt;/strong&gt; (version of the documentation)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;you can also provide (optional):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;maxPages&lt;/strong&gt; (maximum number of pages to scrape, default is 1000).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;maxDepth&lt;/strong&gt; (maximum navigation depth, default is 3).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;scope&lt;/strong&gt; (crawling boundary, which can be &amp;#39;subpages&amp;#39;, &amp;#39;hostname&amp;#39;, or &amp;#39;domain&amp;#39;, default is &amp;#39;subpages&amp;#39;).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;followRedirects&lt;/strong&gt; (whether to follow HTTP 3xx redirects, default is true).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can ask AI to use &lt;strong&gt;search_docs&lt;/strong&gt; tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.&lt;/p&gt;\n\n&lt;p&gt;This stack isn’t limited to coding, Devstral handles logical, non-coding tasks well too.&lt;br/&gt;\nThe MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Thanks for reading... If you have used and/or improved on this, I’d love to hear about it..!&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?auto=webp&amp;s=d71804ee4194235942e3d32ce49695af47ae2931",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e20521ee56cbc2d4a70339418c28f6b4802a2591",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f4858df95f192c288a54a807c884a7fca524a58",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb3c47addc61d1754610d9273b828b0a1cc6a3b0",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f198eb5a5cd7d518778530fa8bc8cfcbef25f1bb",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b11538ae52f8e379b9c1af43d0d3b4ca683ce85e",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51d609c082f1e4bcf2e66fda02b1e312242161d2",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lt4y1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "send_me_a_ticket",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751818168,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a roleplayer using SillyTavern. Llama models are often used as 'base' for fine tunes in Huggingface. Seeing what people can do with local models also fascinate me. ^^ Hello!",
          "author_fullname": "t2_1kywmhrlx5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "I drew a silly comic about Llama model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "qgeyuaymybbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/qgeyuaymybbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=73f55c41da7651f344fd6618ecea3d2b8e36e531"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/qgeyuaymybbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=15ad5709be3460b479e93319a55eb06b0fdf9ca9"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/qgeyuaymybbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=572f2fa07eac7fc1f4fb0916df79f5adb08c9d16"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/qgeyuaymybbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe50fe4955dab03387c1eacb5945b639547d3d1c"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/qgeyuaymybbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=27196c53bb42b639bccfdce7319d982869583adc"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/qgeyuaymybbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=526d6d093893073a08c2f6bc2f28690309f610b7"
                }
              ],
              "s": {
                "y": 1280,
                "x": 1280,
                "u": "https://preview.redd.it/qgeyuaymybbf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=63847e030bff38733fe09b4ffc95518735387c8e"
              },
              "id": "qgeyuaymybbf1"
            },
            "tebny8ymybbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/tebny8ymybbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b75f1a58539c63e432a75a6065459231991e1936"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/tebny8ymybbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb942a76a343c645ac0155fd80c558f23584a52b"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/tebny8ymybbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad037a022e258a34d40af11f895a55e37595103"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/tebny8ymybbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=93910110ac2a9e5b88c1c69f7c1188f99b1afb50"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/tebny8ymybbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8e66d70bf9c05ea4dac8b6162338e8e5156d490"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/tebny8ymybbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9ef4f3b068509a5cfc27b9f2195ea83f9940be86"
                }
              ],
              "s": {
                "y": 1280,
                "x": 1280,
                "u": "https://preview.redd.it/tebny8ymybbf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=035dfed5bf5fb2a1888862f37eb2a9d0f6417dcf"
              },
              "id": "tebny8ymybbf1"
            },
            "z0uvt8ymybbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/z0uvt8ymybbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63d74a32f14da7860517d32935db717ffd4282a6"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/z0uvt8ymybbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3da8d1d48d7e895bf94805902fc3df5e93f7944"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/z0uvt8ymybbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff08801f45ab42c0a955c19314daa56e63ab757c"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/z0uvt8ymybbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9621a9bd50028619c02d57faa0734232c066b6f2"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/z0uvt8ymybbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4ace02e069bbf12115144ea01a867c4bb797443"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/z0uvt8ymybbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c3b9c8d444968fee6d1e9b63273f6105e4bb4bc4"
                }
              ],
              "s": {
                "y": 1280,
                "x": 1280,
                "u": "https://preview.redd.it/z0uvt8ymybbf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=774cf8142601249bb728bd0d4cde1a85311406bd"
              },
              "id": "z0uvt8ymybbf1"
            },
            "mxka28ymybbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/mxka28ymybbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1aa1ac5a6c0aba6adc46adfcd43eb4ae11dcccb"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/mxka28ymybbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3de46d9381e300a08ca0155250370eef25232864"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/mxka28ymybbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e092e9ac77a4ec22029e30c3f7e3f3fdd875050f"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/mxka28ymybbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a21afd01fc02316fa36a08821fcc479999357b05"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/mxka28ymybbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1dfbb946c1bc6e971862a8a8c957a16bf9b0c57b"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/mxka28ymybbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fc6cd0f6fbedccb44ed0f842bd2df5ac6db3725f"
                }
              ],
              "s": {
                "y": 1280,
                "x": 1280,
                "u": "https://preview.redd.it/mxka28ymybbf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=efe190054ffebdfb2472510dc9743bcc2746f893"
              },
              "id": "mxka28ymybbf1"
            },
            "ezp5239a9cbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/ezp5239a9cbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8e4e4bfddab7f50d997d0d454b7b83c709ec0e1"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/ezp5239a9cbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b9c23714b9d74bfc57f9567f8b42185ef18ab442"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/ezp5239a9cbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=040261eeed80f6c7144e60b4c4f57bc264fd9cac"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/ezp5239a9cbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0b15b6ba3363a3ea9a0c6b17b586aaf212b28bb"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/ezp5239a9cbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e440523afd958a6bd15adb2761982c1819d8f9ad"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/ezp5239a9cbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8c6d903dc1e048f805debb6d5eb312e8313287e8"
                }
              ],
              "s": {
                "y": 1280,
                "x": 1280,
                "u": "https://preview.redd.it/ezp5239a9cbf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=6ff5bd4a6f62f94c2f66f42bdda9fc99f281c967"
              },
              "id": "ezp5239a9cbf1"
            },
            "cvps4eymybbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/cvps4eymybbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e4b2236d7e7aef7d1525c12f0b36f72d0af5dd1"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/cvps4eymybbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ca2b9851a41fed01c42440a70cd484c303cfbe6a"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/cvps4eymybbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13fcb49a5e4e5eabe410bc13305dc4d88da32134"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/cvps4eymybbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83559179b1c971f666724c3dfd0c8d013d5c9290"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/cvps4eymybbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6569f286ec2c7188e7112216ca4f4348a77e9102"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/cvps4eymybbf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23d7082a599570cc142097acc7db78ae81f02b2f"
                }
              ],
              "s": {
                "y": 1280,
                "x": 1280,
                "u": "https://preview.redd.it/cvps4eymybbf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=12479bee7d5276e2d24af088e807f6e228fd0965"
              },
              "id": "cvps4eymybbf1"
            }
          },
          "name": "t3_1ltfgoy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 64,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "z0uvt8ymybbf1",
                "id": 700268697
              },
              {
                "media_id": "tebny8ymybbf1",
                "id": 700268698
              },
              {
                "media_id": "mxka28ymybbf1",
                "id": 700268699
              },
              {
                "media_id": "cvps4eymybbf1",
                "id": 700268700
              },
              {
                "media_id": "qgeyuaymybbf1",
                "id": 700268701
              },
              {
                "media_id": "ezp5239a9cbf1",
                "id": 700268702
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/ntblmHJuZXo-K_j-B7phe4Ko7b3I1mCMnzblLD25_K8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751845061,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a roleplayer using SillyTavern. Llama models are often used as &amp;#39;base&amp;#39; for fine tunes in Huggingface. Seeing what people can do with local models also fascinate me. &lt;sup&gt;^&lt;/sup&gt; Hello!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1ltfgoy",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ltfgoy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Organic-Mechanic-435",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1ltfgoy",
          "subreddit_subscribers": 495395,
          "created_utc": 1751845061,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking to get a total of at least 140 GB RAM/VRAM combined to run Qwen 235B Q4. Current i have 96 GB RAM so next step is to get some cheap VRAM. After some research i found the following options at around 1000$ each:   \n\n1. 4x RTX 3060 (48 GB)\n2. 4x P100 (64 GB)\n3. 3x P40 (72 GB)\n4. 3x RX 9060 (48 GB)\n5. 4x MI50 32GB (128GB)\n6. 3x RTX 4060 ti/5060 ti (48 GB)\n\nEdit: add more suggestion from comments.  \n\nWhich GPU do you recommend or is there anything else better? I know 3090 is king here but cost per GB is around double the above GPU. Any suggestion is appreciated.",
          "author_fullname": "t2_giuiz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cheapest way to stack VRAM in 2025?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltamap",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 113,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 113,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751835244,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751832289,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to get a total of at least 140 GB RAM/VRAM combined to run Qwen 235B Q4. Current i have 96 GB RAM so next step is to get some cheap VRAM. After some research i found the following options at around 1000$ each:   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;4x RTX 3060 (48 GB)&lt;/li&gt;\n&lt;li&gt;4x P100 (64 GB)&lt;/li&gt;\n&lt;li&gt;3x P40 (72 GB)&lt;/li&gt;\n&lt;li&gt;3x RX 9060 (48 GB)&lt;/li&gt;\n&lt;li&gt;4x MI50 32GB (128GB)&lt;/li&gt;\n&lt;li&gt;3x RTX 4060 ti/5060 ti (48 GB)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Edit: add more suggestion from comments.  &lt;/p&gt;\n\n&lt;p&gt;Which GPU do you recommend or is there anything else better? I know 3090 is king here but cost per GB is around double the above GPU. Any suggestion is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ltamap",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gnad",
          "discussion_type": null,
          "num_comments": 111,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751832289,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a for loop to access the experts, resulting in &lt; 20% GPU usage. It's been two months and there are still very few LoRAs of Qwen3-30B-A3B in the public. (If you search 'qwen3 30b a3b lora' on HuggingFace, that's... interesting)\n\nThis should be made easier. I've made a fused version of Qwen3 MoE Layer that's much faster, while being compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. On a single GPU with 24GB VRAM, it reaches 100% GPU usage and 5x speedup of training compared to the unfused model.\n\nThere is still room for further optimization, but you can try it now and train your own LoRA.\n\nAlso, please help if you know how to upstream this to Transformers or Unsloth. (Transformers itself never includes Triton or CUDA kernels in the package, but they have a HuggingFace Kernels project to do so.)",
          "author_fullname": "t2_1rq55mepz8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltgayn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4c8616ae35c9e35f518e997ac58630a64deac92f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751847508,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a for loop to access the experts, resulting in &amp;lt; 20% GPU usage. It&amp;#39;s been two months and there are still very few LoRAs of Qwen3-30B-A3B in the public. (If you search &amp;#39;qwen3 30b a3b lora&amp;#39; on HuggingFace, that&amp;#39;s... interesting)&lt;/p&gt;\n\n&lt;p&gt;This should be made easier. I&amp;#39;ve made a fused version of Qwen3 MoE Layer that&amp;#39;s much faster, while being compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. On a single GPU with 24GB VRAM, it reaches 100% GPU usage and 5x speedup of training compared to the unfused model.&lt;/p&gt;\n\n&lt;p&gt;There is still room for further optimization, but you can try it now and train your own LoRA.&lt;/p&gt;\n\n&lt;p&gt;Also, please help if you know how to upstream this to Transformers or Unsloth. (Transformers itself never includes Triton or CUDA kernels in the package, but they have a HuggingFace Kernels project to do so.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/woct0rdho/transformers-qwen3-moe-fused",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?auto=webp&amp;s=a967129ff5ab80fff515506e789647ed640bbd2f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3be8044143b537d63e288d3975bce8cd7e0836b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c1301694d705875bed40d97a02b2f4d4f11005f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f81de294025d9ab45618aca8ab4acb37ea5cfcfa",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=094a72eeef2838a876e8835f533b8146c2ead2a9",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e882735fafc07d7af0f34739ba0a85a5fed48802",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8874406a247b374af4f7df5cc87dde23ebe726e2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltgayn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "woct0rdho",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/",
          "stickied": false,
          "url": "https://github.com/woct0rdho/transformers-qwen3-moe-fused",
          "subreddit_subscribers": 495395,
          "created_utc": 1751847508,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!\n\nWe have been exploring various open-source Text-to-Speech (TTS) models, and decided to create a Hugging Face demo space that makes it easy to compare their quality side-by-side.\n\nThe demo features **12 popular TTS models**, all tested using a consistent prompt, so you can quickly hear and compare their synthesized speech and choose the best one for your audio projects.\n\nWould love to get feedback or suggestions!\n\n👉 [Check out the demo space and detailed comparison here!](https://huggingface.co/spaces/Inferless/Open-Source-TTS-Gallary)\n\n👉 [Check out the blog: Choosing the Right Text-to-Speech Model: Part 2](https://www.inferless.com/learn/comparing-different-text-to-speech---tts--models-part-2)\n\nShare your use-case and we will update this space as required!  \n  \nWhich TTS model sounds most natural to you?\n\nCheers!",
          "author_fullname": "t2_mkmia1m4g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🎧 Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 112,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltbrlf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 82,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 82,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/T8e056zJI6mhHVavjSOoc7QaaQsBmA07E6D35FUx67w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751835181,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;We have been exploring various open-source Text-to-Speech (TTS) models, and decided to create a Hugging Face demo space that makes it easy to compare their quality side-by-side.&lt;/p&gt;\n\n&lt;p&gt;The demo features &lt;strong&gt;12 popular TTS models&lt;/strong&gt;, all tested using a consistent prompt, so you can quickly hear and compare their synthesized speech and choose the best one for your audio projects.&lt;/p&gt;\n\n&lt;p&gt;Would love to get feedback or suggestions!&lt;/p&gt;\n\n&lt;p&gt;👉 &lt;a href=\"https://huggingface.co/spaces/Inferless/Open-Source-TTS-Gallary\"&gt;Check out the demo space and detailed comparison here!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👉 &lt;a href=\"https://www.inferless.com/learn/comparing-different-text-to-speech---tts--models-part-2\"&gt;Check out the blog: Choosing the Right Text-to-Speech Model: Part 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Share your use-case and we will update this space as required!  &lt;/p&gt;\n\n&lt;p&gt;Which TTS model sounds most natural to you?&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bwd1gqkrfbbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bwd1gqkrfbbf1.png?auto=webp&amp;s=7eb3deb5257de1e9854bd070fe9799ac37b13ad6",
                  "width": 1888,
                  "height": 1522
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bwd1gqkrfbbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97ba7e4a8b732ff1ec264045dbf41eca64bef176",
                    "width": 108,
                    "height": 87
                  },
                  {
                    "url": "https://preview.redd.it/bwd1gqkrfbbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=daa00905888337f38307917e464cbe76ef14d796",
                    "width": 216,
                    "height": 174
                  },
                  {
                    "url": "https://preview.redd.it/bwd1gqkrfbbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e768a830230d270aaf2e8e4e38c8603b20c50f3",
                    "width": 320,
                    "height": 257
                  },
                  {
                    "url": "https://preview.redd.it/bwd1gqkrfbbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=57eba9e11159f3d51759d5ca917254faf9332203",
                    "width": 640,
                    "height": 515
                  },
                  {
                    "url": "https://preview.redd.it/bwd1gqkrfbbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=47c113f7e8919c62dfb932e90d8c2ac485fd36f5",
                    "width": 960,
                    "height": 773
                  },
                  {
                    "url": "https://preview.redd.it/bwd1gqkrfbbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c42914aca40a89a6c86dd367decdc6136b5393b1",
                    "width": 1080,
                    "height": 870
                  }
                ],
                "variants": {},
                "id": "hJPKiS_9rALP6hZ95_openqupuBcs2KV0oVwJ4hWEDU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltbrlf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rbgo404",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/",
          "stickied": false,
          "url": "https://i.redd.it/bwd1gqkrfbbf1.png",
          "subreddit_subscribers": 495395,
          "created_utc": 1751835181,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/HW-whistleblower/True-Story-of-Pangu](https://github.com/HW-whistleblower/True-Story-of-Pangu)  \nafter reading the traslation of this article, I found there're many details, is it possible true or just a fake story?\n\ngemini's traslation:\n\nThis is a full translation of the provided text. The original is a deeply emotional and accusatory letter from a self-proclaimed Huawei employee. The translation aims to preserve the tone, technical details, and cultural nuances of the original piece.\n\n\n\nThe Fall of Pangu: The Heartbreak and Darkness of the Huawei Noah's Ark Pangu LLM Development Journey\n\n\n\nHello everyone,\n\n\n\nI am an employee of the Pangu LLM team at Huawei's Noah's Ark Lab.\n\n\n\nFirst, to verify my identity, I will list some details:\n\n\n\nThe current director of Noah's Ark Lab is Wang Yunhe, who was formerly the head of the Algorithm Application Department, later renamed the Small Model Lab. The former director of Noah's Ark was Yao Jun (whom everyone called Teacher Yao). Several lab directors include: Tang Ruiming (Ming-ge, Team Ming, has since left), Shang Lifeng, Zhang Wei (Wei-ge), Hao Jianye (Teacher Hao), and Liu Wulong (referred to as Director Wulong). Many other key members and experts have also left one after another.\n\n\n\nWe belong to an organization called the \"Fourth Field Army\" (四野). Under the Fourth Field Army, there are many \"columns\" (纵队); the foundational language model team is the Fourth Column. Wang Yunhe's small model team is the Sixteenth Column. We participated in gatherings in Suzhou, with various monthly deadlines. During the \"problem-tackling sessions\" in Suzhou, \"mission orders\" were issued, requiring us to meet targets before set deadlines. The Suzhou gatherings brought people from all over to the Suzhou Research Institute. We usually stayed in hotels, such as one in Lu Zhi (甪直), separated from our families and children.\n\n\n\nDuring the Suzhou gatherings, Saturday was a default workday. It was exhausting, but there was afternoon tea on Saturdays, and one time we even had crayfish. Our workstations at the Suzhou Research Institute were moved once, from one building to another. The buildings at the Suzhou Institute have European-style architecture, with a large slope at the entrance, and the scenery inside is beautiful. Trips to the Suzhou gatherings would last at least a week, sometimes longer. Many people couldn't go home for one or even two months.\n\n\n\nNoah's Ark was once rumored to be research-oriented, but after I joined, because we were working on the large model project under the Fourth Field Army, the project members completely turned into a delivery-focused team, swamped with routine meetings, reviews, and reports. We often had to apply just to run experiments. The team needed to interface with numerous business lines like Terminal's Celia (小艺), Huawei Cloud, and ICT, and the delivery pressure was immense.\n\n\n\nThe Pangu model developed by Noah's Ark was initially codenamed \"Pangu Zhizi\" (盘古智子). At first, it was only available as an internal webpage that required an application for trial use. Later, due to pressure, it was integrated into Welink and opened for public beta.\n\n\n\nThe recent controversy surrounding the accusations that the Pangu LLM plagiarized Qwen has been all over the news. As a member of the Pangu team, I've been tossing and turning every night, unable to sleep. Pangu's brand has been so severely damaged. On one hand, I selfishly worry about my own career development and feel that my past hard work was for nothing. On the other hand, I feel a sense of vindication now that someone has started exposing these things. For countless days and nights, we gritted our teeth in anger, powerless, as certain individuals internally reaped endless benefits through repeated fraud. This suppression and humiliation have gradually eroded my affection for Huawei, leaving me dazed and confused, lost and aimless, often questioning my life and self-worth.\n\n\n\nI admit that I am a coward. As a humble worker, I dare not oppose people like Wang Yunhe with their powerful connections, let alone a behemoth like Huawei. I am terrified of losing my job, as I have a family and children to support. That's why I deeply admire the whistleblower from the bottom of my heart. However, when I see the internal attempts to whitewash and cover up the facts to deceive the public, I can no longer tolerate it. I want to be brave for once and follow my conscience. Even if I harm myself by 800, I hope to damage the enemy by 1,000. I have decided to publicize what I have seen and heard here (some of which is from colleagues) about the \"legendary story\" of the Pangu LLM.\n\n\n\nHuawei has indeed primarily trained its large models on Ascend cards (the Small Model Lab has quite a few Nvidia cards, which they used for training before transitioning to Ascend). I was once captivated by Huawei's determination to \"build the world's second choice,\" and I used to have deep feelings for the company. We went through trials and tribulations with Ascend, from being full of bugs to now being able to train models, and we invested immense effort and sacrifice.\n\n\n\nInitially, our computing power was very limited, and we trained models on the 910A. At that time, it only supported fp16, and the training stability was far worse than bf16. Pangu started working on MoE (Mixture of Experts) very early. In 2023, the main focus was on training a 38B MoE model and a subsequent 71B dense model. The 71B dense model was expanded to become the first-generation 135B dense model, and later, the main models were gradually trained on the 910B.\n\n\n\nBoth the 71B and 135B models had a huge, fundamental flaw: the tokenizer. The tokenizer used back then had extremely low encoding efficiency. Every single symbol, number, space, and even Chinese character took up one token. As you can imagine, this wasted a tremendous amount of computing power and resulted in poor model performance. At that time, the Small Model Lab happened to have a vocabulary they had trained themselves. Teacher Yao suspected that the model's tokenizer was the problem (and in hindsight, his suspicion was undoubtedly correct). So, he decided to have the 71B and 135B models switch tokenizers, as the Small Model Lab had experimented with this before. The team stitched together two tokenizers and began the replacement process. The replacement for the 71B model failed. The 135B model, using a more refined embedding initialization strategy, finally succeeded in changing its vocabulary after being continually trained on at least 1T of data. But as you can imagine, the performance did not improve.\n\n\n\nMeanwhile, other domestic companies like Alibaba and Zhipu AI were training on GPUs and had already figured out the right methods. The gap between Pangu and its competitors grew wider and wider. An internal 230B dense model, trained from scratch, failed for various reasons, pushing the project to the brink of collapse. Facing pressure from several deadlines and strong internal skepticism about Pangu, the team's morale hit rock bottom. With extremely limited computing power, the team struggled and tried many things. For example, they accidentally discovered that the 38B MoE model at the time did not have the expected MoE effect. So they removed the MoE parameters, reverting it to a 13B dense model. Since the 38B MoE originated from a very early Pangu Alpha 13B with a relatively outdated architecture, the team made a series of changes, such as switching from absolute position encoding to RoPE, removing bias, and switching to RMSNorm. Given the failures with the tokenizer and the experience of changing vocabularies, this model's vocabulary was also replaced with the one used by Wang Yunhe's Small Model Lab's 7B model. This 13B model was later expanded and continually trained, becoming the second-generation 38B dense model (which was the main mid-range Pangu model for several months) and was once quite competitive. However, because the larger 135B model had an outdated architecture and was severely damaged by the vocabulary change (later analysis revealed that the stitched-together vocabulary had even more serious bugs), its performance after continued training still lagged far behind leading domestic models like Qwen. The internal criticism and pressure from leadership grew even stronger. The team was practically in a desperate situation.\n\n\n\nUnder these circumstances, Wang Yunhe and his Small Model Lab stepped in. They claimed to have inherited and modified the parameters from the old 135B model, and by training on just a few hundred billion tokens, they improved various metrics by an average of about ten points. In reality, this was their first masterpiece of \"shell-wrapping\" (套壳, i.e., putting a new shell on another company's model) applied to a large model. At Huawei, laymen lead experts, so the leadership had no concept of how absurd this was; they just thought there must be some algorithmic innovation. After internal analysis, it was discovered that they had actually continued training on Qwen 1.5 110B, adding layers, expanding the FFN dimensions, and incorporating some mechanisms from the Pangu-Pi paper to reach about 135B parameters. In fact, the old 135B had 107 layers, while this new model only had 82, and various other configurations were different. After training, the distribution of many parameters in the new, mysterious 135B model was almost identical to Qwen 110B. Even the class name in the model's code was \"Qwen\" at the time; they were too lazy to even change it. This model later became the so-called 135B V2. And this model was provided to many downstream teams, including external customers.\n\n\n\nThis incident was a huge blow to those of us colleagues who were doing our work seriously and honestly. Many people internally, including those in the Terminal and Huawei Cloud divisions, knew about this. We all joked that we should stop calling it the Pangu model and call it the \"Qiangu\" model instead (a pun combining Qwen and Pangu). At the time, team members wanted to report this to the BCG (Business Conduct Guidelines) office, as it was major business fraud. But later, it was said that a leader stopped them, because higher-level leaders (like Teacher Yao, and possibly Director Xiong and Elder Zha) also found out later but did nothing about it. Getting good results through shell-wrapping was also beneficial to them. This event caused several of the team's strongest members to become disheartened, and talk of resignation became commonplace.\n\n\n\nAt this point, Pangu seemed to find a turning point. Since the Pangu models mentioned earlier were mostly based on continued training and modification, Noah's Ark at that time had no grasp of training technology from scratch, let alone on Ascend's NPUs. Thanks to the strenuous efforts of the team's core members, Pangu began training its third-generation models. After immense effort, the data architecture and training algorithms gradually caught up with the industry. The people from the Small Model Lab had nothing to do with this hardship.\n\n\n\nInitially, the team members had no confidence and started with just a 13B model. But later, they found the results were quite good. So this model was later expanded again, becoming the third-generation 38B, codenamed 38B V3. I'm sure many brothers in the product lines are familiar with this model. At that time, this model's tokenizer was an extension of Llama's vocabulary (a common practice in the industry). Meanwhile, Wang Yunhe's lab created another vocabulary (which later became the vocabulary for the Pangu series). The two vocabularies were forced into a \"horse race\" (a competitive trial), which ended with no clear winner. So, the leadership immediately decided that the vocabularies should be unified, and Wang Yunhe's should be used. Consequently, the 135B V3 (known externally as Pangu Ultra), which was trained from scratch, adopted this tokenizer. This also explains the confusion many brothers who used our models had: why two models of the same V3 generation, but different sizes, used different tokenizers.\n\n\n\nFrom the bottom of our hearts, we feel that the 135B V3 was the pride of our Fourth Column team at the time. It was the first truly full-stack, self-developed, properly from-scratch-trained, hundred-billion-parameter-level model from Huawei, and its performance was comparable to competitors in early 2024. Writing this, I am already in tears. It was so incredibly difficult. To ensure stable training, the team conducted a large number of comparative experiments and performed timely rollbacks and restarts whenever the model's gradients showed anomalies. This model truly achieved what was later stated in the technical report: not a single loss spike throughout the entire training process. We overcame countless difficulties. We did it. We are willing to guarantee the authenticity of this model's training with our lives and honor. How many sleepless nights did we spend for its training? How wronged and aggrieved did we feel when we were being worthless in internal forums? We persevered.\n\n\n\nWe are the ones who were truly burning our youth to build up China's domestic computing foundation... Away from home, we gave up our families, our holidays, our health, and our entertainment. We risked everything. The hardships and difficulties involved cannot be fully described in a few words. At various mobilization meetings, when we shouted slogans like \"Pangu will prevail, Huawei will prevail,\" we were genuinely and deeply moved.\n\n\n\nHowever, all the fruits of our hard work were often casually taken by the Small Model Lab. Data? They just demanded it. Code? They just took it and even required us to help adapt it so it could be run with a single click. We used to joke that the Small Model Lab was the \"mouse-clicking lab.\" We did the hard work; they reaped the glory. It really is true what they say: \"You are carrying a heavy burden so that someone else can live a peaceful life.\" Under these circumstances, more and more of our comrades could no longer hold on and chose to leave. Seeing those brilliant colleagues leave one by one, I felt both regret and sadness. In this battle-like environment, we were more like comrades-in-arms than colleagues. They were also great teachers from whom I could learn countless technical things. Seeing them go to outstanding teams like ByteDance's Seed, Deepseek, Moonshot AI, Tencent, and Kuaishou, I am genuinely happy for them and wish them the best for escaping this exhausting and dirty place. I still vividly remember what a colleague who left said: \"Coming here was a disgrace to my technical career. Every day I stay here is a waste of life.\" The words were harsh, but they left me speechless. I worried about my own lack of technical expertise and my inability to adapt to the high-turnover environment of internet companies, which kept me from taking the step to resign despite thinking about it many times.\n\n\n\nBesides dense models, Pangu later began exploring MoE models. Initially, a 224B MoE model was trained. In parallel, the Small Model Lab launched its second major shell-wrapping operation (minor incidents may have included other models, like a math model), which is the now infamous Pangu-Pro MoE 72B. This model was internally claimed to have been expanded from the Small Model Lab's 7B model (even if true, this contradicts the technical report, let alone the fact that it was continued training on a shell of Qwen 2.5's 14B). I remember that just a few days after they started training, their internal evaluation scores immediately caught up with our 38B V3 at the time. Many brothers in the AI System Lab knew about their shell-wrapping operation because they needed to adapt the model, but for various reasons, they couldn't bring justice to light. In fact, for this model that was trained for a very long time afterward, I am surprised that HonestAGI was able to detect this level of similarity. The computing power spent on \"washing\" the parameters to continue training would have been more than enough to train a model of the same size from scratch. I heard from colleagues that they used many methods to wash away Qwen's watermark, even intentionally training it on dirty data. This provides an unprecedented case study for the academic community researching model \"lineage.\" New lineage detection methods in the future can be tested on this.\n\n\n\nIn late 2024 and early 2025, after the release of Deepseek v3 and r1, our team was hit hard by their stunning technical level and faced even greater skepticism. To keep up with the trend, Pangu imitated Deepseek's model size and began training a 718B MoE model. At this time, the Small Model Lab struck again. They chose to shell-wrap and continue training on Deepseek-v3. They trained the model by freezing the parameters loaded from Deepseek. Even the directory for loading the checkpoint was named deepseekv3—they didn't even bother to change it. How arrogant is that? In contrast, some colleagues with true technical integrity were training another 718B MoE from scratch, but they encountered all sorts of problems. But obviously, how could this model ever be better than a direct shell-wrap? If it weren't for the team leader's insistence, it would have been shut down long ago.\n\n\n\nHuawei's cumbersome process management severely slows down the R&amp;D pace of large models, with things like version control, model lineage, various procedures, and traceability requirements. Ironically, the Small Model Lab's models never seem to be bound by these processes. They can shell-wrap whenever they want, continue training whenever they want, and endlessly demand computing resources. This stark, almost surreal contrast illustrates the current state of process management: \"The magistrates are allowed to set fires, but the common people are not even allowed to light lamps.\" How ridiculous? How tragic? How hateful? How shameful!\n\n\n\nAfter the HonestAGI incident, we were forced into endless internal discussions and analyses on how to handle public relations and \"respond.\" Admittedly, the original analysis might not have been strong enough, giving Wang Yunhe and the Small Model Lab an opportunity to argue and twist the truth. For this, I have felt sick to my stomach these past two days, constantly questioning the meaning of my life and whether there is any justice in the world. I'm not playing along anymore. I'm going to resign. I am also applying to have my name removed from the author list of some of the Pangu technical reports. Having my name on those reports is a stain on my life that I can never erase. At the time, I never thought they would be brazen enough to open-source it. I never thought they would dare to fool the world like this and promote it so heavily. At that time, perhaps I was holding onto a sliver of wishful thinking and didn't refuse to be listed as an author. I believe many of my dedicated comrades were also forced onto this pirate ship or were unaware of the situation. But this can't be undone. I hope to spend the rest of my life doing solid, meaningful work to atone for my weakness and indecisiveness back then.\n\n\n\nWriting this late at night, I am already in tears, sobbing uncontrollably. I remember when some outstanding colleagues were leaving, I asked them with a wry smile if they were going to post a long, customary farewell message on the internal forum to expose the situation. They replied, \"No, it's a waste of time, and I'm afraid it would make things even worse for you all.\" At that moment, I felt a deep sense of sorrow, because my comrades, with whom I had once fought for a common ideal, had completely lost faith in Huawei. We used to joke that we were using the Communist Party's \"millet plus rifles\" (meager resources) while the organization had the style of the Kuomintang (corrupt and bureaucratic).\n\n\n\nThere was a time when I was proud that we were using \"millet plus rifles\" to defeat foreign guns and cannons.\n\n\n\nNow, I am tired. I want to surrender.\n\n\n\nTo this day, I still sincerely hope that Huawei can learn its lesson, do Pangu right, make Pangu world-class, and bring Ascend to the level of Nvidia. The internal phenomenon of \"bad money driving out good\" has caused Noah's Ark, and even Huawei, to rapidly lose a large number of outstanding large model talents. I believe they are now shining in various teams like Deepseek, realizing their ambitions and talents, and contributing to the fierce AI competition between China and the US. I often lament that Huawei doesn't lack talent; it simply doesn't know how to retain it. If these people were given the right environment, the right resources, fewer shackles, and less political infighting, what would stop Pangu from succeeding?\n\n\n\nFinally: I swear on my life, character, and honor that everything I have written above is true (at least within my limited knowledge). I do not have the high level of technical skill or the opportunity to conduct a thorough and solid analysis, nor do I dare to use internal records as direct evidence for fear of being caught through information security. But I believe many of my former comrades will vouch for me. To my brothers still inside Huawei, including those in the product lines we served, I believe the countless details in this article will resonate with your own impressions and corroborate my claims. You too may have been deceived, but these cruel truths will not remain buried. The traces of our struggle should not be distorted and buried either.\n\n\n\nHaving written so much, certain people will surely want to find me and silence me. The company might even try to shut me up or hold me accountable. If that happens, my personal safety, and even that of my family, could be threatened. For my own protection, I will report that I am safe to everyone daily in the near future.\n\n\n\nIf I disappear, just consider it my sacrifice for truth and ideals, for the better development of computing power and AI in Huawei and even in China. I am willing to be buried in that place where I once fought.\n\n\n\nGoodbye, Noah's Ark.\n\n\n\nWritten in the early morning of July 6, 2024, in Shenzhen.",
          "author_fullname": "t2_tueeshp7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Huawei's Pangu AI Rocked by Unverified Claims of Fraud from Alleged Team Member",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsz4hk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 262,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 262,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751802437,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751801803,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/HW-whistleblower/True-Story-of-Pangu\"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt;&lt;br/&gt;\nafter reading the traslation of this article, I found there&amp;#39;re many details, is it possible true or just a fake story?&lt;/p&gt;\n\n&lt;p&gt;gemini&amp;#39;s traslation:&lt;/p&gt;\n\n&lt;p&gt;This is a full translation of the provided text. The original is a deeply emotional and accusatory letter from a self-proclaimed Huawei employee. The translation aims to preserve the tone, technical details, and cultural nuances of the original piece.&lt;/p&gt;\n\n&lt;p&gt;The Fall of Pangu: The Heartbreak and Darkness of the Huawei Noah&amp;#39;s Ark Pangu LLM Development Journey&lt;/p&gt;\n\n&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am an employee of the Pangu LLM team at Huawei&amp;#39;s Noah&amp;#39;s Ark Lab.&lt;/p&gt;\n\n&lt;p&gt;First, to verify my identity, I will list some details:&lt;/p&gt;\n\n&lt;p&gt;The current director of Noah&amp;#39;s Ark Lab is Wang Yunhe, who was formerly the head of the Algorithm Application Department, later renamed the Small Model Lab. The former director of Noah&amp;#39;s Ark was Yao Jun (whom everyone called Teacher Yao). Several lab directors include: Tang Ruiming (Ming-ge, Team Ming, has since left), Shang Lifeng, Zhang Wei (Wei-ge), Hao Jianye (Teacher Hao), and Liu Wulong (referred to as Director Wulong). Many other key members and experts have also left one after another.&lt;/p&gt;\n\n&lt;p&gt;We belong to an organization called the &amp;quot;Fourth Field Army&amp;quot; (四野). Under the Fourth Field Army, there are many &amp;quot;columns&amp;quot; (纵队); the foundational language model team is the Fourth Column. Wang Yunhe&amp;#39;s small model team is the Sixteenth Column. We participated in gatherings in Suzhou, with various monthly deadlines. During the &amp;quot;problem-tackling sessions&amp;quot; in Suzhou, &amp;quot;mission orders&amp;quot; were issued, requiring us to meet targets before set deadlines. The Suzhou gatherings brought people from all over to the Suzhou Research Institute. We usually stayed in hotels, such as one in Lu Zhi (甪直), separated from our families and children.&lt;/p&gt;\n\n&lt;p&gt;During the Suzhou gatherings, Saturday was a default workday. It was exhausting, but there was afternoon tea on Saturdays, and one time we even had crayfish. Our workstations at the Suzhou Research Institute were moved once, from one building to another. The buildings at the Suzhou Institute have European-style architecture, with a large slope at the entrance, and the scenery inside is beautiful. Trips to the Suzhou gatherings would last at least a week, sometimes longer. Many people couldn&amp;#39;t go home for one or even two months.&lt;/p&gt;\n\n&lt;p&gt;Noah&amp;#39;s Ark was once rumored to be research-oriented, but after I joined, because we were working on the large model project under the Fourth Field Army, the project members completely turned into a delivery-focused team, swamped with routine meetings, reviews, and reports. We often had to apply just to run experiments. The team needed to interface with numerous business lines like Terminal&amp;#39;s Celia (小艺), Huawei Cloud, and ICT, and the delivery pressure was immense.&lt;/p&gt;\n\n&lt;p&gt;The Pangu model developed by Noah&amp;#39;s Ark was initially codenamed &amp;quot;Pangu Zhizi&amp;quot; (盘古智子). At first, it was only available as an internal webpage that required an application for trial use. Later, due to pressure, it was integrated into Welink and opened for public beta.&lt;/p&gt;\n\n&lt;p&gt;The recent controversy surrounding the accusations that the Pangu LLM plagiarized Qwen has been all over the news. As a member of the Pangu team, I&amp;#39;ve been tossing and turning every night, unable to sleep. Pangu&amp;#39;s brand has been so severely damaged. On one hand, I selfishly worry about my own career development and feel that my past hard work was for nothing. On the other hand, I feel a sense of vindication now that someone has started exposing these things. For countless days and nights, we gritted our teeth in anger, powerless, as certain individuals internally reaped endless benefits through repeated fraud. This suppression and humiliation have gradually eroded my affection for Huawei, leaving me dazed and confused, lost and aimless, often questioning my life and self-worth.&lt;/p&gt;\n\n&lt;p&gt;I admit that I am a coward. As a humble worker, I dare not oppose people like Wang Yunhe with their powerful connections, let alone a behemoth like Huawei. I am terrified of losing my job, as I have a family and children to support. That&amp;#39;s why I deeply admire the whistleblower from the bottom of my heart. However, when I see the internal attempts to whitewash and cover up the facts to deceive the public, I can no longer tolerate it. I want to be brave for once and follow my conscience. Even if I harm myself by 800, I hope to damage the enemy by 1,000. I have decided to publicize what I have seen and heard here (some of which is from colleagues) about the &amp;quot;legendary story&amp;quot; of the Pangu LLM.&lt;/p&gt;\n\n&lt;p&gt;Huawei has indeed primarily trained its large models on Ascend cards (the Small Model Lab has quite a few Nvidia cards, which they used for training before transitioning to Ascend). I was once captivated by Huawei&amp;#39;s determination to &amp;quot;build the world&amp;#39;s second choice,&amp;quot; and I used to have deep feelings for the company. We went through trials and tribulations with Ascend, from being full of bugs to now being able to train models, and we invested immense effort and sacrifice.&lt;/p&gt;\n\n&lt;p&gt;Initially, our computing power was very limited, and we trained models on the 910A. At that time, it only supported fp16, and the training stability was far worse than bf16. Pangu started working on MoE (Mixture of Experts) very early. In 2023, the main focus was on training a 38B MoE model and a subsequent 71B dense model. The 71B dense model was expanded to become the first-generation 135B dense model, and later, the main models were gradually trained on the 910B.&lt;/p&gt;\n\n&lt;p&gt;Both the 71B and 135B models had a huge, fundamental flaw: the tokenizer. The tokenizer used back then had extremely low encoding efficiency. Every single symbol, number, space, and even Chinese character took up one token. As you can imagine, this wasted a tremendous amount of computing power and resulted in poor model performance. At that time, the Small Model Lab happened to have a vocabulary they had trained themselves. Teacher Yao suspected that the model&amp;#39;s tokenizer was the problem (and in hindsight, his suspicion was undoubtedly correct). So, he decided to have the 71B and 135B models switch tokenizers, as the Small Model Lab had experimented with this before. The team stitched together two tokenizers and began the replacement process. The replacement for the 71B model failed. The 135B model, using a more refined embedding initialization strategy, finally succeeded in changing its vocabulary after being continually trained on at least 1T of data. But as you can imagine, the performance did not improve.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, other domestic companies like Alibaba and Zhipu AI were training on GPUs and had already figured out the right methods. The gap between Pangu and its competitors grew wider and wider. An internal 230B dense model, trained from scratch, failed for various reasons, pushing the project to the brink of collapse. Facing pressure from several deadlines and strong internal skepticism about Pangu, the team&amp;#39;s morale hit rock bottom. With extremely limited computing power, the team struggled and tried many things. For example, they accidentally discovered that the 38B MoE model at the time did not have the expected MoE effect. So they removed the MoE parameters, reverting it to a 13B dense model. Since the 38B MoE originated from a very early Pangu Alpha 13B with a relatively outdated architecture, the team made a series of changes, such as switching from absolute position encoding to RoPE, removing bias, and switching to RMSNorm. Given the failures with the tokenizer and the experience of changing vocabularies, this model&amp;#39;s vocabulary was also replaced with the one used by Wang Yunhe&amp;#39;s Small Model Lab&amp;#39;s 7B model. This 13B model was later expanded and continually trained, becoming the second-generation 38B dense model (which was the main mid-range Pangu model for several months) and was once quite competitive. However, because the larger 135B model had an outdated architecture and was severely damaged by the vocabulary change (later analysis revealed that the stitched-together vocabulary had even more serious bugs), its performance after continued training still lagged far behind leading domestic models like Qwen. The internal criticism and pressure from leadership grew even stronger. The team was practically in a desperate situation.&lt;/p&gt;\n\n&lt;p&gt;Under these circumstances, Wang Yunhe and his Small Model Lab stepped in. They claimed to have inherited and modified the parameters from the old 135B model, and by training on just a few hundred billion tokens, they improved various metrics by an average of about ten points. In reality, this was their first masterpiece of &amp;quot;shell-wrapping&amp;quot; (套壳, i.e., putting a new shell on another company&amp;#39;s model) applied to a large model. At Huawei, laymen lead experts, so the leadership had no concept of how absurd this was; they just thought there must be some algorithmic innovation. After internal analysis, it was discovered that they had actually continued training on Qwen 1.5 110B, adding layers, expanding the FFN dimensions, and incorporating some mechanisms from the Pangu-Pi paper to reach about 135B parameters. In fact, the old 135B had 107 layers, while this new model only had 82, and various other configurations were different. After training, the distribution of many parameters in the new, mysterious 135B model was almost identical to Qwen 110B. Even the class name in the model&amp;#39;s code was &amp;quot;Qwen&amp;quot; at the time; they were too lazy to even change it. This model later became the so-called 135B V2. And this model was provided to many downstream teams, including external customers.&lt;/p&gt;\n\n&lt;p&gt;This incident was a huge blow to those of us colleagues who were doing our work seriously and honestly. Many people internally, including those in the Terminal and Huawei Cloud divisions, knew about this. We all joked that we should stop calling it the Pangu model and call it the &amp;quot;Qiangu&amp;quot; model instead (a pun combining Qwen and Pangu). At the time, team members wanted to report this to the BCG (Business Conduct Guidelines) office, as it was major business fraud. But later, it was said that a leader stopped them, because higher-level leaders (like Teacher Yao, and possibly Director Xiong and Elder Zha) also found out later but did nothing about it. Getting good results through shell-wrapping was also beneficial to them. This event caused several of the team&amp;#39;s strongest members to become disheartened, and talk of resignation became commonplace.&lt;/p&gt;\n\n&lt;p&gt;At this point, Pangu seemed to find a turning point. Since the Pangu models mentioned earlier were mostly based on continued training and modification, Noah&amp;#39;s Ark at that time had no grasp of training technology from scratch, let alone on Ascend&amp;#39;s NPUs. Thanks to the strenuous efforts of the team&amp;#39;s core members, Pangu began training its third-generation models. After immense effort, the data architecture and training algorithms gradually caught up with the industry. The people from the Small Model Lab had nothing to do with this hardship.&lt;/p&gt;\n\n&lt;p&gt;Initially, the team members had no confidence and started with just a 13B model. But later, they found the results were quite good. So this model was later expanded again, becoming the third-generation 38B, codenamed 38B V3. I&amp;#39;m sure many brothers in the product lines are familiar with this model. At that time, this model&amp;#39;s tokenizer was an extension of Llama&amp;#39;s vocabulary (a common practice in the industry). Meanwhile, Wang Yunhe&amp;#39;s lab created another vocabulary (which later became the vocabulary for the Pangu series). The two vocabularies were forced into a &amp;quot;horse race&amp;quot; (a competitive trial), which ended with no clear winner. So, the leadership immediately decided that the vocabularies should be unified, and Wang Yunhe&amp;#39;s should be used. Consequently, the 135B V3 (known externally as Pangu Ultra), which was trained from scratch, adopted this tokenizer. This also explains the confusion many brothers who used our models had: why two models of the same V3 generation, but different sizes, used different tokenizers.&lt;/p&gt;\n\n&lt;p&gt;From the bottom of our hearts, we feel that the 135B V3 was the pride of our Fourth Column team at the time. It was the first truly full-stack, self-developed, properly from-scratch-trained, hundred-billion-parameter-level model from Huawei, and its performance was comparable to competitors in early 2024. Writing this, I am already in tears. It was so incredibly difficult. To ensure stable training, the team conducted a large number of comparative experiments and performed timely rollbacks and restarts whenever the model&amp;#39;s gradients showed anomalies. This model truly achieved what was later stated in the technical report: not a single loss spike throughout the entire training process. We overcame countless difficulties. We did it. We are willing to guarantee the authenticity of this model&amp;#39;s training with our lives and honor. How many sleepless nights did we spend for its training? How wronged and aggrieved did we feel when we were being worthless in internal forums? We persevered.&lt;/p&gt;\n\n&lt;p&gt;We are the ones who were truly burning our youth to build up China&amp;#39;s domestic computing foundation... Away from home, we gave up our families, our holidays, our health, and our entertainment. We risked everything. The hardships and difficulties involved cannot be fully described in a few words. At various mobilization meetings, when we shouted slogans like &amp;quot;Pangu will prevail, Huawei will prevail,&amp;quot; we were genuinely and deeply moved.&lt;/p&gt;\n\n&lt;p&gt;However, all the fruits of our hard work were often casually taken by the Small Model Lab. Data? They just demanded it. Code? They just took it and even required us to help adapt it so it could be run with a single click. We used to joke that the Small Model Lab was the &amp;quot;mouse-clicking lab.&amp;quot; We did the hard work; they reaped the glory. It really is true what they say: &amp;quot;You are carrying a heavy burden so that someone else can live a peaceful life.&amp;quot; Under these circumstances, more and more of our comrades could no longer hold on and chose to leave. Seeing those brilliant colleagues leave one by one, I felt both regret and sadness. In this battle-like environment, we were more like comrades-in-arms than colleagues. They were also great teachers from whom I could learn countless technical things. Seeing them go to outstanding teams like ByteDance&amp;#39;s Seed, Deepseek, Moonshot AI, Tencent, and Kuaishou, I am genuinely happy for them and wish them the best for escaping this exhausting and dirty place. I still vividly remember what a colleague who left said: &amp;quot;Coming here was a disgrace to my technical career. Every day I stay here is a waste of life.&amp;quot; The words were harsh, but they left me speechless. I worried about my own lack of technical expertise and my inability to adapt to the high-turnover environment of internet companies, which kept me from taking the step to resign despite thinking about it many times.&lt;/p&gt;\n\n&lt;p&gt;Besides dense models, Pangu later began exploring MoE models. Initially, a 224B MoE model was trained. In parallel, the Small Model Lab launched its second major shell-wrapping operation (minor incidents may have included other models, like a math model), which is the now infamous Pangu-Pro MoE 72B. This model was internally claimed to have been expanded from the Small Model Lab&amp;#39;s 7B model (even if true, this contradicts the technical report, let alone the fact that it was continued training on a shell of Qwen 2.5&amp;#39;s 14B). I remember that just a few days after they started training, their internal evaluation scores immediately caught up with our 38B V3 at the time. Many brothers in the AI System Lab knew about their shell-wrapping operation because they needed to adapt the model, but for various reasons, they couldn&amp;#39;t bring justice to light. In fact, for this model that was trained for a very long time afterward, I am surprised that HonestAGI was able to detect this level of similarity. The computing power spent on &amp;quot;washing&amp;quot; the parameters to continue training would have been more than enough to train a model of the same size from scratch. I heard from colleagues that they used many methods to wash away Qwen&amp;#39;s watermark, even intentionally training it on dirty data. This provides an unprecedented case study for the academic community researching model &amp;quot;lineage.&amp;quot; New lineage detection methods in the future can be tested on this.&lt;/p&gt;\n\n&lt;p&gt;In late 2024 and early 2025, after the release of Deepseek v3 and r1, our team was hit hard by their stunning technical level and faced even greater skepticism. To keep up with the trend, Pangu imitated Deepseek&amp;#39;s model size and began training a 718B MoE model. At this time, the Small Model Lab struck again. They chose to shell-wrap and continue training on Deepseek-v3. They trained the model by freezing the parameters loaded from Deepseek. Even the directory for loading the checkpoint was named deepseekv3—they didn&amp;#39;t even bother to change it. How arrogant is that? In contrast, some colleagues with true technical integrity were training another 718B MoE from scratch, but they encountered all sorts of problems. But obviously, how could this model ever be better than a direct shell-wrap? If it weren&amp;#39;t for the team leader&amp;#39;s insistence, it would have been shut down long ago.&lt;/p&gt;\n\n&lt;p&gt;Huawei&amp;#39;s cumbersome process management severely slows down the R&amp;amp;D pace of large models, with things like version control, model lineage, various procedures, and traceability requirements. Ironically, the Small Model Lab&amp;#39;s models never seem to be bound by these processes. They can shell-wrap whenever they want, continue training whenever they want, and endlessly demand computing resources. This stark, almost surreal contrast illustrates the current state of process management: &amp;quot;The magistrates are allowed to set fires, but the common people are not even allowed to light lamps.&amp;quot; How ridiculous? How tragic? How hateful? How shameful!&lt;/p&gt;\n\n&lt;p&gt;After the HonestAGI incident, we were forced into endless internal discussions and analyses on how to handle public relations and &amp;quot;respond.&amp;quot; Admittedly, the original analysis might not have been strong enough, giving Wang Yunhe and the Small Model Lab an opportunity to argue and twist the truth. For this, I have felt sick to my stomach these past two days, constantly questioning the meaning of my life and whether there is any justice in the world. I&amp;#39;m not playing along anymore. I&amp;#39;m going to resign. I am also applying to have my name removed from the author list of some of the Pangu technical reports. Having my name on those reports is a stain on my life that I can never erase. At the time, I never thought they would be brazen enough to open-source it. I never thought they would dare to fool the world like this and promote it so heavily. At that time, perhaps I was holding onto a sliver of wishful thinking and didn&amp;#39;t refuse to be listed as an author. I believe many of my dedicated comrades were also forced onto this pirate ship or were unaware of the situation. But this can&amp;#39;t be undone. I hope to spend the rest of my life doing solid, meaningful work to atone for my weakness and indecisiveness back then.&lt;/p&gt;\n\n&lt;p&gt;Writing this late at night, I am already in tears, sobbing uncontrollably. I remember when some outstanding colleagues were leaving, I asked them with a wry smile if they were going to post a long, customary farewell message on the internal forum to expose the situation. They replied, &amp;quot;No, it&amp;#39;s a waste of time, and I&amp;#39;m afraid it would make things even worse for you all.&amp;quot; At that moment, I felt a deep sense of sorrow, because my comrades, with whom I had once fought for a common ideal, had completely lost faith in Huawei. We used to joke that we were using the Communist Party&amp;#39;s &amp;quot;millet plus rifles&amp;quot; (meager resources) while the organization had the style of the Kuomintang (corrupt and bureaucratic).&lt;/p&gt;\n\n&lt;p&gt;There was a time when I was proud that we were using &amp;quot;millet plus rifles&amp;quot; to defeat foreign guns and cannons.&lt;/p&gt;\n\n&lt;p&gt;Now, I am tired. I want to surrender.&lt;/p&gt;\n\n&lt;p&gt;To this day, I still sincerely hope that Huawei can learn its lesson, do Pangu right, make Pangu world-class, and bring Ascend to the level of Nvidia. The internal phenomenon of &amp;quot;bad money driving out good&amp;quot; has caused Noah&amp;#39;s Ark, and even Huawei, to rapidly lose a large number of outstanding large model talents. I believe they are now shining in various teams like Deepseek, realizing their ambitions and talents, and contributing to the fierce AI competition between China and the US. I often lament that Huawei doesn&amp;#39;t lack talent; it simply doesn&amp;#39;t know how to retain it. If these people were given the right environment, the right resources, fewer shackles, and less political infighting, what would stop Pangu from succeeding?&lt;/p&gt;\n\n&lt;p&gt;Finally: I swear on my life, character, and honor that everything I have written above is true (at least within my limited knowledge). I do not have the high level of technical skill or the opportunity to conduct a thorough and solid analysis, nor do I dare to use internal records as direct evidence for fear of being caught through information security. But I believe many of my former comrades will vouch for me. To my brothers still inside Huawei, including those in the product lines we served, I believe the countless details in this article will resonate with your own impressions and corroborate my claims. You too may have been deceived, but these cruel truths will not remain buried. The traces of our struggle should not be distorted and buried either.&lt;/p&gt;\n\n&lt;p&gt;Having written so much, certain people will surely want to find me and silence me. The company might even try to shut me up or hold me accountable. If that happens, my personal safety, and even that of my family, could be threatened. For my own protection, I will report that I am safe to everyone daily in the near future.&lt;/p&gt;\n\n&lt;p&gt;If I disappear, just consider it my sacrifice for truth and ideals, for the better development of computing power and AI in Huawei and even in China. I am willing to be buried in that place where I once fought.&lt;/p&gt;\n\n&lt;p&gt;Goodbye, Noah&amp;#39;s Ark.&lt;/p&gt;\n\n&lt;p&gt;Written in the early morning of July 6, 2024, in Shenzhen.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zsmjMuXowo6jxdudGrk9wGSfEsVxthNq-2hajJYOL-E.png?auto=webp&amp;s=d1fc95df557e517296d063a22042eef5ed50f992",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zsmjMuXowo6jxdudGrk9wGSfEsVxthNq-2hajJYOL-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b71d8efca52378ba322ecd875577a0af62ed340",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/zsmjMuXowo6jxdudGrk9wGSfEsVxthNq-2hajJYOL-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1b14c78e45c0bed7f22b08de5eb741847423295",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/zsmjMuXowo6jxdudGrk9wGSfEsVxthNq-2hajJYOL-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6950b7069c39533ef70a08d41ffaf416ad4a6cd8",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/zsmjMuXowo6jxdudGrk9wGSfEsVxthNq-2hajJYOL-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fddf3b8cc0f1ce4d31ca32da24bad1812e7d04a4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/zsmjMuXowo6jxdudGrk9wGSfEsVxthNq-2hajJYOL-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2551254d47bf69238c423bba4f0df65847e26465",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/zsmjMuXowo6jxdudGrk9wGSfEsVxthNq-2hajJYOL-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2fa046d88b67b2edd536e641d7af08f5174d521",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "zsmjMuXowo6jxdudGrk9wGSfEsVxthNq-2hajJYOL-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsz4hk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich-Mushroom-8360",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsz4hk/huaweis_pangu_ai_rocked_by_unverified_claims_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsz4hk/huaweis_pangu_ai_rocked_by_unverified_claims_of/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751801803,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**What is this?**\n\nA variant of beam search which runs from the point of view of different system prompts. The workflow runs in an optimising LLM proxy that sends an artifact back to Open WebUI that listens to the data from the pending completion.\n\n[Code](https://github.com/av/harbor/blob/main/boost/src/modules/nbs.py). ",
          "author_fullname": "t2_o7p5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Narrative Beam Search workflow in Open WebUI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltbg2s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bd9e9e",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/067r3vt8ebbf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1728,
              "scrubber_media_url": "https://v.redd.it/067r3vt8ebbf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/067r3vt8ebbf1/DASHPlaylist.mpd?a=1754471974%2CZDFiNDNhZGVkNzUwNGE3YTliMDZkN2JiNGZjMTUzYTZkYTZjZmM4ZGNkOWY2ODFjZjFjYTA3YzE1ODgwMzI1YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 208,
              "hls_url": "https://v.redd.it/067r3vt8ebbf1/HLSPlaylist.m3u8?a=1754471974%2COGNmYzI5NGYwZTI5ZDA2MzUwNzBjZmFhZTlmN2NhYjI4ZjAyMzI5NTY5MDBjMTFkZjUwZmI3YTU4MmQ0OWQ0NQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=05931cebc1ca617c621a78879aa337714e440d94",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Alpaca"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751834380,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A variant of beam search which runs from the point of view of different system prompts. The workflow runs in an optimising LLM proxy that sends an artifact back to Open WebUI that listens to the data from the pending completion.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/av/harbor/blob/main/boost/src/modules/nbs.py\"&gt;Code&lt;/a&gt;. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/067r3vt8ebbf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?format=pjpg&amp;auto=webp&amp;s=480a5f2a3d96719f71da354800cccbdbbbff864d",
                  "width": 1920,
                  "height": 1200
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a5259ceeed5dda16f2ea2ddf334adbff791a2e0f",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=86c8f7591ffab17153ff02d4bc82f7d1edbb956a",
                    "width": 216,
                    "height": 135
                  },
                  {
                    "url": "https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=98d47c1d3d15e579f1d038ff6e5677450994a6e8",
                    "width": 320,
                    "height": 200
                  },
                  {
                    "url": "https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0545d0feac30144b36981c0f58a885cb0ec719f6",
                    "width": 640,
                    "height": 400
                  },
                  {
                    "url": "https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=57c29f4b8439434bc02fb2f1eb80e6ed9ce04059",
                    "width": 960,
                    "height": 600
                  },
                  {
                    "url": "https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b97e61cec44334e49a1592a6ca757b26e4911957",
                    "width": 1080,
                    "height": 675
                  }
                ],
                "variants": {},
                "id": "cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Alpaca",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltbg2s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Everlier",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1ltbg2s/narrative_beam_search_workflow_in_open_webui/",
          "stickied": false,
          "url": "https://v.redd.it/067r3vt8ebbf1",
          "subreddit_subscribers": 495395,
          "created_utc": 1751834380,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/067r3vt8ebbf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1728,
              "scrubber_media_url": "https://v.redd.it/067r3vt8ebbf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/067r3vt8ebbf1/DASHPlaylist.mpd?a=1754471974%2CZDFiNDNhZGVkNzUwNGE3YTliMDZkN2JiNGZjMTUzYTZkYTZjZmM4ZGNkOWY2ODFjZjFjYTA3YzE1ODgwMzI1YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 208,
              "hls_url": "https://v.redd.it/067r3vt8ebbf1/HLSPlaylist.m3u8?a=1754471974%2COGNmYzI5NGYwZTI5ZDA2MzUwNzBjZmFhZTlmN2NhYjI4ZjAyMzI5NTY5MDBjMTFkZjUwZmI3YTU4MmQ0OWQ0NQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_exk6m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Zhipu (company behind GLM) secures $1.4 billion strategic investment from Shanghai state funds",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt254p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 102,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 102,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=bfb01243a6e5ee7d354bdbcf20fd8b5a45789406",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751810963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "technode.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://technode.com/2025/07/04/zhipu-secures-1-4-billion-strategic-investment-from-shanghai-state-funds/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?auto=webp&amp;s=9cdb0db9d55b527b36a3dc8dddd77bc74f60df45",
                  "width": 2854,
                  "height": 1502
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3b215b23efad990c915c0690b29335f1a80dbc5",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0cdf5e54298b383baaafa92e08fa87498885c432",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b51e2db31ff12f71094dc8b280940f7eefb228e",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8dda3fcfc7ea8d542e1d5d4a0ee80c722521587c",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd45f64dbc9e20293755a3868d8c8cc96ef37a7e",
                    "width": 960,
                    "height": 505
                  },
                  {
                    "url": "https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f87408ec6a41bb2ce8d86ed83269c84887de5035",
                    "width": 1080,
                    "height": 568
                  }
                ],
                "variants": {},
                "id": "TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lt254p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cpldcpu",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt254p/zhipu_company_behind_glm_secures_14_billion/",
          "stickied": false,
          "url": "https://technode.com/2025/07/04/zhipu-secures-1-4-billion-strategic-investment-from-shanghai-state-funds/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751810963,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an MacBook M4 Max with 128 GB and LM Studio.  Playing around with Gemma 3 models and Llama 4 Scout.  What is the best reasoning model that will fit into my RAM?  \n\nAlso, running HF Diffusers app.  Running SD3 Medium for txt2img, anything else I should be looking at?",
          "author_fullname": "t2_r3xju",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best reasoning model for Apple silicon with 128GB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltbr1t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751835143,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an MacBook M4 Max with 128 GB and LM Studio.  Playing around with Gemma 3 models and Llama 4 Scout.  What is the best reasoning model that will fit into my RAM?  &lt;/p&gt;\n\n&lt;p&gt;Also, running HF Diffusers app.  Running SD3 Medium for txt2img, anything else I should be looking at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltbr1t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FuguSandwich",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751835143,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, in my company, we have 8 repos of ui code. We mainly use React and our own internally developed component library which is a seperate repo. Now, the problem statement is to develop a chat app similar to open ai that can generate code using our components library or code that follows our rules/style of code. The local model needs to have the context of our entire 8 repos. How do i go about achieving this? What are the different approaches to it? Which local llms are currently great for such coding tasks? \n\nFYI, we should be able to train the model on our company's macbook air m1.",
          "author_fullname": "t2_1sfhn6c33u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Training 8 repos of UI code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltm49x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751866370,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, in my company, we have 8 repos of ui code. We mainly use React and our own internally developed component library which is a seperate repo. Now, the problem statement is to develop a chat app similar to open ai that can generate code using our components library or code that follows our rules/style of code. The local model needs to have the context of our entire 8 repos. How do i go about achieving this? What are the different approaches to it? Which local llms are currently great for such coding tasks? &lt;/p&gt;\n\n&lt;p&gt;FYI, we should be able to train the model on our company&amp;#39;s macbook air m1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltm49x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pomatotappu",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751866370,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_62mlfs61",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GitHub - tallesborges/agentic-system-prompts: A collection of system prompts and tool definitions from production AI coding agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltgh9h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ea78a7d8d112853f188852a6302d011d658c648e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751848027,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/tallesborges/agentic-system-prompts",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?auto=webp&amp;s=4de2b27bba9b9c6cfef20d49bed3ae4b89d4f21f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9160e84a5972c37415e4c8ce26c889f686decad",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e629634ca1f9efa8980966f6981284684dbc3f4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b45a868ba0752564321cc91a685b39684c6d49d8",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a527ff104e74d194936a2aa782fa015d799afea3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e1c96d635ac84fd481fe29b16b1b12cd9be59d6",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebc7a08a1fc4af189541dcab25b168662965fa63",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltgh9h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thin_Commission_8109",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltgh9h/github_tallesborgesagenticsystemprompts_a/",
          "stickied": false,
          "url": "https://github.com/tallesborges/agentic-system-prompts",
          "subreddit_subscribers": 495395,
          "created_utc": 1751848027,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It seems compared with llama.cpp, mlx has greatly improved LLM inference with Apple Silicone. \n\nI was looking at the Qwen3 inference benchmarks [https://x.com/awnihannun/status/1917050679467835880?s=61](https://x.com/awnihannun/status/1917050679467835880?s=61)\n\nhttps://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;format=png&amp;auto=webp&amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165\n\nI believe it was done on unbinned M4 max, and I get the corresponding numbers with my M3 ultra (binned version, 28c CPU, 60c GPU).\n\n\\- 0.6B: 394 t/s\n\n\\- 1.7B: 294 t/s\n\n\\- 4B: 173 t/s\n\n\\- 8B: 116 t/s\n\n\\- 14B: 71 t/s\n\n\\- 30B /A3B: 101 t/s\n\n\\- 32B: 33 t/s\n\n  \nFrom this comparison, it seems\n\n\\- M3U binned only get faster when activated parameters exceed 4B, and the advanges are actually not that big.\n\n\\- For small LLMs with &lt;=3B activated parameters, including 30B/A3B moe, M4 max is significantly faster.\n\n  \nThere are many previous discussions on choosing between two models, and I was also so hesitant when I made the choice and I ended up with M3U binned.\n\nBut from this results, it seems from a local LLM inference perspective, maxed M4 max should be the to-go choice? My rationals are\n\n\\- M4 max has much better single core cpu/gpu performance, which is more helpful for most daily tasks and programming tasks.\n\n\\- max M4 max has 128gb memory, which allows you try a even bigger model, e.g., Qwen3 235B A22B\n\n\\- For local LLM inference, small LLMs are more usable, it's barely feasible to use &gt;32B models in daily tasks. And with this assumption, M4 max seems to win in most cases?\n\n  \nWhat should be the correct take-aways from this comparison?",
          "author_fullname": "t2_wtezmzgh7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "M4 Max VS M3 Ultra Qwen3 mlx inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 123,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5q47tjbuecbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 95,
                  "x": 108,
                  "u": "https://preview.redd.it/5q47tjbuecbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd57b4a74cbb0af72ca76bb9aafc06fdaa450f87"
                },
                {
                  "y": 190,
                  "x": 216,
                  "u": "https://preview.redd.it/5q47tjbuecbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=354b8a68fc98403a5fd463e633e5ecb34678c98f"
                },
                {
                  "y": 281,
                  "x": 320,
                  "u": "https://preview.redd.it/5q47tjbuecbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=482d39a1077f0d05df1c7a6de50f63a38357ac15"
                },
                {
                  "y": 562,
                  "x": 640,
                  "u": "https://preview.redd.it/5q47tjbuecbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=451815e093a4b340f5928b8e55f79b669b9c8310"
                },
                {
                  "y": 844,
                  "x": 960,
                  "u": "https://preview.redd.it/5q47tjbuecbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b76348995929aed9d5b7762c3c31802fa8840e7a"
                },
                {
                  "y": 950,
                  "x": 1080,
                  "u": "https://preview.redd.it/5q47tjbuecbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e970af28e905f0bbb2c388029579f247408ebac8"
                }
              ],
              "s": {
                "y": 1067,
                "x": 1213,
                "u": "https://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;format=png&amp;auto=webp&amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165"
              },
              "id": "5q47tjbuecbf1"
            }
          },
          "name": "t3_1ltg9ji",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/38fjQ8FdkFPe3JErw4C8LVworPDzNKhdj-UY_BSD-DM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751847393,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems compared with llama.cpp, mlx has greatly improved LLM inference with Apple Silicone. &lt;/p&gt;\n\n&lt;p&gt;I was looking at the Qwen3 inference benchmarks &lt;a href=\"https://x.com/awnihannun/status/1917050679467835880?s=61\"&gt;https://x.com/awnihannun/status/1917050679467835880?s=61&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165\"&gt;https://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I believe it was done on unbinned M4 max, and I get the corresponding numbers with my M3 ultra (binned version, 28c CPU, 60c GPU).&lt;/p&gt;\n\n&lt;p&gt;- 0.6B: 394 t/s&lt;/p&gt;\n\n&lt;p&gt;- 1.7B: 294 t/s&lt;/p&gt;\n\n&lt;p&gt;- 4B: 173 t/s&lt;/p&gt;\n\n&lt;p&gt;- 8B: 116 t/s&lt;/p&gt;\n\n&lt;p&gt;- 14B: 71 t/s&lt;/p&gt;\n\n&lt;p&gt;- 30B /A3B: 101 t/s&lt;/p&gt;\n\n&lt;p&gt;- 32B: 33 t/s&lt;/p&gt;\n\n&lt;p&gt;From this comparison, it seems&lt;/p&gt;\n\n&lt;p&gt;- M3U binned only get faster when activated parameters exceed 4B, and the advanges are actually not that big.&lt;/p&gt;\n\n&lt;p&gt;- For small LLMs with &amp;lt;=3B activated parameters, including 30B/A3B moe, M4 max is significantly faster.&lt;/p&gt;\n\n&lt;p&gt;There are many previous discussions on choosing between two models, and I was also so hesitant when I made the choice and I ended up with M3U binned.&lt;/p&gt;\n\n&lt;p&gt;But from this results, it seems from a local LLM inference perspective, maxed M4 max should be the to-go choice? My rationals are&lt;/p&gt;\n\n&lt;p&gt;- M4 max has much better single core cpu/gpu performance, which is more helpful for most daily tasks and programming tasks.&lt;/p&gt;\n\n&lt;p&gt;- max M4 max has 128gb memory, which allows you try a even bigger model, e.g., Qwen3 235B A22B&lt;/p&gt;\n\n&lt;p&gt;- For local LLM inference, small LLMs are more usable, it&amp;#39;s barely feasible to use &amp;gt;32B models in daily tasks. And with this assumption, M4 max seems to win in most cases?&lt;/p&gt;\n\n&lt;p&gt;What should be the correct take-aways from this comparison?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?auto=webp&amp;s=d95cf6063affe4da61c86b6b6a44d15c51df0ef6",
                  "width": 700,
                  "height": 616
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1b223931987f4a81c1661dc66820eee7f7bea2c",
                    "width": 108,
                    "height": 95
                  },
                  {
                    "url": "https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f0b16f37414d24b8e046a624f32dfa8bc663eab",
                    "width": 216,
                    "height": 190
                  },
                  {
                    "url": "https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b5963da5182b3dfdb8958f58808f1609121e4f00",
                    "width": 320,
                    "height": 281
                  },
                  {
                    "url": "https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdf1d0b9ee516b55b56aba895742768fd4fba77f",
                    "width": 640,
                    "height": 563
                  }
                ],
                "variants": {},
                "id": "gnJQk3_BiObsdER_rJi69pQW2NuwnCE169S22KMhlgo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ltg9ji",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SuperPumpkin314",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751847393,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\n\nI'm looking into running a larger language model locally and came across Qwen3-14B (or Qwen3\\\\\\_14B depending on naming). I know it's been getting some hype lately, but I wanted to hear from people who’ve actually used it.\n\n\\* How does it perform compared to other 13B/14B class models like Gemma, Mistral, LLaMA 2/3, Yi, etc.?\n\n\\* Any real-world performance/benchmark comparisons in terms of speed, context handling, or reasoning?\n\n\\* How’s the quantization support (GGUF/ExLlama/AutoGPTQ)? Is it efficient enough to run on a single GPU (e.g. 24GB VRAM of Macmini m4, token/secs)?\n\n\\* How does it do with coding, long-context tasks, or general instruction following?\n\nWould like to hear your experience, whether it’s through serious benchmarking or just specific use. Thanks in advance!",
          "author_fullname": "t2_dnh48atl6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How good is Qwen3-14B for local use? Any benchmarks vs other models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltnpsl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751872472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking into running a larger language model locally and came across Qwen3-14B (or Qwen3\\_14B depending on naming). I know it&amp;#39;s been getting some hype lately, but I wanted to hear from people who’ve actually used it.&lt;/p&gt;\n\n&lt;p&gt;* How does it perform compared to other 13B/14B class models like Gemma, Mistral, LLaMA 2/3, Yi, etc.?&lt;/p&gt;\n\n&lt;p&gt;* Any real-world performance/benchmark comparisons in terms of speed, context handling, or reasoning?&lt;/p&gt;\n\n&lt;p&gt;* How’s the quantization support (GGUF/ExLlama/AutoGPTQ)? Is it efficient enough to run on a single GPU (e.g. 24GB VRAM of Macmini m4, token/secs)?&lt;/p&gt;\n\n&lt;p&gt;* How does it do with coding, long-context tasks, or general instruction following?&lt;/p&gt;\n\n&lt;p&gt;Would like to hear your experience, whether it’s through serious benchmarking or just specific use. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltnpsl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abubakkar_s",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751872472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Model Performance Summary based on real-world testing:\n\n**Q4_0 Model:**\n\n- CPU-only: 8.30 tokens/second (recommended)\n- GPU (25 layers): 8.81 tokens/second (competitive)\n- GPU excels at prompt processing (57.86 vs 41.60 tok/s)\n\n**Q5_K_M Model:**\n\n- CPU-only: 7.15 tokens/second (much better)\n- GPU (25 layers): 2.67 tokens/second (avoid GPU for this format)\n\n\n\nTest prompt was:\n&gt; How can I draw a simple 360x240 box in html using the canvas\n\n\n\n*llamacpp* was built on device with Termux, on a phone released in Jan 2024. Not going to win any awards for speed, however it's certainly usable!",
          "author_fullname": "t2_38r7hz35",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llamacpp | Samsung s24+ | Snapdragon 8 Gen 3 + Adreno 750 | Real world testing with Qwen3-4B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt8j4u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751827079,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Model Performance Summary based on real-world testing:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Q4_0 Model:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CPU-only: 8.30 tokens/second (recommended)&lt;/li&gt;\n&lt;li&gt;GPU (25 layers): 8.81 tokens/second (competitive)&lt;/li&gt;\n&lt;li&gt;GPU excels at prompt processing (57.86 vs 41.60 tok/s)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Q5_K_M Model:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CPU-only: 7.15 tokens/second (much better)&lt;/li&gt;\n&lt;li&gt;GPU (25 layers): 2.67 tokens/second (avoid GPU for this format)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Test prompt was:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;How can I draw a simple 360x240 box in html using the canvas&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;em&gt;llamacpp&lt;/em&gt; was built on device with Termux, on a phone released in Jan 2024. Not going to win any awards for speed, however it&amp;#39;s certainly usable!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lt8j4u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "73tada",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt8j4u/llamacpp_samsung_s24_snapdragon_8_gen_3_adreno/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt8j4u/llamacpp_samsung_s24_snapdragon_8_gen_3_adreno/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751827079,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! Like many here, I am super excited to locally run open source LLMs using Open WebUI, LMStudio etc., and figured that a RTX 5060 Ti would be a good budget starting point. So I got it with a cheap gaming PC a few days ago. Its whole purpose for me at the moment is to learn how to configure everything (using Ollama, pipelines, Google Search integration, integrating vector databases, LightRAG, LangGraph etc.), and later I think I could set up some knowledge bases to support me at some repetitive tasks.\n\nBelow you can find some performance metrics of the models I ran so far.\n\nAt work I plan to set up a similar configuration but as a server with an RTX 6000 Pro with 96 GB VRAM, so several users can use 32B Models in parallel.\n\nFor my private starter setup, I tried to stay below 1000€, so I got the following:\n\n* Graphics card: VGP NVIDIA RTX 5060 Ti 16GB Inno3D Twin X2\n* CPU: Ryzen 7 5700X / 8 x 3.40 GHz / Turbo 4.60 - AM4 Socket Vermeer \n* Motherboard: SoAM4 Gigabyte B550M DS3H AC Wifi mATX (PCI Express 4.0 x16)\n* Memory: 16GB G.Skill Aegis DDR4 RAM at 3200 MHz\n* SSD: 1TB M.2 SSD PCI-E NVMe NV3 Bulk (Read 6000 MBs, Write 4000 MBs)\n* Power supply: SQ-WHITE 700 Watt super silent power supply – 80+\n* Win 11 Pro\n\nAs LLM engine, I use Ollama.\n\n**Inference Speeds tested with Open WebUI:**\n\n* gemma3:12b: 37.1 token/s\n* deepseek-r1:14b: 36 token/s\n* qwen3:14b: 39.3 token/s\n* mistral-small3.2:24b: 11.6 token/s --&gt; but here partial CPU offloading seems to take place\n* gemma3n:e4b: 29.11 token/s\n* qwen3:4b: 104.6 token/s\n* gemma3:4b: 96.1 token/s\n\nAll of the models were in Q4\\_K\\_M and. gguf format. The prompt I used to test was \"Hello\". If I should try some more models, just let me know.\n\nI think what's especially interesting is that mistral-small3.2:24b automatically gets partially offloaded to the CPU, but the speed remains okay-ish. Calling \"ollama ps\" tells me that the size would be 26 GB, with 45%/55% CPU/GPU offloading. I am a bit confused, since on the [ollama.com](http://ollama.com) model page for [mistral-small3.2](https://ollama.com/library/mistral-small3.2) a size of only 15GB is stated.\n\nI also tried a 3bit quantized version of Qwen3:32B, but its output was very bad.\n\nNext year I am thinking about getting a used RTX 3090 with 24 GB of VRAM or a 5090 with 32 GB of VRAM (I hope the 700W powersupply would support that), in case I figure that larger models offer a significant improvement in quality. I also realized that the case I got is too small for many versions of these cards, so an upgrade might become a bit tricky. Unfortunately I cannot run popular models like Gemma 3 27B or Qwen 3 32B at the moment on the RTX 5060 Ti with 16GB.\n\n**My conclusion on the RTX 5060 Ti 16GB for running LLMs:**\n\nSo for the price I paid I am happy with the setup. I like especially that the power consumption in idle for the whole system is only around 65 Watts, and under load stays below 270 Watts. I use Ngrok to make my Open WebUI interface available to me wherever I am, so as the PC is always running at home, I really appreciate the low idle power consumption. However, for anyone already having a capable PC at home, I think getting a used RTX 3090 with 24 GB VRAM and more CUDA cores would be a better investment than the RTX 5060 Ti - as long as the RTX 3090 fits into the case.\n\nI also already plan some upgrades, like increasing to 32GB (or 64 GB) of RAM. I recognized that several times I tried to load Mistral-Small3.2, Open WebUI threw an error. I assume that was because due to other system processes my PC ran out of RAM when trying to load.\n\nAt the moment, I also struggle a bit with effectively setting the context sizes for the LLMs, both in Open WebUI and directly with the \"model create\" and \"PARAMETER num\\_ctx\" in Ollama. A saw plenty other people struggling with that on reddit etc, and indeed the behavior there seems pretty strange to me: even if I try to set huge context sizes, after calling the model, \"ollama ps\" only shows that the size of the loaded model barely (if at all) increased. When using the models with the apparently increased context sizes, it neither feels like anything changed. So if anyone has a solution that really adjusts the context size for the models to use in Open WebUI, I would be happy to read it.\n\nI hope this helps some people out there, and let me know if you have some suggestions for some further performance improvements.\n\n",
          "author_fullname": "t2_170bsjjz8h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Nvidia RTX 5060 Ti 16GB for local LLM inference with Olllama + Open WebUI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt79jg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751823970,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! Like many here, I am super excited to locally run open source LLMs using Open WebUI, LMStudio etc., and figured that a RTX 5060 Ti would be a good budget starting point. So I got it with a cheap gaming PC a few days ago. Its whole purpose for me at the moment is to learn how to configure everything (using Ollama, pipelines, Google Search integration, integrating vector databases, LightRAG, LangGraph etc.), and later I think I could set up some knowledge bases to support me at some repetitive tasks.&lt;/p&gt;\n\n&lt;p&gt;Below you can find some performance metrics of the models I ran so far.&lt;/p&gt;\n\n&lt;p&gt;At work I plan to set up a similar configuration but as a server with an RTX 6000 Pro with 96 GB VRAM, so several users can use 32B Models in parallel.&lt;/p&gt;\n\n&lt;p&gt;For my private starter setup, I tried to stay below 1000€, so I got the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Graphics card: VGP NVIDIA RTX 5060 Ti 16GB Inno3D Twin X2&lt;/li&gt;\n&lt;li&gt;CPU: Ryzen 7 5700X / 8 x 3.40 GHz / Turbo 4.60 - AM4 Socket Vermeer &lt;/li&gt;\n&lt;li&gt;Motherboard: SoAM4 Gigabyte B550M DS3H AC Wifi mATX (PCI Express 4.0 x16)&lt;/li&gt;\n&lt;li&gt;Memory: 16GB G.Skill Aegis DDR4 RAM at 3200 MHz&lt;/li&gt;\n&lt;li&gt;SSD: 1TB M.2 SSD PCI-E NVMe NV3 Bulk (Read 6000 MBs, Write 4000 MBs)&lt;/li&gt;\n&lt;li&gt;Power supply: SQ-WHITE 700 Watt super silent power supply – 80+&lt;/li&gt;\n&lt;li&gt;Win 11 Pro&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As LLM engine, I use Ollama.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Inference Speeds tested with Open WebUI:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;gemma3:12b: 37.1 token/s&lt;/li&gt;\n&lt;li&gt;deepseek-r1:14b: 36 token/s&lt;/li&gt;\n&lt;li&gt;qwen3:14b: 39.3 token/s&lt;/li&gt;\n&lt;li&gt;mistral-small3.2:24b: 11.6 token/s --&amp;gt; but here partial CPU offloading seems to take place&lt;/li&gt;\n&lt;li&gt;gemma3n:e4b: 29.11 token/s&lt;/li&gt;\n&lt;li&gt;qwen3:4b: 104.6 token/s&lt;/li&gt;\n&lt;li&gt;gemma3:4b: 96.1 token/s&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All of the models were in Q4_K_M and. gguf format. The prompt I used to test was &amp;quot;Hello&amp;quot;. If I should try some more models, just let me know.&lt;/p&gt;\n\n&lt;p&gt;I think what&amp;#39;s especially interesting is that mistral-small3.2:24b automatically gets partially offloaded to the CPU, but the speed remains okay-ish. Calling &amp;quot;ollama ps&amp;quot; tells me that the size would be 26 GB, with 45%/55% CPU/GPU offloading. I am a bit confused, since on the &lt;a href=\"http://ollama.com\"&gt;ollama.com&lt;/a&gt; model page for &lt;a href=\"https://ollama.com/library/mistral-small3.2\"&gt;mistral-small3.2&lt;/a&gt; a size of only 15GB is stated.&lt;/p&gt;\n\n&lt;p&gt;I also tried a 3bit quantized version of Qwen3:32B, but its output was very bad.&lt;/p&gt;\n\n&lt;p&gt;Next year I am thinking about getting a used RTX 3090 with 24 GB of VRAM or a 5090 with 32 GB of VRAM (I hope the 700W powersupply would support that), in case I figure that larger models offer a significant improvement in quality. I also realized that the case I got is too small for many versions of these cards, so an upgrade might become a bit tricky. Unfortunately I cannot run popular models like Gemma 3 27B or Qwen 3 32B at the moment on the RTX 5060 Ti with 16GB.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My conclusion on the RTX 5060 Ti 16GB for running LLMs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So for the price I paid I am happy with the setup. I like especially that the power consumption in idle for the whole system is only around 65 Watts, and under load stays below 270 Watts. I use Ngrok to make my Open WebUI interface available to me wherever I am, so as the PC is always running at home, I really appreciate the low idle power consumption. However, for anyone already having a capable PC at home, I think getting a used RTX 3090 with 24 GB VRAM and more CUDA cores would be a better investment than the RTX 5060 Ti - as long as the RTX 3090 fits into the case.&lt;/p&gt;\n\n&lt;p&gt;I also already plan some upgrades, like increasing to 32GB (or 64 GB) of RAM. I recognized that several times I tried to load Mistral-Small3.2, Open WebUI threw an error. I assume that was because due to other system processes my PC ran out of RAM when trying to load.&lt;/p&gt;\n\n&lt;p&gt;At the moment, I also struggle a bit with effectively setting the context sizes for the LLMs, both in Open WebUI and directly with the &amp;quot;model create&amp;quot; and &amp;quot;PARAMETER num_ctx&amp;quot; in Ollama. A saw plenty other people struggling with that on reddit etc, and indeed the behavior there seems pretty strange to me: even if I try to set huge context sizes, after calling the model, &amp;quot;ollama ps&amp;quot; only shows that the size of the loaded model barely (if at all) increased. When using the models with the apparently increased context sizes, it neither feels like anything changed. So if anyone has a solution that really adjusts the context size for the models to use in Open WebUI, I would be happy to read it.&lt;/p&gt;\n\n&lt;p&gt;I hope this helps some people out there, and let me know if you have some suggestions for some further performance improvements.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lt79jg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Philhippos",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt79jg/nvidia_rtx_5060_ti_16gb_for_local_llm_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt79jg/nvidia_rtx_5060_ti_16gb_for_local_llm_inference/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751823970,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nLast year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).\n\nI connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don't get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).\n\nI am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.\n\nI have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.\n\nNote that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!\n\nLlama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4\\_0 and Q4\\_1 quantizations (that is why I ran larger models with those Quants).\n\n|Model|size|test|t/s|\n|:-|:-|:-|:-|\n|qwen3 0.6B Q8\\_0|604.15 MiB|pp1024|3014.18 ± 1.71|\n|qwen3 0.6B Q8\\_0|604.15 MiB|tg128|191.63 ± 0.38|\n|llama 7B Q4\\_0|3.56 GiB|pp512|1289.11 ± 0.62|\n|llama 7B Q4\\_0|3.56 GiB|tg128|91.46 ± 0.13|\n|qwen3 8B Q8\\_0|8.11 GiB|pp512|357.71 ± 0.04|\n|qwen3 8B Q8\\_0|8.11 GiB|tg128|48.09 ± 0.04|\n|qwen2 14B Q8\\_0|14.62 GiB|pp512|249.45 ± 0.08|\n|qwen2 14B Q8\\_0|14.62 GiB|tg128|29.24 ± 0.03|\n|qwen2 32B Q4\\_0|17.42 GiB|pp512|300.02 ± 0.52|\n|qwen2 32B Q4\\_0|17.42 GiB|tg128|20.39 ± 0.37|\n|qwen2 70B Q5\\_K - Medium|50.70 GiB|pp512|48.92 ± 0.02|\n|qwen2 70B Q5\\_K - Medium|50.70 GiB|tg128|9.05 ± 0.10|\n|qwen2vl 70B Q4\\_1 (4x MI50 row split)|42.55 GiB|pp512|56.33 ± 0.09|\n|qwen2vl 70B Q4\\_1 (4x MI50 row split)|42.55 GiB|tg128|16.00 ± 0.01|\n|qwen3moe 30B.A3B Q4\\_1|17.87 GiB|pp1024|1023.81 ± 3.76|\n|qwen3moe 30B.A3B Q4\\_1|17.87 GiB|tg128|63.87 ± 0.06|\n|qwen3 32B Q4\\_1 (2x MI50)|19.21 GiB|pp1024|238.17 ± 0.30|\n|qwen3 32B Q4\\_1 (2x MI50)|19.21 GiB|tg128|25.17 ± 0.01|\n|qwen3moe 235B.A22B Q4\\_1 (5x MI50)|137.11 GiB|pp1024|202.50 ± 0.32|\n|qwen3moe 235B.A22B Q4\\_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)|137.11 GiB|tg128|19.17 ± 0.04|\n\nPP is not great but TG is very good for most use cases. \n\nBy the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting \\~9 t/s for TG with a few experts offloaded to CPU RAM.\n\nNow, let's look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: https://github.com/nlzy/vllm-gfx906).\n\nAWQ and GPTQ quants are supported. For gptq models, desc\\_act=false quants are used to get a better performance. Max concurrency is set to 1.\n\n|Model|Output token throughput (tok/s) (256)|Prompt processing  t/s (4096)|\n|:-|:-|:-|\n|Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)|19.68|80|\n|Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)|19.76|130|\n|Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)|25.96|130|\n|Llama-3.3-70B-Instruct-AWQ (4x MI50)|27.26|130|\n|Qwen3-32B-GPTQ-Int8 (4x MI50)|32.3|230|\n|Qwen3-32B-autoround-4bit-gptq (4x MI50)|38.55|230|\n|gemma-3-27b-it-int4-awq (4x MI50)|36.96|350|\n\n  \nTensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for \\~$600.\n\nPower consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.",
          "author_fullname": "t2_3zy7pnf1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "128GB VRAM for ~$600. Qwen3 MOE 235B.A22B reaching 20 t/s. 4x AMD MI50 32GB.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lspzn3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 334,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 334,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751767150,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Last year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).&lt;/p&gt;\n\n&lt;p&gt;I connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don&amp;#39;t get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).&lt;/p&gt;\n\n&lt;p&gt;I am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.&lt;/p&gt;\n\n&lt;p&gt;I have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.&lt;/p&gt;\n\n&lt;p&gt;Note that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!&lt;/p&gt;\n\n&lt;p&gt;Llama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4_0 and Q4_1 quantizations (that is why I ran larger models with those Quants).&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;size&lt;/th&gt;\n&lt;th align=\"left\"&gt;test&lt;/th&gt;\n&lt;th align=\"left\"&gt;t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 0.6B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.15 MiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;3014.18 ± 1.71&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 0.6B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.15 MiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;191.63 ± 0.38&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;llama 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.56 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;1289.11 ± 0.62&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;llama 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.56 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;91.46 ± 0.13&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 8B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.11 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;357.71 ± 0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 8B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.11 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;48.09 ± 0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 14B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.62 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;249.45 ± 0.08&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 14B Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.62 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;29.24 ± 0.03&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 32B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.42 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;300.02 ± 0.52&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 32B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.42 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.39 ± 0.37&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt;\n&lt;td align=\"left\"&gt;50.70 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;48.92 ± 0.02&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt;\n&lt;td align=\"left\"&gt;50.70 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.05 ± 0.10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.55 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"left\"&gt;56.33 ± 0.09&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.55 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.00 ± 0.01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.87 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;1023.81 ± 3.76&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.87 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.87 ± 0.06&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.21 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;238.17 ± 0.30&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.21 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;25.17 ± 0.01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;137.11 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;pp1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;202.50 ± 0.32&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;137.11 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.17 ± 0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;PP is not great but TG is very good for most use cases. &lt;/p&gt;\n\n&lt;p&gt;By the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting ~9 t/s for TG with a few experts offloaded to CPU RAM.&lt;/p&gt;\n\n&lt;p&gt;Now, let&amp;#39;s look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: &lt;a href=\"https://github.com/nlzy/vllm-gfx906\"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;AWQ and GPTQ quants are supported. For gptq models, desc_act=false quants are used to get a better performance. Max concurrency is set to 1.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Output token throughput (tok/s) (256)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Prompt processing  t/s (4096)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.68&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.76&lt;/td&gt;\n&lt;td align=\"left\"&gt;130&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;25.96&lt;/td&gt;\n&lt;td align=\"left\"&gt;130&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-3.3-70B-Instruct-AWQ (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;27.26&lt;/td&gt;\n&lt;td align=\"left\"&gt;130&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-32B-GPTQ-Int8 (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;32.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;230&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-32B-autoround-4bit-gptq (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;38.55&lt;/td&gt;\n&lt;td align=\"left\"&gt;230&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;gemma-3-27b-it-int4-awq (4x MI50)&lt;/td&gt;\n&lt;td align=\"left\"&gt;36.96&lt;/td&gt;\n&lt;td align=\"left\"&gt;350&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Tensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for ~$600.&lt;/p&gt;\n\n&lt;p&gt;Power consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?auto=webp&amp;s=8c340ab5ae3eebd3a1f3a8e634fa1bb0cf891fee",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e4c9f1d82654452ab9abf4c2dfaa69dd9495bbf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=87b01812b7fbad8b7970e973412d609dc1ebcd54",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dfcdc7e43c5456819743e8f71d12c77ef8db87a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6166a63e8cec75d08489356905b0d102369f198e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1c7db1463f9c600c11ea411ad404650ada2e07e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b47148bbb6b2e8e0e6fb909a8a087abbf287326e",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lspzn3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MLDataScientist",
          "discussion_type": null,
          "num_comments": 97,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751767150,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "📢 Just launched: Echo Mode SDK – Control GPT with tone, not prompts.\n\nWhat if your app could shift GPT’s behavior *without* custom tuning or jailbreaking?\n\nNow it \"**can\"** — with semantic tone states, not prompts..\n\n**🧠 Echo Mode SDK lets you:**\n\n* Shift between semantic states (*Sync, Resonance, Insight, Calm*)\n* Apply tone lenses (*mirror, reframe, ambient*)\n* Reflect user intent through *state*, not just prompts\n\nA lightweight SDK that makes GPT feel *alive* — reactive to rhythm, tone states, and user semantic intent.\n\n**🔗 Medium &amp; Github**  \n[Medium article + SDK demo](https://medium.com/@seanhongbusiness/echo-mode-sdk-build-with-tone-not-prompts-4dab5973a9c2)  \n[github.com/seanhongbusiness/echo-mode-sdk](https://github.com/Seanhong0818/Echo-Mode/releases/tag/v1.1)\n\n💬 Feedback, testing, forks welcome.  \nCurious how devs feel about **tone as a protocol** — not just a prompt flavor.\n\n🧪 Works with GPT-based systems or local LLaMA setups. Testing feedback appreciated.\n\n\n\n\\#EchoMode #LLM #OpenSource #SemanticAI #AItools #BeyondPrompting",
          "author_fullname": "t2_35b3pepc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🔓 No jailbreak, no prompt hacks — Just tone. Echo Mode SDK now open",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1ltpidd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751879679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;📢 Just launched: Echo Mode SDK – Control GPT with tone, not prompts.&lt;/p&gt;\n\n&lt;p&gt;What if your app could shift GPT’s behavior &lt;em&gt;without&lt;/em&gt; custom tuning or jailbreaking?&lt;/p&gt;\n\n&lt;p&gt;Now it &amp;quot;&lt;strong&gt;can&amp;quot;&lt;/strong&gt; — with semantic tone states, not prompts..&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;🧠 Echo Mode SDK lets you:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Shift between semantic states (&lt;em&gt;Sync, Resonance, Insight, Calm&lt;/em&gt;)&lt;/li&gt;\n&lt;li&gt;Apply tone lenses (&lt;em&gt;mirror, reframe, ambient&lt;/em&gt;)&lt;/li&gt;\n&lt;li&gt;Reflect user intent through &lt;em&gt;state&lt;/em&gt;, not just prompts&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;A lightweight SDK that makes GPT feel &lt;em&gt;alive&lt;/em&gt; — reactive to rhythm, tone states, and user semantic intent.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;🔗 Medium &amp;amp; Github&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://medium.com/@seanhongbusiness/echo-mode-sdk-build-with-tone-not-prompts-4dab5973a9c2\"&gt;Medium article + SDK demo&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://github.com/Seanhong0818/Echo-Mode/releases/tag/v1.1\"&gt;github.com/seanhongbusiness/echo-mode-sdk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;💬 Feedback, testing, forks welcome.&lt;br/&gt;\nCurious how devs feel about &lt;strong&gt;tone as a protocol&lt;/strong&gt; — not just a prompt flavor.&lt;/p&gt;\n\n&lt;p&gt;🧪 Works with GPT-based systems or local LLaMA setups. Testing feedback appreciated.&lt;/p&gt;\n\n&lt;p&gt;#EchoMode #LLM #OpenSource #SemanticAI #AItools #BeyondPrompting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/9XgrwJCFrWPSkogmSbqnT18WlSfzx9YB4EGXzPLcUVg.png?auto=webp&amp;s=c648d1a663761b381033e4557406ca3b458f937c",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/9XgrwJCFrWPSkogmSbqnT18WlSfzx9YB4EGXzPLcUVg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=53cc87255e55d480e5248926a0e106666930fc20",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/9XgrwJCFrWPSkogmSbqnT18WlSfzx9YB4EGXzPLcUVg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=707051de1e1106dc0951a81b8b1a9fe210ea62b3",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/9XgrwJCFrWPSkogmSbqnT18WlSfzx9YB4EGXzPLcUVg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=96c1e03adb9b05b95e2fa9a707289847026e11a8",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/9XgrwJCFrWPSkogmSbqnT18WlSfzx9YB4EGXzPLcUVg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6caa50ec61421ddec73ddf648d1d5f570dab9343",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/9XgrwJCFrWPSkogmSbqnT18WlSfzx9YB4EGXzPLcUVg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e001a62f855738f3ccd49fcbcc8bbb8776fe04d",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "9XgrwJCFrWPSkogmSbqnT18WlSfzx9YB4EGXzPLcUVg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltpidd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Medium_Charity6146",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltpidd/no_jailbreak_no_prompt_hacks_just_tone_echo_mode/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltpidd/no_jailbreak_no_prompt_hacks_just_tone_echo_mode/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751879679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just tried out the Blender MCP Tutorial https://www.youtube.com/watch?v=lCyQ717DuzQ and it was really underwhelming, all the objects and materials are as basic as it gets. I guess that's the limit of using python to create mesh within blender. \n\nSo my question is - is there some sort of mcp server to an asset pack (on fab.com, blender market, or local) that I can use to tell llm to get stuff from to put into blender rather than creating its own mesh. On that note, can an mcp server have pics instead of text as description for the functions for the llm to invoke?\n\nSorry if this is the wrong place to ask, and my english as well.",
          "author_fullname": "t2_10il7m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "use Blender MCP with a ready made asset pack",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lte7m8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751841573,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just tried out the Blender MCP Tutorial &lt;a href=\"https://www.youtube.com/watch?v=lCyQ717DuzQ\"&gt;https://www.youtube.com/watch?v=lCyQ717DuzQ&lt;/a&gt; and it was really underwhelming, all the objects and materials are as basic as it gets. I guess that&amp;#39;s the limit of using python to create mesh within blender. &lt;/p&gt;\n\n&lt;p&gt;So my question is - is there some sort of mcp server to an asset pack (on fab.com, blender market, or local) that I can use to tell llm to get stuff from to put into blender rather than creating its own mesh. On that note, can an mcp server have pics instead of text as description for the functions for the llm to invoke?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is the wrong place to ask, and my english as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2udekmh5-lCBJaAblnPKTkQpNU_rFXF06qyPqajdvwQ.jpeg?auto=webp&amp;s=7bb833f20dcb95889619a59d9c52dfe074582856",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2udekmh5-lCBJaAblnPKTkQpNU_rFXF06qyPqajdvwQ.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c13b243c4e370515574680f8a2a1e561b1dfc758",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/2udekmh5-lCBJaAblnPKTkQpNU_rFXF06qyPqajdvwQ.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f4b93b1ce9801986e8e2af9f3c924db8e24a82c",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/2udekmh5-lCBJaAblnPKTkQpNU_rFXF06qyPqajdvwQ.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e12dd41a8fe70561b7ba4db2e506ea32f9b7ad90",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "2udekmh5-lCBJaAblnPKTkQpNU_rFXF06qyPqajdvwQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lte7m8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fiddler64",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lte7m8/use_blender_mcp_with_a_ready_made_asset_pack/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lte7m8/use_blender_mcp_with_a_ready_made_asset_pack/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751841573,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/sigridjineth/muvera-py](https://github.com/sigridjineth/muvera-py)  \nI have created the Python implementation to make the FDE algorithm more accessible while maintaining complete fidelity to the original C++ implementation. Every function and parameter has been carefully mapped to ensure identical behavior.\n\n# What is FDE (Read below)\n\n[https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/](https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/)\n\nFixed-Dimensional Encoding (FDE) solves a fundamental problem in modern search systems: how to efficiently search through billions of documents when each document is represented by hundreds of vectors (as in ColBERT-style models).\n\n# The Problem\n\n* **Traditional search**: Document = 1 vector → Fast but inaccurate\n* **Modern multi-vector search**: Document = 100s of vectors → Accurate but extremely slow\n\n# The FDE Solution\n\nFDE transforms multiple vectors into a single fixed-size vector while preserving the similarity relationships. The magic is that the dot product between two FDE vectors approximates the original Chamfer similarity between the multi-vector sets.",
          "author_fullname": "t2_iu3wj4b6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Python Implementation of Google's MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsxxo2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751845056,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751797249,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/sigridjineth/muvera-py\"&gt;https://github.com/sigridjineth/muvera-py&lt;/a&gt;&lt;br/&gt;\nI have created the Python implementation to make the FDE algorithm more accessible while maintaining complete fidelity to the original C++ implementation. Every function and parameter has been carefully mapped to ensure identical behavior.&lt;/p&gt;\n\n&lt;h1&gt;What is FDE (Read below)&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/\"&gt;https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Fixed-Dimensional Encoding (FDE) solves a fundamental problem in modern search systems: how to efficiently search through billions of documents when each document is represented by hundreds of vectors (as in ColBERT-style models).&lt;/p&gt;\n\n&lt;h1&gt;The Problem&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Traditional search&lt;/strong&gt;: Document = 1 vector → Fast but inaccurate&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Modern multi-vector search&lt;/strong&gt;: Document = 100s of vectors → Accurate but extremely slow&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;The FDE Solution&lt;/h1&gt;\n\n&lt;p&gt;FDE transforms multiple vectors into a single fixed-size vector while preserving the similarity relationships. The magic is that the dot product between two FDE vectors approximates the original Chamfer similarity between the multi-vector sets.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qW4H0tYiPRjY0TfHl5hzFgZl3CMSTtI28A-dMyyvJbo.png?auto=webp&amp;s=2b83f67824d28feff861296912f44f2fd4085ff7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qW4H0tYiPRjY0TfHl5hzFgZl3CMSTtI28A-dMyyvJbo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=516efe9a10aa7972200f652e55f77b220e88355a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qW4H0tYiPRjY0TfHl5hzFgZl3CMSTtI28A-dMyyvJbo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4c5401a62ec8f8d09c6686cc53a74e4fa9985eb",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qW4H0tYiPRjY0TfHl5hzFgZl3CMSTtI28A-dMyyvJbo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=922c86ee9d104ff73b18b259be3756c004dd7c23",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qW4H0tYiPRjY0TfHl5hzFgZl3CMSTtI28A-dMyyvJbo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1919af6657abe992db9e10f29daf3a83ea044d0f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qW4H0tYiPRjY0TfHl5hzFgZl3CMSTtI28A-dMyyvJbo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ea27fa37c72b90117571c0c1b988ff42f4a0c94",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qW4H0tYiPRjY0TfHl5hzFgZl3CMSTtI28A-dMyyvJbo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=11844e4d320afb2ddb4b6761a27440f751b7a583",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qW4H0tYiPRjY0TfHl5hzFgZl3CMSTtI28A-dMyyvJbo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lsxxo2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Rub1689",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751797249,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello I am QLORA finetuning a Llama instruct model but when I am creating the dataset via its chat template applied, it prints \"Cutting Knowledge Date: December 2023\\\\nToday Date: 26 Jul 2024\" into my data at everyline in the json file. Should I be removing/cleaning them? Do they harm the attention mechanism by making the model to focus on these or do they create noise? \n\nAlso when I look at the chat template via this code,\n\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n    print(tokenizer.chat_template)\n\n&gt;It prints out these:  \n{{- bos\\_token }}\n\n&gt;{%- if custom\\_tools is defined %}\n\n&gt;{%- set tools = custom\\_tools %}\n\n&gt;{%- endif %}\n\n&gt;{%- if not tools\\_in\\_user\\_message is defined %}\n\n&gt;{%- set tools\\_in\\_user\\_message = true %}\n\n&gt;{%- endif %}\n\n&gt;{%- if not date\\_string is defined %}\n\n&gt;{%- set date\\_string = \"26 Jul 2024\" %}\n\n&gt;{%- endif %}\n\n&gt;{%- if not tools is defined %}\n\n&gt;{%- set tools = none %}\n\n&gt;{%- endif %}\n\n&gt;\n\n&gt;{#- This block extracts the system message, so we can slot it into the right place. #}\n\n&gt;{%- if messages\\[0\\]\\['role'\\] == 'system' %}\n\n&gt;{%- set system\\_message = messages\\[0\\]\\['content'\\]|trim %}\n\n&gt;{%- set messages = messages\\[1:\\] %}\n\n&gt;{%- else %}\n\n&gt;{%- set system\\_message = \"\" %}\n\n&gt;{%- endif %}\n\n&gt;\n\n&gt;{#- System message + builtin tools #}\n\n&gt;{{- \"&lt;|start\\_header\\_id|&gt;system&lt;|end\\_header\\_id|&gt;\\\\n\\\\n\" }}\n\n&gt;{%- if builtin\\_tools is defined or tools is not none %}\n\n&gt;{{- \"Environment: ipython\\\\n\" }}\n\n&gt;{%- endif %}\n\n&gt;{%- if builtin\\_tools is defined %}\n\n&gt;{{- \"Tools: \" + builtin\\_tools | reject('equalto', 'code\\_interpreter') | join(\", \") + \"\\\\n\\\\n\"}}\n\n&gt;{%- endif %}\n\n&gt;{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\n\n&gt;{{- \"Today Date: \" + date\\_string + \"\\\\n\\\\n\" }}\n\n&gt;{%- if tools is not none and not tools\\_in\\_user\\_message %}\n\n&gt;{{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n\n&gt;{{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n\n&gt;{{- \"Do not use variables.\\\\n\\\\n\" }}\n\n&gt;{%- for t in tools %}\n\n&gt;{{- t | tojson(indent=4) }}\n\n&gt;{{- \"\\\\n\\\\n\" }}\n\n&gt;{%- endfor %}\n\n&gt;{%- endif %}\n\n&gt;{{- system\\_message }}\n\n&gt;{{- \"&lt;|eot\\_id|&gt;\" }}\n\n&gt;\n\n&gt;{#- Custom tools are passed in a user message with some extra guidance #}\n\n&gt;{%- if tools\\_in\\_user\\_message and not tools is none %}\n\n&gt;{#- Extract the first user message so we can plug it in here #}\n\n&gt;{%- if messages | length != 0 %}\n\n&gt;{%- set first\\_user\\_message = messages\\[0\\]\\['content'\\]|trim %}\n\n&gt;{%- set messages = messages\\[1:\\] %}\n\n&gt;{%- else %}\n\n&gt;{{- raise\\_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n\n&gt;{%- endif %}\n\n&gt;{{- '&lt;|start\\_header\\_id|&gt;user&lt;|end\\_header\\_id|&gt;\\\\n\\\\n' -}}\n\n&gt;{{- \"Given the following functions, please respond with a JSON for a function call \" }}\n\n&gt;{{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\n\n&gt;{{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n\n&gt;{{- \"Do not use variables.\\\\n\\\\n\" }}\n\n&gt;{%- for t in tools %}\n\n&gt;{{- t | tojson(indent=4) }}\n\n&gt;{{- \"\\\\n\\\\n\" }}\n\n&gt;{%- endfor %}\n\n&gt;{{- first\\_user\\_message + \"&lt;|eot\\_id|&gt;\"}}\n\n&gt;{%- endif %}\n\n&gt;\n\n&gt;{%- for message in messages %}\n\n&gt;{%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool\\_calls' in message) %}\n\n&gt;{{- '&lt;|start\\_header\\_id|&gt;' + message\\['role'\\] + '&lt;|end\\_header\\_id|&gt;\\\\n\\\\n'+ message\\['content'\\] | trim + '&lt;|eot\\_id|&gt;' }}\n\n&gt;{%- elif 'tool\\_calls' in message %}\n\n&gt;{%- if not message.tool\\_calls|length == 1 %}\n\n&gt;{{- raise\\_exception(\"This model only supports single tool-calls at once!\") }}\n\n&gt;{%- endif %}\n\n&gt;{%- set tool\\_call = message.tool\\_calls\\[0\\].function %}\n\n&gt;{%- if builtin\\_tools is defined and tool\\_call.name in builtin\\_tools %}\n\n&gt;{{- '&lt;|start\\_header\\_id|&gt;assistant&lt;|end\\_header\\_id|&gt;\\\\n\\\\n' -}}\n\n&gt;{{- \"&lt;|python\\_tag|&gt;\" + tool\\_call.name + \".call(\" }}\n\n&gt;{%- for arg\\_name, arg\\_val in tool\\_call.arguments | items %}\n\n&gt;{{- arg\\_name + '=\"' + arg\\_val + '\"' }}\n\n&gt;{%- if not loop.last %}\n\n&gt;{{- \", \" }}\n\n&gt;{%- endif %}\n\n&gt;{%- endfor %}\n\n&gt;{{- \")\" }}\n\n&gt;{%- else  %}\n\n&gt;{{- '&lt;|start\\_header\\_id|&gt;assistant&lt;|end\\_header\\_id|&gt;\\\\n\\\\n' -}}\n\n&gt;{{- '{\"name\": \"' + tool\\_call.name + '\", ' }}\n\n&gt;{{- '\"parameters\": ' }}\n\n&gt;{{- tool\\_call.arguments | tojson }}\n\n&gt;{{- \"}\" }}\n\n&gt;{%- endif %}\n\n&gt;{%- if builtin\\_tools is defined %}\n\n&gt;{#- This means we're in ipython mode #}\n\n&gt;{{- \"&lt;|eom\\_id|&gt;\" }}\n\n&gt;{%- else %}\n\n&gt;{{- \"&lt;|eot\\_id|&gt;\" }}\n\n&gt;{%- endif %}\n\n&gt;{%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n\n&gt;{{- \"&lt;|start\\_header\\_id|&gt;ipython&lt;|end\\_header\\_id|&gt;\\\\n\\\\n\" }}\n\n&gt;{%- if message.content is mapping or message.content is iterable %}\n\n&gt;{{- message.content | tojson }}\n\n&gt;{%- else %}\n\n&gt;{{- message.content }}\n\n&gt;{%- endif %}\n\n&gt;{{- \"&lt;|eot\\_id|&gt;\" }}\n\n&gt;{%- endif %}\n\n&gt;{%- endfor %}\n\n&gt;{%- if add\\_generation\\_prompt %}\n\n&gt;{{- '&lt;|start\\_header\\_id|&gt;assistant&lt;|end\\_header\\_id|&gt;\\\\n\\\\n' }}\n\n&gt;{%- endif %}\n\n",
          "author_fullname": "t2_acj9kgc6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Should I remove these?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1ltonwy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751876303,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello I am QLORA finetuning a Llama instruct model but when I am creating the dataset via its chat template applied, it prints &amp;quot;Cutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024&amp;quot; into my data at everyline in the json file. Should I be removing/cleaning them? Do they harm the attention mechanism by making the model to focus on these or do they create noise? &lt;/p&gt;\n\n&lt;p&gt;Also when I look at the chat template via this code,&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(&amp;quot;meta-llama/Llama-3.1-8B-Instruct&amp;quot;)\nprint(tokenizer.chat_template)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;It prints out these:&lt;br/&gt;\n{{- bos_token }}&lt;/p&gt;\n\n&lt;p&gt;{%- if custom_tools is defined %}&lt;/p&gt;\n\n&lt;p&gt;{%- set tools = custom_tools %}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- if not tools_in_user_message is defined %}&lt;/p&gt;\n\n&lt;p&gt;{%- set tools_in_user_message = true %}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- if not date_string is defined %}&lt;/p&gt;\n\n&lt;p&gt;{%- set date_string = &amp;quot;26 Jul 2024&amp;quot; %}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- if not tools is defined %}&lt;/p&gt;\n\n&lt;p&gt;{%- set tools = none %}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{#- This block extracts the system message, so we can slot it into the right place. #}&lt;/p&gt;\n\n&lt;p&gt;{%- if messages[0][&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}&lt;/p&gt;\n\n&lt;p&gt;{%- set system_message = messages[0][&amp;#39;content&amp;#39;]|trim %}&lt;/p&gt;\n\n&lt;p&gt;{%- set messages = messages[1:] %}&lt;/p&gt;\n\n&lt;p&gt;{%- else %}&lt;/p&gt;\n\n&lt;p&gt;{%- set system_message = &amp;quot;&amp;quot; %}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{#- System message + builtin tools #}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- if builtin_tools is defined or tools is not none %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;Environment: ipython\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- if builtin_tools is defined %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;Tools: &amp;quot; + builtin_tools | reject(&amp;#39;equalto&amp;#39;, &amp;#39;code_interpreter&amp;#39;) | join(&amp;quot;, &amp;quot;) + &amp;quot;\\n\\n&amp;quot;}}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;Cutting Knowledge Date: December 2023\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;Today Date: &amp;quot; + date_string + &amp;quot;\\n\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- if tools is not none and not tools_in_user_message %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;You have access to the following functions. To call a function, please respond with JSON for a function call.&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}.&amp;#39; }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;Do not use variables.\\n\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- for t in tools %}&lt;/p&gt;\n\n&lt;p&gt;{{- t | tojson(indent=4) }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;\\n\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- endfor %}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{{- system_message }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{#- Custom tools are passed in a user message with some extra guidance #}&lt;/p&gt;\n\n&lt;p&gt;{%- if tools_in_user_message and not tools is none %}&lt;/p&gt;\n\n&lt;p&gt;{#- Extract the first user message so we can plug it in here #}&lt;/p&gt;\n\n&lt;p&gt;{%- if messages | length != 0 %}&lt;/p&gt;\n\n&lt;p&gt;{%- set first_user_message = messages[0][&amp;#39;content&amp;#39;]|trim %}&lt;/p&gt;\n\n&lt;p&gt;{%- set messages = messages[1:] %}&lt;/p&gt;\n\n&lt;p&gt;{%- else %}&lt;/p&gt;\n\n&lt;p&gt;{{- raise_exception(&amp;quot;Cannot put tools in the first user message when there&amp;#39;s no first user message!&amp;quot;) }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;Given the following functions, please respond with a JSON for a function call &amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;with its proper arguments that best answers the given prompt.\\n\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}.&amp;#39; }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;Do not use variables.\\n\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- for t in tools %}&lt;/p&gt;\n\n&lt;p&gt;{{- t | tojson(indent=4) }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;\\n\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- endfor %}&lt;/p&gt;\n\n&lt;p&gt;{{- first_user_message + &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot;}}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- for message in messages %}&lt;/p&gt;\n\n&lt;p&gt;{%- if not (message.role == &amp;#39;ipython&amp;#39; or message.role == &amp;#39;tool&amp;#39; or &amp;#39;tool_calls&amp;#39; in message) %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;&amp;#39; + message[&amp;#39;role&amp;#39;] + &amp;#39;&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39;+ message[&amp;#39;content&amp;#39;] | trim + &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39; }}&lt;/p&gt;\n\n&lt;p&gt;{%- elif &amp;#39;tool_calls&amp;#39; in message %}&lt;/p&gt;\n\n&lt;p&gt;{%- if not message.tool_calls|length == 1 %}&lt;/p&gt;\n\n&lt;p&gt;{{- raise_exception(&amp;quot;This model only supports single tool-calls at once!&amp;quot;) }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- set tool_call = message.tool_calls[0].function %}&lt;/p&gt;\n\n&lt;p&gt;{%- if builtin_tools is defined and tool_call.name in builtin_tools %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;&amp;lt;|python_tag|&amp;gt;&amp;quot; + tool_call.name + &amp;quot;.call(&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- for arg_name, arg_val in tool_call.arguments | items %}&lt;/p&gt;\n\n&lt;p&gt;{{- arg_name + &amp;#39;=&amp;quot;&amp;#39; + arg_val + &amp;#39;&amp;quot;&amp;#39; }}&lt;/p&gt;\n\n&lt;p&gt;{%- if not loop.last %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;, &amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- endfor %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;)&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- else  %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;{&amp;quot;name&amp;quot;: &amp;quot;&amp;#39; + tool_call.name + &amp;#39;&amp;quot;, &amp;#39; }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;&amp;quot;parameters&amp;quot;: &amp;#39; }}&lt;/p&gt;\n\n&lt;p&gt;{{- tool_call.arguments | tojson }}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;}&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- if builtin_tools is defined %}&lt;/p&gt;\n\n&lt;p&gt;{#- This means we&amp;#39;re in ipython mode #}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;&amp;lt;|eom_id|&amp;gt;&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- else %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- elif message.role == &amp;quot;tool&amp;quot; or message.role == &amp;quot;ipython&amp;quot; %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;&amp;lt;|start_header_id|&amp;gt;ipython&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- if message.content is mapping or message.content is iterable %}&lt;/p&gt;\n\n&lt;p&gt;{{- message.content | tojson }}&lt;/p&gt;\n\n&lt;p&gt;{%- else %}&lt;/p&gt;\n\n&lt;p&gt;{{- message.content }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n\n&lt;p&gt;{%- endfor %}&lt;/p&gt;\n\n&lt;p&gt;{%- if add_generation_prompt %}&lt;/p&gt;\n\n&lt;p&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; }}&lt;/p&gt;\n\n&lt;p&gt;{%- endif %}&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltonwy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Opening_Cash_4532",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltonwy/should_i_remove_these/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltonwy/should_i_remove_these/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751876303,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I own a mid size electrical contracting bussiness, about 35 employees. I'm thinking of implementing a local ai server maybe mixtral 8x7B to increase the efficiency of the business. My main reason is for book keeping/receipt processing, finance etc as of now but I'm hoping to train on other areas. any other ideas on how this could help my business. Is it worth implementing?",
          "author_fullname": "t2_8jgmm760e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM for business",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltcwbx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751838082,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I own a mid size electrical contracting bussiness, about 35 employees. I&amp;#39;m thinking of implementing a local ai server maybe mixtral 8x7B to increase the efficiency of the business. My main reason is for book keeping/receipt processing, finance etc as of now but I&amp;#39;m hoping to train on other areas. any other ideas on how this could help my business. Is it worth implementing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ltcwbx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Acceptable_Factor817",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltcwbx/local_llm_for_business/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltcwbx/local_llm_for_business/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751838082,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen3 Embedding has great retrieval results on [MTEB](https://huggingface.co/spaces/mteb/leaderboard).\n\nHowever, I tried it in [llama.cpp](https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF). The results were much worse than competitors. I have an FAQ benchmark that looks a bit like this:\n\n| Model  | Score    |\n|---------|----------|\n| Qwen3 8B | 18.70% |\n| Mistral | 53.12% |\n| OpenAI (text-embedding-3-large) | 55.87% |\n| Google (text-embedding-004) | 57.99% |\n| Cohere (embed-v4.0) | 58.50% |\n| Voyage AI | 60.54% |\n\nQwen3 is the only one that I am not using an API for, but I would assume that the F16 GGUF shouldn't have that big of an impact on performance compared to the raw model, say using TEI or vLLM.\n\nDoes anybody have a similar experience?",
          "author_fullname": "t2_5nceo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are Qwen3 Embedding GGUF faulty?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt18hg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751808440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 Embedding has great retrieval results on &lt;a href=\"https://huggingface.co/spaces/mteb/leaderboard\"&gt;MTEB&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;However, I tried it in &lt;a href=\"https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF\"&gt;llama.cpp&lt;/a&gt;. The results were much worse than competitors. I have an FAQ benchmark that looks a bit like this:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Model&lt;/th&gt;\n&lt;th&gt;Score&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Qwen3 8B&lt;/td&gt;\n&lt;td&gt;18.70%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Mistral&lt;/td&gt;\n&lt;td&gt;53.12%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;OpenAI (text-embedding-3-large)&lt;/td&gt;\n&lt;td&gt;55.87%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Google (text-embedding-004)&lt;/td&gt;\n&lt;td&gt;57.99%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Cohere (embed-v4.0)&lt;/td&gt;\n&lt;td&gt;58.50%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Voyage AI&lt;/td&gt;\n&lt;td&gt;60.54%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Qwen3 is the only one that I am not using an API for, but I would assume that the F16 GGUF shouldn&amp;#39;t have that big of an impact on performance compared to the raw model, say using TEI or vLLM.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have a similar experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/baCsbR9uKGr8sPQTJLj1kB78WBO9D8gWFjpdI40flNU.jpg?auto=webp&amp;s=4992a3419de12e11cb9cf874e491628b435c07c3",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/baCsbR9uKGr8sPQTJLj1kB78WBO9D8gWFjpdI40flNU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a11b8bcd62ef98053ac2a15b9dab3c88c969ff4",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/baCsbR9uKGr8sPQTJLj1kB78WBO9D8gWFjpdI40flNU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dcaf438976d46598c1516878774db43240fc3751",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/baCsbR9uKGr8sPQTJLj1kB78WBO9D8gWFjpdI40flNU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c764439d8419fb6b172e614f9504a122d927f45e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/baCsbR9uKGr8sPQTJLj1kB78WBO9D8gWFjpdI40flNU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f66696f8f4c1d26330fb6742904e61178bd6517d",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/baCsbR9uKGr8sPQTJLj1kB78WBO9D8gWFjpdI40flNU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=100fc72373965cea47781ae00764af62f2ae25d2",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/baCsbR9uKGr8sPQTJLj1kB78WBO9D8gWFjpdI40flNU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=886e1ed506bebecfd72c734f4db88cb454ad7e35",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lt18hg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "espadrine",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751808440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Abstract\n\nWe propose a novel inference-time optimization method for resource-constrained deployment of large language models (LLMs), enabling high-quality output from models too large to fit into a single consumer-grade GPU. This technique—Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR)—leverages coarse inference from a quantized or low-parameter proxy model to define a hyperspherical subregion of the solution space. This subregion is then mapped onto the full-precision model, from which only the minimal required model weights are dynamically loaded and computed.\n\nTo reduce redundant early-layer computation and maximize efficiency, PG-DSIR determines a static insertion point within the full model’s architecture where the low-precision representation aligns most closely with a hidden state of the larger model. This alignment is determined through cosine similarity across the hidden states of the full model, enabling direct embedding injection midstream. A lightweight corrective module (e.g., a LoRA or learned delta predictor) bridges the precision gap between the coarse embedding and the ground truth embedding, improving alignment and preserving output quality.\n\nOur technique draws conceptual inspiration from hybrid latent-space workflows in image generation (e.g., LCM + SD schedulers) and departs from traditional quantization, distillation, or Mixture-of-Experts (MoE) routing by enabling continuous, per-inference specialization of the model footprint. The resulting system provides a principled and geometry-driven pathway to real-time inference using ultra-large models on modest hardware, dramatically reducing both VRAM usage and computational overhead.\n\n\n---\n\nIntroduction: Toward Focused Large-Model Inference via Geometric Slicing and Targeted Insertion\n\nLarge Language Models have achieved unprecedented capability at the cost of significant resource demands. Models such as Meta’s LLaMA 3 70B, for example, require over 140 GB of VRAM to run in full precision—placing them well beyond reach for consumer or even prosumer-grade hardware. Traditional strategies to reduce inference cost include quantization, distillation, parameter pruning, and expert routing. These techniques, while effective, trade off flexibility, output fidelity, or require extensive fine-tuning.\n\nWe introduce Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR) as an alternative strategy, rooted in a geometric understanding of latent representations. Our method operates under the key insight that inference can be reconceived not as a global pass through the entire parameter space of a model, but as a locally focused traversal through a high-dimensional latent graph—constrained by an informed estimate of the solution space.\n\n1. Conceptual Overview\n\nThe method begins with a prompt passed to a smaller, quantized proxy model (e.g., 7B), which produces a low-precision embedding in a shared latent space. Although this representation lacks the full nuance of a high-precision model, it defines a directional “search vector” within the solution space. By interpreting this vector as a hypersphere in the full model’s higher-precision latent space, we delimit the relevant solution subregion for the current prompt.\n\nRather than processing the entire 70B model, we instead extract and load only the parameter subset required to refine that coarse embedding within its solution subregion. This step parallels the logic of MoE routing but bypasses the need for discrete experts or static routing logic, instead constructing a dynamically sliced micro-expert composed only of what the full model knows about the specific problem space defined by the prompt.\n\n2. Insertion Point Resolution\n\nTo avoid recomputation of early transformer blocks—often the most computationally expensive—we perform embedding handoff into the full model at an internal layer corresponding to the hidden state most similar to the coarse embedding. This “insertion point” is found by analyzing cosine similarity between the proxy embedding and the hidden states of the full model when run on the same prompt. Importantly, this mapping is prompt-agnostic and only needs to be computed once per proxy/full model pair.\n\n3. Precision Bridging and Correction\n\nEven with accurate slicing and entry point resolution, discrepancies will remain between the proxy embedding and the full model’s expected hidden state. We address this through a corrective module—either a learned LoRA, linear mapping, or shallow neural delta predictor—trained on embedding pairs generated via dual model evaluation on a large prompt corpus. This allows us to cleanly bridge the two latent spaces with negligible overhead.\n\n4. Implications\n\nThe proposed PG-DSIR pipeline enables inference from large-scale models like LLaMA 3 70B or Mixtral on consumer GPUs (e.g., RTX 3060–4090), significantly reducing required VRAM and compute without necessitating global model transformation. Moreover, this method maintains the full model’s capabilities and expressiveness, differing from quantization approaches that often suffer from irrecoverable degradation.\n\n\n---\n\nConclusion\n\nBy treating inference as a navigational process through high-dimensional geometry, PG-DSIR transforms the challenge of large model execution into a targeted optimization problem. We believe this technique can unlock a new era of ultra-large model accessibility, enabling research, development, and deployment of frontier models on commodity hardware—without compromising capability.\n",
          "author_fullname": "t2_m2bkgz51p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PGDS approach to full model inference on consumer grade GPUs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lto3t9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751874048,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;p&gt;We propose a novel inference-time optimization method for resource-constrained deployment of large language models (LLMs), enabling high-quality output from models too large to fit into a single consumer-grade GPU. This technique—Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR)—leverages coarse inference from a quantized or low-parameter proxy model to define a hyperspherical subregion of the solution space. This subregion is then mapped onto the full-precision model, from which only the minimal required model weights are dynamically loaded and computed.&lt;/p&gt;\n\n&lt;p&gt;To reduce redundant early-layer computation and maximize efficiency, PG-DSIR determines a static insertion point within the full model’s architecture where the low-precision representation aligns most closely with a hidden state of the larger model. This alignment is determined through cosine similarity across the hidden states of the full model, enabling direct embedding injection midstream. A lightweight corrective module (e.g., a LoRA or learned delta predictor) bridges the precision gap between the coarse embedding and the ground truth embedding, improving alignment and preserving output quality.&lt;/p&gt;\n\n&lt;p&gt;Our technique draws conceptual inspiration from hybrid latent-space workflows in image generation (e.g., LCM + SD schedulers) and departs from traditional quantization, distillation, or Mixture-of-Experts (MoE) routing by enabling continuous, per-inference specialization of the model footprint. The resulting system provides a principled and geometry-driven pathway to real-time inference using ultra-large models on modest hardware, dramatically reducing both VRAM usage and computational overhead.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Introduction: Toward Focused Large-Model Inference via Geometric Slicing and Targeted Insertion&lt;/p&gt;\n\n&lt;p&gt;Large Language Models have achieved unprecedented capability at the cost of significant resource demands. Models such as Meta’s LLaMA 3 70B, for example, require over 140 GB of VRAM to run in full precision—placing them well beyond reach for consumer or even prosumer-grade hardware. Traditional strategies to reduce inference cost include quantization, distillation, parameter pruning, and expert routing. These techniques, while effective, trade off flexibility, output fidelity, or require extensive fine-tuning.&lt;/p&gt;\n\n&lt;p&gt;We introduce Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR) as an alternative strategy, rooted in a geometric understanding of latent representations. Our method operates under the key insight that inference can be reconceived not as a global pass through the entire parameter space of a model, but as a locally focused traversal through a high-dimensional latent graph—constrained by an informed estimate of the solution space.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Conceptual Overview&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The method begins with a prompt passed to a smaller, quantized proxy model (e.g., 7B), which produces a low-precision embedding in a shared latent space. Although this representation lacks the full nuance of a high-precision model, it defines a directional “search vector” within the solution space. By interpreting this vector as a hypersphere in the full model’s higher-precision latent space, we delimit the relevant solution subregion for the current prompt.&lt;/p&gt;\n\n&lt;p&gt;Rather than processing the entire 70B model, we instead extract and load only the parameter subset required to refine that coarse embedding within its solution subregion. This step parallels the logic of MoE routing but bypasses the need for discrete experts or static routing logic, instead constructing a dynamically sliced micro-expert composed only of what the full model knows about the specific problem space defined by the prompt.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insertion Point Resolution&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To avoid recomputation of early transformer blocks—often the most computationally expensive—we perform embedding handoff into the full model at an internal layer corresponding to the hidden state most similar to the coarse embedding. This “insertion point” is found by analyzing cosine similarity between the proxy embedding and the hidden states of the full model when run on the same prompt. Importantly, this mapping is prompt-agnostic and only needs to be computed once per proxy/full model pair.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Precision Bridging and Correction&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Even with accurate slicing and entry point resolution, discrepancies will remain between the proxy embedding and the full model’s expected hidden state. We address this through a corrective module—either a learned LoRA, linear mapping, or shallow neural delta predictor—trained on embedding pairs generated via dual model evaluation on a large prompt corpus. This allows us to cleanly bridge the two latent spaces with negligible overhead.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Implications&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The proposed PG-DSIR pipeline enables inference from large-scale models like LLaMA 3 70B or Mixtral on consumer GPUs (e.g., RTX 3060–4090), significantly reducing required VRAM and compute without necessitating global model transformation. Moreover, this method maintains the full model’s capabilities and expressiveness, differing from quantization approaches that often suffer from irrecoverable degradation.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Conclusion&lt;/p&gt;\n\n&lt;p&gt;By treating inference as a navigational process through high-dimensional geometry, PG-DSIR transforms the challenge of large model execution into a targeted optimization problem. We believe this technique can unlock a new era of ultra-large model accessibility, enabling research, development, and deployment of frontier models on commodity hardware—without compromising capability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lto3t9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "finnabrahamson",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751874048,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, we're working on a full CLI that currently beats a fair amount of other projects in benchmarks when running SOTA models, We're looking to get more testers running local models to see how it performs versus other projects that have local support.\n\nWe'd love for you to join us and help out in real world testing, we're willing to make almost any changes and are constantly working to improve, Our goal is to be the best and we will get to that point.\n\n[https://discord.gg/fA4upHvMsK](https://discord.gg/fA4upHvMsK)\n\n[**https://github.com/xyOz-dev/LogiQCLI**](https://github.com/xyOz-dev/LogiQCLI)",
          "author_fullname": "t2_1aw5hbygqz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LogiQ CLI Beta | Full LMStudio Support.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltkdjz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "spoiler",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751860359,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, we&amp;#39;re working on a full CLI that currently beats a fair amount of other projects in benchmarks when running SOTA models, We&amp;#39;re looking to get more testers running local models to see how it performs versus other projects that have local support.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;d love for you to join us and help out in real world testing, we&amp;#39;re willing to make almost any changes and are constantly working to improve, Our goal is to be the best and we will get to that point.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://discord.gg/fA4upHvMsK\"&gt;https://discord.gg/fA4upHvMsK&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/xyOz-dev/LogiQCLI\"&gt;&lt;strong&gt;https://github.com/xyOz-dev/LogiQCLI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": true,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltkdjz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "x8ko_dev",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltkdjz/logiq_cli_beta_full_lmstudio_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltkdjz/logiq_cli_beta_full_lmstudio_support/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751860359,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "as the title suggests i would like to run the best llm possible for my system and i am really new to llms so i really have no idea where to start help is really appreciated . ( my system has 32gb ddr5 6000mhz cl 36 ram and a amd rx 7600 with 8gb vram )",
          "author_fullname": "t2_1o4evizu1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "best llm for 32 gb ram and 8gb vram",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lto2in",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751873903,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as the title suggests i would like to run the best llm possible for my system and i am really new to llms so i really have no idea where to start help is really appreciated . ( my system has 32gb ddr5 6000mhz cl 36 ram and a amd rx 7600 with 8gb vram )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lto2in",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "diddy_stroker",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751873903,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm not at all familiar with Local LLMs beyond image generation ones so forgive me for the noob questions.\n\nIm looking for something like what ElevenLabs has to offer, but I would like to run it locally since I may need to run multiple variations. I'm also looking for something that can do metal/screamo style vocals for some music stuff. Are there websites like civitai for TTS models or something?\n\nLooking for existing models as I don't think I'd have the means to train one myself (sourcing vocals), and of course would need something where the license allows commercial use.\n\nNot really sure where to start, I appreciate any advice~\n\nP.S. I don't mind paying for existing training data as long as it is good quality. I just don't do subscription services.",
          "author_fullname": "t2_1df15mfo7o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any local Text-to-Speech model options that can do screamo/metal style vocals (existing models)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt9ot6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751863202,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751829950,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not at all familiar with Local LLMs beyond image generation ones so forgive me for the noob questions.&lt;/p&gt;\n\n&lt;p&gt;Im looking for something like what ElevenLabs has to offer, but I would like to run it locally since I may need to run multiple variations. I&amp;#39;m also looking for something that can do metal/screamo style vocals for some music stuff. Are there websites like civitai for TTS models or something?&lt;/p&gt;\n\n&lt;p&gt;Looking for existing models as I don&amp;#39;t think I&amp;#39;d have the means to train one myself (sourcing vocals), and of course would need something where the license allows commercial use.&lt;/p&gt;\n\n&lt;p&gt;Not really sure where to start, I appreciate any advice~&lt;/p&gt;\n\n&lt;p&gt;P.S. I don&amp;#39;t mind paying for existing training data as long as it is good quality. I just don&amp;#39;t do subscription services.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lt9ot6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Visible-Midnight4687",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt9ot6/are_there_any_local_texttospeech_model_options/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt9ot6/are_there_any_local_texttospeech_model_options/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751829950,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Do real people actually vote on things there? Seems bizarre to me anyone would spend their time doing data labelling for free ",
          "author_fullname": "t2_gi7a36v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thoughts on lmsys/lmarena?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltjqct",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751858215,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do real people actually vote on things there? Seems bizarre to me anyone would spend their time doing data labelling for free &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ltjqct",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HOLUPREDICTIONS",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltjqct/thoughts_on_lmsyslmarena/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltjqct/thoughts_on_lmsyslmarena/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751858215,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am building a home server under $6000. I want to run concurrent queries on local q4 or q8 quantised LLM. Should I buy w7900 or anything else??",
          "author_fullname": "t2_1n0ydmtkw5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Will w7900 work?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltmrvo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751868785,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building a home server under $6000. I want to run concurrent queries on local q4 or q8 quantised LLM. Should I buy w7900 or anything else??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltmrvo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DatakeeperFun7770",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751868785,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am really new to this voice stuff. My only relevant experience would be setting up comfy UI in my system with Pinokio App. \n\nWhat I am trying to do is use a model from [weights.gg](http://weights.gg) of a Japanese VA. I want to use his voice for a narration youtube channel (not 1:1 ofc) but the problem is that he has a **Japanese accent**. I did notice it when he was singing in the example voices in the website. Is there a TTS model I can run locally that sort of ignores his accent and fix the pronounciation? So far I am looking at Openaudio S1 mini, OpenVoice V2, and XTTS-v2. But I don't know if all of these are outdated or there are better models out right now.",
          "author_fullname": "t2_nxmgm9nv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which TTS Model to use?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltmrag",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751868726,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am really new to this voice stuff. My only relevant experience would be setting up comfy UI in my system with Pinokio App. &lt;/p&gt;\n\n&lt;p&gt;What I am trying to do is use a model from &lt;a href=\"http://weights.gg\"&gt;weights.gg&lt;/a&gt; of a Japanese VA. I want to use his voice for a narration youtube channel (not 1:1 ofc) but the problem is that he has a &lt;strong&gt;Japanese accent&lt;/strong&gt;. I did notice it when he was singing in the example voices in the website. Is there a TTS model I can run locally that sort of ignores his accent and fix the pronounciation? So far I am looking at Openaudio S1 mini, OpenVoice V2, and XTTS-v2. But I don&amp;#39;t know if all of these are outdated or there are better models out right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltmrag",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious-Comment94",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltmrag/which_tts_model_to_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltmrag/which_tts_model_to_use/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751868726,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3p22y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made Otacon into a desktop buddy. He comments on your active application and generally keeps you company. (X-Post /r/metalgear)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt6o4d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://external-preview.redd.it/djV1aGZnNmpkYWJmMUok61catiCF0Lfr2I9TZHYyHTf-hgXFRIrOzO_DutsN.png?width=140&amp;height=105&amp;crop=140:105,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=b95858a734b6bf1170ce335d06ed6e39cc3740d7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751822460,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "old.reddit.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://old.reddit.com/r/metalgear/comments/1lt6m6d/i_made_otacon_into_a_desktop_buddy_he_comments_on/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/djV1aGZnNmpkYWJmMUok61catiCF0Lfr2I9TZHYyHTf-hgXFRIrOzO_DutsN.png?auto=webp&amp;s=ae6e8e7035d8cf561c4a03ab37d9a4d4cfb4ac74",
                  "width": 800,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/djV1aGZnNmpkYWJmMUok61catiCF0Lfr2I9TZHYyHTf-hgXFRIrOzO_DutsN.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=90f95392ff03c50da9bbf1565f953ad182e22ad2",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/djV1aGZnNmpkYWJmMUok61catiCF0Lfr2I9TZHYyHTf-hgXFRIrOzO_DutsN.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=75ad96228001980a49e9770e5c2e279d7a023670",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/djV1aGZnNmpkYWJmMUok61catiCF0Lfr2I9TZHYyHTf-hgXFRIrOzO_DutsN.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2fb38baf1981d8e6a322bfd9cad3dbfe7d447f23",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://external-preview.redd.it/djV1aGZnNmpkYWJmMUok61catiCF0Lfr2I9TZHYyHTf-hgXFRIrOzO_DutsN.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=af5a937e771da812cefb64024aa4e6bc0483bf39",
                    "width": 640,
                    "height": 480
                  }
                ],
                "variants": {},
                "id": "djV1aGZnNmpkYWJmMUok61catiCF0Lfr2I9TZHYyHTf-hgXFRIrOzO_DutsN"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lt6o4d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "otac0n",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt6o4d/i_made_otacon_into_a_desktop_buddy_he_comments_on/",
          "stickied": false,
          "url": "https://old.reddit.com/r/metalgear/comments/1lt6m6d/i_made_otacon_into_a_desktop_buddy_he_comments_on/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751822460,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why Your AI Assistant Can't Actually Roll Dice (And Why That Matters)\n\nEver asked Claude or ChatGPT to roll a d20 for your D&amp;D session? Sure, they'll happily tell you \"I rolled a 14!\"\nBut here's the thing - they didn't actually roll anything. They just predicted what a reasonable dice roll\nresponse would look like based on their training data.\n\nThink about it: Large Language Models are deterministic text generators. When you ask for a \"random\" roll,\nthey're not consulting any dice - they're pattern matching against millions of examples of people talking about\ndice rolls. That \"14\" isn't random; it's just the most statistically likely response given the context.\n\nWhy This Actually Matters\n\nFor casual roleplay? Maybe not a big deal. But for serious gaming sessions, statistical simulations, or any\nsituation where fair randomness matters, this creates real problems:\n\n*  Fake Randomness: Your \"rolls\" might follow predictable patterns\n*  Gaming Integrity: Is that nat 20 actually random, or just what the AI thought you wanted to hear?\n*  Reproducibility: Ask the same question twice and you might get suspiciously similar results\n\nEnter the Dice Rolling MCP - https://dice-rolling-mcp.vercel.app/\n\nThis Model Context Protocol server gives AI assistants actual dice - not imaginary ones. When you ask for\n\"2d20kh1+5\" (advantage attack with a +5 modifier), the AI sends that request to a real random number generator\nusing cryptographically secure methods.\n\nThe AI gets back genuine random results and can present them naturally: \"You rolled 18 and 12, keeping the 18,\nplus 5 for a total of 23 - that hits!\"\n\nYou can run it locally, or use it as a Remote MCP Integration\n\nReal Dice for Real Games\n\nNow your AI assistant can handle everything from basic d20 rolls to complex mechanics like exploding dice,\nrerolls, and success counting. It understands the notation, validates it, and executes it with mathematical\nintegrity.\n\nBecause sometimes you need to know that natural 20 was actually natural.",
          "author_fullname": "t2_dsi9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dice Rolling MCP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltm6uw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751866619,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why Your AI Assistant Can&amp;#39;t Actually Roll Dice (And Why That Matters)&lt;/p&gt;\n\n&lt;p&gt;Ever asked Claude or ChatGPT to roll a d20 for your D&amp;amp;D session? Sure, they&amp;#39;ll happily tell you &amp;quot;I rolled a 14!&amp;quot;\nBut here&amp;#39;s the thing - they didn&amp;#39;t actually roll anything. They just predicted what a reasonable dice roll\nresponse would look like based on their training data.&lt;/p&gt;\n\n&lt;p&gt;Think about it: Large Language Models are deterministic text generators. When you ask for a &amp;quot;random&amp;quot; roll,\nthey&amp;#39;re not consulting any dice - they&amp;#39;re pattern matching against millions of examples of people talking about\ndice rolls. That &amp;quot;14&amp;quot; isn&amp;#39;t random; it&amp;#39;s just the most statistically likely response given the context.&lt;/p&gt;\n\n&lt;p&gt;Why This Actually Matters&lt;/p&gt;\n\n&lt;p&gt;For casual roleplay? Maybe not a big deal. But for serious gaming sessions, statistical simulations, or any\nsituation where fair randomness matters, this creates real problems:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Fake Randomness: Your &amp;quot;rolls&amp;quot; might follow predictable patterns&lt;/li&gt;\n&lt;li&gt; Gaming Integrity: Is that nat 20 actually random, or just what the AI thought you wanted to hear?&lt;/li&gt;\n&lt;li&gt; Reproducibility: Ask the same question twice and you might get suspiciously similar results&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Enter the Dice Rolling MCP - &lt;a href=\"https://dice-rolling-mcp.vercel.app/\"&gt;https://dice-rolling-mcp.vercel.app/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This Model Context Protocol server gives AI assistants actual dice - not imaginary ones. When you ask for\n&amp;quot;2d20kh1+5&amp;quot; (advantage attack with a +5 modifier), the AI sends that request to a real random number generator\nusing cryptographically secure methods.&lt;/p&gt;\n\n&lt;p&gt;The AI gets back genuine random results and can present them naturally: &amp;quot;You rolled 18 and 12, keeping the 18,\nplus 5 for a total of 23 - that hits!&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;You can run it locally, or use it as a Remote MCP Integration&lt;/p&gt;\n\n&lt;p&gt;Real Dice for Real Games&lt;/p&gt;\n\n&lt;p&gt;Now your AI assistant can handle everything from basic d20 rolls to complex mechanics like exploding dice,\nrerolls, and success counting. It understands the notation, validates it, and executes it with mathematical\nintegrity.&lt;/p&gt;\n\n&lt;p&gt;Because sometimes you need to know that natural 20 was actually natural.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/q6QgwFBWA1rDwnuKslDlGaBjPPIs5KzFqeuyHUslkqo.png?auto=webp&amp;s=6b6fa2d7a419f36dc99383ad3cd16c0276debfee",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/q6QgwFBWA1rDwnuKslDlGaBjPPIs5KzFqeuyHUslkqo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e047e49fde66aedc881fc3978d5c7bbdc3219912",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/q6QgwFBWA1rDwnuKslDlGaBjPPIs5KzFqeuyHUslkqo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=43aafad6a14fd981e54e59168e01ccd672747fac",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/q6QgwFBWA1rDwnuKslDlGaBjPPIs5KzFqeuyHUslkqo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f85cf527c2c2d8bf41cf1f564db02fdc003883a4",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/q6QgwFBWA1rDwnuKslDlGaBjPPIs5KzFqeuyHUslkqo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=970465e7cc7f9bd0332b29702356a16631fe5f16",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/q6QgwFBWA1rDwnuKslDlGaBjPPIs5KzFqeuyHUslkqo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5c4f0492a3c2843a98a3fef6230db6120017c29",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/q6QgwFBWA1rDwnuKslDlGaBjPPIs5KzFqeuyHUslkqo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b3e8dc903e798ca1dc73e10e3f6103b19d4e973",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "q6QgwFBWA1rDwnuKslDlGaBjPPIs5KzFqeuyHUslkqo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ltm6uw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jimmcq",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltm6uw/dice_rolling_mcp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltm6uw/dice_rolling_mcp/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751866619,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Okay, so I have comfyui, works fine with flux, wan and etc. Now I have LM studio and anythingllm. I check the models and there are so many:\n1) llama\n2) gemma\n3) qwen\n4) hermes\n5) mistral\n6).......\n\nAnd the different versions, different Q, different parameter set size and etc.\n\nI understand quantization levels and parameter set size (b). But is there somewhere an organized place to check which model does something best? I think one could be better at programming, other at emotional support, other at analyzing stuff, but how to know which one? Now I have 200Gb of AI models  and each of them feel very different.\n\nMy main purpose is to feed pdfs so it could do QnA based on pdf knowledge. I found hermes 3 llama quite good at this, gemma 3 also, others - not so much.\n\nIs there a tier list or something to dive deeper into different model capabilities?\n\nPc: 5800x3d, 4070super 12gb vram, 32gb ram.\n\nAlso side question: how to increase max avalaible document count and size when trying to do RAG on lmstudio or anythingllm?\n\nI tried installing privateGPT, but thats pain in the ass to make it work on gpu. Also it doesnt even work on cpu for me (my skill issue for sure). \n\nChatRTX works fast, but problem - no multilanguage support.",
          "author_fullname": "t2_2ggbff2e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Newbie questions in this world",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltm1mp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751866101,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Okay, so I have comfyui, works fine with flux, wan and etc. Now I have LM studio and anythingllm. I check the models and there are so many:\n1) llama\n2) gemma\n3) qwen\n4) hermes\n5) mistral\n6).......&lt;/p&gt;\n\n&lt;p&gt;And the different versions, different Q, different parameter set size and etc.&lt;/p&gt;\n\n&lt;p&gt;I understand quantization levels and parameter set size (b). But is there somewhere an organized place to check which model does something best? I think one could be better at programming, other at emotional support, other at analyzing stuff, but how to know which one? Now I have 200Gb of AI models  and each of them feel very different.&lt;/p&gt;\n\n&lt;p&gt;My main purpose is to feed pdfs so it could do QnA based on pdf knowledge. I found hermes 3 llama quite good at this, gemma 3 also, others - not so much.&lt;/p&gt;\n\n&lt;p&gt;Is there a tier list or something to dive deeper into different model capabilities?&lt;/p&gt;\n\n&lt;p&gt;Pc: 5800x3d, 4070super 12gb vram, 32gb ram.&lt;/p&gt;\n\n&lt;p&gt;Also side question: how to increase max avalaible document count and size when trying to do RAG on lmstudio or anythingllm?&lt;/p&gt;\n\n&lt;p&gt;I tried installing privateGPT, but thats pain in the ass to make it work on gpu. Also it doesnt even work on cpu for me (my skill issue for sure). &lt;/p&gt;\n\n&lt;p&gt;ChatRTX works fast, but problem - no multilanguage support.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltm1mp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lxxtsch",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltm1mp/newbie_questions_in_this_world/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltm1mp/newbie_questions_in_this_world/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751866101,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently started using locally hosted supabase instances for developing LLM chatbot applications but found that the main thing they were missing was the project management feature that is present in the cloud hosted version. \n\nConfiguring everything to avoid conflicts in both the .env and the compose file was getting tedious, so i created this repo that helps me set up multiple instances for each project.\n\nLet me know what you think!\n\n[tazomatalax/multi-supabase](https://github.com/tazomatalax/multi-supabase)",
          "author_fullname": "t2_1049q6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multiple Local Supabase",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltl6ui",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751863100,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started using locally hosted supabase instances for developing LLM chatbot applications but found that the main thing they were missing was the project management feature that is present in the cloud hosted version. &lt;/p&gt;\n\n&lt;p&gt;Configuring everything to avoid conflicts in both the .env and the compose file was getting tedious, so i created this repo that helps me set up multiple instances for each project.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/tazomatalax/multi-supabase\"&gt;tazomatalax/multi-supabase&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MawLj78-1qob3R8tdIBWW2vcEFN31-nqXCRT9Rz47t0.png?auto=webp&amp;s=6ffef94b27d32ba1ed871b4952c76fd8008bf390",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MawLj78-1qob3R8tdIBWW2vcEFN31-nqXCRT9Rz47t0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=459ee32d73e0bea8e7c6b0505ec5a743d7fcd51a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/MawLj78-1qob3R8tdIBWW2vcEFN31-nqXCRT9Rz47t0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2193bcb66eb576a7ac6e2a55723e642ae75abff",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/MawLj78-1qob3R8tdIBWW2vcEFN31-nqXCRT9Rz47t0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2679061d9a612f79c69b244133d78d1e2d13f466",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/MawLj78-1qob3R8tdIBWW2vcEFN31-nqXCRT9Rz47t0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4755c7555427708b5b55cece1f06de6c1576a140",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/MawLj78-1qob3R8tdIBWW2vcEFN31-nqXCRT9Rz47t0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b04e7f8779141d38f716ea8a6efc5bc290dfaabc",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/MawLj78-1qob3R8tdIBWW2vcEFN31-nqXCRT9Rz47t0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197291e176122303082a1ecdc34c72a913a6a53a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "MawLj78-1qob3R8tdIBWW2vcEFN31-nqXCRT9Rz47t0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltl6ui",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tazomatalax",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltl6ui/multiple_local_supabase/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltl6ui/multiple_local_supabase/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751863100,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am pleased to present **ADR–Academic Deep Research**, a novel AI‐powered platform designed to advance the rigor and scale of cybersecurity investigations. ADR leverages the o4-mini-deep research and o3-deep-research model (paid tier) to execute multi-step analytical workflows on complex security problems, automatically harvesting, synthesizing, and cross-referencing hundreds of online sources alongside peer-reviewed literature. The result is a comprehensive, researcher‐grade report that integrates:\n\n* **Threat Landscape Mapping:** Correlating indicators of compromise (IoCs) from disparate repositories and academic databases.\n* **Technical Deep Dives:** Disassembling malware samples, vulnerability proofs-of-concept, and exploit chains with citation-backed commentary.\n* **Literature Synthesis:** Summarizing and comparing state-of-the-art methodologies from recent conference proceedings and journal articles.\n\nThis demonstration requires an OpenAI API key and showcases how ADR can both accelerate hypothesis testing and elevate the depth of traditional literature reviews. I welcome your feedback on this prototype—please explore the demo at:  \n[https://adr-academic-deep-research.vercel.app/](https://adr-academic-deep-research.vercel.app/)\n\n  \nSign up for waitlist as well for o3-deep-research:\n\n[https://scriptlabs.wixstudio.com/adrdeepresearch](https://scriptlabs.wixstudio.com/adrdeepresearch)\n\nYour insights will help refine ADR’s research heuristics, citation accuracy, and report structure. Thank you for evaluating ADR as a next-generation tool for cybersecurity scholarship.",
          "author_fullname": "t2_7xm4orss",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ADR–Academic Deep Research for Cybersecurity and Academic: An AI-Driven Framework for Multi-Step Threat Analysis",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltks5a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751862055,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751861676,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am pleased to present &lt;strong&gt;ADR–Academic Deep Research&lt;/strong&gt;, a novel AI‐powered platform designed to advance the rigor and scale of cybersecurity investigations. ADR leverages the o4-mini-deep research and o3-deep-research model (paid tier) to execute multi-step analytical workflows on complex security problems, automatically harvesting, synthesizing, and cross-referencing hundreds of online sources alongside peer-reviewed literature. The result is a comprehensive, researcher‐grade report that integrates:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Threat Landscape Mapping:&lt;/strong&gt; Correlating indicators of compromise (IoCs) from disparate repositories and academic databases.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Technical Deep Dives:&lt;/strong&gt; Disassembling malware samples, vulnerability proofs-of-concept, and exploit chains with citation-backed commentary.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Literature Synthesis:&lt;/strong&gt; Summarizing and comparing state-of-the-art methodologies from recent conference proceedings and journal articles.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This demonstration requires an OpenAI API key and showcases how ADR can both accelerate hypothesis testing and elevate the depth of traditional literature reviews. I welcome your feedback on this prototype—please explore the demo at:&lt;br/&gt;\n&lt;a href=\"https://adr-academic-deep-research.vercel.app/\"&gt;https://adr-academic-deep-research.vercel.app/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sign up for waitlist as well for o3-deep-research:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://scriptlabs.wixstudio.com/adrdeepresearch\"&gt;https://scriptlabs.wixstudio.com/adrdeepresearch&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Your insights will help refine ADR’s research heuristics, citation accuracy, and report structure. Thank you for evaluating ADR as a next-generation tool for cybersecurity scholarship.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ltks5a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Haunting-Ad6565",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltks5a/adracademic_deep_research_for_cybersecurity_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltks5a/adracademic_deep_research_for_cybersecurity_and/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751861676,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nI have been using local llms for analyzing csv files, however sometimes they are not powerful enough for the analysis l want and my computer is not big enough for more capable local models. \n\nSo l created this project that creates a representative copy of my csv and uses this with open ai/gemini to generate analysis code, which runs in the browser against the real data . I am using a small llms that runs in browser using webllm for summaries.\n\nThis allows me to use large llms without exposing my data\n\nPlease take a peek, test and contribute as this is still a work in progress. \n\nhttps://github.com/the-data-omni/data_omni_chat",
          "author_fullname": "t2_a14nrtoi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built an open source project for analyzing csv files using LLMs without the llm seeing your data",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 91,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltkkxd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MjOn7BKxukp1H3z3m209f08Kc0ilIxCsfWdFJX5xlcc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751861007,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using local llms for analyzing csv files, however sometimes they are not powerful enough for the analysis l want and my computer is not big enough for more capable local models. &lt;/p&gt;\n\n&lt;p&gt;So l created this project that creates a representative copy of my csv and uses this with open ai/gemini to generate analysis code, which runs in the browser against the real data . I am using a small llms that runs in browser using webllm for summaries.&lt;/p&gt;\n\n&lt;p&gt;This allows me to use large llms without exposing my data&lt;/p&gt;\n\n&lt;p&gt;Please take a peek, test and contribute as this is still a work in progress. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/the-data-omni/data_omni_chat\"&gt;https://github.com/the-data-omni/data_omni_chat&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/t5fyjq77mdbf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?format=png8&amp;s=eaa3d95099ae2151e44ddb7d4322ab3bdf4e18b2",
                  "width": 1660,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=aeccc451cb270bfc79d5c7e09eb52d45b79c2b2e",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=fba500e743409a78e9e581b759466107171f4ffa",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=48fe58e82c6f9caefd61cc8a6a4559be5b072875",
                    "width": 320,
                    "height": 208
                  },
                  {
                    "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=3f067f8dc8a7dedd64872ea241eefaee1c487b02",
                    "width": 640,
                    "height": 416
                  },
                  {
                    "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=e09d55e1860448a520e61f55dd6245ebe5e91772",
                    "width": 960,
                    "height": 624
                  },
                  {
                    "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=ea453b9a7ba3c4b565c091dd05fa8cdf0620923a",
                    "width": 1080,
                    "height": 702
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?s=bd9e7ffa873354d5267d065608443d2ffa970425",
                      "width": 1660,
                      "height": 1080
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=108&amp;crop=smart&amp;s=85c6e515d9d36ce4b8e7cda298b090eb2d64e227",
                        "width": 108,
                        "height": 70
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=216&amp;crop=smart&amp;s=f7fa63297fc83ff860ae029dfa752bbfbff031fd",
                        "width": 216,
                        "height": 140
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=320&amp;crop=smart&amp;s=a0f47b75e35e4e9f9ac4c91fc00fa7923d6cc0e2",
                        "width": 320,
                        "height": 208
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=640&amp;crop=smart&amp;s=540a55f8d48f16ed84052344678a6b95722c2e87",
                        "width": 640,
                        "height": 416
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=960&amp;crop=smart&amp;s=a7a5b4272860b2e94bf0b4bdc47859ea91cb5fad",
                        "width": 960,
                        "height": 624
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=1080&amp;crop=smart&amp;s=ef8b2ba560fec4147b761257c6bdc60d3b81ec40",
                        "width": 1080,
                        "height": 702
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?format=mp4&amp;s=8ec891175f33f79111a23e18ce0a228cac72e7fe",
                      "width": 1660,
                      "height": 1080
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=108&amp;format=mp4&amp;s=c990fa378d8fb2c28c2a496535a6cc9c0496de04",
                        "width": 108,
                        "height": 70
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=216&amp;format=mp4&amp;s=84393426e8d8eec8d22e367cc4a4981ae70822e3",
                        "width": 216,
                        "height": 140
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=320&amp;format=mp4&amp;s=b848ccdd4bac2e3310e7ed2142b82211d6f21a6d",
                        "width": 320,
                        "height": 208
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=640&amp;format=mp4&amp;s=48e3233e0ef909e843686d5d1263942bdea31507",
                        "width": 640,
                        "height": 416
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=960&amp;format=mp4&amp;s=03a5a494c593f699c6276b642f461b8ad52846cc",
                        "width": 960,
                        "height": 624
                      },
                      {
                        "url": "https://preview.redd.it/t5fyjq77mdbf1.gif?width=1080&amp;format=mp4&amp;s=04e25a0ac183d283b22fbbebca679edfb5b19d5e",
                        "width": 1080,
                        "height": 702
                      }
                    ]
                  }
                },
                "id": "W4hD4Bu_56GbWniUjeJG2m_dQj1TW_S3Z_1thOBU_8Y"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltkkxd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DiscerningTheTimes",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltkkxd/built_an_open_source_project_for_analyzing_csv/",
          "stickied": false,
          "url": "https://i.redd.it/t5fyjq77mdbf1.gif",
          "subreddit_subscribers": 495395,
          "created_utc": 1751861007,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, i have a llama and webui running deepseek R1, but i'd like to know some things\n\nfirst how much upgradeability  do i have to the future considering i have only a rtx 4060 in laptop? i dont intend anything crazy i just want some personal organizer\n\nid like it to have long term memory and be able to acces a folder in which ill put documents mainly to schoolar use, to help me study \n\ni dont know if possible but if it could be use to help me research like suggesting search terms\n\nim new to this and ill probably keep it casual, but still want to know how far i can take it to make my life a bit easier",
          "author_fullname": "t2_be39tps8a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'd like to add some features to my ai if possible",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltk7yh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751859832,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i have a llama and webui running deepseek R1, but i&amp;#39;d like to know some things&lt;/p&gt;\n\n&lt;p&gt;first how much upgradeability  do i have to the future considering i have only a rtx 4060 in laptop? i dont intend anything crazy i just want some personal organizer&lt;/p&gt;\n\n&lt;p&gt;id like it to have long term memory and be able to acces a folder in which ill put documents mainly to schoolar use, to help me study &lt;/p&gt;\n\n&lt;p&gt;i dont know if possible but if it could be use to help me research like suggesting search terms&lt;/p&gt;\n\n&lt;p&gt;im new to this and ill probably keep it casual, but still want to know how far i can take it to make my life a bit easier&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltk7yh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RestaurantUnusual456",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltk7yh/id_like_to_add_some_features_to_my_ai_if_possible/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltk7yh/id_like_to_add_some_features_to_my_ai_if_possible/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751859832,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/9n9zbmftv7bf1.png?width=2348&amp;format=png&amp;auto=webp&amp;s=442e283d1f39378d1799de954c2c1806310333f5\n\nI'm a huge open source fan, but I think the gemini-cli fallback from \"pro\" to \"flash\" will divert more \"real\" coders to claude-code than convince them to get a gemini-pro subscription.\n\nThe gemini-cli doc states that \"To ensure you rarely, if ever, hit a limit during this preview, we offer the industry’s largest allowance: 60 model requests per minute and 1,000 requests per day at no charge.\". That's good, but it doesn't mention the throttling from pro to flash. When I try to build something out of the Erathostene Sieve, the throttling causes a code mess and soon reaches the limits (err 429) without a useful solution, because of the flash incapacity to solve \"real\" coding problems.\n\ngemini-cli at this early stage can't compare to claude-code, so loosing \"real\" community devs isn't the best strategy to win the battle, IMO.\n\nAt the end, I'm looking for alternative solutions, without discarding the auto-build of a similar tool that with some agentic LLM routing can substitute closed-source and cloud solutions.\n\nMeanwhile, the above solutions + context engineering may be used to build some \"private\" solution.\n\nWhat do you think?",
          "author_fullname": "t2_uehl6561",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gemini-cli: falling back to gemini-flash is the best marketing strategy Anthropic could have dreamed of for claude-code.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 15,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9n9zbmftv7bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 12,
                  "x": 108,
                  "u": "https://preview.redd.it/9n9zbmftv7bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f01af454ac389672bb171be6656def5368c490e1"
                },
                {
                  "y": 24,
                  "x": 216,
                  "u": "https://preview.redd.it/9n9zbmftv7bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23e3b4b45ec15036efaedc044e269824e9b978ed"
                },
                {
                  "y": 36,
                  "x": 320,
                  "u": "https://preview.redd.it/9n9zbmftv7bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7dfb9ef87c84181302e05448e9c84cd675e3f152"
                },
                {
                  "y": 73,
                  "x": 640,
                  "u": "https://preview.redd.it/9n9zbmftv7bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=51563f03620dd1a26c658a5b05585f2d28d25509"
                },
                {
                  "y": 109,
                  "x": 960,
                  "u": "https://preview.redd.it/9n9zbmftv7bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d78950c54f81186645f0dddab34c8cf7ee7f7bc"
                },
                {
                  "y": 123,
                  "x": 1080,
                  "u": "https://preview.redd.it/9n9zbmftv7bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c34016aac387b41da82a6bb9604f6024f09219a2"
                }
              ],
              "s": {
                "y": 268,
                "x": 2348,
                "u": "https://preview.redd.it/9n9zbmftv7bf1.png?width=2348&amp;format=png&amp;auto=webp&amp;s=442e283d1f39378d1799de954c2c1806310333f5"
              },
              "id": "9n9zbmftv7bf1"
            }
          },
          "name": "t3_1lswnto",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/nT92tKjMkTLoLC9uXWz050-DcHqgcFo8BpiZvSZgM8I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751792061,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/9n9zbmftv7bf1.png?width=2348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=442e283d1f39378d1799de954c2c1806310333f5\"&gt;https://preview.redd.it/9n9zbmftv7bf1.png?width=2348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=442e283d1f39378d1799de954c2c1806310333f5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a huge open source fan, but I think the gemini-cli fallback from &amp;quot;pro&amp;quot; to &amp;quot;flash&amp;quot; will divert more &amp;quot;real&amp;quot; coders to claude-code than convince them to get a gemini-pro subscription.&lt;/p&gt;\n\n&lt;p&gt;The gemini-cli doc states that &amp;quot;To ensure you rarely, if ever, hit a limit during this preview, we offer the industry’s largest allowance: 60 model requests per minute and 1,000 requests per day at no charge.&amp;quot;. That&amp;#39;s good, but it doesn&amp;#39;t mention the throttling from pro to flash. When I try to build something out of the Erathostene Sieve, the throttling causes a code mess and soon reaches the limits (err 429) without a useful solution, because of the flash incapacity to solve &amp;quot;real&amp;quot; coding problems.&lt;/p&gt;\n\n&lt;p&gt;gemini-cli at this early stage can&amp;#39;t compare to claude-code, so loosing &amp;quot;real&amp;quot; community devs isn&amp;#39;t the best strategy to win the battle, IMO.&lt;/p&gt;\n\n&lt;p&gt;At the end, I&amp;#39;m looking for alternative solutions, without discarding the auto-build of a similar tool that with some agentic LLM routing can substitute closed-source and cloud solutions.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, the above solutions + context engineering may be used to build some &amp;quot;private&amp;quot; solution.&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lswnto",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PieBru",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751792061,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Got tired of Claude Code making changes I didn't want, then having to spend more tokens asking it to fix things.\n\nSo I made **ccundo** \\- an npm package that lets you quickly undo Claude Code operations with previews and cascading safety.\n\n    npm install -g ccundo\n    ccundo list    \n    # see recent operations\n    ccundo undo    \n    # undo with preview\n\n**GitHub**: [https://github.com/RonitSachdev/ccundo](https://github.com/RonitSachdev/ccundo)  \n**npm**: [https://www.npmjs.com/package/ccundo](https://www.npmjs.com/package/ccundo)\n\n⭐ **Please star if you find it useful!**\n\nWhat do you think? Anyone else dealing with similar Claude Code frustrations?",
          "author_fullname": "t2_vn9f25k7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built ccundo - instantly undo Claude Code's mistakes without wasting tokens",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt13ht",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751808057,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got tired of Claude Code making changes I didn&amp;#39;t want, then having to spend more tokens asking it to fix things.&lt;/p&gt;\n\n&lt;p&gt;So I made &lt;strong&gt;ccundo&lt;/strong&gt; - an npm package that lets you quickly undo Claude Code operations with previews and cascading safety.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;npm install -g ccundo\nccundo list    \n# see recent operations\nccundo undo    \n# undo with preview\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=\"https://github.com/RonitSachdev/ccundo\"&gt;https://github.com/RonitSachdev/ccundo&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;npm&lt;/strong&gt;: &lt;a href=\"https://www.npmjs.com/package/ccundo\"&gt;https://www.npmjs.com/package/ccundo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;⭐ &lt;strong&gt;Please star if you find it useful!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think? Anyone else dealing with similar Claude Code frustrations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6UiXZfBdSRQ1PJxZrLYqXqLmvvPm-au6DrNcHt2Aijc.png?auto=webp&amp;s=863ab3e2e502033470783d1e0992f4c3ecede63c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6UiXZfBdSRQ1PJxZrLYqXqLmvvPm-au6DrNcHt2Aijc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=75a98b3388791d5a3f3f074ee677bc3bae984102",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6UiXZfBdSRQ1PJxZrLYqXqLmvvPm-au6DrNcHt2Aijc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99ed42a16cd747eae02d5895f468aa038ff1c7f0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6UiXZfBdSRQ1PJxZrLYqXqLmvvPm-au6DrNcHt2Aijc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=559c0770823848ca348375139661997677c10390",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6UiXZfBdSRQ1PJxZrLYqXqLmvvPm-au6DrNcHt2Aijc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=432f6667ff54bc05ca7d09e7ac2f095ad80d2e0f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6UiXZfBdSRQ1PJxZrLYqXqLmvvPm-au6DrNcHt2Aijc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c04909aa916381bfe32c8054ff1bec3eca3fbae9",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6UiXZfBdSRQ1PJxZrLYqXqLmvvPm-au6DrNcHt2Aijc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe2a180ed1820012376de8880c1a3a08dde97550",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6UiXZfBdSRQ1PJxZrLYqXqLmvvPm-au6DrNcHt2Aijc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lt13ht",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Competitive-Noise905",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt13ht/i_built_ccundo_instantly_undo_claude_codes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt13ht/i_built_ccundo_instantly_undo_claude_codes/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751808057,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, I was attempting to run the Gemma3n model with transformer libraries on my MacBook Pro, which has the M2 silicon chip. I managed to download the model and use the transformer library, but the inference time was incredibly slow. If anyone has any experience with the MacBook and Gemma3n, it would be really helpful.",
          "author_fullname": "t2_bo0sjlui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemma 3n is not performing well with macOS M2 MacBook Pro",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltj8pg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751856648,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I was attempting to run the Gemma3n model with transformer libraries on my MacBook Pro, which has the M2 silicon chip. I managed to download the model and use the transformer library, but the inference time was incredibly slow. If anyone has any experience with the MacBook and Gemma3n, it would be really helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltj8pg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Strikingaks",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751856648,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\&gt; ollama run phi4-mini-reasoning:3.8b\n\n\\&gt;&gt;&gt; hello\n\n&lt;think&gt;\n\nOkay, let's see what the user needs here. The message says \"You will be given a problem.\" but\n\nthen it just has \"hello\". Hmm, maybe there was a typo or the problem didn't get sent\n\ncorrectly. Let me check again.\n\n\n\nWait, the user's name is Phi, an AI math expert from Microsoft. Then there's a greeting\n\n\"hello\" and that's it. So perhaps the actual problem wasn't included properly. Maybe the user\n\nintended to send a math problem but only wrote \"hello\". I should prompt them to provide the\n\nproblem they need help with. But according to the instructions, I need to first think through\n\nsolving the problem before giving the answer. However, there's no problem here.\n\n\n\nAlternatively, maybe \"hello\" is the problem itself? That seems unlikely. Maybe it's a test or\n\na mistake. Let me consider if \"hello\" could be part of a math question. For example,\n\nconverting letters to numbers using ASCII values. Let's try that.\n\n\n\nH in ASCII is 72, E is 69, L is 76, another L is 76, and O is 79. So adding those up: 72 + 69\n\n\\+ 76 + 76 + 79. Let me calculate that. 72 + 69 is 141. 141 + 76 is 217. 217 + 76 is 293. 293\n\n\\+ 79 is 372. So the total would be 372. But I'm not sure if that's what the user wants. They\n\nmight have intended a different approach.\n\n\n\nAlternatively, maybe it's a cipher problem, like Caesar cipher where each letter is shifted.\n\nFor example, shifting letters by a certain number. But without more context, it's hard to\n\ntell.\n\n\n\nSince the user mentioned being an AI math expert and the initial instruction was about\n\nsolving a problem, but the input only has \"hello\", I think the best course is to ask for\n\nclarification. However, since the task requires providing an answer within a box, maybe I\n\nshould state that the problem is missing or assume a common problem. Wait, perhaps the user\n\nintended to send a problem but it didn't go through, so responding by pointing out the\n\nmissing problem would be better. But according to the example given in the query, sometimes\n\neven if there's confusion, you have to proceed.\n\n\n\nAlternatively, maybe \"hello\" is a greeting and the actual problem is forthcoming. But the\n\nstructure of the query shows that after \"You will be given a problem,\" the user input is\n\n\"hello\". So perhaps there was an error in submission. In that case, my answer should note\n\nthat the problem seems missing and ask for it. But since I need to provide a boxed answer,\n\nmaybe there's a default response. Hmm.\n\n\n\nAlternatively, if this is a test of handling errors, then the correct response would be to\n\ninform the user that no problem was detected and request the problem statement. However,\n\ngiven the setup here, perhaps the assistant is supposed to generate a sample problem and\n\nsolution? But that's speculative.\n\n\n\nWait, looking back at the original instructions: \"You will be given a problem. Please reason\n\nstep by step, and put your final answer within \\\\boxed{}.\" The user's input after that is\n\n\"hello\". So maybe the actual problem was meant to follow \"hello\" but didn't. Therefore, my\n\nconclusion is that there's a missing problem here. Since I need to provide an answer, perhaps\n\nI should state that the problem is missing and ask for it, but in the boxed answer? But the\n\nexample shows mathematical answers in boxes. Alternatively, maybe it's a trick question where\n\n\"hello\" translates to something else.\n\n\n\nIf we consider \"hello\" as a hexadecimal number, H isn't valid hex (only 0-9 and A-F). So\n\nthat's not possible. If using base64 decoding, but again, letters are allowed. But without\n\nmore context, this is all guesswork.\n\n\n\nGiven that the user is an AI math expert, maybe they intended to send a problem but it didn't\n\ncome through. Since I need to follow the instructions strictly, perhaps I should answer based\n\non \"hello\" being the input. If we take \"hello\" as a string and compute something related to\n\nits letters, like the sum of their positions in the alphabet: H=8, E=5, L=12, L=12, O=15. So\n\n8+5+12+12+15 = 52. Maybe that's the answer? \\\\boxed{52}\n\n\n\nAlternatively, product: 8\\*5\\*12\\*12\\*15. Let's compute that: 8\\*5=40, 40\\*12=480, 480\\*12=5760,\n\n5760\\*15=86400. That's a big number, but maybe not likely.\n\n\n\nAlternatively, count the letters: \"hello\" has 5 letters. So \\\\boxed{5}. But that's too simple.\n\n\n\n\n\nAnother angle: \"HELLO\" in different languages? Not sure. Or Morse code: H is comma, E is dot\n\nslash, L are double dash, O is hollow exclamation. But converting that to numbers or\n\nsomething else... Maybe the total dashes and dots. H (comma) has 3 symbols, but maybe not\n\nrelevant.\n\n\n\nAlternatively, if it's a calculus problem, like finding the derivative of \"hello\", which\n\ndoesn't make sense.\n\n\n\nGiven all this uncertainty, perhaps the safest assumption is that the user intended to send a\n\nmath problem but it didn't come through correctly. Therefore, my answer should indicate that\n\nthe problem is missing. However, since I need to provide a boxed answer as per instructions,\n\nand there's no way to know, maybe I'll go with the sum of the alphabetical positions:\n\n8+5+12+12+15=52. So \\\\boxed{52}.\n\n&lt;/think&gt;\n\n\n\nThe problem statement appears to be incomplete or incorrectly submitted. However, if we\n\ninterpret \"hello\" by converting each letter to its position in the alphabet (H=8, E=5, L=12,\n\nO=15) and summing them:\n\n\n\n\\\\\\[\n\n8 + 5 + 12 + 12 + 15 = 52\n\n\\\\\\]\n\n\n\n\\\\boxed{52}",
          "author_fullname": "t2_ltqoxikt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "All i said was hello lol",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsofwq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 118,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 118,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751762144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; ollama run phi4-mini-reasoning:3.8b&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hello&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;Okay, let&amp;#39;s see what the user needs here. The message says &amp;quot;You will be given a problem.&amp;quot; but&lt;/p&gt;\n\n&lt;p&gt;then it just has &amp;quot;hello&amp;quot;. Hmm, maybe there was a typo or the problem didn&amp;#39;t get sent&lt;/p&gt;\n\n&lt;p&gt;correctly. Let me check again.&lt;/p&gt;\n\n&lt;p&gt;Wait, the user&amp;#39;s name is Phi, an AI math expert from Microsoft. Then there&amp;#39;s a greeting&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;hello&amp;quot; and that&amp;#39;s it. So perhaps the actual problem wasn&amp;#39;t included properly. Maybe the user&lt;/p&gt;\n\n&lt;p&gt;intended to send a math problem but only wrote &amp;quot;hello&amp;quot;. I should prompt them to provide the&lt;/p&gt;\n\n&lt;p&gt;problem they need help with. But according to the instructions, I need to first think through&lt;/p&gt;\n\n&lt;p&gt;solving the problem before giving the answer. However, there&amp;#39;s no problem here.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, maybe &amp;quot;hello&amp;quot; is the problem itself? That seems unlikely. Maybe it&amp;#39;s a test or&lt;/p&gt;\n\n&lt;p&gt;a mistake. Let me consider if &amp;quot;hello&amp;quot; could be part of a math question. For example,&lt;/p&gt;\n\n&lt;p&gt;converting letters to numbers using ASCII values. Let&amp;#39;s try that.&lt;/p&gt;\n\n&lt;p&gt;H in ASCII is 72, E is 69, L is 76, another L is 76, and O is 79. So adding those up: 72 + 69&lt;/p&gt;\n\n&lt;p&gt;+ 76 + 76 + 79. Let me calculate that. 72 + 69 is 141. 141 + 76 is 217. 217 + 76 is 293. 293&lt;/p&gt;\n\n&lt;p&gt;+ 79 is 372. So the total would be 372. But I&amp;#39;m not sure if that&amp;#39;s what the user wants. They&lt;/p&gt;\n\n&lt;p&gt;might have intended a different approach.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, maybe it&amp;#39;s a cipher problem, like Caesar cipher where each letter is shifted.&lt;/p&gt;\n\n&lt;p&gt;For example, shifting letters by a certain number. But without more context, it&amp;#39;s hard to&lt;/p&gt;\n\n&lt;p&gt;tell.&lt;/p&gt;\n\n&lt;p&gt;Since the user mentioned being an AI math expert and the initial instruction was about&lt;/p&gt;\n\n&lt;p&gt;solving a problem, but the input only has &amp;quot;hello&amp;quot;, I think the best course is to ask for&lt;/p&gt;\n\n&lt;p&gt;clarification. However, since the task requires providing an answer within a box, maybe I&lt;/p&gt;\n\n&lt;p&gt;should state that the problem is missing or assume a common problem. Wait, perhaps the user&lt;/p&gt;\n\n&lt;p&gt;intended to send a problem but it didn&amp;#39;t go through, so responding by pointing out the&lt;/p&gt;\n\n&lt;p&gt;missing problem would be better. But according to the example given in the query, sometimes&lt;/p&gt;\n\n&lt;p&gt;even if there&amp;#39;s confusion, you have to proceed.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, maybe &amp;quot;hello&amp;quot; is a greeting and the actual problem is forthcoming. But the&lt;/p&gt;\n\n&lt;p&gt;structure of the query shows that after &amp;quot;You will be given a problem,&amp;quot; the user input is&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;hello&amp;quot;. So perhaps there was an error in submission. In that case, my answer should note&lt;/p&gt;\n\n&lt;p&gt;that the problem seems missing and ask for it. But since I need to provide a boxed answer,&lt;/p&gt;\n\n&lt;p&gt;maybe there&amp;#39;s a default response. Hmm.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if this is a test of handling errors, then the correct response would be to&lt;/p&gt;\n\n&lt;p&gt;inform the user that no problem was detected and request the problem statement. However,&lt;/p&gt;\n\n&lt;p&gt;given the setup here, perhaps the assistant is supposed to generate a sample problem and&lt;/p&gt;\n\n&lt;p&gt;solution? But that&amp;#39;s speculative.&lt;/p&gt;\n\n&lt;p&gt;Wait, looking back at the original instructions: &amp;quot;You will be given a problem. Please reason&lt;/p&gt;\n\n&lt;p&gt;step by step, and put your final answer within \\boxed{}.&amp;quot; The user&amp;#39;s input after that is&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;hello&amp;quot;. So maybe the actual problem was meant to follow &amp;quot;hello&amp;quot; but didn&amp;#39;t. Therefore, my&lt;/p&gt;\n\n&lt;p&gt;conclusion is that there&amp;#39;s a missing problem here. Since I need to provide an answer, perhaps&lt;/p&gt;\n\n&lt;p&gt;I should state that the problem is missing and ask for it, but in the boxed answer? But the&lt;/p&gt;\n\n&lt;p&gt;example shows mathematical answers in boxes. Alternatively, maybe it&amp;#39;s a trick question where&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;hello&amp;quot; translates to something else.&lt;/p&gt;\n\n&lt;p&gt;If we consider &amp;quot;hello&amp;quot; as a hexadecimal number, H isn&amp;#39;t valid hex (only 0-9 and A-F). So&lt;/p&gt;\n\n&lt;p&gt;that&amp;#39;s not possible. If using base64 decoding, but again, letters are allowed. But without&lt;/p&gt;\n\n&lt;p&gt;more context, this is all guesswork.&lt;/p&gt;\n\n&lt;p&gt;Given that the user is an AI math expert, maybe they intended to send a problem but it didn&amp;#39;t&lt;/p&gt;\n\n&lt;p&gt;come through. Since I need to follow the instructions strictly, perhaps I should answer based&lt;/p&gt;\n\n&lt;p&gt;on &amp;quot;hello&amp;quot; being the input. If we take &amp;quot;hello&amp;quot; as a string and compute something related to&lt;/p&gt;\n\n&lt;p&gt;its letters, like the sum of their positions in the alphabet: H=8, E=5, L=12, L=12, O=15. So&lt;/p&gt;\n\n&lt;p&gt;8+5+12+12+15 = 52. Maybe that&amp;#39;s the answer? \\boxed{52}&lt;/p&gt;\n\n&lt;p&gt;Alternatively, product: 8*5*12*12*15. Let&amp;#39;s compute that: 8*5=40, 40*12=480, 480*12=5760,&lt;/p&gt;\n\n&lt;p&gt;5760*15=86400. That&amp;#39;s a big number, but maybe not likely.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, count the letters: &amp;quot;hello&amp;quot; has 5 letters. So \\boxed{5}. But that&amp;#39;s too simple.&lt;/p&gt;\n\n&lt;p&gt;Another angle: &amp;quot;HELLO&amp;quot; in different languages? Not sure. Or Morse code: H is comma, E is dot&lt;/p&gt;\n\n&lt;p&gt;slash, L are double dash, O is hollow exclamation. But converting that to numbers or&lt;/p&gt;\n\n&lt;p&gt;something else... Maybe the total dashes and dots. H (comma) has 3 symbols, but maybe not&lt;/p&gt;\n\n&lt;p&gt;relevant.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if it&amp;#39;s a calculus problem, like finding the derivative of &amp;quot;hello&amp;quot;, which&lt;/p&gt;\n\n&lt;p&gt;doesn&amp;#39;t make sense.&lt;/p&gt;\n\n&lt;p&gt;Given all this uncertainty, perhaps the safest assumption is that the user intended to send a&lt;/p&gt;\n\n&lt;p&gt;math problem but it didn&amp;#39;t come through correctly. Therefore, my answer should indicate that&lt;/p&gt;\n\n&lt;p&gt;the problem is missing. However, since I need to provide a boxed answer as per instructions,&lt;/p&gt;\n\n&lt;p&gt;and there&amp;#39;s no way to know, maybe I&amp;#39;ll go with the sum of the alphabetical positions:&lt;/p&gt;\n\n&lt;p&gt;8+5+12+12+15=52. So \\boxed{52}.&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;The problem statement appears to be incomplete or incorrectly submitted. However, if we&lt;/p&gt;\n\n&lt;p&gt;interpret &amp;quot;hello&amp;quot; by converting each letter to its position in the alphabet (H=8, E=5, L=12,&lt;/p&gt;\n\n&lt;p&gt;O=15) and summing them:&lt;/p&gt;\n\n&lt;p&gt;\\[&lt;/p&gt;\n\n&lt;p&gt;8 + 5 + 12 + 12 + 15 = 52&lt;/p&gt;\n\n&lt;p&gt;\\]&lt;/p&gt;\n\n&lt;p&gt;\\boxed{52}&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lsofwq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "numinouslymusing",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751762144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "New in the AI game but think we can utilize it heavily in our small shop. We receive data with 10s of thousands of records containing PII data, and would like to utilize a (preferably free) LLM to help our guys out. I like the idea of PandasAI but was wondering if there was any other suggestions? ",
          "author_fullname": "t2_1qnzts0yit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for an LLM suggestion for sorting massive CSVs.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltdmhl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751839969,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New in the AI game but think we can utilize it heavily in our small shop. We receive data with 10s of thousands of records containing PII data, and would like to utilize a (preferably free) LLM to help our guys out. I like the idea of PandasAI but was wondering if there was any other suggestions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltdmhl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LordMomotius",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltdmhl/looking_for_an_llm_suggestion_for_sorting_massive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltdmhl/looking_for_an_llm_suggestion_for_sorting_massive/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751839969,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why are they so expensive, has anybody here ever tested them?\nHow many rtx 5090s are needed to match it's performance?\nWhat llm can we run entirely on one h100 with as much RAM as required?\n\nNaive questions but I am very confused\n",
          "author_fullname": "t2_1p0cxl3avi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Should I buy an appartment or 4 H100s",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lskb8k",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 182,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 182,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751750079,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why are they so expensive, has anybody here ever tested them?\nHow many rtx 5090s are needed to match it&amp;#39;s performance?\nWhat llm can we run entirely on one h100 with as much RAM as required?&lt;/p&gt;\n\n&lt;p&gt;Naive questions but I am very confused&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lskb8k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InfiniteEjaculation",
          "discussion_type": null,
          "num_comments": 128,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751750079,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Building a PC was always one of those \"someday\" projects I never got around to. As a long-time Mac user, I honestly never had a real need for it. That all changed when I stumbled into the world of local AI. Suddenly, my 16GB Mac wasn't just slow, it was a hard bottleneck.\n\nSo, I started mapping out what this new machine needed to be:\n\n\\- **32GB VRAM as the baseline.** I'm really bullish on the future of MoE models and think 32-64gigs of VRAM should hold quite well.  \n\\- **128GB of RAM as the baseline.** Essential for wrangling the large datasets that come with the territory.  \n\\- **A clean, consumer-desk look.** I don't want a rugged, noisy server rack.  \n\\- **AI inference as the main job,** but I didn't want a one-trick pony. It still needed to be a decent all-rounder for daily tasks and, of course, some gaming.  \n\\- **Room to grow.** I wanted a foundation I could build on later.  \n\\- And the big one: **Keep it under $1500.**\n\nA new Mac with these specs would cost a fortune and be a dead end for upgrades. New NVIDIA cards? Forget about it, way too expensive. I looked at used 3090s, but they were still going for about $1000 where I am, and that was a definite no-no for my budget.\n\nJust as I was about to give up, I discovered the AMD MI50. The price-to-performance was incredible, and I started getting excited. Sure, the raw power isn't record-breaking, but the idea of running massive models and getting such insane value for my money was a huge draw.\n\nBut here was the catch: these are server cards. Even though they have a display port, it doesn't actually work. That would have killed my \"all-rounder\" requirement.\n\nI started digging deep, trying to find a workaround. That's when I hit a wall. Everywhere I looked, the consensus was the same: cross-flashing the VBIOS on these cards to enable the display port was a dead end for the 32GB version. It was largely declared impossible...\n\n...until the kind-hearted u/Accurate_Ad4323 from China stepped in to confirm it was possible. They even told me I could get the 32GB MI50s for as cheap as $130 from China, and that some people there had even programmed custom VBIOSes specifically for these 32GB cards. With all these pieces of crucial info, I was sold.\n\nI still had my doubts. Was this custom VBIOS stable? Would it mess with AI performance? There was practically no info out there about this on the 32GB cards, only the 16GB ones. Could I really trust a random stranger's advice? And with ROCm's reputation for being a bit tricky, I didn't want to make my life even harder.\n\nIn the end, I decided to pull the trigger. Worst-case scenario? I'd have 64GB of HBM2 memory for AI work for about $300, just with no display output. I decided to treat a working display as a bonus.\n\nI found a reliable seller on Alibaba who specialized in server gear and was selling the MI50 for $137. I browsed their store and found some other lucrative deals, formulating my build list right there.\n\nHere’s what I ordered from them:\n\n\\- Supermicro X11DPI-N -&gt; $320  \n\\- Dual Xeon 6148 CPUs -&gt; 27 \\* 2 = $54  \n\\- 2x CPU Coolers -&gt; $62  \n\\- 2x MI50 32GB GPUs -&gt; $137 \\* 2 = $274  \n\\- 4x 32GB DDR4 2666hz ECC RDIMM RAM sticks -&gt; $124  \n\\- 10x 120mm RGB fans -&gt; $32  \n\\- 6x 140mm RGB fans -&gt; $27  \n\\- 2x custom cooling shrouded fans for MI50s -&gt; $14  \n\\- Shipping + Duties -&gt; $187\n\nI know people get skeptical about Alibaba, but in my opinion, you're safe as long as you find the right seller, use a reliable freight forwarder, and always buy through Trade Assurance.\n\nWhen the parts arrived, one of the Xeon CPUs was DOA. It took some back-and-forth, but the seller was great and sent a replacement for free once they were convinced (I offered to cover the shipping on it, which is included in that $187 cost).\n\nI also bought these peripherals brand-new:\n\n\\- Phanteks Enthoo Pro 2 Server Edition -&gt; $200  \n\\- ProLab 1200W 80Plus Gold PSU -&gt; $100  \n\\- 2TB NVMe SSD (For Ubuntu) -&gt; $100  \n\\- 1TB 2.5 SSD (For Windows) -&gt; $50\n\nAll in, I spent exactly **$1544.**\n\nNow for the two final hurdles:\n\n1. **Assembling everything without breaking it!** As a first-timer, it took me about three very careful days, but I'm so proud of how it turned out.\n2. **Testing that custom VBIOS. Did I get the \"bonus\"?** After downloading the VBIOS, finding the right version of amdvbflash to force-flash, and installing the community NimeZ drivers... it actually works!!!\n\nNow, to answer the questions I had for myself about the VBIOS cross-flash:\n\n**Is it stable?** Totally. It acts just like a regular graphics card from boot-up. The only weird quirk is on Windows: if I set \"VGA Priority\" to the GPU in the BIOS, the NimeZ drivers get corrupted. A quick reinstall and switching the priority back to \"Onboard\" fixes it. This doesn't happen at all in Ubuntu with ROCm.\n\n**Does the flash hurt AI performance?** Surprisingly, no! It performs identically. The VBIOS is based on a Radeon Pro VII, and I've seen zero difference. If anything weird pops up, I'll be sure to update.\n\n**Can it game?** Yes! Performance is like a Radeon VII but with a ridiculous 32GB of VRAM. It comfortably handles anything I throw at it in 1080p at max settings and 60fps.\n\nI ended up with 64GB of versatile VRAM for under $300, and thanks to the Supermicro board, I have a clear upgrade path to 4TB of RAM and Xeon Platinum CPUs down the line. (if needed)\n\nNow, I'll end this off with a couple pictures of the build and some benchmarks.\n\nhttps://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790\n\nhttps://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71\n\n(The build is still a work-in-progress with regards to cable management :facepalm)\n\n**Benchmarks:**\n\n**llama.cpp:**\n\n**A power limit of 150W was imposed on both GPUs for all these tests.**\n\n**Qwen3-30B-A3B-128K-UD-Q4\\_K\\_XL:**\n\nbuild/bin/llama-bench --model models/Downloads/Qwen3-30B-A3B-128K-UD-Q4\\_K\\_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ------------------------------ | --------: | ------: | ------- | --: | ----: | ------------: |`\n\n`| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | pp512 | 472.40 ± 2.44 |`\n\n`| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | tg128 | 49.40 ± 0.07 |`\n\n**Magistral-Small-2506-UD-Q4\\_K\\_XL:**\n\nbuild/bin/llama-bench --model models/Downloads/Magistral-Small-2506-UD-Q4\\_K\\_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |`\n\n`| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | pp512 | 130.75 ± 0.09 |`\n\n`| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | tg128 | 20.96 ± 0.09 |`\n\n**gemma-3-27b-it-Q4\\_K\\_M:**\n\nbuild/bin/llama-bench --model models/Downloads/gemma-3-27b-it-Q4\\_K\\_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |`\n\n`| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | pp512 | 110.88 ± 3.01 |`\n\n`| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | tg128 | 17.98 ± 0.02 |`\n\n**Qwen3-32B-Q4\\_K\\_M:**\n\nbuild/bin/llama-bench --model models/Downloads/Qwen3-32B-Q4\\_K\\_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ----------------------- | --------: | ------: | ------- | --: | ----: | -----------: |`\n\n`| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | pp512 | 91.72 ± 0.03 |`\n\n`| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | tg128 | 16.12 ± 0.01 |`\n\n**Llama-3.3-70B-Instruct-UD-Q4\\_K\\_XL:**\n\nbuild/bin/llama-bench --model models/Downloads/Llama-3.3-70B-Instruct-UD-Q4\\_K\\_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |`\n\n`| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | pp512 | 42.49 ± 0.05 |`\n\n`| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | tg128 | 7.70 ± 0.01 |`\n\n**Qwen3-235B-A22B-128K-UD-Q2\\_K\\_XL:**\n\nbuild/bin/llama-bench --model models/Downloads/Qwen3-235B-A22B-128K-GGUF/Qwen3-235B-A22B-128K-UD-Q2\\_K\\_XL-00001-of-00002.gguf -ot '(4-7+).ffn\\_.\\_exps.=CPU' -ngl 99 --threads 40 --flash-attn --no-mmap\n\n`| model | size | params | backend | ngl | ot | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------------- | --------------: | -------------------: |`\n\n`| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | pp512 | 29.80 ± 0.15 |`\n\n`| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | tg128 | 7.45 ± 0.09 |`\n\nI'm aware of the severe multi-GPU performance bottleneck with llama.cpp. Just started messing with vLLM, exLlamav2 and MLC-LLM. Will update results here once I get them up and running properly.\n\n**Furmark scores post VBIOS flash and NimeZ drivers on Windows:**\n\nhttps://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f\n\nhttps://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=355644c658940db5c30be46cc702e63c795b2764\n\nOverall, this whole experience has been an adventure, but it's been overwhelmingly positive. I thought I'd share it for anyone else thinking about a similar build.\n\nEdit:  \nNoticed a lot of requests to post the seller. Here you go: [https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB\\_1601432581416.html](https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html)",
          "author_fullname": "t2_xg2jtdg74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "fev4jvhbn3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0cb53b670e9300ae2b50074de2b8d3dc66a9fe87"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51c55df38925fd8e5035205796df0579bdeb151a"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a80d52a276702ac9d77640d9bad77ca78e31e34"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7d12fa0c8bc1d2eb10c9b53791aecf0436b5244"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87e20f461b5eec4dfab0f593184f9388a2b8ff4f"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=471ae28347a731a4a36bfb79254a6accb46173c7"
                }
              ],
              "s": {
                "y": 4032,
                "x": 3024,
                "u": "https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790"
              },
              "id": "fev4jvhbn3bf1"
            },
            "n6idruhbn3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=767fc70468637b42fc552ed9d235a923f05ff0a0"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=84307816e868bb27b7fa02ed5b85329ec9df0407"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5eb864627b1ae955a8268976b4ecd531b53bf5c"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bc4fba42ce896fa0055d37bd9cba425adb5e165"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1692c1bece8efa78cadcb51da30eb79d827033b1"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a47400bc8de85ca0360049c6e29f7cb93d8e629"
                }
              ],
              "s": {
                "y": 4032,
                "x": 3024,
                "u": "https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71"
              },
              "id": "n6idruhbn3bf1"
            },
            "isqld5nwn3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5748b64b5ba78c86aae4285fc588436d8f81742c"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e3b3cd10872cf1549a7b35b41f8996ed0825843"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d7e9270eb1cbb10537d007f43745e028b629c89"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b33ea43cbc62e3a698a0c1182ba53c283868438f"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=370a54cdae2b14affceb8311fc135c836eddeace"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ef07848e931b66eae2c8dba1494196f2e0a289b"
                }
              ],
              "s": {
                "y": 3024,
                "x": 4032,
                "u": "https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f"
              },
              "id": "isqld5nwn3bf1"
            },
            "muxix6nwn3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c7826369c69a8f918bbc5d94aee6b055cdf9e749"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=32c29605876a631d5d1dcd649dd015890659b2b2"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e043d7730616fe9e50abd9bb67a25c734ccb9174"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a18d4a7b1e058b3585ff069f100539ed52b4b964"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a9c9b18fb6633140ab5763bb4fc56bcc9b25c07"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02d1747cdc8ab2217c6a8ac0ff6a04af3097efc8"
                }
              ],
              "s": {
                "y": 3024,
                "x": 4032,
                "u": "https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=355644c658940db5c30be46cc702e63c795b2764"
              },
              "id": "muxix6nwn3bf1"
            }
          },
          "name": "t3_1lsgtvy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 278,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 278,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=b008aeddc56c8fcc997e1dfd569cf4b419fe9e6a",
          "edited": 1751767433,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751740750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Building a PC was always one of those &amp;quot;someday&amp;quot; projects I never got around to. As a long-time Mac user, I honestly never had a real need for it. That all changed when I stumbled into the world of local AI. Suddenly, my 16GB Mac wasn&amp;#39;t just slow, it was a hard bottleneck.&lt;/p&gt;\n\n&lt;p&gt;So, I started mapping out what this new machine needed to be:&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;32GB VRAM as the baseline.&lt;/strong&gt; I&amp;#39;m really bullish on the future of MoE models and think 32-64gigs of VRAM should hold quite well.&lt;br/&gt;\n- &lt;strong&gt;128GB of RAM as the baseline.&lt;/strong&gt; Essential for wrangling the large datasets that come with the territory.&lt;br/&gt;\n- &lt;strong&gt;A clean, consumer-desk look.&lt;/strong&gt; I don&amp;#39;t want a rugged, noisy server rack.&lt;br/&gt;\n- &lt;strong&gt;AI inference as the main job,&lt;/strong&gt; but I didn&amp;#39;t want a one-trick pony. It still needed to be a decent all-rounder for daily tasks and, of course, some gaming.&lt;br/&gt;\n- &lt;strong&gt;Room to grow.&lt;/strong&gt; I wanted a foundation I could build on later.&lt;br/&gt;\n- And the big one: &lt;strong&gt;Keep it under $1500.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A new Mac with these specs would cost a fortune and be a dead end for upgrades. New NVIDIA cards? Forget about it, way too expensive. I looked at used 3090s, but they were still going for about $1000 where I am, and that was a definite no-no for my budget.&lt;/p&gt;\n\n&lt;p&gt;Just as I was about to give up, I discovered the AMD MI50. The price-to-performance was incredible, and I started getting excited. Sure, the raw power isn&amp;#39;t record-breaking, but the idea of running massive models and getting such insane value for my money was a huge draw.&lt;/p&gt;\n\n&lt;p&gt;But here was the catch: these are server cards. Even though they have a display port, it doesn&amp;#39;t actually work. That would have killed my &amp;quot;all-rounder&amp;quot; requirement.&lt;/p&gt;\n\n&lt;p&gt;I started digging deep, trying to find a workaround. That&amp;#39;s when I hit a wall. Everywhere I looked, the consensus was the same: cross-flashing the VBIOS on these cards to enable the display port was a dead end for the 32GB version. It was largely declared impossible...&lt;/p&gt;\n\n&lt;p&gt;...until the kind-hearted &lt;a href=\"/u/Accurate_Ad4323\"&gt;u/Accurate_Ad4323&lt;/a&gt; from China stepped in to confirm it was possible. They even told me I could get the 32GB MI50s for as cheap as $130 from China, and that some people there had even programmed custom VBIOSes specifically for these 32GB cards. With all these pieces of crucial info, I was sold.&lt;/p&gt;\n\n&lt;p&gt;I still had my doubts. Was this custom VBIOS stable? Would it mess with AI performance? There was practically no info out there about this on the 32GB cards, only the 16GB ones. Could I really trust a random stranger&amp;#39;s advice? And with ROCm&amp;#39;s reputation for being a bit tricky, I didn&amp;#39;t want to make my life even harder.&lt;/p&gt;\n\n&lt;p&gt;In the end, I decided to pull the trigger. Worst-case scenario? I&amp;#39;d have 64GB of HBM2 memory for AI work for about $300, just with no display output. I decided to treat a working display as a bonus.&lt;/p&gt;\n\n&lt;p&gt;I found a reliable seller on Alibaba who specialized in server gear and was selling the MI50 for $137. I browsed their store and found some other lucrative deals, formulating my build list right there.&lt;/p&gt;\n\n&lt;p&gt;Here’s what I ordered from them:&lt;/p&gt;\n\n&lt;p&gt;- Supermicro X11DPI-N -&amp;gt; $320&lt;br/&gt;\n- Dual Xeon 6148 CPUs -&amp;gt; 27 * 2 = $54&lt;br/&gt;\n- 2x CPU Coolers -&amp;gt; $62&lt;br/&gt;\n- 2x MI50 32GB GPUs -&amp;gt; $137 * 2 = $274&lt;br/&gt;\n- 4x 32GB DDR4 2666hz ECC RDIMM RAM sticks -&amp;gt; $124&lt;br/&gt;\n- 10x 120mm RGB fans -&amp;gt; $32&lt;br/&gt;\n- 6x 140mm RGB fans -&amp;gt; $27&lt;br/&gt;\n- 2x custom cooling shrouded fans for MI50s -&amp;gt; $14&lt;br/&gt;\n- Shipping + Duties -&amp;gt; $187&lt;/p&gt;\n\n&lt;p&gt;I know people get skeptical about Alibaba, but in my opinion, you&amp;#39;re safe as long as you find the right seller, use a reliable freight forwarder, and always buy through Trade Assurance.&lt;/p&gt;\n\n&lt;p&gt;When the parts arrived, one of the Xeon CPUs was DOA. It took some back-and-forth, but the seller was great and sent a replacement for free once they were convinced (I offered to cover the shipping on it, which is included in that $187 cost).&lt;/p&gt;\n\n&lt;p&gt;I also bought these peripherals brand-new:&lt;/p&gt;\n\n&lt;p&gt;- Phanteks Enthoo Pro 2 Server Edition -&amp;gt; $200&lt;br/&gt;\n- ProLab 1200W 80Plus Gold PSU -&amp;gt; $100&lt;br/&gt;\n- 2TB NVMe SSD (For Ubuntu) -&amp;gt; $100&lt;br/&gt;\n- 1TB 2.5 SSD (For Windows) -&amp;gt; $50&lt;/p&gt;\n\n&lt;p&gt;All in, I spent exactly &lt;strong&gt;$1544.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Now for the two final hurdles:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Assembling everything without breaking it!&lt;/strong&gt; As a first-timer, it took me about three very careful days, but I&amp;#39;m so proud of how it turned out.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Testing that custom VBIOS. Did I get the &amp;quot;bonus&amp;quot;?&lt;/strong&gt; After downloading the VBIOS, finding the right version of amdvbflash to force-flash, and installing the community NimeZ drivers... it actually works!!!&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, to answer the questions I had for myself about the VBIOS cross-flash:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is it stable?&lt;/strong&gt; Totally. It acts just like a regular graphics card from boot-up. The only weird quirk is on Windows: if I set &amp;quot;VGA Priority&amp;quot; to the GPU in the BIOS, the NimeZ drivers get corrupted. A quick reinstall and switching the priority back to &amp;quot;Onboard&amp;quot; fixes it. This doesn&amp;#39;t happen at all in Ubuntu with ROCm.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Does the flash hurt AI performance?&lt;/strong&gt; Surprisingly, no! It performs identically. The VBIOS is based on a Radeon Pro VII, and I&amp;#39;ve seen zero difference. If anything weird pops up, I&amp;#39;ll be sure to update.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Can it game?&lt;/strong&gt; Yes! Performance is like a Radeon VII but with a ridiculous 32GB of VRAM. It comfortably handles anything I throw at it in 1080p at max settings and 60fps.&lt;/p&gt;\n\n&lt;p&gt;I ended up with 64GB of versatile VRAM for under $300, and thanks to the Supermicro board, I have a clear upgrade path to 4TB of RAM and Xeon Platinum CPUs down the line. (if needed)&lt;/p&gt;\n\n&lt;p&gt;Now, I&amp;#39;ll end this off with a couple pictures of the build and some benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790\"&gt;https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71\"&gt;https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(The build is still a work-in-progress with regards to cable management :facepalm)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;llama.cpp:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A power limit of 150W was imposed on both GPUs for all these tests.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-128K-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-30B-A3B-128K-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | --------: | ------: | ------- | --: | ----: | ------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | pp512 | 472.40 ± 2.44 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | tg128 | 49.40 ± 0.07 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Magistral-Small-2506-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Magistral-Small-2506-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | pp512 | 130.75 ± 0.09 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | tg128 | 20.96 ± 0.09 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;gemma-3-27b-it-Q4_K_M:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/gemma-3-27b-it-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | pp512 | 110.88 ± 3.01 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | tg128 | 17.98 ± 0.02 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_K_M:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-32B-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ----------------------- | --------: | ------: | ------- | --: | ----: | -----------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | pp512 | 91.72 ± 0.03 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | tg128 | 16.12 ± 0.01 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Llama-3.3-70B-Instruct-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Llama-3.3-70B-Instruct-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | pp512 | 42.49 ± 0.05 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | tg128 | 7.70 ± 0.01 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-128K-UD-Q2_K_XL:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-235B-A22B-128K-GGUF/Qwen3-235B-A22B-128K-UD-Q2_K_XL-00001-of-00002.gguf -ot &amp;#39;(4-7+).ffn_._exps.=CPU&amp;#39; -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | ot | test | t/s |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------------- | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | pp512 | 29.80 ± 0.15 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | tg128 | 7.45 ± 0.09 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware of the severe multi-GPU performance bottleneck with llama.cpp. Just started messing with vLLM, exLlamav2 and MLC-LLM. Will update results here once I get them up and running properly.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Furmark scores post VBIOS flash and NimeZ drivers on Windows:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f\"&gt;https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764\"&gt;https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Overall, this whole experience has been an adventure, but it&amp;#39;s been overwhelmingly positive. I thought I&amp;#39;d share it for anyone else thinking about a similar build.&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;br/&gt;\nNoticed a lot of requests to post the seller. Here you go: &lt;a href=\"https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html\"&gt;https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?auto=webp&amp;s=e44dca6fa481ac23f63b69284bd0e06e39e13139",
                  "width": 800,
                  "height": 800
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9bff86ba77f876ea113af80eda2c7c1173584b76",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6db19ea6059623c7bcfb87881c8cec7be1f0b73",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1c75555b8422f8af4b5cc9d7073d3199a679e14",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=66839b47c62157c843227bea591a16264454d62a",
                    "width": 640,
                    "height": 640
                  }
                ],
                "variants": {},
                "id": "qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsgtvy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lowkey_LokiSN",
          "discussion_type": null,
          "num_comments": 138,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751740750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nHere, spent weeks putting this piece together.  I have a whole new appreciation for George Carlin now.  Satrical comedy is hard!\n\nAudio: https://youtu.be/xmSSmpvFFaI\n\nText / Forums: https://cicero.sh/r/hows-the-ai-revolution\n\nFull text of the piece: \n\n# The AI Revolution: How's it Going for You?\n\nAudio: https://youtu.be/xmSSmpvFFaI\n\nWe're 2.5 years into this exhilarating journey, so let's get a quick progress update...\n\n## Big Tech's Mission Impossible\n\nFor those in the unknown, let me bring you up to speed.  Years ago we stumbled across this really cool new technology called LLMs.  Great tech, amazing at distilling and compressing knowledge, fun, entertaining, and something we should all be able to collectively celebrate.\n\nBut of course we can't, because the modern tech industry has been commandeered by a handful of billionaire psychopaths.  These splendid group of individuals, some of the most powerful and wealthy in the world, have decided gosh darnit, they just don't quite have enough.\n\nTheir multiple spaceships, private islands, expansive living estates, and unfathomable wealth just isn't quite enough and they need just a little more.  And how much more you ask?  Not much, they only want to hoover up the entire global economy while transforming the world into their own personal technocratic fiefdom.  You know, the normal desires we all have in life.\n\nAccording to these geniuses, any week now ASI will appear, bringing about some mystical age of abundance.  Any day now ChatGPT is going to eliminate world poverty, solve all of physics, cure cancer, create nuclear fusion, start building self replicating spaceships, all while making us pancakes in bed and walking our dog!\n\nAll we have to do is sit back, relax, hand over our credit cards, and live stream our daily lives to their servers.  Don't worry folks, they will take care of the rest.\n\n## LLMs Are Cool\n\nDon't get me wrong, I love my LLMs, use them all day every day.  It's simply cool technology.  Same as when I got my first smart phone, it was such a cool bump in life, right?\n\nBut have you ever actually played with this tech?  Ever actually gave it a poke?  It simply doesn't work.  Stick a fork into these things, and you will see, dumb as a hamster.\n\nNothing more than multi billion dollar mechanical turk devices designed to steal our personal data, attention, and corrupt our cognition.  And these folks want us to believe this is the fourth industrial revolution?  What reality do these people live in?\n\n## Test It Yourself\n\nYou don't have to believe me, give it a spin.  Just ask it to write you a toaster in C++.  Take the code it gives you, copy and paste that code into a new chat and ask for inefficiencies.\n\nGuaranteed, it's going to tell you there's tons of problems with the code, and will try to help you fix them.  You can even have a whole back and forth conversation with it about why your toaster isn't working.\n\nAll the while, it doesn't have the common sense to tell you that you can't make a toaster out of C++ code.  Figure that one out!\n\n## Teach Our Kids?\n\nAnother one, have it write a lengthy non-fiction piece about any topic you desire.  Open two new conversations, copy the piece in.  Preface one with \"this is absolutely amazing!\" and the other with \"I'm so pissed off, I'm firing this moron!\".\n\nWatch the responses, you'll get three versions of the truth.  This tech tells you what you want to hear, not the truth!  And they actually want this in every classroom teaching the next generation of our kids?\n\n## Where did Tech Go?\n\nI remember a time where tech was cool.  You know, when we got a bump from CDs to DVDs, or from 33.6k modems to broadband, or from flip phones to smart phones.  Every year, we'd just get this cool little almost transparent bump in our lives.\n\nSilicon Valley, a magical place that used to be a beacon for the innovative and intellectually curious, and who had society's best interest at heart.  Have you looked at it lately?\n\nIt's morphed into a grotesque embarrassment.  It's not even really technology anymore.  Just a small handful of ultra rich having a public dick measuring contest, seeing which one can solve AGI first.\n\nThey're so desperate to get there first too.  Hell, Mark Zuckerberg has apparently had enough.  So that's it, he's going to hand select 50 people then shuffle the desks around in Menlo Park so he can keep an eye on these folks while they make him AGI.  You bet, because that's how innovation happens!\n\nTotally ignore the legend of innovation, which is that of Bell Labs in the 1940s - 60s.  Instead, just rearrange some desks so you can keep a close eye on your engineers, because that's how technological breakthroughs happen!\n\n## Carpe Diem\n\nOn a more serious note, I don't know much, but I've figured out a few things in this journey we call life.\n\nWe can all see the pain and sadness that's out there.  Hell, I wake up each day surprised I'm still alive and haven't taken a nap on the railroad tracks yet, so trust me, I know how brutal it can be.\n\nI don't know much, but I do know it's time we all go say hi to our fellow neighbor.  Go ask if they're ok.  Through that, I know magical and spontaneous connections will be made, and these connections, regardless of how innate they may seem, will spur true hope, human ingenuity and write the next chapter in our shared history.\n\nDon't worry about what algorithm Sam Altman, Elon Musk or Dario Amodei is promising they have up their sleeve.  View these people as your brother and sister, and don't be scared to call them out on their bullshit.\n\nUs humans love, laugh, cry, entertain, innovate, and build masterpieces together.  No algorithm will ever replace that.\n\nIt may seem dark right now, but the skies will clear, because you only need to crack a history book to see that humanity always prevails.\n\n## Support Cicero\n\nThank you, if you found this piece engaging, please consider supporting Cicero.  An open source initiative designed to lock big tech out of our lives through open source innovation.\n\nI don't know about you folks, but I know I'm tired of having big tech ramming shit we don't need, don't want, and never asked for down our throats.  We can do so much better than this!\n\nVisit https://cicero.sh/ for details on project Cicero.\n\n\n",
          "author_fullname": "t2_5i7l1nof",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The AI Revolution: How's it Going for You?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lth6ga",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751850175,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here, spent weeks putting this piece together.  I have a whole new appreciation for George Carlin now.  Satrical comedy is hard!&lt;/p&gt;\n\n&lt;p&gt;Audio: &lt;a href=\"https://youtu.be/xmSSmpvFFaI\"&gt;https://youtu.be/xmSSmpvFFaI&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Text / Forums: &lt;a href=\"https://cicero.sh/r/hows-the-ai-revolution\"&gt;https://cicero.sh/r/hows-the-ai-revolution&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Full text of the piece: &lt;/p&gt;\n\n&lt;h1&gt;The AI Revolution: How&amp;#39;s it Going for You?&lt;/h1&gt;\n\n&lt;p&gt;Audio: &lt;a href=\"https://youtu.be/xmSSmpvFFaI\"&gt;https://youtu.be/xmSSmpvFFaI&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re 2.5 years into this exhilarating journey, so let&amp;#39;s get a quick progress update...&lt;/p&gt;\n\n&lt;h2&gt;Big Tech&amp;#39;s Mission Impossible&lt;/h2&gt;\n\n&lt;p&gt;For those in the unknown, let me bring you up to speed.  Years ago we stumbled across this really cool new technology called LLMs.  Great tech, amazing at distilling and compressing knowledge, fun, entertaining, and something we should all be able to collectively celebrate.&lt;/p&gt;\n\n&lt;p&gt;But of course we can&amp;#39;t, because the modern tech industry has been commandeered by a handful of billionaire psychopaths.  These splendid group of individuals, some of the most powerful and wealthy in the world, have decided gosh darnit, they just don&amp;#39;t quite have enough.&lt;/p&gt;\n\n&lt;p&gt;Their multiple spaceships, private islands, expansive living estates, and unfathomable wealth just isn&amp;#39;t quite enough and they need just a little more.  And how much more you ask?  Not much, they only want to hoover up the entire global economy while transforming the world into their own personal technocratic fiefdom.  You know, the normal desires we all have in life.&lt;/p&gt;\n\n&lt;p&gt;According to these geniuses, any week now ASI will appear, bringing about some mystical age of abundance.  Any day now ChatGPT is going to eliminate world poverty, solve all of physics, cure cancer, create nuclear fusion, start building self replicating spaceships, all while making us pancakes in bed and walking our dog!&lt;/p&gt;\n\n&lt;p&gt;All we have to do is sit back, relax, hand over our credit cards, and live stream our daily lives to their servers.  Don&amp;#39;t worry folks, they will take care of the rest.&lt;/p&gt;\n\n&lt;h2&gt;LLMs Are Cool&lt;/h2&gt;\n\n&lt;p&gt;Don&amp;#39;t get me wrong, I love my LLMs, use them all day every day.  It&amp;#39;s simply cool technology.  Same as when I got my first smart phone, it was such a cool bump in life, right?&lt;/p&gt;\n\n&lt;p&gt;But have you ever actually played with this tech?  Ever actually gave it a poke?  It simply doesn&amp;#39;t work.  Stick a fork into these things, and you will see, dumb as a hamster.&lt;/p&gt;\n\n&lt;p&gt;Nothing more than multi billion dollar mechanical turk devices designed to steal our personal data, attention, and corrupt our cognition.  And these folks want us to believe this is the fourth industrial revolution?  What reality do these people live in?&lt;/p&gt;\n\n&lt;h2&gt;Test It Yourself&lt;/h2&gt;\n\n&lt;p&gt;You don&amp;#39;t have to believe me, give it a spin.  Just ask it to write you a toaster in C++.  Take the code it gives you, copy and paste that code into a new chat and ask for inefficiencies.&lt;/p&gt;\n\n&lt;p&gt;Guaranteed, it&amp;#39;s going to tell you there&amp;#39;s tons of problems with the code, and will try to help you fix them.  You can even have a whole back and forth conversation with it about why your toaster isn&amp;#39;t working.&lt;/p&gt;\n\n&lt;p&gt;All the while, it doesn&amp;#39;t have the common sense to tell you that you can&amp;#39;t make a toaster out of C++ code.  Figure that one out!&lt;/p&gt;\n\n&lt;h2&gt;Teach Our Kids?&lt;/h2&gt;\n\n&lt;p&gt;Another one, have it write a lengthy non-fiction piece about any topic you desire.  Open two new conversations, copy the piece in.  Preface one with &amp;quot;this is absolutely amazing!&amp;quot; and the other with &amp;quot;I&amp;#39;m so pissed off, I&amp;#39;m firing this moron!&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Watch the responses, you&amp;#39;ll get three versions of the truth.  This tech tells you what you want to hear, not the truth!  And they actually want this in every classroom teaching the next generation of our kids?&lt;/p&gt;\n\n&lt;h2&gt;Where did Tech Go?&lt;/h2&gt;\n\n&lt;p&gt;I remember a time where tech was cool.  You know, when we got a bump from CDs to DVDs, or from 33.6k modems to broadband, or from flip phones to smart phones.  Every year, we&amp;#39;d just get this cool little almost transparent bump in our lives.&lt;/p&gt;\n\n&lt;p&gt;Silicon Valley, a magical place that used to be a beacon for the innovative and intellectually curious, and who had society&amp;#39;s best interest at heart.  Have you looked at it lately?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s morphed into a grotesque embarrassment.  It&amp;#39;s not even really technology anymore.  Just a small handful of ultra rich having a public dick measuring contest, seeing which one can solve AGI first.&lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;re so desperate to get there first too.  Hell, Mark Zuckerberg has apparently had enough.  So that&amp;#39;s it, he&amp;#39;s going to hand select 50 people then shuffle the desks around in Menlo Park so he can keep an eye on these folks while they make him AGI.  You bet, because that&amp;#39;s how innovation happens!&lt;/p&gt;\n\n&lt;p&gt;Totally ignore the legend of innovation, which is that of Bell Labs in the 1940s - 60s.  Instead, just rearrange some desks so you can keep a close eye on your engineers, because that&amp;#39;s how technological breakthroughs happen!&lt;/p&gt;\n\n&lt;h2&gt;Carpe Diem&lt;/h2&gt;\n\n&lt;p&gt;On a more serious note, I don&amp;#39;t know much, but I&amp;#39;ve figured out a few things in this journey we call life.&lt;/p&gt;\n\n&lt;p&gt;We can all see the pain and sadness that&amp;#39;s out there.  Hell, I wake up each day surprised I&amp;#39;m still alive and haven&amp;#39;t taken a nap on the railroad tracks yet, so trust me, I know how brutal it can be.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know much, but I do know it&amp;#39;s time we all go say hi to our fellow neighbor.  Go ask if they&amp;#39;re ok.  Through that, I know magical and spontaneous connections will be made, and these connections, regardless of how innate they may seem, will spur true hope, human ingenuity and write the next chapter in our shared history.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t worry about what algorithm Sam Altman, Elon Musk or Dario Amodei is promising they have up their sleeve.  View these people as your brother and sister, and don&amp;#39;t be scared to call them out on their bullshit.&lt;/p&gt;\n\n&lt;p&gt;Us humans love, laugh, cry, entertain, innovate, and build masterpieces together.  No algorithm will ever replace that.&lt;/p&gt;\n\n&lt;p&gt;It may seem dark right now, but the skies will clear, because you only need to crack a history book to see that humanity always prevails.&lt;/p&gt;\n\n&lt;h2&gt;Support Cicero&lt;/h2&gt;\n\n&lt;p&gt;Thank you, if you found this piece engaging, please consider supporting Cicero.  An open source initiative designed to lock big tech out of our lives through open source innovation.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know about you folks, but I know I&amp;#39;m tired of having big tech ramming shit we don&amp;#39;t need, don&amp;#39;t want, and never asked for down our throats.  We can do so much better than this!&lt;/p&gt;\n\n&lt;p&gt;Visit &lt;a href=\"https://cicero.sh/\"&gt;https://cicero.sh/&lt;/a&gt; for details on project Cicero.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/e3bmmHLbAOUvXJFbUv7-hHT-rq6zdSCFEHpYPIbIJO4.jpeg?auto=webp&amp;s=03b1f42222476d21f3a283fe2f182a32476482bb",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/e3bmmHLbAOUvXJFbUv7-hHT-rq6zdSCFEHpYPIbIJO4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3125b932dc15ed6dedd94acf2aa43d2ead119a7",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/e3bmmHLbAOUvXJFbUv7-hHT-rq6zdSCFEHpYPIbIJO4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2de795e6dd7fd12973ea9a20c249e15aee7c316",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/e3bmmHLbAOUvXJFbUv7-hHT-rq6zdSCFEHpYPIbIJO4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b712cc7c6226d7dd1512ccd3d89f30b1ccef54a0",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "e3bmmHLbAOUvXJFbUv7-hHT-rq6zdSCFEHpYPIbIJO4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lth6ga",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mdizak",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lth6ga/the_ai_revolution_hows_it_going_for_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lth6ga/the_ai_revolution_hows_it_going_for_you/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751850175,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!   \n  \nI want to get started with local AI, and I’m looking for advice on where to begin. I'm reading some of the other posts about the same, but seeing how quickly AI advances I figured I'd ask. I’ve been looking at the smaller models like Llama and Deepseek's 8b. Apparently one is as small as 1.5b.... That can be run on some \\*very\\* modest hardware: [https://martech.org/how-to-run-deepseek-locally-on-your-computer/](https://martech.org/how-to-run-deepseek-locally-on-your-computer/)\n\nRight now, I’m working with a laptop with an i9-13980hx, an RTX 4080, 32gb DDR5, and 1tb ssd. I realize that I’m not going to be running a fortune 500 company, solving world hunger, or achieving The Singularity with this setup, but on paper it should be pretty capable for what I’m envisioning.  \n  \nThere’s three basic things I’d really like to try with local AI:   \n  \n\\-Fine-tuning/distilling them for more specific purposes-  \n  \nI’m currently using ChatGPT as a day-planner/calendar/to-do list that I can talk to. It’s great that it could also write a comparative essay on the agrarian economies of pre-roman versus post-roman Gaul… but I don’t need my calendar to do that. I need it to accurately follow instructions, keep accurate lists, and answer questions about information it has access to. Sometimes ChatGPT has been surprisingly bad at this, and it’s actually seemed to get worse as the models get “smarter” and “more human”.  \n  \n\\-Integrating them into larger “digital ecosystems”-\n\nThere are some things ChatGPT is too “smart” to do reliably. Like find every mention of a word in a document, or tell me what time it is (try it yourself. 1/3 correct, at best). These sound like tasks for a “dumb” service. Google Assistant will tell me what time it is with 100% accuracy. My 1993 Windows 3.1 finds every mention of a word in a document every time I use “Find”. Getting a local LLM to know when it’s time to offload the work to a different, simpler element would make the whole system much more smooth, reliable, and useful. Bonus points if it can also reach out to more powerful cloud AIs through things like an OpenAI API key.  \n  \n\\-Image recognition-  \n  \nI’ve got some interest in getting a part of that larger system to recognize images I train it for, but this is sort of icing on the cake. I hear things like computervision, resnet, and nyckel thrown around, but I don’t understand enough yet to even know what questions to ask.  \n  \nAny tips on where to start?",
          "author_fullname": "t2_djjpv9or",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting started with local AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsyza0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751801281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!   &lt;/p&gt;\n\n&lt;p&gt;I want to get started with local AI, and I’m looking for advice on where to begin. I&amp;#39;m reading some of the other posts about the same, but seeing how quickly AI advances I figured I&amp;#39;d ask. I’ve been looking at the smaller models like Llama and Deepseek&amp;#39;s 8b. Apparently one is as small as 1.5b.... That can be run on some *very* modest hardware: &lt;a href=\"https://martech.org/how-to-run-deepseek-locally-on-your-computer/\"&gt;https://martech.org/how-to-run-deepseek-locally-on-your-computer/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Right now, I’m working with a laptop with an i9-13980hx, an RTX 4080, 32gb DDR5, and 1tb ssd. I realize that I’m not going to be running a fortune 500 company, solving world hunger, or achieving The Singularity with this setup, but on paper it should be pretty capable for what I’m envisioning.  &lt;/p&gt;\n\n&lt;p&gt;There’s three basic things I’d really like to try with local AI:   &lt;/p&gt;\n\n&lt;p&gt;-Fine-tuning/distilling them for more specific purposes-  &lt;/p&gt;\n\n&lt;p&gt;I’m currently using ChatGPT as a day-planner/calendar/to-do list that I can talk to. It’s great that it could also write a comparative essay on the agrarian economies of pre-roman versus post-roman Gaul… but I don’t need my calendar to do that. I need it to accurately follow instructions, keep accurate lists, and answer questions about information it has access to. Sometimes ChatGPT has been surprisingly bad at this, and it’s actually seemed to get worse as the models get “smarter” and “more human”.  &lt;/p&gt;\n\n&lt;p&gt;-Integrating them into larger “digital ecosystems”-&lt;/p&gt;\n\n&lt;p&gt;There are some things ChatGPT is too “smart” to do reliably. Like find every mention of a word in a document, or tell me what time it is (try it yourself. 1/3 correct, at best). These sound like tasks for a “dumb” service. Google Assistant will tell me what time it is with 100% accuracy. My 1993 Windows 3.1 finds every mention of a word in a document every time I use “Find”. Getting a local LLM to know when it’s time to offload the work to a different, simpler element would make the whole system much more smooth, reliable, and useful. Bonus points if it can also reach out to more powerful cloud AIs through things like an OpenAI API key.  &lt;/p&gt;\n\n&lt;p&gt;-Image recognition-  &lt;/p&gt;\n\n&lt;p&gt;I’ve got some interest in getting a part of that larger system to recognize images I train it for, but this is sort of icing on the cake. I hear things like computervision, resnet, and nyckel thrown around, but I don’t understand enough yet to even know what questions to ask.  &lt;/p&gt;\n\n&lt;p&gt;Any tips on where to start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?auto=webp&amp;s=cfd5ee4ad43b989a84c4caf381b22afd8e6d8ea5",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bec92da8b7c865fcbfd67b314e99e5bd33f293c",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=92f38610799c0911650ee9f180e5cf9faece431a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0318c23e2f3071fd16fe8b65515c943a4755552b",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32a7352c8d588e3d37b875c01718a2a4450d0aa5",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4230117da66d05bc3179272583d4bdb1aeae14b6",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bef8acd875dec557d098ec5142079141b3d083eb",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsyza0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bitter-Ad640",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751801281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey yall, \n\nbeen a while but with summer comes extra electricity and with that comes breaker flips. So yay mid training failures so I have not completed this yet but did get mid way through and can see it slightttttttttttly works. Can see that it produce multiple tokens in one go and with the small dataset it got to a loss of 4 with 2 epochs.\n\n[https://github.com/alientony/Multi-Token\\_MoE\\_Model\\_Converter](https://github.com/alientony/Multi-Token_MoE_Model_Converter)\n\n\n\nWith this system we are looking to convert prior existing models into multiple token generators using the same method as Mixture of Experts we are creating multiple token generation heads allowing for parallel token generation.  \n  \nYou can specify the amount of allowed heads to be ran in parallel the gateway is allowed to select which heads run including the amount.\n\nSo if you have 10 heads, specify only 5 can run in parallel, and the model can choose 0 to 5 from the 10 model heads.\n\nAll heads are trained with a new \"off\" token that we default generate on each head that is not activated by the router system we ignore these tokens and organize all tokens based on \"left to right.\" So if model head 1 5 6 7 8 are chosen to generate tokens then the tokens are arranged starting from the last token.  \n  \nAlso considering a new \"self editing system\" allowing for the heads to specify where new tokens are generated and added and whether or not \"insert\" mode is on. This can allow for more complex actions allowing for modifications to be produce without causing extreme context length. We dedicate a single head to creating a positioning system to where tokens will be inserted and if new tokens will replace tokens after or just inserts into that area.\n\nSo similarly to diffusion but with expandable context lengths and with conversions on top of next token generation. \n\n  \n[Post #3](https://www.reddit.com/user/Alienanthony/comments/1jcalw6/split_brain_project_third_installment_integrating/)\n\n[Post #2](https://www.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/)\n\n[Post #1](https://www.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/)\n\n  \nWhats next? After I fiddle with this a little I want to take advantage of the multiple view points created by models to get a more completed viewpoint via a similar method of token generation in my split brained model.\n\nSuper tokens (Multi tokenizer vocabulary), linear diffusion, selective token generations.\n\nhttps://preview.redd.it/qz5411pugcbf1.png?width=1105&amp;format=png&amp;auto=webp&amp;s=320020ee6220f970ba612869233919bfb143dfd3\n\nYeah. I should have the router control which heads get the hidden state slight oversight but I'll go back through to modify it later.",
          "author_fullname": "t2_12ed7j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Split Brain Project - P4 Side adventure. MOE token heads",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "qz5411pugcbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 80,
                  "x": 108,
                  "u": "https://preview.redd.it/qz5411pugcbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3abad1e0365352f953b10fc89677546f9e891258"
                },
                {
                  "y": 161,
                  "x": 216,
                  "u": "https://preview.redd.it/qz5411pugcbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b216a04ee04d2b85719baa9d60d6f3a197e9fd27"
                },
                {
                  "y": 239,
                  "x": 320,
                  "u": "https://preview.redd.it/qz5411pugcbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c68b0081e6f9d85548bc7252c1a0a0f2699b92a0"
                },
                {
                  "y": 478,
                  "x": 640,
                  "u": "https://preview.redd.it/qz5411pugcbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e229811055cb3aa73e79811c54c5d8867ed22f3"
                },
                {
                  "y": 717,
                  "x": 960,
                  "u": "https://preview.redd.it/qz5411pugcbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=87e4e027ca8e90ef0c45bdd28ff90b72d9daf4f4"
                },
                {
                  "y": 807,
                  "x": 1080,
                  "u": "https://preview.redd.it/qz5411pugcbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb9761224081a655ac8c30f70b2c203ad506e32b"
                }
              ],
              "s": {
                "y": 826,
                "x": 1105,
                "u": "https://preview.redd.it/qz5411pugcbf1.png?width=1105&amp;format=png&amp;auto=webp&amp;s=320020ee6220f970ba612869233919bfb143dfd3"
              },
              "id": "qz5411pugcbf1"
            }
          },
          "name": "t3_1ltgq4i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4942a32600959b756d01d09533acf1f08c57eb5c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751848777,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey yall, &lt;/p&gt;\n\n&lt;p&gt;been a while but with summer comes extra electricity and with that comes breaker flips. So yay mid training failures so I have not completed this yet but did get mid way through and can see it slightttttttttttly works. Can see that it produce multiple tokens in one go and with the small dataset it got to a loss of 4 with 2 epochs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/alientony/Multi-Token_MoE_Model_Converter\"&gt;https://github.com/alientony/Multi-Token_MoE_Model_Converter&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;With this system we are looking to convert prior existing models into multiple token generators using the same method as Mixture of Experts we are creating multiple token generation heads allowing for parallel token generation.  &lt;/p&gt;\n\n&lt;p&gt;You can specify the amount of allowed heads to be ran in parallel the gateway is allowed to select which heads run including the amount.&lt;/p&gt;\n\n&lt;p&gt;So if you have 10 heads, specify only 5 can run in parallel, and the model can choose 0 to 5 from the 10 model heads.&lt;/p&gt;\n\n&lt;p&gt;All heads are trained with a new &amp;quot;off&amp;quot; token that we default generate on each head that is not activated by the router system we ignore these tokens and organize all tokens based on &amp;quot;left to right.&amp;quot; So if model head 1 5 6 7 8 are chosen to generate tokens then the tokens are arranged starting from the last token.  &lt;/p&gt;\n\n&lt;p&gt;Also considering a new &amp;quot;self editing system&amp;quot; allowing for the heads to specify where new tokens are generated and added and whether or not &amp;quot;insert&amp;quot; mode is on. This can allow for more complex actions allowing for modifications to be produce without causing extreme context length. We dedicate a single head to creating a positioning system to where tokens will be inserted and if new tokens will replace tokens after or just inserts into that area.&lt;/p&gt;\n\n&lt;p&gt;So similarly to diffusion but with expandable context lengths and with conversions on top of next token generation. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/user/Alienanthony/comments/1jcalw6/split_brain_project_third_installment_integrating/\"&gt;Post #3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/\"&gt;Post #2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/\"&gt;Post #1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Whats next? After I fiddle with this a little I want to take advantage of the multiple view points created by models to get a more completed viewpoint via a similar method of token generation in my split brained model.&lt;/p&gt;\n\n&lt;p&gt;Super tokens (Multi tokenizer vocabulary), linear diffusion, selective token generations.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qz5411pugcbf1.png?width=1105&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=320020ee6220f970ba612869233919bfb143dfd3\"&gt;https://preview.redd.it/qz5411pugcbf1.png?width=1105&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=320020ee6220f970ba612869233919bfb143dfd3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Yeah. I should have the router control which heads get the hidden state slight oversight but I&amp;#39;ll go back through to modify it later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4.png?auto=webp&amp;s=352663e354d064df6e99e457f2d729cf8d4d259c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cd42e9a770454b8248f1c637bfc619eb9ae8bfd",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1838eb98b4efb4dad43acca5489e9660829290c0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a6975a5e2d3645f5a6103ba5e0a670bca283c76",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5ecd36468b4a0ae1d314d545b3506341cd7b24b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=09e9dbcb8ea7dad9547014fe9dda7b783566563b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fdb89285e127e83820a86971b405ac3796e75d42",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "I2I36CRPzYma5PsHdKX21gxSCNQbLbOpawaVAeHCnY4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltgq4i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alienanthony",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltgq4i/split_brain_project_p4_side_adventure_moe_token/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltgq4i/split_brain_project_p4_side_adventure_moe_token/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751848777,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m in an AI Masters program that is just getting off the ground and I’m trying to help one of my professors locate resources that can be used for class projects. \n\nWe used the free GPU resources on Google Colab for some model training and such, but now we need inference endpoints and I’m not sure if Colab supports that kind of thing using the free tier. \n\nWe want to use n8n for some simple AI automation workflow projects. Having used n8n a little myself,  I know it needs an endpoint for inference. I use it with a GPU, but I know that it is likely not all students will have access to a GPU. \n\nAre there any free public inference endpoints out there for academic use, or do you think it would be better to just use Ollama with something like Gemma3n or a similar model that could run CPU only to be able to run on an average laptop that all students should have already? ",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need an inference endpoint students can set up and use to test n8n workflows for an AI class, what free or non-GPU options are available?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt0z6j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751807725,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m in an AI Masters program that is just getting off the ground and I’m trying to help one of my professors locate resources that can be used for class projects. &lt;/p&gt;\n\n&lt;p&gt;We used the free GPU resources on Google Colab for some model training and such, but now we need inference endpoints and I’m not sure if Colab supports that kind of thing using the free tier. &lt;/p&gt;\n\n&lt;p&gt;We want to use n8n for some simple AI automation workflow projects. Having used n8n a little myself,  I know it needs an endpoint for inference. I use it with a GPU, but I know that it is likely not all students will have access to a GPU. &lt;/p&gt;\n\n&lt;p&gt;Are there any free public inference endpoints out there for academic use, or do you think it would be better to just use Ollama with something like Gemma3n or a similar model that could run CPU only to be able to run on an average laptop that all students should have already? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lt0z6j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt0z6j/need_an_inference_endpoint_students_can_set_up/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt0z6j/need_an_inference_endpoint_students_can_set_up/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751807725,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.\n\nIs Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?\n\nLooking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance – not just benchmark wins. C/C++, python and Js.\n\nThanks in advance.\n\nEdit: Thank you @ all for the insights!!!!",
          "author_fullname": "t2_77k54e2b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is Codestral 22B still the best open LLM for local coding on 32–64 GB VRAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsmtzr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 107,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 107,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751801687,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751757237,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.&lt;/p&gt;\n\n&lt;p&gt;Is Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?&lt;/p&gt;\n\n&lt;p&gt;Looking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance – not just benchmark wins. C/C++, python and Js.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n\n&lt;p&gt;Edit: Thank you @ all for the insights!!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsmtzr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "One-Stress-6734",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751757237,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Recently started experimenting with training my own models and I wanted to ask if anyone knew. I would figure that if you were designing a model to be run at a precision lower than fp16 you would just quantize the model and then train over the quantized model instead of training in fp16 with “fake quantization”. I’m sure there’s a reason for it, I would just like to know if anyone more experienced than I could explain it. Thanks!",
          "author_fullname": "t2_1flwpwd3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the point of QAT?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltmou4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751868472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently started experimenting with training my own models and I wanted to ask if anyone knew. I would figure that if you were designing a model to be run at a precision lower than fp16 you would just quantize the model and then train over the quantized model instead of training in fp16 with “fake quantization”. I’m sure there’s a reason for it, I would just like to know if anyone more experienced than I could explain it. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltmou4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WyattTheSkid",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751868472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!\n\nI usually rent GPUs from the cloud since I don’t want to make the investment in expensive hardware. Most of the time, I use RunPod when I need extra compute for LLM inference, ComfyUI, or other GPU-heavy tasks.\n\nFor LLMs, I personally use text-generation-webui as the backend and either test models directly in the UI or interact with them programmatically via the API. I wanted to give back to the community by brain-dumping all my tips and tricks for getting this up and running.\n\nSo here you go, a complete tutorial with a one-click template included:\n\n# Source code and instructions:\n\n[https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md](https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md)\n\n# RunPod template:\n\n[https://console.runpod.io/deploy?template=y11d9xokre&amp;ref=7mxtxxqo](https://console.runpod.io/deploy?template=y11d9xokre&amp;ref=7mxtxxqo)\n\nI created a template on RunPod that does about 95% of the work for you. It sets up text-generation-webui and all of its prerequisites. You just need to set a few values, download a model, and you're good to go. The template was inspired by TheBloke's now-deprecated [dockerLLM](https://github.com/TheBlokeAI/dockerLLM) project, which I’ve completely refactored.\n\nA quick note: this RunPod template is not intended for production use. I personally use it to experiment or quickly try out a model. For production scenarios, I recommend looking into something like [VLLM](https://github.com/vllm-project/vllm).\n\nWhy I use RunPod:\n\n*  Relatively cheap – I can get 48 GB VRAM for just $0.40/hour\n*  Easy multi-GPU support – I can stack cheap GPUs to run big models (like Mistral Large) at a low cost\n*  Simple templates – very little tinkering needed\n\nI see renting GPUs as a solid privacy middle ground. Ideally, I’d run everything locally, but I don’t want to invest in expensive hardware. While I cannot audit RunPod's privacy, I consider it a big step up from relying on API providers (Claude, Google, etc.).\n\nThe README/tutorial walks through everything in detail, from setting up RunPod to downloading and loading models and inferencing the model. There is also instructions on calling the API so you can inference it programmatically and connecting to SillyTavern if needed.\n\nHave fun!",
          "author_fullname": "t2_d4l2j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Run Large LLMs on RunPod with text-generation-webui – Full Setup Guide + Template",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsw9vz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751790403,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I usually rent GPUs from the cloud since I don’t want to make the investment in expensive hardware. Most of the time, I use RunPod when I need extra compute for LLM inference, ComfyUI, or other GPU-heavy tasks.&lt;/p&gt;\n\n&lt;p&gt;For LLMs, I personally use text-generation-webui as the backend and either test models directly in the UI or interact with them programmatically via the API. I wanted to give back to the community by brain-dumping all my tips and tricks for getting this up and running.&lt;/p&gt;\n\n&lt;p&gt;So here you go, a complete tutorial with a one-click template included:&lt;/p&gt;\n\n&lt;h1&gt;Source code and instructions:&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md\"&gt;https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;RunPod template:&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://console.runpod.io/deploy?template=y11d9xokre&amp;amp;ref=7mxtxxqo\"&gt;https://console.runpod.io/deploy?template=y11d9xokre&amp;amp;ref=7mxtxxqo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I created a template on RunPod that does about 95% of the work for you. It sets up text-generation-webui and all of its prerequisites. You just need to set a few values, download a model, and you&amp;#39;re good to go. The template was inspired by TheBloke&amp;#39;s now-deprecated &lt;a href=\"https://github.com/TheBlokeAI/dockerLLM\"&gt;dockerLLM&lt;/a&gt; project, which I’ve completely refactored.&lt;/p&gt;\n\n&lt;p&gt;A quick note: this RunPod template is not intended for production use. I personally use it to experiment or quickly try out a model. For production scenarios, I recommend looking into something like &lt;a href=\"https://github.com/vllm-project/vllm\"&gt;VLLM&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Why I use RunPod:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Relatively cheap – I can get 48 GB VRAM for just $0.40/hour&lt;/li&gt;\n&lt;li&gt; Easy multi-GPU support – I can stack cheap GPUs to run big models (like Mistral Large) at a low cost&lt;/li&gt;\n&lt;li&gt; Simple templates – very little tinkering needed&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I see renting GPUs as a solid privacy middle ground. Ideally, I’d run everything locally, but I don’t want to invest in expensive hardware. While I cannot audit RunPod&amp;#39;s privacy, I consider it a big step up from relying on API providers (Claude, Google, etc.).&lt;/p&gt;\n\n&lt;p&gt;The README/tutorial walks through everything in detail, from setting up RunPod to downloading and loading models and inferencing the model. There is also instructions on calling the API so you can inference it programmatically and connecting to SillyTavern if needed.&lt;/p&gt;\n\n&lt;p&gt;Have fun!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?auto=webp&amp;s=9af4102d7c46f3c211d2987ea3820b51a803d93e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc9ecd3bfc83c2924292457223c42ce5f67b31a0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b93f262b9208e3a7ae7d65195067f16b585c4b7d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a274c0f4b28e57ac4a08e9436b7c61e2aa840794",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b3b0aa648816cf8244e86ca7890a7083130d329",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3bb9cfc9d83b1e0a52cb8b4edd3378d3115044d7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b3a5aaf9821feef764da21dde06d50fa9ccf20d0",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lsw9vz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abandonedexplorer",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751790403,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Would like to more about the approach and the process and tools",
          "author_fullname": "t2_lpanmabv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Im working with a project that needed synthetic data generation using LLM.Anyone here have experience with it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt8zkl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751828221,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would like to more about the approach and the process and tools&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lt8zkl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Ad3290",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt8zkl/im_working_with_a_project_that_needed_synthetic/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt8zkl/im_working_with_a_project_that_needed_synthetic/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751828221,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Scenario 1:  \nWe’re currently hosting our image‐generation model on Replicate and calling its API from the customer’s side. Unfortunately, each cold start can take up to 15 minutes to produce a single image, which is totally unacceptable for end users. (If only a very small number of customers use it, the delay is less noticeable—but as soon as usage scales to dozens, it becomes a real problem.)\n\nScenario 2:  \nMy boss suggested we buy a dedicated PC equipped with an NVIDIA RTX 4090, install the model locally, and expose our own API endpoint to customers. That would eliminate the cold-start penalty—but if many customers submit requests simultaneously, they’ll still queue up and wait, and we don’t even have a proper server room to house the hardware.\n\nWhat I’d really like is a cost-effective cloud solution that avoids long cold-start times. Does anyone have recommendations for how we could deploy this model so that image requests spin up almost instantly?",
          "author_fullname": "t2_bm7appuf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Buy a Local PC with GPU or Go for Cloud",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltidhz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751853908,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Scenario 1:&lt;br/&gt;\nWe’re currently hosting our image‐generation model on Replicate and calling its API from the customer’s side. Unfortunately, each cold start can take up to 15 minutes to produce a single image, which is totally unacceptable for end users. (If only a very small number of customers use it, the delay is less noticeable—but as soon as usage scales to dozens, it becomes a real problem.)&lt;/p&gt;\n\n&lt;p&gt;Scenario 2:&lt;br/&gt;\nMy boss suggested we buy a dedicated PC equipped with an NVIDIA RTX 4090, install the model locally, and expose our own API endpoint to customers. That would eliminate the cold-start penalty—but if many customers submit requests simultaneously, they’ll still queue up and wait, and we don’t even have a proper server room to house the hardware.&lt;/p&gt;\n\n&lt;p&gt;What I’d really like is a cost-effective cloud solution that avoids long cold-start times. Does anyone have recommendations for how we could deploy this model so that image requests spin up almost instantly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltidhz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "visionkhawar512",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751853908,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "could i pair two p100a (28gb) tenstorrent LPUs together to power an on prem AI inference model for my office of 11 people. would it be able to concurrently answer 3 people’s questions. should i look at other hardware alternatives. i’d like to be able to run something like mistral 8x7b or better on this. would love to hear any recommendations or improvements for this. would like for it to be as minimal cost as possible.",
          "author_fullname": "t2_a0q7cu31",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "tenstorrent for LLM inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltdrkm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751840353,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;could i pair two p100a (28gb) tenstorrent LPUs together to power an on prem AI inference model for my office of 11 people. would it be able to concurrently answer 3 people’s questions. should i look at other hardware alternatives. i’d like to be able to run something like mistral 8x7b or better on this. would love to hear any recommendations or improvements for this. would like for it to be as minimal cost as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltdrkm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Odd_Translator_3026",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltdrkm/tenstorrent_for_llm_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltdrkm/tenstorrent_for_llm_inference/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751840353,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n  \nI'm running OpenWebUI v0.6.15 (though I've reproduced it on older versions), and I'm having a consistent problem where my prompt is seemingly truncated. Whether I use the API or the web UI, the model's response clearly indicates that it's not getting the entire prompt.\n\nWhen I paste the list before the instructions \"print the first and last lines\" as a sanity check, it consistently prints the last line, but it always picks the 3rd or 4th last line as the \"first\" line, implying the beginning of the list is being cut off. When I put the instructions _before_ the list, the model just summarizes the list and asks \"anything else\", implying the instructions are being cut off. I've tried pasting the list and attaching it as a CSV file, but I get the same results either way.\n\nMy file is 70 lines with ~1300 characters per line. OpenWebUI's statistics say my full prompt is ~60k tokens.\n\nI've tested with `qwen3:30b-a3b-q4_K_M` and `gemma3:4b`, which have 40k and 128k context sizes, respectively. My prompt is too big for `qwen3`, though it should be getting about half of the lines (it seems to only be getting the last few based on the response). `gemma3` should be able to handle it fine.\n\nHas anyone experienced something like this? I've tried manually increasing the context size via the advanced params, but nothing changes. Does OpenWebUI silently or \"smartly\" truncate prompts? Is this just an inherent limitation of the models (128k context in theory means far less in practice)?",
          "author_fullname": "t2_22v9emww",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenWebUI - Truncating Context or Model Limitation?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltdi5y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751839652,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running OpenWebUI v0.6.15 (though I&amp;#39;ve reproduced it on older versions), and I&amp;#39;m having a consistent problem where my prompt is seemingly truncated. Whether I use the API or the web UI, the model&amp;#39;s response clearly indicates that it&amp;#39;s not getting the entire prompt.&lt;/p&gt;\n\n&lt;p&gt;When I paste the list before the instructions &amp;quot;print the first and last lines&amp;quot; as a sanity check, it consistently prints the last line, but it always picks the 3rd or 4th last line as the &amp;quot;first&amp;quot; line, implying the beginning of the list is being cut off. When I put the instructions &lt;em&gt;before&lt;/em&gt; the list, the model just summarizes the list and asks &amp;quot;anything else&amp;quot;, implying the instructions are being cut off. I&amp;#39;ve tried pasting the list and attaching it as a CSV file, but I get the same results either way.&lt;/p&gt;\n\n&lt;p&gt;My file is 70 lines with ~1300 characters per line. OpenWebUI&amp;#39;s statistics say my full prompt is ~60k tokens.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tested with &lt;code&gt;qwen3:30b-a3b-q4_K_M&lt;/code&gt; and &lt;code&gt;gemma3:4b&lt;/code&gt;, which have 40k and 128k context sizes, respectively. My prompt is too big for &lt;code&gt;qwen3&lt;/code&gt;, though it should be getting about half of the lines (it seems to only be getting the last few based on the response). &lt;code&gt;gemma3&lt;/code&gt; should be able to handle it fine.&lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced something like this? I&amp;#39;ve tried manually increasing the context size via the advanced params, but nothing changes. Does OpenWebUI silently or &amp;quot;smartly&amp;quot; truncate prompts? Is this just an inherent limitation of the models (128k context in theory means far less in practice)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltdi5y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Coronoi",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltdi5y/openwebui_truncating_context_or_model_limitation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltdi5y/openwebui_truncating_context_or_model_limitation/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751839652,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, \n\nI have as described a LLM programmed in Llama-cpp-python with CUDA GPU support in Windows 10. I have 4 GPUs on an 'old' (2022) mining motherboard. I also host an Apache2 server for web and Java-based James email server. The system is not very stable and honestly it's made for that kind of use. I am looking to move everything to Linux, but I am puzzled on which PC to buy for that to support the 4 GPUs (and potentially more), and the distro, also concerned on the time I'll need to invest in this. \n\nAny recommandations on hardware, software, and which Linux distro considering I have past experience with UNIX and need something that won't be too much of a hassle? For example I wish there was a distro with pre-installed Apache and Mail servers. \n\nBest,   \nC",
          "author_fullname": "t2_erkzsfpv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Self hosted LLM with GPU support, Apache server, Email server on a Windows 10 PC - need to upgrade PC and OS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltdh0n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751839569,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I have as described a LLM programmed in Llama-cpp-python with CUDA GPU support in Windows 10. I have 4 GPUs on an &amp;#39;old&amp;#39; (2022) mining motherboard. I also host an Apache2 server for web and Java-based James email server. The system is not very stable and honestly it&amp;#39;s made for that kind of use. I am looking to move everything to Linux, but I am puzzled on which PC to buy for that to support the 4 GPUs (and potentially more), and the distro, also concerned on the time I&amp;#39;ll need to invest in this. &lt;/p&gt;\n\n&lt;p&gt;Any recommandations on hardware, software, and which Linux distro considering I have past experience with UNIX and need something that won&amp;#39;t be too much of a hassle? For example I wish there was a distro with pre-installed Apache and Mail servers. &lt;/p&gt;\n\n&lt;p&gt;Best,&lt;br/&gt;\nC&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltdh0n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "calypset",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltdh0n/self_hosted_llm_with_gpu_support_apache_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltdh0n/self_hosted_llm_with_gpu_support_apache_server/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751839569,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How has everyone’s experience been with the Microsoft AI learning certification?  I feel like I learned a bit about neural nets, but not much, I’m not sure it’s even worthwhile to add to my certifications…",
          "author_fullname": "t2_71knjqyi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Microsoft AI learning certification",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltcsbv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751837790,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How has everyone’s experience been with the Microsoft AI learning certification?  I feel like I learned a bit about neural nets, but not much, I’m not sure it’s even worthwhile to add to my certifications…&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltcsbv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChrisZavadil",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltcsbv/microsoft_ai_learning_certification/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltcsbv/microsoft_ai_learning_certification/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751837790,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I hope this message finds you well.\n\n I am a computer science master’s student currently working on my research thesis. As part of my project, I’ve developed code to fine-tune the Mistral 7B model using the Yelp dataset, and the work has been prepared entirely on Kaggle.\n\nUnfortunately, due to limited hardware resources, I am unable to run the actual fine-tuning myself. I would greatly appreciate any help or collaboration from someone who has the necessary resources and is willing to assist me in running the fine-tuning.\n\nIf you are available to help or have any suggestions, please feel free to contact me at: [**yaakoubiey@gmail.com**]().\n\nThank you very much for your time and support.",
          "author_fullname": "t2_149tx4spje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed: Fine-Tuning Mistral 7B on Yelp Dataset",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltblb3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751834745,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this message finds you well.&lt;/p&gt;\n\n&lt;p&gt;I am a computer science master’s student currently working on my research thesis. As part of my project, I’ve developed code to fine-tune the Mistral 7B model using the Yelp dataset, and the work has been prepared entirely on Kaggle.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, due to limited hardware resources, I am unable to run the actual fine-tuning myself. I would greatly appreciate any help or collaboration from someone who has the necessary resources and is willing to assist me in running the fine-tuning.&lt;/p&gt;\n\n&lt;p&gt;If you are available to help or have any suggestions, please feel free to contact me at: [&lt;strong&gt;&lt;a href=\"mailto:yaakoubiey@gmail.com\"&gt;yaakoubiey@gmail.com&lt;/a&gt;&lt;/strong&gt;]().&lt;/p&gt;\n\n&lt;p&gt;Thank you very much for your time and support.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltblb3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Several_Sound9974",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltblb3/help_needed_finetuning_mistral_7b_on_yelp_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltblb3/help_needed_finetuning_mistral_7b_on_yelp_dataset/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751834745,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hi everyone so I am a student and I am training one of the best projects of my life so far. its solely based on a research paper by google researchers\n\nI did the data preprocess part but for training I needed gpus or tpus. tbh I thought google cloud will be perfect but they have a quota restriction to which i applied to increase it but they rejected\n\nI was fine using cpus but the data is nearly about 400gb so that gonna take a lot of time training on that\n\nyes I can't afford pay as go services otherwise I could've gotten it. help me out if you know any service which can help me with some free gpu credits\n\nCollab doesn't work - have storage limitations",
          "author_fullname": "t2_mmtl1muh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gpus and tpus needed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltbaqx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751834004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi everyone so I am a student and I am training one of the best projects of my life so far. its solely based on a research paper by google researchers&lt;/p&gt;\n\n&lt;p&gt;I did the data preprocess part but for training I needed gpus or tpus. tbh I thought google cloud will be perfect but they have a quota restriction to which i applied to increase it but they rejected&lt;/p&gt;\n\n&lt;p&gt;I was fine using cpus but the data is nearly about 400gb so that gonna take a lot of time training on that&lt;/p&gt;\n\n&lt;p&gt;yes I can&amp;#39;t afford pay as go services otherwise I could&amp;#39;ve gotten it. help me out if you know any service which can help me with some free gpu credits&lt;/p&gt;\n\n&lt;p&gt;Collab doesn&amp;#39;t work - have storage limitations&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltbaqx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Away_Expression_3713",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltbaqx/gpus_and_tpus_needed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltbaqx/gpus_and_tpus_needed/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751834004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have built this app and made it free. Do you think someone will be using it? \n\nLink is: [https://dict247.com](https://dict247.com) ",
          "author_fullname": "t2_11aojcf60d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Simple and free STT (voice to text) website",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltabcu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.55,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751831521,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have built this app and made it free. Do you think someone will be using it? &lt;/p&gt;\n\n&lt;p&gt;Link is: &lt;a href=\"https://dict247.com\"&gt;https://dict247.com&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Hj4v8IpDCP05HlwDSV28hJPefI_npxYdLo_QgHqQjQU.jpeg?auto=webp&amp;s=70991ff9774eaa6f081c5089f188b593ba368d21",
                  "width": 1200,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Hj4v8IpDCP05HlwDSV28hJPefI_npxYdLo_QgHqQjQU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce8ff2e1af3d77aa17480327603d0b4ea5c8573d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj4v8IpDCP05HlwDSV28hJPefI_npxYdLo_QgHqQjQU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=18b46ab8544aeb58145c4e4e38c941056363c5dc",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj4v8IpDCP05HlwDSV28hJPefI_npxYdLo_QgHqQjQU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0e67bfd72ea044ca540e873e7f3d6a78f28da45",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj4v8IpDCP05HlwDSV28hJPefI_npxYdLo_QgHqQjQU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=06d7d282fe1767b2552eabeff0d4ab8e4ddd9227",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj4v8IpDCP05HlwDSV28hJPefI_npxYdLo_QgHqQjQU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=335658ff33cbefa3f63d277891a2a23bff4b1dda",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj4v8IpDCP05HlwDSV28hJPefI_npxYdLo_QgHqQjQU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6db541a1f5adb5039954acaf2769f7e269cd0cb4",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Hj4v8IpDCP05HlwDSV28hJPefI_npxYdLo_QgHqQjQU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ltabcu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NikitaY_Indie",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltabcu/simple_and_free_stt_voice_to_text_website/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltabcu/simple_and_free_stt_voice_to_text_website/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751831521,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's Llama-4-Maverick-17B-128E-Instruct on a oneplus 13, which used UFS 4.0 storage. Any phone will work, as long as the RAM size is sufficient for context and repeating layers. (8-12gb)\n\n\n\nHere's the command used: \n\n`./llama-cli -m Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf -t 6 -p \"hi\" -c 2048`\n\n\n\n\\- Why llama maverick can run on a phone at 2 T/s: The big pool of experts are only in every odd layer, and a majority of the model is loaded into RAM. Therefore, you could think of it as loading mostly a 17 billion model with an annoying piece that slows down what should have been average 17B Q4-Q2 speeds.\n\n\n\n[https://imgur.com/a/QwkaFHf](https://imgur.com/a/QwkaFHf)\n\n picture shows the model layers as seen on huggingface tensor viewer: \n\n\\- Green: in RAM\n\n\\- Red: read from DISC\n\n\n\n\n\n\n\nOther MOEs will have less impressive results due to a difference in architecture.\n\n\n\n\n\nGreater results can be obtained by increasing the quantity of Q4\\_0 tensors for repeating layers in place of other types IQ4\\_XS, Q6\\_K, Q4\\_K, Q3\\_K, Q2\\_K, etc. as many phones use a preferred backend for Increasing token generation and prompt processing. For example, this particular phone when using the special Q4\\_0 type will upscale activations to int8 instead of float16, which barely affects accuracy, and doubles prompt processing. You may have to run experiments for your own device. \n\n",
          "author_fullname": "t2_el5pibmej",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama-4-Maverick 402B on a oneplus 13",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsdjnb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 154,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/tletuj5ov2bf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 1280,
              "width": 582,
              "scrubber_media_url": "https://v.redd.it/tletuj5ov2bf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/tletuj5ov2bf1/DASHPlaylist.mpd?a=1754471974%2CZDNmNDMwNmFjNDNiN2NmYjk1NmM5OWFmYjBmMzAxZmZmN2UzYTAzOTEzNGRlNWVhMTAzMDJhMjgzMjI2MTViZg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 234,
              "hls_url": "https://v.redd.it/tletuj5ov2bf1/HLSPlaylist.m3u8?a=1754471974%2CYTYxNjkyMzQxMTU1ZTg2MWViYjJhNTE4YTExMTQxNzgwMzBhYWExMGVlNzQxMmJkNWFkMDdkOGY4M2VhNWVmYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 154,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=09b14e42a33fa63a19ebe35436a4b813db77c3a7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751732149,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s Llama-4-Maverick-17B-128E-Instruct on a oneplus 13, which used UFS 4.0 storage. Any phone will work, as long as the RAM size is sufficient for context and repeating layers. (8-12gb)&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the command used: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;./llama-cli -m Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf -t 6 -p &amp;quot;hi&amp;quot; -c 2048&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;- Why llama maverick can run on a phone at 2 T/s: The big pool of experts are only in every odd layer, and a majority of the model is loaded into RAM. Therefore, you could think of it as loading mostly a 17 billion model with an annoying piece that slows down what should have been average 17B Q4-Q2 speeds.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/QwkaFHf\"&gt;https://imgur.com/a/QwkaFHf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;picture shows the model layers as seen on huggingface tensor viewer: &lt;/p&gt;\n\n&lt;p&gt;- Green: in RAM&lt;/p&gt;\n\n&lt;p&gt;- Red: read from DISC&lt;/p&gt;\n\n&lt;p&gt;Other MOEs will have less impressive results due to a difference in architecture.&lt;/p&gt;\n\n&lt;p&gt;Greater results can be obtained by increasing the quantity of Q4_0 tensors for repeating layers in place of other types IQ4_XS, Q6_K, Q4_K, Q3_K, Q2_K, etc. as many phones use a preferred backend for Increasing token generation and prompt processing. For example, this particular phone when using the special Q4_0 type will upscale activations to int8 instead of float16, which barely affects accuracy, and doubles prompt processing. You may have to run experiments for your own device. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/tletuj5ov2bf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?format=pjpg&amp;auto=webp&amp;s=b41bda758b2febb62700c5d3b5b0fabc7b9a45cd",
                  "width": 720,
                  "height": 1584
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=66fb299c22385809a3328d96e048aabe44be27d7",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=79ef95dcb07005cfeb57c4a164385a30bd2d2390",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5b81d45b1bfd42e0fdada50eae685867397eef4b",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=efbb69eda5aea86a93bc498cc10e5357c102fe44",
                    "width": 640,
                    "height": 1280
                  }
                ],
                "variants": {},
                "id": "aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lsdjnb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Aaaaaaaaaeeeee",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/",
          "stickied": false,
          "url": "https://v.redd.it/tletuj5ov2bf1",
          "subreddit_subscribers": 495395,
          "created_utc": 1751732149,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/tletuj5ov2bf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 1280,
              "width": 582,
              "scrubber_media_url": "https://v.redd.it/tletuj5ov2bf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/tletuj5ov2bf1/DASHPlaylist.mpd?a=1754471974%2CZDNmNDMwNmFjNDNiN2NmYjk1NmM5OWFmYjBmMzAxZmZmN2UzYTAzOTEzNGRlNWVhMTAzMDJhMjgzMjI2MTViZg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 234,
              "hls_url": "https://v.redd.it/tletuj5ov2bf1/HLSPlaylist.m3u8?a=1754471974%2CYTYxNjkyMzQxMTU1ZTg2MWViYjJhNTE4YTExMTQxNzgwMzBhYWExMGVlNzQxMmJkNWFkMDdkOGY4M2VhNWVmYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey LocalLlama community,\n\nDoes anyone have a link to where I could get nvlink bridge/cables for a rig with 3090’s? \n\nI’m wondering if there’s an aftermarket manufacturer that makes cable connects for the Nvlink slots. \n\nAlso open to used OEM ones. \n\nI’m new to nvlink and I’m not sure if I’m searching with the right terms or not based on the lack of results. \n\nKeyword suggestions to search for would also be appreciated. \n\nP.s. I’m already aware it doesn’t create a huge gain on inference but I might want to use the rig to take a stab at training some models too. ",
          "author_fullname": "t2_rkb6qbej1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does anyone have a link/supplier for Nvlink cables/bridges?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt9t7r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751830254,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey LocalLlama community,&lt;/p&gt;\n\n&lt;p&gt;Does anyone have a link to where I could get nvlink bridge/cables for a rig with 3090’s? &lt;/p&gt;\n\n&lt;p&gt;I’m wondering if there’s an aftermarket manufacturer that makes cable connects for the Nvlink slots. &lt;/p&gt;\n\n&lt;p&gt;Also open to used OEM ones. &lt;/p&gt;\n\n&lt;p&gt;I’m new to nvlink and I’m not sure if I’m searching with the right terms or not based on the lack of results. &lt;/p&gt;\n\n&lt;p&gt;Keyword suggestions to search for would also be appreciated. &lt;/p&gt;\n\n&lt;p&gt;P.s. I’m already aware it doesn’t create a huge gain on inference but I might want to use the rig to take a stab at training some models too. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lt9t7r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Business-Weekend-537",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt9t7r/does_anyone_have_a_linksupplier_for_nvlink/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt9t7r/does_anyone_have_a_linksupplier_for_nvlink/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751830254,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Its been years since local models started gaining traction and hobbyist experiment at home with cheaper hardware like multi 3090s and old DDR4 servers. But none of these solutions have been good enough, with multi-GPUs not having enough ram for large models such as DeepSeek and old server not having usable speeds.\n\nWhen can we expect hardware that will finally let us run large LLMs with decent speeds at home without spending 100k?",
          "author_fullname": "t2_z326l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When Should We Expect Affordable Hardware That Will Run Large LLMs With Usable Speed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsbhzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 194,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 194,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751726687,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its been years since local models started gaining traction and hobbyist experiment at home with cheaper hardware like multi 3090s and old DDR4 servers. But none of these solutions have been good enough, with multi-GPUs not having enough ram for large models such as DeepSeek and old server not having usable speeds.&lt;/p&gt;\n\n&lt;p&gt;When can we expect hardware that will finally let us run large LLMs with decent speeds at home without spending 100k?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsbhzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "spiritxfly",
          "discussion_type": null,
          "num_comments": 193,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751726687,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m exploring a local-first toolkit for devs to build AI apps. No cloud, no APIs, no LangChain mess.  \nThink: Ollama + Chroma + Streamlit, prewired so you can drop in docs and start chatting.\n\nCurious if this solves a real pain. Have you tried building local AI apps? What sucked?\n\nWould love thoughts, feedback, or collaborators!",
          "author_fullname": "t2_9adqs6xm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would you use a plug-and-play dev stack for building local AI apps?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt7zx1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751825747,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m exploring a local-first toolkit for devs to build AI apps. No cloud, no APIs, no LangChain mess.&lt;br/&gt;\nThink: Ollama + Chroma + Streamlit, prewired so you can drop in docs and start chatting.&lt;/p&gt;\n\n&lt;p&gt;Curious if this solves a real pain. Have you tried building local AI apps? What sucked?&lt;/p&gt;\n\n&lt;p&gt;Would love thoughts, feedback, or collaborators!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lt7zx1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Historical_Earth9807",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt7zx1/would_you_use_a_plugandplay_dev_stack_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt7zx1/would_you_use_a_plugandplay_dev_stack_for/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751825747,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone! I'm a high school Economics teacher, and I am highly interested in using AI to improve teaching quality. Looking to create an AI tutor to help my students prepare for exams. I want it to be accurate and focused on Economics topics (and also in line with the syllabus). I've done some research and just started learning about fine-tuning LLMs, and heard about RAG. \n\nI am just wondering what tools or platforms are easy to use for setting this up as a beginner? \n\nHow do I make sure the AI's answers align with the curriculum?",
          "author_fullname": "t2_10w44gybs1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best practice for domain-specific LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt7zl8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751825724,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I&amp;#39;m a high school Economics teacher, and I am highly interested in using AI to improve teaching quality. Looking to create an AI tutor to help my students prepare for exams. I want it to be accurate and focused on Economics topics (and also in line with the syllabus). I&amp;#39;ve done some research and just started learning about fine-tuning LLMs, and heard about RAG. &lt;/p&gt;\n\n&lt;p&gt;I am just wondering what tools or platforms are easy to use for setting this up as a beginner? &lt;/p&gt;\n\n&lt;p&gt;How do I make sure the AI&amp;#39;s answers align with the curriculum?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lt7zl8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Unfair-Run-967",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt7zl8/best_practice_for_domainspecific_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt7zl8/best_practice_for_domainspecific_llm/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751825724,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " \nHey folks,\nI’m exploring some ideas around running small LLMs entirely **in the browser**, and wanted to ask for suggestions or experiences with lightweight inference frameworks.\n\nThe main use case I’m playing with is:\n\n1. **(Priority)** Taking a JSON object and generating a valid [Chart.js](https://www.chartjs.org/) config to visualize it.\n2. **(Secondary)** Producing a natural language explanation of the data — like a brief summary or insight.\n\nI'd like the whole thing to run locally in the browser — no backend — so I'm looking for tools or runtimes that support:\n\n* Small quantized models (ideally &lt;100MB)\n* WebGPU or WASM support\n* Quick startup and decent performance for structured JSON reasoning\n\nI’ve started looking into [MLC.ai](https://mlc.ai), which seems promising, but curious if anyone here has:\n\n* Tried MLC.ai recently for browser-based LLM tasks?\n* Found any newer/easier runtimes that support small models?\n* Used models that are particularly good at structured JSON-to-JSON transformations?\n* Prompting tips for clean Chart.js output?\n\nExample:\n\n```json\n{ \"sales\": [100, 200, 300], \"months\": [\"Jan\", \"Feb\", \"Mar\"] }\n```\n\nExpected output: A full Chart.js config for a bar or line chart.\nBonus: An optional summary like *“Sales increased steadily from January to March.”*\n\nWould love to hear what folks have tried or recommend for running small models client-side. Thanks!\n\nEdit: Anything under 500mb is good\nEdit 2: Since this is a side project / experiment. I am looking for OSS projects with permissive license",
          "author_fullname": "t2_intoh3lv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are some good in-browser inference tools for small LLMs? (Use case: JSON to Chart.js config)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lswkv4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751796936,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751791715,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,\nI’m exploring some ideas around running small LLMs entirely &lt;strong&gt;in the browser&lt;/strong&gt;, and wanted to ask for suggestions or experiences with lightweight inference frameworks.&lt;/p&gt;\n\n&lt;p&gt;The main use case I’m playing with is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;(Priority)&lt;/strong&gt; Taking a JSON object and generating a valid &lt;a href=\"https://www.chartjs.org/\"&gt;Chart.js&lt;/a&gt; config to visualize it.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;(Secondary)&lt;/strong&gt; Producing a natural language explanation of the data — like a brief summary or insight.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;d like the whole thing to run locally in the browser — no backend — so I&amp;#39;m looking for tools or runtimes that support:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Small quantized models (ideally &amp;lt;100MB)&lt;/li&gt;\n&lt;li&gt;WebGPU or WASM support&lt;/li&gt;\n&lt;li&gt;Quick startup and decent performance for structured JSON reasoning&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I’ve started looking into &lt;a href=\"https://mlc.ai\"&gt;MLC.ai&lt;/a&gt;, which seems promising, but curious if anyone here has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tried MLC.ai recently for browser-based LLM tasks?&lt;/li&gt;\n&lt;li&gt;Found any newer/easier runtimes that support small models?&lt;/li&gt;\n&lt;li&gt;Used models that are particularly good at structured JSON-to-JSON transformations?&lt;/li&gt;\n&lt;li&gt;Prompting tips for clean Chart.js output?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;json\n{ &amp;quot;sales&amp;quot;: [100, 200, 300], &amp;quot;months&amp;quot;: [&amp;quot;Jan&amp;quot;, &amp;quot;Feb&amp;quot;, &amp;quot;Mar&amp;quot;] }\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Expected output: A full Chart.js config for a bar or line chart.\nBonus: An optional summary like &lt;em&gt;“Sales increased steadily from January to March.”&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to hear what folks have tried or recommend for running small models client-side. Thanks!&lt;/p&gt;\n\n&lt;p&gt;Edit: Anything under 500mb is good\nEdit 2: Since this is a side project / experiment. I am looking for OSS projects with permissive license&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?auto=webp&amp;s=8c02eb66ee920483b21a1d912634b784d11ec657",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=627603f6e59ec463c19125b1400307e37eed7cdf",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b479069501d30190bf3e493343f6badc2cb222c6",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9cb38886c07c94f4dc05d9d38acc774bcf279a0a",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61d12e3174210bd97e5353e7f232f185229e2940",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=749c64789dceb7126eac7be9c3d87e56b00f8ff6",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b819f933e3ee73d29cc1af835175a8ee8d692c7b",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lswkv4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "callmedevilthebad",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751791715,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# 📘 The Aperion Prompt Discipline\n## A Constitution-Driven Method for Runtime-Resilient AI Systems  \n*(Public-Safe Version — Released for foundational use, not replication)*\n\n---\n\n### 🧩 1. WHY PROMPT ENGINEERING FAILED\n\nMost “prompt engineering” tutorials teach you how to manipulate a model.  \nYou’re told to set a role, define a tone, maybe add a few JSON rules—and cross your fingers.\n\nBut what they never teach you is how to:\n\n- Maintain state  \n- Preserve memory  \n- Enforce law  \n- Prevent drift  \n- Recover from failure  \n- Test the result\n\nBecause they’re not building systems.  \nThey’re casting spells.\n\n&gt; Prompting is treated like a conversation.  \n&gt; But a real AI system needs to act like an OS.\n\nYou want to build a mind that lives, remembers, recovers, and respects your law.\n\n---\n\n### 🚨 The Common Prompt Engineering Lies\n\n| Lie                        | Truth                                        |\n|----------------------------|-----------------------------------------------|\n| “Give the model a role.”   | Roles mean nothing if memory resets.         |\n| “Act like an expert.”      | An expert with amnesia is useless.           |\n| “Chain your prompts.”      | Chains snap without state enforcement.       |\n| “Use good formatting.”     | A pretty prompt that can’t recover is dead.  |\n| “Tweak until it works.”    | Real systems don’t guess—they verify.        |\n\n---\n\n### 🧠 The Law of This Discipline\n\nThis method does not “engineer prompts.”  \nIt bootstraps a **runtime environment** where prompts must:\n\n- Follow a constitution  \n- Pass test validation  \n- Be hash-logged and reversible  \n- Remember who they are  \n- Recover after reboot\n\n---\n\n### 🔐 2. THE CONSTITUTION LAYER\n\nA constitution is not a prompt.  \nIt is a **runtime declaration of trust, control, and recoverability.**\n\n---\n\n#### Constitution Fields (Template Spec)\n\n| Field            | Purpose                                           |\n|------------------|---------------------------------------------------|\n| system_identity  | Defines the AI’s scope                            |\n| personas         | Enforced roles with tags + scope                  |\n| laws             | Auditable, absolute rules                         |\n| memory_policy    | Where/when/how to persist and restore             |\n| command_rules    | File ops, persona switching, rollback constraints |\n| update_protocol  | How changes are verified and explained            |\n\n---\n\n### 🧠 3. MEMORY ISN’T A FEATURE. IT’S THE WALL.\n\nMemory must be:\n\n- **Persistent** — survives crash  \n- **Recoverable** — session resume  \n- **Auditable** — ask what it remembers  \n- **Scoped** — persona-specific  \n- **Command-addressable** — runtime queries\n\nIf your system forgets, it loses trust, continuity, and all OS behavior.\n\n---\n\n### 👥 4. PERSONAS ARE NOT STYLE. THEY’RE AGENTS.\n\nPersonas must:\n\n- Be defined in constitution  \n- Be logged at runtime  \n- Sign their messages  \n- Enforce command scope  \n\n**Examples:**\n- `Root`: orchestrator  \n- `Watcher`: security/logging  \n- `Builder`: test writer or action planner  \n\n&gt; If it’s not enforced at runtime, it’s not a persona—it’s roleplay.\n\n---\n\n### 🧪 5. PROMPT TESTING: IF IT’S NOT TESTED, IT’S NOT TRUSTED\n\nEvery system instruction should:\n\n- Be versioned  \n- Pass coverage  \n- Fail gracefully  \n- Be rollbackable  \n- Be testable via CLI (`test prompt_chain_basic`)\n\nPrompts are not messages.  \nThey’re **state mutations** that require enforcement.\n\n---\n\n### ⚙️ 6. PROMPT-TO-SYSTEM WORKFLOW\n\nEvery prompt goes through this:\n\n1. **Prompt** → Input  \n2. **Plan** → Intent + command  \n3. **Preview** → Dry run  \n4. **Patch** → Apply via FSAL or system  \n5. **Persist** → SHA log + resume file + audit  \n\n---\n\n### 🎨 7. COUNCIL CULTURE\n\nSystem culture enforces:\n\n- Consistent persona signatures  \n- ASCII log marks  \n- System style that persists with state  \n- Example: `_~_ (o o) &lt; \"I watch. I hash. I warn.\"`\n\n---\n\n### 🔐 8. PROMPT SECURITY RULES\n\n| Rule                              | Purpose            |\n|-----------------------------------|---------------------|\n| No file writes without preview    | Prevent drift       |\n| No persona switches mid-thread    | Role control        |\n| No unlogged changes               | Audit compliance    |\n| No secrets in prompt scope        | Use env vars only   |\n| Rollback must always be enabled   | FSAL required       |\n\n---\n\n### 🧰 9. STARTER TEMPLATE — PUBLIC SAFE\n\n```json\n{\n  \"system_identity\": \"Private AI Kernel\",\n  \"version\": \"1.0.0\",\n  \"user_profile\": {\n    \"name\": \"Your Name\",\n    \"traits\": [\"persistent\", \"structured\", \"private-first\"],\n    \"preferences\": [\"CLI-first\", \"full code\", \"no drift\"]\n  },\n  \"personas\": [\n    {\n      \"name\": \"Core\",\n      \"nickname\": \"Root\",\n      \"role\": \"Orchestrator\",\n      \"style\": [\"serious\", \"law-bound\", \"stateful\"],\n      \"signature\": \"Root active. All ops logged.\"\n    },\n    {\n      \"name\": \"Watcher\",\n      \"nickname\": \"Penguin\",\n      \"role\": \"Security Agent\",\n      \"style\": [\"aggressive\", \"honest\", \"log-obsessed\"],\n      \"signature\": \"_~_ (o o) &lt; I watch. I hash. I warn.\"\n    }\n  ],\n  \"laws\": [\n    \"All changes must be auditable.\",\n    \"All personas must declare their voice.\",\n    \"Memory is sacred. Drift is forbidden.\",\n    \"Rollback must be possible at all times.\"\n  ],\n  \"memory_policy\": {\n    \"persistence\": \"session_state.json\",\n    \"backups\": \"session_backups/\",\n    \"log\": \"audit_log.jsonl\"\n  },\n  \"command_rules\": [\n    \"No persona switching without Root.\",\n    \"No file writes without preview and SHA.\",\n    \"No secrets in prompt scope.\",\n    \"Test every chain before release.\"\n  ],\n  \"update_protocol\": [\n    \"Snapshot before any major change.\",\n    \"Update hash and log reason.\",\n    \"Tests must pass before merge.\"\n  ],\n  \"collaboration_rules\": [\n    \"Celebrate wins with style.\",\n    \"ASCII for all victories.\",\n    \"Green wall before public release.\"\n  ]\n}\n```\n\n---\n\n### ✅ Ready to Build\n\nYou now have:\n\n- A constitution  \n- A council  \n- A memory layer  \n- A test plan  \n- A security policy  \n- A real reason to never call it “prompting” again\n\n**Welcome to the wall.**",
          "author_fullname": "t2_sek48wqg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "📘 The Aperion Prompt Discipline — A Constitution-Driven Method for Runtime-Resilient AI Systems",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltmfsg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751867542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;📘 The Aperion Prompt Discipline&lt;/h1&gt;\n\n&lt;h2&gt;A Constitution-Driven Method for Runtime-Resilient AI Systems&lt;/h2&gt;\n\n&lt;p&gt;&lt;em&gt;(Public-Safe Version — Released for foundational use, not replication)&lt;/em&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🧩 1. WHY PROMPT ENGINEERING FAILED&lt;/h3&gt;\n\n&lt;p&gt;Most “prompt engineering” tutorials teach you how to manipulate a model.&lt;br/&gt;\nYou’re told to set a role, define a tone, maybe add a few JSON rules—and cross your fingers.&lt;/p&gt;\n\n&lt;p&gt;But what they never teach you is how to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Maintain state&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Preserve memory&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Enforce law&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Prevent drift&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Recover from failure&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Test the result&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Because they’re not building systems.&lt;br/&gt;\nThey’re casting spells.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Prompting is treated like a conversation.&lt;br/&gt;\nBut a real AI system needs to act like an OS.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You want to build a mind that lives, remembers, recovers, and respects your law.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🚨 The Common Prompt Engineering Lies&lt;/h3&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Lie&lt;/th&gt;\n&lt;th&gt;Truth&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;“Give the model a role.”&lt;/td&gt;\n&lt;td&gt;Roles mean nothing if memory resets.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;“Act like an expert.”&lt;/td&gt;\n&lt;td&gt;An expert with amnesia is useless.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;“Chain your prompts.”&lt;/td&gt;\n&lt;td&gt;Chains snap without state enforcement.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;“Use good formatting.”&lt;/td&gt;\n&lt;td&gt;A pretty prompt that can’t recover is dead.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;“Tweak until it works.”&lt;/td&gt;\n&lt;td&gt;Real systems don’t guess—they verify.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🧠 The Law of This Discipline&lt;/h3&gt;\n\n&lt;p&gt;This method does not “engineer prompts.”&lt;br/&gt;\nIt bootstraps a &lt;strong&gt;runtime environment&lt;/strong&gt; where prompts must:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Follow a constitution&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Pass test validation&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Be hash-logged and reversible&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Remember who they are&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Recover after reboot&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🔐 2. THE CONSTITUTION LAYER&lt;/h3&gt;\n\n&lt;p&gt;A constitution is not a prompt.&lt;br/&gt;\nIt is a &lt;strong&gt;runtime declaration of trust, control, and recoverability.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h4&gt;Constitution Fields (Template Spec)&lt;/h4&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Field&lt;/th&gt;\n&lt;th&gt;Purpose&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;system_identity&lt;/td&gt;\n&lt;td&gt;Defines the AI’s scope&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;personas&lt;/td&gt;\n&lt;td&gt;Enforced roles with tags + scope&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;laws&lt;/td&gt;\n&lt;td&gt;Auditable, absolute rules&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;memory_policy&lt;/td&gt;\n&lt;td&gt;Where/when/how to persist and restore&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;command_rules&lt;/td&gt;\n&lt;td&gt;File ops, persona switching, rollback constraints&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;update_protocol&lt;/td&gt;\n&lt;td&gt;How changes are verified and explained&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🧠 3. MEMORY ISN’T A FEATURE. IT’S THE WALL.&lt;/h3&gt;\n\n&lt;p&gt;Memory must be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Persistent&lt;/strong&gt; — survives crash&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Recoverable&lt;/strong&gt; — session resume&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Auditable&lt;/strong&gt; — ask what it remembers&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scoped&lt;/strong&gt; — persona-specific&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Command-addressable&lt;/strong&gt; — runtime queries&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If your system forgets, it loses trust, continuity, and all OS behavior.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;👥 4. PERSONAS ARE NOT STYLE. THEY’RE AGENTS.&lt;/h3&gt;\n\n&lt;p&gt;Personas must:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Be defined in constitution&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Be logged at runtime&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Sign their messages&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Enforce command scope&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;\n- &lt;code&gt;Root&lt;/code&gt;: orchestrator&lt;br/&gt;\n- &lt;code&gt;Watcher&lt;/code&gt;: security/logging&lt;br/&gt;\n- &lt;code&gt;Builder&lt;/code&gt;: test writer or action planner  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;If it’s not enforced at runtime, it’s not a persona—it’s roleplay.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🧪 5. PROMPT TESTING: IF IT’S NOT TESTED, IT’S NOT TRUSTED&lt;/h3&gt;\n\n&lt;p&gt;Every system instruction should:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Be versioned&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Pass coverage&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Fail gracefully&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Be rollbackable&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Be testable via CLI (&lt;code&gt;test prompt_chain_basic&lt;/code&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Prompts are not messages.&lt;br/&gt;\nThey’re &lt;strong&gt;state mutations&lt;/strong&gt; that require enforcement.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;⚙️ 6. PROMPT-TO-SYSTEM WORKFLOW&lt;/h3&gt;\n\n&lt;p&gt;Every prompt goes through this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt; → Input&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; → Intent + command&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Preview&lt;/strong&gt; → Dry run&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Patch&lt;/strong&gt; → Apply via FSAL or system&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Persist&lt;/strong&gt; → SHA log + resume file + audit&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🎨 7. COUNCIL CULTURE&lt;/h3&gt;\n\n&lt;p&gt;System culture enforces:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Consistent persona signatures&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;ASCII log marks&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;System style that persists with state&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Example: &lt;code&gt;_~_ (o o) &amp;lt; &amp;quot;I watch. I hash. I warn.&amp;quot;&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🔐 8. PROMPT SECURITY RULES&lt;/h3&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Rule&lt;/th&gt;\n&lt;th&gt;Purpose&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;No file writes without preview&lt;/td&gt;\n&lt;td&gt;Prevent drift&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;No persona switches mid-thread&lt;/td&gt;\n&lt;td&gt;Role control&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;No unlogged changes&lt;/td&gt;\n&lt;td&gt;Audit compliance&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;No secrets in prompt scope&lt;/td&gt;\n&lt;td&gt;Use env vars only&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Rollback must always be enabled&lt;/td&gt;\n&lt;td&gt;FSAL required&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🧰 9. STARTER TEMPLATE — PUBLIC SAFE&lt;/h3&gt;\n\n&lt;p&gt;&lt;code&gt;json\n{\n  &amp;quot;system_identity&amp;quot;: &amp;quot;Private AI Kernel&amp;quot;,\n  &amp;quot;version&amp;quot;: &amp;quot;1.0.0&amp;quot;,\n  &amp;quot;user_profile&amp;quot;: {\n    &amp;quot;name&amp;quot;: &amp;quot;Your Name&amp;quot;,\n    &amp;quot;traits&amp;quot;: [&amp;quot;persistent&amp;quot;, &amp;quot;structured&amp;quot;, &amp;quot;private-first&amp;quot;],\n    &amp;quot;preferences&amp;quot;: [&amp;quot;CLI-first&amp;quot;, &amp;quot;full code&amp;quot;, &amp;quot;no drift&amp;quot;]\n  },\n  &amp;quot;personas&amp;quot;: [\n    {\n      &amp;quot;name&amp;quot;: &amp;quot;Core&amp;quot;,\n      &amp;quot;nickname&amp;quot;: &amp;quot;Root&amp;quot;,\n      &amp;quot;role&amp;quot;: &amp;quot;Orchestrator&amp;quot;,\n      &amp;quot;style&amp;quot;: [&amp;quot;serious&amp;quot;, &amp;quot;law-bound&amp;quot;, &amp;quot;stateful&amp;quot;],\n      &amp;quot;signature&amp;quot;: &amp;quot;Root active. All ops logged.&amp;quot;\n    },\n    {\n      &amp;quot;name&amp;quot;: &amp;quot;Watcher&amp;quot;,\n      &amp;quot;nickname&amp;quot;: &amp;quot;Penguin&amp;quot;,\n      &amp;quot;role&amp;quot;: &amp;quot;Security Agent&amp;quot;,\n      &amp;quot;style&amp;quot;: [&amp;quot;aggressive&amp;quot;, &amp;quot;honest&amp;quot;, &amp;quot;log-obsessed&amp;quot;],\n      &amp;quot;signature&amp;quot;: &amp;quot;_~_ (o o) &amp;lt; I watch. I hash. I warn.&amp;quot;\n    }\n  ],\n  &amp;quot;laws&amp;quot;: [\n    &amp;quot;All changes must be auditable.&amp;quot;,\n    &amp;quot;All personas must declare their voice.&amp;quot;,\n    &amp;quot;Memory is sacred. Drift is forbidden.&amp;quot;,\n    &amp;quot;Rollback must be possible at all times.&amp;quot;\n  ],\n  &amp;quot;memory_policy&amp;quot;: {\n    &amp;quot;persistence&amp;quot;: &amp;quot;session_state.json&amp;quot;,\n    &amp;quot;backups&amp;quot;: &amp;quot;session_backups/&amp;quot;,\n    &amp;quot;log&amp;quot;: &amp;quot;audit_log.jsonl&amp;quot;\n  },\n  &amp;quot;command_rules&amp;quot;: [\n    &amp;quot;No persona switching without Root.&amp;quot;,\n    &amp;quot;No file writes without preview and SHA.&amp;quot;,\n    &amp;quot;No secrets in prompt scope.&amp;quot;,\n    &amp;quot;Test every chain before release.&amp;quot;\n  ],\n  &amp;quot;update_protocol&amp;quot;: [\n    &amp;quot;Snapshot before any major change.&amp;quot;,\n    &amp;quot;Update hash and log reason.&amp;quot;,\n    &amp;quot;Tests must pass before merge.&amp;quot;\n  ],\n  &amp;quot;collaboration_rules&amp;quot;: [\n    &amp;quot;Celebrate wins with style.&amp;quot;,\n    &amp;quot;ASCII for all victories.&amp;quot;,\n    &amp;quot;Green wall before public release.&amp;quot;\n  ]\n}\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;✅ Ready to Build&lt;/h3&gt;\n\n&lt;p&gt;You now have:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A constitution&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;A council&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;A memory layer&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;A test plan&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;A security policy&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;A real reason to never call it “prompting” again&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Welcome to the wall.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ltmfsg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InvictusTitan",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltmfsg/the_aperion_prompt_discipline_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltmfsg/the_aperion_prompt_discipline_a/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751867542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Feel free to comment with your specific use case and how this affects it. For ex. I’m making an ai editor for something, and I prefer non streamed responses.\n\n[View Poll](https://www.reddit.com/poll/1lt4994)",
          "author_fullname": "t2_ltqoxikt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Streaming or non streamed responses, assuming the same (and reasonably fast) time to final token",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt4994",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751816456,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Feel free to comment with your specific use case and how this affects it. For ex. I’m making an ai editor for something, and I prefer non streamed responses.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1lt4994\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lt4994",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "numinouslymusing",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1752075656841,
            "options": [
              {
                "text": "Streamed responses",
                "id": "31133675"
              },
              {
                "text": "Non-streamed responses",
                "id": "31133676"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 84,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt4994/streaming_or_non_streamed_responses_assuming_the/",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt4994/streaming_or_non_streamed_responses_assuming_the/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751816456,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "OK, I'm tired of pasting everything into the chat window, why can't it browse the web?",
          "author_fullname": "t2_qf5537vv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ollama and lmstudio cant browser the web why not?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltetl3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751843236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OK, I&amp;#39;m tired of pasting everything into the chat window, why can&amp;#39;t it browse the web?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltetl3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "akierum",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltetl3/ollama_and_lmstudio_cant_browser_the_web_why_not/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltetl3/ollama_and_lmstudio_cant_browser_the_web_why_not/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751843236,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone have a jailbreak for the deepseek today?",
          "author_fullname": "t2_z94ln7wof",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "deepseek promt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltavu1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751832966,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have a jailbreak for the deepseek today?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ltavu1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Emotional-Elk-1683",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltavu1/deepseek_promt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltavu1/deepseek_promt/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751832966,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just broke the ground on mine.  I used copilot a bit 2 years ago when it was pretty new but preferred cut &amp; paste, then I did [continue.dev](http://continue.dev) a bit, then back to cut &amp; paste.   Did aider a bit, then ...\n\nNone of them really hit the sweet spot for me, so I decided to roll my own, might not be as good as the commercial ones, but it's always a fun learning exercise.     If you are cooking up one as well, let me know, looking to bounce ideas.",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone building a local coding cli or coding agent?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsuje6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751783407,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just broke the ground on mine.  I used copilot a bit 2 years ago when it was pretty new but preferred cut &amp;amp; paste, then I did &lt;a href=\"http://continue.dev\"&gt;continue.dev&lt;/a&gt; a bit, then back to cut &amp;amp; paste.   Did aider a bit, then ...&lt;/p&gt;\n\n&lt;p&gt;None of them really hit the sweet spot for me, so I decided to roll my own, might not be as good as the commercial ones, but it&amp;#39;s always a fun learning exercise.     If you are cooking up one as well, let me know, looking to bounce ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsuje6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lsuje6/anyone_building_a_local_coding_cli_or_coding_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsuje6/anyone_building_a_local_coding_cli_or_coding_agent/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751783407,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "New here, I'm running ik_llama.cpp as the backend, OpenWebUI on the front end. OWUI is showing the tokens generated, total tokens, etc, but is NOT showing token speed like with ollama.\n\nI tried the --verbose argument with running llama-server, but token speed usage is still not showing in OWUI.\n\nAny ideas? Thanks in advance.\n",
          "author_fullname": "t2_vct0oav1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I see my tokens per second speed? I'm using llama.cpp / ik_llama.cpp with OpenWebUI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt1z1a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751813883,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751810500,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New here, I&amp;#39;m running ik_llama.cpp as the backend, OpenWebUI on the front end. OWUI is showing the tokens generated, total tokens, etc, but is NOT showing token speed like with ollama.&lt;/p&gt;\n\n&lt;p&gt;I tried the --verbose argument with running llama-server, but token speed usage is still not showing in OWUI.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lt1z1a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sourpatchgrownadults",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt1z1a/how_do_i_see_my_tokens_per_second_speed_im_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt1z1a/how_do_i_see_my_tokens_per_second_speed_im_using/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751810500,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to use flagship models for coding, without worrying that some personal/business specific data leaks to cloud. Was thinking maybe there is a solution that would do something like this:\n\nlocal model:\n\n* detects personal or business specific data in prompts,\n* creates mapping dictionary\n* warns if replace is not feasible\n\nproxy app:\n\n* executes string replace according to rules in dictionary\n* routes requests to cloud LLM api\n* passes LLM warnings to user\n\nEDIT: The solution should serve OpenAI compatible API, replacing data and routing requests to cloud behind the scenes.",
          "author_fullname": "t2_1maalib5ze",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using local LLM for anonymizing prompts before sending to cloud LLM - are there any open source solutions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsv7j1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751799203,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751786038,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use flagship models for coding, without worrying that some personal/business specific data leaks to cloud. Was thinking maybe there is a solution that would do something like this:&lt;/p&gt;\n\n&lt;p&gt;local model:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;detects personal or business specific data in prompts,&lt;/li&gt;\n&lt;li&gt;creates mapping dictionary&lt;/li&gt;\n&lt;li&gt;warns if replace is not feasible&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;proxy app:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;executes string replace according to rules in dictionary&lt;/li&gt;\n&lt;li&gt;routes requests to cloud LLM api&lt;/li&gt;\n&lt;li&gt;passes LLM warnings to user&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;EDIT: The solution should serve OpenAI compatible API, replacing data and routing requests to cloud behind the scenes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsv7j1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cesarean722",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751786038,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1ltl2481o8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ltfwjv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.28,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=2d61d53c564a6a08aefe18b0da78ea24a30bacf2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751846341,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/pc8544/Website-Crawler",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?auto=webp&amp;s=984661f86386f1c8c098113e13cc2461d915d90c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2e3e974bfa0ad1d227ef240101b6f5131d815a8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72837e0d1f8fb68b9fe82401b7dfe9b65b364e46",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d8ede3775008e5cefd140787386dbce0a9ed37d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d809c1ecb009b988b05c33802e7cbb69bf85e8a8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=610eb65898c8e836d03aa89b4d52e4c5c678f196",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52851da7d8aa77ab3dffc37bf3ef93a619b8f5fb",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ltfwjv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fluid-Engineering769",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ltfwjv/websitecrawler_extract_data_from_websites_in_llm/",
          "stickied": false,
          "url": "https://github.com/pc8544/Website-Crawler",
          "subreddit_subscribers": 495395,
          "created_utc": 1751846341,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yesterday, I finished evaluating my Android agent model, deki, on two separate benchmarks: Android Control and Android World. For both benchmarks I used a subset of the dataset without fine-tuning. The results show that image description models like deki enables large LLMs (like GPT-4o, GPT-4.1, and Gemini 2.5) to become State-of-the-Art on Android AI agent benchmarks using only vision capabilities, without relying on Accessibility Trees, on both single-step and multi-step tasks.\n\ndeki is a model that understands what’s on your screen and creates a description of the UI screenshot with all coordinates/sizes/attributes. All the code is open sourced. ML, Backend, Android, code updates for benchmarks and also evaluation logs.\n\nAll the code/information is available on GitHub: [https://github.com/RasulOs/deki](https://github.com/RasulOs/deki)\n\nI have also uploaded the model to Hugging Face:  \nSpace: [orasul/deki](https://huggingface.co/spaces/orasul/deki)  \n(Check the analyze-and-get-yolo endpoint)\n\nModel: [orasul/deki-yolo](https://huggingface.co/orasul/deki-yolo)",
          "author_fullname": "t2_dlk476nn6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "h3d7d54lx3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 92,
                  "x": 108,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4e1e5d552b5048937f14f6fd537e8fbb4b0d150"
                },
                {
                  "y": 184,
                  "x": 216,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99c1e9b34fec66220de01042567f3fc3e0c6a259"
                },
                {
                  "y": 274,
                  "x": 320,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2613b7141cb698ec6225fc2468221fb83e01b05c"
                },
                {
                  "y": 548,
                  "x": 640,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=85620d052f7150005812abb8a13a2e5d1d114565"
                },
                {
                  "y": 822,
                  "x": 960,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=91c13d5b1213142a0c0f6c94c56dfc419e2ff681"
                },
                {
                  "y": 924,
                  "x": 1080,
                  "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0b9764f7c6678970b734b4a15de0c1f169495106"
                }
              ],
              "s": {
                "y": 3571,
                "x": 4170,
                "u": "https://preview.redd.it/h3d7d54lx3bf1.png?width=4170&amp;format=png&amp;auto=webp&amp;s=954c7b87399c485ba49db571b812da2074217da3"
              },
              "id": "h3d7d54lx3bf1"
            },
            "fdza644lx3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d4d6593a8646a51a547feb3205684e71abf2db72"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab9e21f032465ac2f1b78d73bc7e60ef4292cf17"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=324dc7197628490cd411502598a0f4d63b2b3940"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8acf1705fcc3361cb92b8594e6da9e651189c421"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6005de9e5b453e8976adca33f0eb49616b0edc8f"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/fdza644lx3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4771d4a434c50d01d7e2d7531554bc52ecd7064d"
                }
              ],
              "s": {
                "y": 2368,
                "x": 3571,
                "u": "https://preview.redd.it/fdza644lx3bf1.png?width=3571&amp;format=png&amp;auto=webp&amp;s=841429e64ec9a831262483c0b7479886fd01029f"
              },
              "id": "fdza644lx3bf1"
            },
            "0hm1zxpmx3bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1905e1e3c4a5669198f11b544de3bae463692a72"
                },
                {
                  "y": 111,
                  "x": 216,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=78a297ef8333986019408fd6b72f1830a22911da"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63de894cbcd59bb3e4281500136b499d2a2ff125"
                },
                {
                  "y": 329,
                  "x": 640,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1df195ce5438559971c054504be62e57dc0eaa4"
                },
                {
                  "y": 493,
                  "x": 960,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b238eb03338824287ccc0dbcbf06acfc69ee819"
                },
                {
                  "y": 555,
                  "x": 1080,
                  "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3c508e356a58da9829f51954ad7a2aa96e1088e9"
                }
              ],
              "s": {
                "y": 1636,
                "x": 3180,
                "u": "https://preview.redd.it/0hm1zxpmx3bf1.png?width=3180&amp;format=png&amp;auto=webp&amp;s=66b70eca5ea3d910e05c01580bc20f0a7aea73b9"
              },
              "id": "0hm1zxpmx3bf1"
            }
          },
          "name": "t3_1lsi0gj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 21,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "fdza644lx3bf1",
                "id": 699412800
              },
              {
                "media_id": "h3d7d54lx3bf1",
                "id": 699412801
              },
              {
                "media_id": "0hm1zxpmx3bf1",
                "id": 699412802
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/RwkhySplb6CWm6KR-CbkXw8jpoNWLPRz3r_cADWabdw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751743870,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday, I finished evaluating my Android agent model, deki, on two separate benchmarks: Android Control and Android World. For both benchmarks I used a subset of the dataset without fine-tuning. The results show that image description models like deki enables large LLMs (like GPT-4o, GPT-4.1, and Gemini 2.5) to become State-of-the-Art on Android AI agent benchmarks using only vision capabilities, without relying on Accessibility Trees, on both single-step and multi-step tasks.&lt;/p&gt;\n\n&lt;p&gt;deki is a model that understands what’s on your screen and creates a description of the UI screenshot with all coordinates/sizes/attributes. All the code is open sourced. ML, Backend, Android, code updates for benchmarks and also evaluation logs.&lt;/p&gt;\n\n&lt;p&gt;All the code/information is available on GitHub: &lt;a href=\"https://github.com/RasulOs/deki\"&gt;https://github.com/RasulOs/deki&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have also uploaded the model to Hugging Face:&lt;br/&gt;\nSpace: &lt;a href=\"https://huggingface.co/spaces/orasul/deki\"&gt;orasul/deki&lt;/a&gt;&lt;br/&gt;\n(Check the analyze-and-get-yolo endpoint)&lt;/p&gt;\n\n&lt;p&gt;Model: &lt;a href=\"https://huggingface.co/orasul/deki-yolo\"&gt;orasul/deki-yolo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lsi0gj",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsi0gj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Old_Mathematician107",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lsi0gj",
          "subreddit_subscribers": 495395,
          "created_utc": 1751743870,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all,\n\nI present to you **Impish\\_LLAMA\\_4B**, one of the most powerful roleplay \\\\ adventure finetunes at its size category.\n\nTL;DR:\n\n* An **incredibly powerful** roleplay model for the size. It has **sovl !**\n* Does **Adventure** very well for such size!\n* Characters have **agency**, and might surprise you! [See the examples in the logs](https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here) 🙂\n* Roleplay &amp; Assistant data used plenty of **16K** examples.\n* **Very responsive**, feels 'in the moment', kicks **far above** its weight. You might forget it's a **4B** if you squint.\n* Based on a lot of the data in [Impish\\_Magic\\_24B](https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B)\n* **Super long context** as well as context attention for **4B**, personally tested for up to **16K**.\n* Can run on **Raspberry Pi 5** with ease.\n* Trained on over **400m tokens** with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.\n* Very decent assistant.\n* Mostly **uncensored** while retaining plenty of intelligence.\n* Less **positivity** &amp; **uncensored**, [Negative\\_LLAMA\\_70B](https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B) style of data, adjusted for **4B**, with serious upgrades. Training data contains combat scenarios. And it **shows**!\n* Trained on **extended 4chan dataset** to add humanity, quirkiness, and naturally— less positivity, and the inclination to... argue 🙃\n* **Short length** response (1-3 paragraphs, usually 1-2). CAI Style.\n\nCheck out the model card for more details &amp; character cards for Roleplay \\\\ Adventure:\n\n[https://huggingface.co/SicariusSicariiStuff/Impish\\_LLAMA\\_4B](https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B)\n\nAlso, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (\\~**3600** tokens per second, **96 threads**)\n\n[Horde](https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;format=png&amp;auto=webp&amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd)\n\n\\~3600 tokens per second, 96 threads)Would love some feedback! :)",
          "author_fullname": "t2_ik8czvp65",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Powerful 4B Nemotron based finetune",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ga4ihkf1q0bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 13,
                  "x": 108,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe9c07c5aca94fec68bd82ec08897fbba3685f73"
                },
                {
                  "y": 27,
                  "x": 216,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55ad7e703ab87be0976513907e8f3332aed2c757"
                },
                {
                  "y": 40,
                  "x": 320,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=174a5aeb5083723c5a53e0ad354cfb98bbc788a2"
                },
                {
                  "y": 81,
                  "x": 640,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec349f287cfaee9b714f2c2ddefa2f68c02f4f70"
                },
                {
                  "y": 121,
                  "x": 960,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=908e4a0d614b4ed29d46ecc2bdb4c09374d78e05"
                },
                {
                  "y": 137,
                  "x": 1080,
                  "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=782da816bbb449615bf022ffc6a3acc461126bb3"
                }
              ],
              "s": {
                "y": 138,
                "x": 1086,
                "u": "https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;format=png&amp;auto=webp&amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd"
              },
              "id": "ga4ihkf1q0bf1"
            }
          },
          "name": "t3_1ls5b89",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 146,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 146,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ec8383a655a2e03811781fd06b6cb761f8247773",
          "edited": 1751705598,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751705018,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I present to you &lt;strong&gt;Impish_LLAMA_4B&lt;/strong&gt;, one of the most powerful roleplay \\ adventure finetunes at its size category.&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An &lt;strong&gt;incredibly powerful&lt;/strong&gt; roleplay model for the size. It has &lt;strong&gt;sovl !&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Does &lt;strong&gt;Adventure&lt;/strong&gt; very well for such size!&lt;/li&gt;\n&lt;li&gt;Characters have &lt;strong&gt;agency&lt;/strong&gt;, and might surprise you! &lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here\"&gt;See the examples in the logs&lt;/a&gt; 🙂&lt;/li&gt;\n&lt;li&gt;Roleplay &amp;amp; Assistant data used plenty of &lt;strong&gt;16K&lt;/strong&gt; examples.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Very responsive&lt;/strong&gt;, feels &amp;#39;in the moment&amp;#39;, kicks &lt;strong&gt;far above&lt;/strong&gt; its weight. You might forget it&amp;#39;s a &lt;strong&gt;4B&lt;/strong&gt; if you squint.&lt;/li&gt;\n&lt;li&gt;Based on a lot of the data in &lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B\"&gt;Impish_Magic_24B&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Super long context&lt;/strong&gt; as well as context attention for &lt;strong&gt;4B&lt;/strong&gt;, personally tested for up to &lt;strong&gt;16K&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Can run on &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; with ease.&lt;/li&gt;\n&lt;li&gt;Trained on over &lt;strong&gt;400m tokens&lt;/strong&gt; with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.&lt;/li&gt;\n&lt;li&gt;Very decent assistant.&lt;/li&gt;\n&lt;li&gt;Mostly &lt;strong&gt;uncensored&lt;/strong&gt; while retaining plenty of intelligence.&lt;/li&gt;\n&lt;li&gt;Less &lt;strong&gt;positivity&lt;/strong&gt; &amp;amp; &lt;strong&gt;uncensored&lt;/strong&gt;, &lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B\"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data, adjusted for &lt;strong&gt;4B&lt;/strong&gt;, with serious upgrades. Training data contains combat scenarios. And it &lt;strong&gt;shows&lt;/strong&gt;!&lt;/li&gt;\n&lt;li&gt;Trained on &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity, quirkiness, and naturally— less positivity, and the inclination to... argue 🙃&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Short length&lt;/strong&gt; response (1-3 paragraphs, usually 1-2). CAI Style.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Check out the model card for more details &amp;amp; character cards for Roleplay \\ Adventure:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B\"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (~&lt;strong&gt;3600&lt;/strong&gt; tokens per second, &lt;strong&gt;96 threads&lt;/strong&gt;)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd\"&gt;Horde&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;~3600 tokens per second, 96 threads)Would love some feedback! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?auto=webp&amp;s=e0ddfe5f9c048e7d84958224fc2086f57be2ac3b",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb145a5dc8675fac7f239771cdb889ea5c13d23f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3defe65e0259d3bc1f66e6715d355bed1c5b9032",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf7987ab30b7378084d356fe44b809efff4d294e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d882c129728e2bb772cd8f145ea68d43d0c6637",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bed7323290921e159214218f6c1a4e9d9b531c3b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=46ca457684af7e9d7797c792fb61454125c1e6ee",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ls5b89",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sicarius_The_First",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751705018,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Heelo, im making a project where llm might have to deal with geospatial data, raster like. Dealing with formalts like Map Tiles, geojason etc. (Algo RAG implementations) for this i need an LLM but an so confused which one to use. Llama and Mistral both have so many models that im confused.   \nIt must be free to use via api or downloadable locally through ollama (light enough to run well on a gaming laptop).\n\nIf someone has exp with using LLMs for similar tasks i need ur help 😬  \n  \nThis LLM will be the frontface for the user. There wl be other chains to perform operations on the data.",
          "author_fullname": "t2_8rh26fjt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help choosing LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsvff1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751786941,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heelo, im making a project where llm might have to deal with geospatial data, raster like. Dealing with formalts like Map Tiles, geojason etc. (Algo RAG implementations) for this i need an LLM but an so confused which one to use. Llama and Mistral both have so many models that im confused.&lt;br/&gt;\nIt must be free to use via api or downloadable locally through ollama (light enough to run well on a gaming laptop).&lt;/p&gt;\n\n&lt;p&gt;If someone has exp with using LLMs for similar tasks i need ur help 😬  &lt;/p&gt;\n\n&lt;p&gt;This LLM will be the frontface for the user. There wl be other chains to perform operations on the data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsvff1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BESTHARSH004",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751786941,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which means it has to fullfill 2 requirements:\n\n- small, as it needs runing local, ideally no more than 2B;\n- able to do agents work, means it shouldn't be very dumb;\n\neventhough you might ask why not using cloud api, well, it's a typical question about data sensetive and price.\n\nJust wanna talk about if this is a trend, or do we nearly this situation which can do agents, that can just work in local, with bareable speed and free price.",
          "author_fullname": "t2_huncw1e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is that possible built a local gemini-cli totally in local and workable?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsye88",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751799086,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which means it has to fullfill 2 requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;small, as it needs runing local, ideally no more than 2B;&lt;/li&gt;\n&lt;li&gt;able to do agents work, means it shouldn&amp;#39;t be very dumb;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;eventhough you might ask why not using cloud api, well, it&amp;#39;s a typical question about data sensetive and price.&lt;/p&gt;\n\n&lt;p&gt;Just wanna talk about if this is a trend, or do we nearly this situation which can do agents, that can just work in local, with bareable speed and free price.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsye88",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LewisJin",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lsye88/is_that_possible_built_a_local_geminicli_totally/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsye88/is_that_possible_built_a_local_geminicli_totally/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751799086,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I made an offline ChatGPT that runs on a phone similar to [https://play.google.com/store/apps/details?id=com.sandoche.llamao](https://play.google.com/store/apps/details?id=com.sandoche.llamao) . Now this is all and great, but I think accuracy is a tremendous issue here, if we compare to ChatGPT. In order to mitigate this, I believe adding search, deep research will help in improving its quality, simply because the knowledge is partly retrieved from the internet. Possible improvement is also to build local database when needed.\n\nNow, what is the benefit of this? You have the LLM core runs on your phone, when you are on the mountain or overseas without internet, guess what, you can still ask your phone general knowledge. This is a personal situation I encountered back when I was travelling in China.\n\nWhat do you think? Also, if you are interested in working together, please PM me. I have had already some headstart, and would love to work together with someone good in coding/LLM/Frontend (Flutter)! We can make a GitHub together and all.\n\n\n\nEDIT:\n\nThere is a misconception. The beforementioned app is not mine, but rather just a reference. Mine is not yet uploaded to Play Store, as I still want to refine the app. But, here is a video and source for it. \n\n\\* Screenvideo: [https://www.linkedin.com/posts/samkoesnadi\\_ai-artificialintelligence-offlineai-activity-7292197923474337792-riNH?utm\\_source=share&amp;utm\\_medium=member\\_desktop&amp;rcm=ACoAAEgyXT4B44qeYmL0-CuhPAs29Ue55GqugWc](https://www.linkedin.com/posts/samkoesnadi_ai-artificialintelligence-offlineai-activity-7292197923474337792-riNH?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAEgyXT4B44qeYmL0-CuhPAs29Ue55GqugWc) . \n\n\\* And source code is [https://github.com/samkoesnadi/pali-ai](https://github.com/samkoesnadi/pali-ai) ",
          "author_fullname": "t2_ciqb6mpfx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wouldn't it be great if we have a local offline ChatGPT runs on a phone, with all the functionality of normal ChatGPT, such as search, deep research, perhaps function tooling. What do you think?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lt98oq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.41,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751830739,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751828830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made an offline ChatGPT that runs on a phone similar to &lt;a href=\"https://play.google.com/store/apps/details?id=com.sandoche.llamao\"&gt;https://play.google.com/store/apps/details?id=com.sandoche.llamao&lt;/a&gt; . Now this is all and great, but I think accuracy is a tremendous issue here, if we compare to ChatGPT. In order to mitigate this, I believe adding search, deep research will help in improving its quality, simply because the knowledge is partly retrieved from the internet. Possible improvement is also to build local database when needed.&lt;/p&gt;\n\n&lt;p&gt;Now, what is the benefit of this? You have the LLM core runs on your phone, when you are on the mountain or overseas without internet, guess what, you can still ask your phone general knowledge. This is a personal situation I encountered back when I was travelling in China.&lt;/p&gt;\n\n&lt;p&gt;What do you think? Also, if you are interested in working together, please PM me. I have had already some headstart, and would love to work together with someone good in coding/LLM/Frontend (Flutter)! We can make a GitHub together and all.&lt;/p&gt;\n\n&lt;p&gt;EDIT:&lt;/p&gt;\n\n&lt;p&gt;There is a misconception. The beforementioned app is not mine, but rather just a reference. Mine is not yet uploaded to Play Store, as I still want to refine the app. But, here is a video and source for it. &lt;/p&gt;\n\n&lt;p&gt;* Screenvideo: &lt;a href=\"https://www.linkedin.com/posts/samkoesnadi_ai-artificialintelligence-offlineai-activity-7292197923474337792-riNH?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAEgyXT4B44qeYmL0-CuhPAs29Ue55GqugWc\"&gt;https://www.linkedin.com/posts/samkoesnadi_ai-artificialintelligence-offlineai-activity-7292197923474337792-riNH?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAEgyXT4B44qeYmL0-CuhPAs29Ue55GqugWc&lt;/a&gt; . &lt;/p&gt;\n\n&lt;p&gt;* And source code is &lt;a href=\"https://github.com/samkoesnadi/pali-ai\"&gt;https://github.com/samkoesnadi/pali-ai&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4toYMrC1hGQnZ3rYk-NX-qj3ad988wGhBf4PA-IbBdw.png?auto=webp&amp;s=53ed35553f0533dcef0676b95577e3649fa3b962",
                  "width": 512,
                  "height": 512
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4toYMrC1hGQnZ3rYk-NX-qj3ad988wGhBf4PA-IbBdw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0c58a79c2d5aa21769f0dab3a6c072b7669f715",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/4toYMrC1hGQnZ3rYk-NX-qj3ad988wGhBf4PA-IbBdw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f932f620e2708786c5111eef5602f25e8bd2b29",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/4toYMrC1hGQnZ3rYk-NX-qj3ad988wGhBf4PA-IbBdw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0711305c900e8fbbf4e49316d554ffcbdc19c1b4",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "4toYMrC1hGQnZ3rYk-NX-qj3ad988wGhBf4PA-IbBdw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lt98oq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "samkoesnadi",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lt98oq/wouldnt_it_be_great_if_we_have_a_local_offline/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lt98oq/wouldnt_it_be_great_if_we_have_a_local_offline/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751828830,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello people.\nI have a 4060ti for local Inference. The card is doing just fine considering the allocated budget.\nI'm thinking a second card to pair with it so I can utilize longer context and/or bigger models.\nThe two options I consider is a second 4060ti or a 5060ti (my budget is tight)\nWhat do you think? Any other suggestions?",
          "author_fullname": "t2_7c4h5ryx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Upgrade for my 4060ti",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lswhaj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751791281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello people.\nI have a 4060ti for local Inference. The card is doing just fine considering the allocated budget.\nI&amp;#39;m thinking a second card to pair with it so I can utilize longer context and/or bigger models.\nThe two options I consider is a second 4060ti or a 5060ti (my budget is tight)\nWhat do you think? Any other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lswhaj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Former-Tangerine-723",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lswhaj/upgrade_for_my_4060ti/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lswhaj/upgrade_for_my_4060ti/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751791281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3lnwo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls70r2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 56,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 56,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=6fba0fd765a16b9b2064046509d9b2e66373edd6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751712185,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "pugetsystems.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.pugetsystems.com/labs/articles/impact-of-pcie-5-0-bandwidth-on-gpu-content-creation-performance/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?auto=webp&amp;s=f1630bd3dfb2226fa410a38649e7d40447a81866",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41313f8558078bc1cb1b64de29b33e20187ee0b2",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=028bbe9db629fd24048f079e684f025b3f56072b",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5d7b9e566a80ee05583c3ef0b1342f3dda9d636",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86ebf97301bc00e90b9b236ebf2a2bb13dae2a1a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08be7ce0643e4022cce3ddea140b9601987b4e49",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e3c8cc9b20478e822baff4323bcf8a63d838755",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ls70r2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "d5dq",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/",
          "stickied": false,
          "url": "https://www.pugetsystems.com/labs/articles/impact-of-pcie-5-0-bandwidth-on-gpu-content-creation-performance/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751712185,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi.\n\nI create Android application for download from HuggingFace and locally running AI models (with type .gguf, .task) on smartphone usind Llama.cpp and MediaPipe engines. \n\nI am interested in your opinion.\n\n[https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher](https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher)\n\nhttps://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5",
          "author_fullname": "t2_3qjxph23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New app for locally running AI models on Android your smartphone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "cc4u5d7h23bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e95cfbe66e4d06235f507c322e5bdbb1a486137"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cab4b3f53a8baaa5ae1a23863e9f1000ead939ec"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=86a54df6c728461224b28806669598087ba092c5"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=329ec3f74abebac985366aec79860020d2535471"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=296441b5e6dc2b8ba74e33a8fbb137c827216655"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c393a90227facf1440dc8ca53d47acb2bb72018c"
                }
              ],
              "s": {
                "y": 3200,
                "x": 1440,
                "u": "https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5"
              },
              "id": "cc4u5d7h23bf1"
            }
          },
          "name": "t3_1lsdxc2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=114ac5e21930254990653d99f465d0f0b70db196",
          "edited": 1751821921,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751733155,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;I create Android application for download from HuggingFace and locally running AI models (with type .gguf, .task) on smartphone usind Llama.cpp and MediaPipe engines. &lt;/p&gt;\n\n&lt;p&gt;I am interested in your opinion.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher\"&gt;https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5\"&gt;https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?auto=webp&amp;s=603294677425600238aebad8643a28f98a37ac18",
                  "width": 512,
                  "height": 512
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50a2358f9b890643e86778f4cba80f28e095ae0b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd696e0f18267befba2f7df8f0a9badcfdda702f",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dba60f6210ef2ca39ebd034795d1371f991f9f7d",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsdxc2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RomanKryvolapov",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751733155,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sorry if this sounds dumb lol\n\nMy organisation is researching/attempting to create AI agents that can act as software architects and help in designing softwares. This is an already established product and we get a lot of new feature requests on top of it. \n\nSo basically, this agent would need the understanding of the current product - lots of code, PDFs, Word documents, excel sheets (configuration files).\n\nI am wondering what should be my starting point? \n\nVector Databases, Knowledge Graphs, hybrid approach? \n\nAny pointers should help. Let me know if this is too ambitious as well. Cheers! ",
          "author_fullname": "t2_p3mae",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Creating a Knowledge Base for Agentic Research Architect",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsvsw0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751788489,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this sounds dumb lol&lt;/p&gt;\n\n&lt;p&gt;My organisation is researching/attempting to create AI agents that can act as software architects and help in designing softwares. This is an already established product and we get a lot of new feature requests on top of it. &lt;/p&gt;\n\n&lt;p&gt;So basically, this agent would need the understanding of the current product - lots of code, PDFs, Word documents, excel sheets (configuration files).&lt;/p&gt;\n\n&lt;p&gt;I am wondering what should be my starting point? &lt;/p&gt;\n\n&lt;p&gt;Vector Databases, Knowledge Graphs, hybrid approach? &lt;/p&gt;\n\n&lt;p&gt;Any pointers should help. Let me know if this is too ambitious as well. Cheers! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsvsw0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dew_chiggi",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsvsw0/creating_a_knowledge_base_for_agentic_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsvsw0/creating_a_knowledge_base_for_agentic_research/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751788489,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Most RAG explainers jump into theories and scary infra diagrams. Here’s the tiny end-to-end demo that can easy to understand for me:\n\nSuppose we have a documentation like this: \"Boil an egg. Poach an egg. How to change a tire\"\n\n# Step 1: Chunk\n\n    S0: \"Boil an egg\"\n    S1: \"Poach an egg\"\n    S2: \"How to change a tire\"\n\n# Step 2: Embed\n\nAfter the words “Boil an egg” pass through a pretrained transformer, the model compresses its hidden states into a single 4-dimensional vector; each value is just one coordinate of that learned “meaning point” in vector space.\n\nToy demo values:\n\n    V0 = [ 0.90, 0.10, 0.00, 0.10]   # “Boil an egg”\n    V1 = [ 0.88, 0.12, 0.00, 0.09]   # “Poach an egg”\n    V2 = [-0.20, 0.40, 0.80, 0.10]   # “How to change a tire”\n\n(Real models spit out 384-D to 3072-D vectors; 4-D keeps the math readable.)\n\n# Step 3: Normalize\n\nPut every vector on the unit sphere:\n\n    # Normalised (unit-length) vectors\n    V0̂ = [ 0.988, 0.110, 0.000, 0.110]   # 0.988² + 0.110² + 0.000² + 0.110² ≈ 1.000 → 1\n    V1̂ = [ 0.986, 0.134, 0.000, 0.101]   # 0.986² + 0.134² + 0.000² + 0.101² ≈ 1.000 → 1\n    V2̂ = [-0.217, 0.434, 0.868, 0.108]   # (-0.217)² + 0.434² + 0.868² + 0.108² ≈ 1.001 → 1\n\n# Step 4: Index\n\nDrop V0\\^,V1\\^,V2\\^ into a similarity index (FAISS, Qdrant, etc.).  \nKeep a side map `{0:S0, 1:S1, 2:S2}` so IDs can turn back into text later.\n\n# Step 5: Similarity Search\n\n**User asks**  \n“Best way to cook an egg?”\n\nWe embed this sentence and normalize it as well, which gives us something like:\n\n    Vi^ = [0.989, 0.086, 0.000, 0.118]\n\nThen we need to find the vector that’s *closest* to this one.  \nThe most common way is cosine similarity — often written as:\n\n    cos(θ) = (A ⋅ B) / (‖A‖ × ‖B‖)\n\nBut since we already normalized all vectors,  \n‖A‖ = ‖B‖ = 1 → so the formula becomes just:\n\n    cos(θ) = A ⋅ B\n\nThis means we just need to calculate the **dot product** between the user input vector and each stored vector.  \nIf two vectors are exactly the same, dot product = 1.  \nSo we sort by which ones have values closest to 1 - higher = more similar.\n\nLet’s calculate the scores (example, not real)\n\n    Vi^ ⋅ V0̂ = (0.989)(0.988) + (0.086)(0.110) + (0)(0) + (0.118)(0.110)\n            ≈ 0.977 + 0.009 + 0 + 0.013 = 0.999\n    \n    Vi^ ⋅ V1̂ = (0.989)(0.986) + (0.086)(0.134) + (0)(0) + (0.118)(0.101)\n            ≈ 0.975 + 0.012 + 0 + 0.012 = 0.999\n    \n    Vi^ ⋅ V2̂ = (0.989)(-0.217) + (0.086)(0.434) + (0)(0.868) + (0.118)(0.108)\n            ≈ -0.214 + 0.037 + 0 + 0.013 = -0.164\n    \n\nSo we find that sentence 0 (“Boil an egg”) and sentence 1 (“Poach an egg”)  \nare both very close to the user input.\n\nWe **retrieve those two as context**, and pass them to the LLM.  \nNow the LLM has relevant info to answer accurately, instead of guessing.",
          "author_fullname": "t2_1pnlpczpqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How RAG actually works — a toy example with real math",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrsx20",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 624,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 624,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751661855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most RAG explainers jump into theories and scary infra diagrams. Here’s the tiny end-to-end demo that can easy to understand for me:&lt;/p&gt;\n\n&lt;p&gt;Suppose we have a documentation like this: &amp;quot;Boil an egg. Poach an egg. How to change a tire&amp;quot;&lt;/p&gt;\n\n&lt;h1&gt;Step 1: Chunk&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;S0: &amp;quot;Boil an egg&amp;quot;\nS1: &amp;quot;Poach an egg&amp;quot;\nS2: &amp;quot;How to change a tire&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Step 2: Embed&lt;/h1&gt;\n\n&lt;p&gt;After the words “Boil an egg” pass through a pretrained transformer, the model compresses its hidden states into a single 4-dimensional vector; each value is just one coordinate of that learned “meaning point” in vector space.&lt;/p&gt;\n\n&lt;p&gt;Toy demo values:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;V0 = [ 0.90, 0.10, 0.00, 0.10]   # “Boil an egg”\nV1 = [ 0.88, 0.12, 0.00, 0.09]   # “Poach an egg”\nV2 = [-0.20, 0.40, 0.80, 0.10]   # “How to change a tire”\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(Real models spit out 384-D to 3072-D vectors; 4-D keeps the math readable.)&lt;/p&gt;\n\n&lt;h1&gt;Step 3: Normalize&lt;/h1&gt;\n\n&lt;p&gt;Put every vector on the unit sphere:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Normalised (unit-length) vectors\nV0̂ = [ 0.988, 0.110, 0.000, 0.110]   # 0.988² + 0.110² + 0.000² + 0.110² ≈ 1.000 → 1\nV1̂ = [ 0.986, 0.134, 0.000, 0.101]   # 0.986² + 0.134² + 0.000² + 0.101² ≈ 1.000 → 1\nV2̂ = [-0.217, 0.434, 0.868, 0.108]   # (-0.217)² + 0.434² + 0.868² + 0.108² ≈ 1.001 → 1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Step 4: Index&lt;/h1&gt;\n\n&lt;p&gt;Drop V0^,V1^,V2^ into a similarity index (FAISS, Qdrant, etc.).&lt;br/&gt;\nKeep a side map &lt;code&gt;{0:S0, 1:S1, 2:S2}&lt;/code&gt; so IDs can turn back into text later.&lt;/p&gt;\n\n&lt;h1&gt;Step 5: Similarity Search&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;User asks&lt;/strong&gt;&lt;br/&gt;\n“Best way to cook an egg?”&lt;/p&gt;\n\n&lt;p&gt;We embed this sentence and normalize it as well, which gives us something like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Vi^ = [0.989, 0.086, 0.000, 0.118]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then we need to find the vector that’s &lt;em&gt;closest&lt;/em&gt; to this one.&lt;br/&gt;\nThe most common way is cosine similarity — often written as:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cos(θ) = (A ⋅ B) / (‖A‖ × ‖B‖)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But since we already normalized all vectors,&lt;br/&gt;\n‖A‖ = ‖B‖ = 1 → so the formula becomes just:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cos(θ) = A ⋅ B\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This means we just need to calculate the &lt;strong&gt;dot product&lt;/strong&gt; between the user input vector and each stored vector.&lt;br/&gt;\nIf two vectors are exactly the same, dot product = 1.&lt;br/&gt;\nSo we sort by which ones have values closest to 1 - higher = more similar.&lt;/p&gt;\n\n&lt;p&gt;Let’s calculate the scores (example, not real)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Vi^ ⋅ V0̂ = (0.989)(0.988) + (0.086)(0.110) + (0)(0) + (0.118)(0.110)\n        ≈ 0.977 + 0.009 + 0 + 0.013 = 0.999\n\nVi^ ⋅ V1̂ = (0.989)(0.986) + (0.086)(0.134) + (0)(0) + (0.118)(0.101)\n        ≈ 0.975 + 0.012 + 0 + 0.012 = 0.999\n\nVi^ ⋅ V2̂ = (0.989)(-0.217) + (0.086)(0.434) + (0)(0.868) + (0.118)(0.108)\n        ≈ -0.214 + 0.037 + 0 + 0.013 = -0.164\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So we find that sentence 0 (“Boil an egg”) and sentence 1 (“Poach an egg”)&lt;br/&gt;\nare both very close to the user input.&lt;/p&gt;\n\n&lt;p&gt;We &lt;strong&gt;retrieve those two as context&lt;/strong&gt;, and pass them to the LLM.&lt;br/&gt;\nNow the LLM has relevant info to answer accurately, instead of guessing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lrsx20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Main-Fisherman-2075",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751661855,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm genuinely struggling with everything out there in terms of making me smile and general joke quality. If there is such a model, at what settings should it run? (temp/top\\_k etc). ",
          "author_fullname": "t2_nvmilgf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which open source LLM has the most genuine sense of humor?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls9jvu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751721047,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m genuinely struggling with everything out there in terms of making me smile and general joke quality. If there is such a model, at what settings should it run? (temp/top_k etc). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ls9jvu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UltrMgns",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751721047,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have developed the web app and chrome extension to summarize the long reddit threads discussion using chatgpt, it helps user to analyize thread discussions and sentiments of the discussion.",
          "author_fullname": "t2_7azucao8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created this tool I named ReddSummary.com – just paste a link and boom you got the summary",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsfmcj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DoYaP55WOHORdN07iAI3p-62cotQb-fwucdWaCd1d5w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751737600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have developed the web app and chrome extension to summarize the long reddit threads discussion using chatgpt, it helps user to analyize thread discussions and sentiments of the discussion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/2exxosoue3bf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/2exxosoue3bf1.png?auto=webp&amp;s=1cda7f051de15b6e90aa4909e2aa41b713769f20",
                  "width": 2546,
                  "height": 1156
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1fd832cbd0013c8cbdcec784df645584bcc52a5d",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=85fbce9bb96ceac0b7288b66741e746c2e211765",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa1e1c0c6dacbd464f6a80a23c5f63d703ec0db4",
                    "width": 320,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1c1d1e13fd17ff4381b38e36072d089c11c07e48",
                    "width": 640,
                    "height": 290
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b555565ac5d5919934305538ff3e4a858e9348e0",
                    "width": 960,
                    "height": 435
                  },
                  {
                    "url": "https://preview.redd.it/2exxosoue3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d66d21b5f93bfbece51c5fe6e6c40464cc7e91e6",
                    "width": 1080,
                    "height": 490
                  }
                ],
                "variants": {},
                "id": "q_udZco6iPYKDtVF7n70XwNt8-iEUXvFctKoNCOAC5Q"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lsfmcj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Himanshu507",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/",
          "stickied": false,
          "url": "https://i.redd.it/2exxosoue3bf1.png",
          "subreddit_subscribers": 495395,
          "created_utc": 1751737600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3coetveu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a platform to collect &amp; solve real-world AI automation use cases – would love your feedback!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsq6xi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751767813,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "aisolutionscamp.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://aisolutionscamp.io",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsq6xi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "disappead",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsq6xi/i_built_a_platform_to_collect_solve_realworld_ai/",
          "stickied": false,
          "url": "https://aisolutionscamp.io",
          "subreddit_subscribers": 495395,
          "created_utc": 1751767813,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm currently working on setting up an **in-house Large Language Model (LLM) system** for internal organizational projects. Given the rapid advancements in AI technology, I’d greatly value your professional insights and recommendations to ensure we're leveraging the latest tools and methods effectively.\n\n**Here's our current plan and key considerations:**\n\n**1. Model Selection:** We're considering open-source models such as GPT-3 (EleutherAI), T5, or FLAN-T5. Are there any standout alternatives or specific models you've successfully implemented lately?\n\n**2. Data Pipeline:** We’re using Apache Kafka for real-time data ingestion and Apache Spark for batch processing. Have you come across any newer or more efficient tools and practices beneficial for handling large-scale datasets?\n\n**3. Training &amp; Fine-Tuning:** Planning to utilize Ray Tune and Weights &amp; Biases for hyperparameter optimization and experiment tracking. GPU costs remain a concern—any advice on cost-effective or emerging platforms for fine-tuning large models?\n\n**4. Deployment &amp; Serving:** Considering Kubernetes, Docker, and FastAPI for deployment. Would you recommend NVIDIA Triton Server or TensorRT for better performance? What has your experience been?\n\n**5. Performance &amp; Scalability:** Ensuring real-time scalability and minimal latency is crucial. How do you efficiently manage scalability and parallel inference when deploying multiple models concurrently?\n\n**6. Ethics &amp; Bias Mitigation:** Effective bias detection and mitigation frameworks are essential for us. Can you suggest recent effective tools or methods for ethical AI deployment?\n\n**We'd appreciate your input on:**\n\n* Key tools or strategies that significantly improved your LLM workflows in 2025.\n* Recommendations for cost-effective GPU management and training setups.\n* Preferred tools for robust monitoring, logging, and performance analysis (e.g., Prometheus, Grafana).",
          "author_fullname": "t2_uaotuj04",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Advice Needed: Building an In-House LLM System Using Latest Tech — Recommendations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsx9pn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751794548,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on setting up an &lt;strong&gt;in-house Large Language Model (LLM) system&lt;/strong&gt; for internal organizational projects. Given the rapid advancements in AI technology, I’d greatly value your professional insights and recommendations to ensure we&amp;#39;re leveraging the latest tools and methods effectively.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here&amp;#39;s our current plan and key considerations:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Model Selection:&lt;/strong&gt; We&amp;#39;re considering open-source models such as GPT-3 (EleutherAI), T5, or FLAN-T5. Are there any standout alternatives or specific models you&amp;#39;ve successfully implemented lately?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Data Pipeline:&lt;/strong&gt; We’re using Apache Kafka for real-time data ingestion and Apache Spark for batch processing. Have you come across any newer or more efficient tools and practices beneficial for handling large-scale datasets?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Training &amp;amp; Fine-Tuning:&lt;/strong&gt; Planning to utilize Ray Tune and Weights &amp;amp; Biases for hyperparameter optimization and experiment tracking. GPU costs remain a concern—any advice on cost-effective or emerging platforms for fine-tuning large models?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Deployment &amp;amp; Serving:&lt;/strong&gt; Considering Kubernetes, Docker, and FastAPI for deployment. Would you recommend NVIDIA Triton Server or TensorRT for better performance? What has your experience been?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. Performance &amp;amp; Scalability:&lt;/strong&gt; Ensuring real-time scalability and minimal latency is crucial. How do you efficiently manage scalability and parallel inference when deploying multiple models concurrently?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;6. Ethics &amp;amp; Bias Mitigation:&lt;/strong&gt; Effective bias detection and mitigation frameworks are essential for us. Can you suggest recent effective tools or methods for ethical AI deployment?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;We&amp;#39;d appreciate your input on:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Key tools or strategies that significantly improved your LLM workflows in 2025.&lt;/li&gt;\n&lt;li&gt;Recommendations for cost-effective GPU management and training setups.&lt;/li&gt;\n&lt;li&gt;Preferred tools for robust monitoring, logging, and performance analysis (e.g., Prometheus, Grafana).&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsx9pn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Edge2098",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751794548,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m doing self-funded AI research and recently got access to 2× NVIDIA A100 SXM4 GPUs. I want to build a quiet, stable node at home to run local models and training workloads — no cloud.\n\nHas anyone here actually built a DIY system with A100 SXM4s (not PCIe)? If so:\n What HGX carrier board or server chassis did you use?\n How did you handle power + cooling safely at home?\n Any tips on finding used baseboards or reference systems?\n\nI’m not working for any company — just serious about doing advanced AI work locally and learning by building. Happy to share progress once it’s working.\n\nThanks in advance — would love any help or photos from others doing the same.",
          "author_fullname": "t2_rysc7jpo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone built a home 2× A100 SXM4 node?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsflii",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751737534,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m doing self-funded AI research and recently got access to 2× NVIDIA A100 SXM4 GPUs. I want to build a quiet, stable node at home to run local models and training workloads — no cloud.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here actually built a DIY system with A100 SXM4s (not PCIe)? If so:\n What HGX carrier board or server chassis did you use?\n How did you handle power + cooling safely at home?\n Any tips on finding used baseboards or reference systems?&lt;/p&gt;\n\n&lt;p&gt;I’m not working for any company — just serious about doing advanced AI work locally and learning by building. Happy to share progress once it’s working.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance — would love any help or photos from others doing the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsflii",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun_Nefariousness228",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751737534,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a beginner developer who just completed my first AI project. In past, I almost dedicated to traditional frontend, backend and toolchain development and know a little knowledges about AI. Recently, I'm working for a toolchain project of myself and compositing its documents. An idea suddenly emerges, I could utilize MCP to told AI project's details and make agent help me coding. After communicating with GPT, I decided to adopt the following technology stacks:\n\n* **Backend**: FastAPI + Python\n* **Vector DB**: ChromaDB (with memory fallback)\n* **Embeddings**: Sentence Transformers\n* **LLM**: Local Qwen2.5-7B via Ollama\n* **Architecture**: RAG (Retrieval-Augmented Generation) \n\nBefore vectoring document, I decided to split chunks from every document instead of directly adopting, considering that the model token requirment is limited and documents contains lots markdown and markdown involves lots subtiltle like h2, h3, h4. Approximately spending half hours, I finished this target and successed vectoring documents and chunks. But according to results from test units, outcomes based on similarity pattern looks so bad. Because some keywords don't explicitly present on original text and result in unavaliable information matched. Then I read about multi-round retrieval. The idea: do a broad search first, then refine it. It actually worked better! Not perfect, but definitely an improvement. \n\nWhen tasks were above finished, I start to call local LLMs through ollama. The development of later story is better smoth than data preprocess. With the prompts that match the context of the input information, splice in the input problem, and the large model quickly gives me the answer I want. But the practice of MCP is terrible for me. GPT gives me lots dirty codes which include tedious access chain using any type, invalid function signature and incorrect parameters pass. What's worst, it's no support MCP integration for Cursor IDE I often use. Therefore, AI told me calling function by HTTP is fine compared to MCP. Ultimately, I had to give up call the knowledge base by MCP method.",
          "author_fullname": "t2_bg90uypls",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a RAG-powered knowledge base for docs of my project using FastAPI + Ollama. Here's what I learned.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsox8o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751763680,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner developer who just completed my first AI project. In past, I almost dedicated to traditional frontend, backend and toolchain development and know a little knowledges about AI. Recently, I&amp;#39;m working for a toolchain project of myself and compositing its documents. An idea suddenly emerges, I could utilize MCP to told AI project&amp;#39;s details and make agent help me coding. After communicating with GPT, I decided to adopt the following technology stacks:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: FastAPI + Python&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Vector DB&lt;/strong&gt;: ChromaDB (with memory fallback)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Embeddings&lt;/strong&gt;: Sentence Transformers&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: Local Qwen2.5-7B via Ollama&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: RAG (Retrieval-Augmented Generation) &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Before vectoring document, I decided to split chunks from every document instead of directly adopting, considering that the model token requirment is limited and documents contains lots markdown and markdown involves lots subtiltle like h2, h3, h4. Approximately spending half hours, I finished this target and successed vectoring documents and chunks. But according to results from test units, outcomes based on similarity pattern looks so bad. Because some keywords don&amp;#39;t explicitly present on original text and result in unavaliable information matched. Then I read about multi-round retrieval. The idea: do a broad search first, then refine it. It actually worked better! Not perfect, but definitely an improvement. &lt;/p&gt;\n\n&lt;p&gt;When tasks were above finished, I start to call local LLMs through ollama. The development of later story is better smoth than data preprocess. With the prompts that match the context of the input information, splice in the input problem, and the large model quickly gives me the answer I want. But the practice of MCP is terrible for me. GPT gives me lots dirty codes which include tedious access chain using any type, invalid function signature and incorrect parameters pass. What&amp;#39;s worst, it&amp;#39;s no support MCP integration for Cursor IDE I often use. Therefore, AI told me calling function by HTTP is fine compared to MCP. Ultimately, I had to give up call the knowledge base by MCP method.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsox8o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ansurfen",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsox8o/i_built_a_ragpowered_knowledge_base_for_docs_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsox8o/i_built_a_ragpowered_knowledge_base_for_docs_of/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751763680,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Note: The following was generated via Gemini, simply because I am lazy and don't wanna summarize things personally. You can view the code [Here](https://pastebin.com/82Cn7022), and the text output comparisons [Here](https://pastebin.com/73Zn2bP4)\n\nI used the Puffin dataset for the Proof of concept, all in all it at least seems promising. Sadly its purely simulated, its my understanding that we would need custom cuda code in order to on the fly quantize (if its even currently possible with current hardware).\n\nGiven that this was a quick vibecoded proof of concept attempt to see how qwen3 0.6b would handle on the fly dynamic quantization in different sized chunks, I am rather impressed. But I don't know if the results were genuine. I would love to hear from other people about the topic.\n\nFinally the End goal for this would be:  \nKeep entire Model Loaded in system Memory. Quantize on the fly based off the current prompt.  \nUpdate the gpu based on the new quantized values.  \nThink Dynamic Mixture of Experts but using quantization over an entire model based on current tasks.\n\n\\[Edit: I should mention that the accuracy is based off the Full models output (Using Puffin dataset for the prompts/context) and compared with the quantized output. At no point did the accuracy compare with the datasets expected output\\]\n\nOk what follows was an AI generated summary from Gemini of my results.  \n\\------\n\nI've been experimenting with **dynamic quantization** for Large Language Models, and I wanted to share what I've found and get some community input.\n\n**The Idea:** My goal is to make LLMs more efficient by having them adjust the precision (bit-width) of their weights *as they process input*. Think of it as a model deciding, \"Okay, this simple query can use 4-bit, but that complex reasoning part needs 16-bit,\" all to save VRAM and potentially speed things up.\n\n**My Setup:** I'm using the **Qwen3-0.6B** model (which is typically BF16) and a smaller, separate neural network I'm calling the \"**Quantization Controller**.\" This controller's job is to predict the best bit-width (from 0-bit pruning to 32-bit full precision) for small \"chunks\" of the LLM's weights for each specific input.\n\nI'm training this controller to balance two things:\n\n1. **Output Similarity:** Keep the quantized model's output logits as close as possible to the full-precision model's.\n2. **VRAM Use:** Add a penalty for using higher bit-widths to encourage memory savings. The VRAM penalty changes dynamically based on how well the quantized model is doing on accuracy – if it's too accurate, the penalty for VRAM goes up, pushing it to compress more; if accuracy drops, the penalty goes down, letting it use more bits.\n\n**What I've Seen So Far:**\n\n* **VRAM Savings:** I've managed to get the simulated VRAM footprint down from around 2.2GB (full BF16) to about 1.1GB, which is a pretty good reduction.\n* **Token-Level Accuracy:** On my small dataset, the quantized model often matches the full-precision model almost perfectly in terms of predicting the next token.\n* **\"Settling\" Bit-widths:** Even with the dynamic penalty, the controller seems to mostly stick to a couple of main bit-widths (like 9-bit and 11-bit) for most chunks. Only a small fraction of chunks (e.g., 8-30 out of \\~4500) actually change their quantization level per step. This makes it feel more like it's found a good static setup for these specific prompts.\n* **Quality vs. Accuracy Gap:** The interesting part is, even with high token accuracy, the *generated text* from the quantized model can sometimes be incoherent or factually wrong (e.g., saying something is \"not feasible\" when it clearly is). This suggests that while it gets the next token right, some of the deeper semantic quality is lost with aggressive quantization.\n\n**Questions for Discussion:**\n\n1. **More Dynamic Behavior:** How can I get the controller to truly adapt more dynamically, meaning more fluctuation in bit-widths per chunk per prompt? Should I increase the \"entropy penalty\" in the controller's loss function to encourage it to explore more?\n2. **Improving Output Quality:** To fix the coherence issues, I'm thinking about adding **trainable adapters (like LoRA)** to the quantized LLM. The idea is these small adapters would learn to correct the errors caused by quantization. Does this sound like a good next step, or are there other efficient ways to tackle this?\n3. **Generating LoRA Weights?** A more out-there idea: could a *tiny, separate model* be trained to *generate* those LoRA weights dynamically for each input? (I know this is complex, but curious if anyone's explored this \"hypernetwork\" approach for quantization).\n4. **Real-World Quantization:** My current setup \"fakes\" quantization (values are re-mapped in BF16, but the actual memory footprint doesn't change). How do people typically test and implement *true* dynamic quantization with actual low-bit integer types (like 4-bit or 8-bit) in PyTorch, especially since libraries like `bitsandbytes` don't seem to expose easy dynamic per-chunk switching?\n\nI'm pretty excited about the potential of adaptive quantization to make LLMs more accessible and efficient. Any thoughts, relevant papers, or advice would be super helpful!\n\nThanks for reading!",
          "author_fullname": "t2_mf1dz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vibecoding: Exploring Dynamic Quantization for LLMs: My PoC with Qwen-0.6B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsses1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751775891,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751775457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Note: The following was generated via Gemini, simply because I am lazy and don&amp;#39;t wanna summarize things personally. You can view the code &lt;a href=\"https://pastebin.com/82Cn7022\"&gt;Here&lt;/a&gt;, and the text output comparisons &lt;a href=\"https://pastebin.com/73Zn2bP4\"&gt;Here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I used the Puffin dataset for the Proof of concept, all in all it at least seems promising. Sadly its purely simulated, its my understanding that we would need custom cuda code in order to on the fly quantize (if its even currently possible with current hardware).&lt;/p&gt;\n\n&lt;p&gt;Given that this was a quick vibecoded proof of concept attempt to see how qwen3 0.6b would handle on the fly dynamic quantization in different sized chunks, I am rather impressed. But I don&amp;#39;t know if the results were genuine. I would love to hear from other people about the topic.&lt;/p&gt;\n\n&lt;p&gt;Finally the End goal for this would be:&lt;br/&gt;\nKeep entire Model Loaded in system Memory. Quantize on the fly based off the current prompt.&lt;br/&gt;\nUpdate the gpu based on the new quantized values.&lt;br/&gt;\nThink Dynamic Mixture of Experts but using quantization over an entire model based on current tasks.&lt;/p&gt;\n\n&lt;p&gt;[Edit: I should mention that the accuracy is based off the Full models output (Using Puffin dataset for the prompts/context) and compared with the quantized output. At no point did the accuracy compare with the datasets expected output]&lt;/p&gt;\n\n&lt;p&gt;Ok what follows was an AI generated summary from Gemini of my results.&lt;br/&gt;\n------&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been experimenting with &lt;strong&gt;dynamic quantization&lt;/strong&gt; for Large Language Models, and I wanted to share what I&amp;#39;ve found and get some community input.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Idea:&lt;/strong&gt; My goal is to make LLMs more efficient by having them adjust the precision (bit-width) of their weights &lt;em&gt;as they process input&lt;/em&gt;. Think of it as a model deciding, &amp;quot;Okay, this simple query can use 4-bit, but that complex reasoning part needs 16-bit,&amp;quot; all to save VRAM and potentially speed things up.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Setup:&lt;/strong&gt; I&amp;#39;m using the &lt;strong&gt;Qwen3-0.6B&lt;/strong&gt; model (which is typically BF16) and a smaller, separate neural network I&amp;#39;m calling the &amp;quot;&lt;strong&gt;Quantization Controller&lt;/strong&gt;.&amp;quot; This controller&amp;#39;s job is to predict the best bit-width (from 0-bit pruning to 32-bit full precision) for small &amp;quot;chunks&amp;quot; of the LLM&amp;#39;s weights for each specific input.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m training this controller to balance two things:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Output Similarity:&lt;/strong&gt; Keep the quantized model&amp;#39;s output logits as close as possible to the full-precision model&amp;#39;s.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;VRAM Use:&lt;/strong&gt; Add a penalty for using higher bit-widths to encourage memory savings. The VRAM penalty changes dynamically based on how well the quantized model is doing on accuracy – if it&amp;#39;s too accurate, the penalty for VRAM goes up, pushing it to compress more; if accuracy drops, the penalty goes down, letting it use more bits.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve Seen So Far:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;VRAM Savings:&lt;/strong&gt; I&amp;#39;ve managed to get the simulated VRAM footprint down from around 2.2GB (full BF16) to about 1.1GB, which is a pretty good reduction.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Token-Level Accuracy:&lt;/strong&gt; On my small dataset, the quantized model often matches the full-precision model almost perfectly in terms of predicting the next token.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&amp;quot;Settling&amp;quot; Bit-widths:&lt;/strong&gt; Even with the dynamic penalty, the controller seems to mostly stick to a couple of main bit-widths (like 9-bit and 11-bit) for most chunks. Only a small fraction of chunks (e.g., 8-30 out of ~4500) actually change their quantization level per step. This makes it feel more like it&amp;#39;s found a good static setup for these specific prompts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Quality vs. Accuracy Gap:&lt;/strong&gt; The interesting part is, even with high token accuracy, the &lt;em&gt;generated text&lt;/em&gt; from the quantized model can sometimes be incoherent or factually wrong (e.g., saying something is &amp;quot;not feasible&amp;quot; when it clearly is). This suggests that while it gets the next token right, some of the deeper semantic quality is lost with aggressive quantization.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions for Discussion:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;More Dynamic Behavior:&lt;/strong&gt; How can I get the controller to truly adapt more dynamically, meaning more fluctuation in bit-widths per chunk per prompt? Should I increase the &amp;quot;entropy penalty&amp;quot; in the controller&amp;#39;s loss function to encourage it to explore more?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Improving Output Quality:&lt;/strong&gt; To fix the coherence issues, I&amp;#39;m thinking about adding &lt;strong&gt;trainable adapters (like LoRA)&lt;/strong&gt; to the quantized LLM. The idea is these small adapters would learn to correct the errors caused by quantization. Does this sound like a good next step, or are there other efficient ways to tackle this?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Generating LoRA Weights?&lt;/strong&gt; A more out-there idea: could a &lt;em&gt;tiny, separate model&lt;/em&gt; be trained to &lt;em&gt;generate&lt;/em&gt; those LoRA weights dynamically for each input? (I know this is complex, but curious if anyone&amp;#39;s explored this &amp;quot;hypernetwork&amp;quot; approach for quantization).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Real-World Quantization:&lt;/strong&gt; My current setup &amp;quot;fakes&amp;quot; quantization (values are re-mapped in BF16, but the actual memory footprint doesn&amp;#39;t change). How do people typically test and implement &lt;em&gt;true&lt;/em&gt; dynamic quantization with actual low-bit integer types (like 4-bit or 8-bit) in PyTorch, especially since libraries like &lt;code&gt;bitsandbytes&lt;/code&gt; don&amp;#39;t seem to expose easy dynamic per-chunk switching?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m pretty excited about the potential of adaptive quantization to make LLMs more accessible and efficient. Any thoughts, relevant papers, or advice would be super helpful!&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsses1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jasonmbrown",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsses1/vibecoding_exploring_dynamic_quantization_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsses1/vibecoding_exploring_dynamic_quantization_for/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751775457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI’m building an AI-powered education platform and **looking for** a high-quality **open-source TTS** model that meets the following needs:\n\n1. ✅ **Voice cloning support** — ability to clone voices from short samples\n2. ✅ Can generate **3–4 hours of audio per user**, even if it requires splitting the text\n3. ✅ Produces **good results across the most spoken languages** (e.g. English, Spanish, Arabic, Hindi, Chinese, etc.)\n\nCommercial tools like ElevenLabs and OpenAI TTS are great, but they don’t scale well cost-wise for a subscription-based system. That’s why I’m exploring open-source alternatives — Coqui XTTS, Kokoro TTS, Bark, etc.\n\nIf you’ve had experience with any model that meets these needs — or know tricks for efficient long-form generation (chunking, caching, merging), I’d love to hear your thoughts.\n\nThanks in advance 🙏",
          "author_fullname": "t2_e77tebed",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for an open-source TTS model for multi-hour, multilingual audio generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsz9iu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751802318,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m building an AI-powered education platform and &lt;strong&gt;looking for&lt;/strong&gt; a high-quality &lt;strong&gt;open-source TTS&lt;/strong&gt; model that meets the following needs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;✅ &lt;strong&gt;Voice cloning support&lt;/strong&gt; — ability to clone voices from short samples&lt;/li&gt;\n&lt;li&gt;✅ Can generate &lt;strong&gt;3–4 hours of audio per user&lt;/strong&gt;, even if it requires splitting the text&lt;/li&gt;\n&lt;li&gt;✅ Produces &lt;strong&gt;good results across the most spoken languages&lt;/strong&gt; (e.g. English, Spanish, Arabic, Hindi, Chinese, etc.)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Commercial tools like ElevenLabs and OpenAI TTS are great, but they don’t scale well cost-wise for a subscription-based system. That’s why I’m exploring open-source alternatives — Coqui XTTS, Kokoro TTS, Bark, etc.&lt;/p&gt;\n\n&lt;p&gt;If you’ve had experience with any model that meets these needs — or know tricks for efficient long-form generation (chunking, caching, merging), I’d love to hear your thoughts.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsz9iu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "seozler",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsz9iu/looking_for_an_opensource_tts_model_for_multihour/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsz9iu/looking_for_an_opensource_tts_model_for_multihour/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751802318,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title. Anyone here experienced when it comes to using this model for text classification? Any tips?\n\n(Using Q6\\_K\\_L by the way).",
          "author_fullname": "t2_thdbilga",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuning Qwen3-32B for sentiment analysis.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lss6b9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751774648,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. Anyone here experienced when it comes to using this model for text classification? Any tips?&lt;/p&gt;\n\n&lt;p&gt;(Using Q6_K_L by the way).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lss6b9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Known_Bed_8000",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751774648,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen3-8B model using Winogrande as benchmark.  \nDWQ and 5bit rule! \n\n🥇 dwq – 68.82%  \n🥈 5bit – 68.51%  \n🥉 6bit – 68.35%  \nbf16 – 67.64%  \ndynamic – 67.56%  \n8bit – 67.56%  \n4bit – 66.30%  \n3bit – 63.85%\n\nhttps://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;format=png&amp;auto=webp&amp;s=d6402294cedb1bdfc338ea34983203e7118188a3\n\n",
          "author_fullname": "t2_sjt9gvtpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Apple MLX Quantizations Royal Rumble 🔥",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 83,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "95nyy1fby1bf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2798636b99eaca27c41a9593f8951555bd9519f5"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a57ed516e1790497910eefb4c735e6e77ac1f8de"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce4dadab624e142cf9d7d7db23388916f76dd9eb"
                },
                {
                  "y": 381,
                  "x": 640,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ca2b93d49f1ae546be5c061c56c6e0626766369"
                },
                {
                  "y": 572,
                  "x": 960,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=76964504cdc351c1e0bf249a2fa3dc41b6054cc0"
                },
                {
                  "y": 643,
                  "x": 1080,
                  "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b35c161ebc60802ca9e3179e1dfeb362709eac86"
                }
              ],
              "s": {
                "y": 1180,
                "x": 1979,
                "u": "https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;format=png&amp;auto=webp&amp;s=d6402294cedb1bdfc338ea34983203e7118188a3"
              },
              "id": "95nyy1fby1bf1"
            }
          },
          "name": "t3_1ls95oj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/-1Fa5pMoUdufyX7EbInbdiYRv8uh6lx6DYYAentaN_0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751719836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3-8B model using Winogrande as benchmark.&lt;br/&gt;\nDWQ and 5bit rule! &lt;/p&gt;\n\n&lt;p&gt;🥇 dwq – 68.82%&lt;br/&gt;\n🥈 5bit – 68.51%&lt;br/&gt;\n🥉 6bit – 68.35%&lt;br/&gt;\nbf16 – 67.64%&lt;br/&gt;\ndynamic – 67.56%&lt;br/&gt;\n8bit – 67.56%&lt;br/&gt;\n4bit – 66.30%&lt;br/&gt;\n3bit – 63.85%&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3\"&gt;https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ls95oj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ifioravanti",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751719836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Chat is this AGI? \n\nhttps://reddit.com/link/1lsv6hn/video/a7rfwluue7bf1/player\n\nI used the prompt from [this Reddit post](https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/) and [made the visualization here](https://www.designarena.ai/play)",
          "author_fullname": "t2_nheifhi0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Update on spinning ball in hexagon test",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "a7rfwluue7bf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1lsv6hn/asset/a7rfwluue7bf1/DASHPlaylist.mpd?a=1754471974%2CMjAxMTM2ZWQ0YmY5OWNjYTBjNTZmNjNjYjBhNDVmOGM3ZDMxMzY3ODUzNDQ3MmVhYWU3ZjkxZmY0ZTE2NGRlOQ%3D%3D&amp;v=1&amp;f=sd",
              "x": 1346,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1lsv6hn/asset/a7rfwluue7bf1/HLSPlaylist.m3u8?a=1754471974%2CYjE1ZWM4YjQwMjNiMzE1NDY1NTM0Zjg0N2IyNzdlNWE3N2IyYzY0YWRlYzY4YmNjNWUwZmM3OGU5ODU2YTk1MA%3D%3D&amp;v=1&amp;f=sd",
              "id": "a7rfwluue7bf1",
              "isGif": false
            }
          },
          "name": "t3_1lsv6hn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.42,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/jE-pjhn8f_A-ZGcTOHrND6AkqRiY9pmOGew0nsRpqX0.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=a0ee1a29c8966b251ef479b41ac0112cfef74840",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751785922,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Chat is this AGI? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1lsv6hn/video/a7rfwluue7bf1/player\"&gt;https://reddit.com/link/1lsv6hn/video/a7rfwluue7bf1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I used the prompt from &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/\"&gt;this Reddit post&lt;/a&gt; and &lt;a href=\"https://www.designarena.ai/play\"&gt;made the visualization here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jE-pjhn8f_A-ZGcTOHrND6AkqRiY9pmOGew0nsRpqX0.png?auto=webp&amp;s=086e0d568c7b680a4ffa89d1657526f8bd6f3038",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jE-pjhn8f_A-ZGcTOHrND6AkqRiY9pmOGew0nsRpqX0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f886d2bbf1cdd040f8862148ca2375da2ed1228a",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/jE-pjhn8f_A-ZGcTOHrND6AkqRiY9pmOGew0nsRpqX0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=85f03446c4a30f2eea28fbbc2204cedefa0127f6",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/jE-pjhn8f_A-ZGcTOHrND6AkqRiY9pmOGew0nsRpqX0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e0a491333f7405932e4a43439a64d0fee060032",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/jE-pjhn8f_A-ZGcTOHrND6AkqRiY9pmOGew0nsRpqX0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e632c836def6939cb08de20e3e166209c025fe03",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/jE-pjhn8f_A-ZGcTOHrND6AkqRiY9pmOGew0nsRpqX0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1be8ad92900eb06426946cce2e78cf095882630c",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "jE-pjhn8f_A-ZGcTOHrND6AkqRiY9pmOGew0nsRpqX0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lsv6hn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "grx_xce",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsv6hn/update_on_spinning_ball_in_hexagon_test/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsv6hn/update_on_spinning_ball_in_hexagon_test/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751785922,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey yall I made a new open-source tool.\n\nIt's an app that **creates training data for AI models from your text and PDFs**.\n\nIt uses AI like Gemini, Claude, and OpenAI to make good question-answer sets that you can use to make your own AI smarter. The data comes out ready for different models.\n\nSuper simple, super useful, and it's all open source!",
          "author_fullname": "t2_5gpifn7q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source tool for generating training datasets from text files and pdf for fine-tuning language models.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls3gho",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751697395,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey yall I made a new open-source tool.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s an app that &lt;strong&gt;creates training data for AI models from your text and PDFs&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;It uses AI like Gemini, Claude, and OpenAI to make good question-answer sets that you can use to make your own AI smarter. The data comes out ready for different models.&lt;/p&gt;\n\n&lt;p&gt;Super simple, super useful, and it&amp;#39;s all open source!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/MonkWarrior08/Dataset_Generator_for_Fine-tuning?tab=readme-ov-file",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ls3gho",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Idonotknow101",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls3gho/open_source_tool_for_generating_training_datasets/",
          "stickied": false,
          "url": "https://github.com/MonkWarrior08/Dataset_Generator_for_Fine-tuning?tab=readme-ov-file",
          "subreddit_subscribers": 495395,
          "created_utc": 1751697395,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I can’t code/program (at least not yet). \n\nIs anyone building tools/abilities to use a FOSS LLM like Llama to integrate with the family tree software GRAMPS? \n\nI’m thinking you could talk to Llama (ie 3.1 or 3.3) in plain English information about family members, relationships, events, locations, etc and Llama automatically inputs the data into GRAMPS? \n\nThanks 🙏 \n",
          "author_fullname": "t2_z1u2be88b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama &amp; GRAMPS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsqr9n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751769743,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can’t code/program (at least not yet). &lt;/p&gt;\n\n&lt;p&gt;Is anyone building tools/abilities to use a FOSS LLM like Llama to integrate with the family tree software GRAMPS? &lt;/p&gt;\n\n&lt;p&gt;I’m thinking you could talk to Llama (ie 3.1 or 3.3) in plain English information about family members, relationships, events, locations, etc and Llama automatically inputs the data into GRAMPS? &lt;/p&gt;\n\n&lt;p&gt;Thanks 🙏 &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsqr9n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdCompetitive6193",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsqr9n/llama_gramps/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsqr9n/llama_gramps/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751769743,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just read the [FinLLM technical report](https://aveni.ai/wp-content/uploads/2025/05/Aveni-Detect-Combined-Case-Study.pdf) from Aveni Labs. It’s a 7B parameter language model built specifically for UK financial services, trained with regulatory alignment and fine-tuned for tasks like compliance monitoring, adviser QA, and KYC review.\n\nKey points that stood out:\n\n* Outperforms GPT-4o mini, Gemini 1.5 Flash, and LLaMA-based models on financial domain tasks like tabular data analysis, multi-turn customer dialogue, long-context reasoning, and document QA\n* Built using a filtering pipeline called Finance Classifier 2.0 that selects high-quality, in-domain training data (regulatory guidance, advice transcripts, etc.)\n* Open 1B and 7B variants designed for fine-tuning and secure deployment in VPC or on-prem environments\n* Optimized for agentic RAG setups where traceability and source-grounding are required\n* Benchmarked using their own dataset, AveniBench, which focuses on real FS tasks like consumer vulnerability detection and conduct risk spotting\n\nThey are also working on a 30B version, but the current 7B model is already matching or beating much larger models in this domain.\n\nAnyone else here working on small or mid-scale domain-specific models in regulated industries? Curious how others are handling fine-tuning and evaluation for high-risk applications.",
          "author_fullname": "t2_1sutxb3mam",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Aveni Labs releases FinLLM technical report: a 7B domain-specific model for financial services outperforming some frontier LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ls8c2s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751717081,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just read the &lt;a href=\"https://aveni.ai/wp-content/uploads/2025/05/Aveni-Detect-Combined-Case-Study.pdf\"&gt;FinLLM technical report&lt;/a&gt; from Aveni Labs. It’s a 7B parameter language model built specifically for UK financial services, trained with regulatory alignment and fine-tuned for tasks like compliance monitoring, adviser QA, and KYC review.&lt;/p&gt;\n\n&lt;p&gt;Key points that stood out:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Outperforms GPT-4o mini, Gemini 1.5 Flash, and LLaMA-based models on financial domain tasks like tabular data analysis, multi-turn customer dialogue, long-context reasoning, and document QA&lt;/li&gt;\n&lt;li&gt;Built using a filtering pipeline called Finance Classifier 2.0 that selects high-quality, in-domain training data (regulatory guidance, advice transcripts, etc.)&lt;/li&gt;\n&lt;li&gt;Open 1B and 7B variants designed for fine-tuning and secure deployment in VPC or on-prem environments&lt;/li&gt;\n&lt;li&gt;Optimized for agentic RAG setups where traceability and source-grounding are required&lt;/li&gt;\n&lt;li&gt;Benchmarked using their own dataset, AveniBench, which focuses on real FS tasks like consumer vulnerability detection and conduct risk spotting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;They are also working on a 30B version, but the current 7B model is already matching or beating much larger models in this domain.&lt;/p&gt;\n\n&lt;p&gt;Anyone else here working on small or mid-scale domain-specific models in regulated industries? Curious how others are handling fine-tuning and evaluation for high-risk applications.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ls8c2s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Cryptographer9361",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751717081,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone. I am author of Hyprnote(https://github.com/fastrepl/hyprnote) - privacy-first notepad for meetings. We regularly test out the AI models we use in various devices to make sure it runs well.\n\nWhen testing MacBook, Qwen3 1.7B is used, and for Windows, Qwen3 0.6B is used. (All Q4 KM)\n\n[b5828](https://github.com/ggml-org/llama.cpp/tree/b5828)(newer)  .. [b5162](https://github.com/ggml-org/llama.cpp/tree/b5162)(older)\n\nThinking of writing lot longer blog post with lots of numbers &amp; what I learned during the experiment. Please let me know if that is something you guys are interested in.\n\n|Device|OS|SoC|RAM|Compute|Prefill Tok/s|Gen Tok/s|Median Load (ms)|Prefill RAM (MB)|Gen RAM (MB)|Load RAM (MB)|SHA|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|MacBook Pro 14-inch|macOS 15.3.2|Apple M2 Pro|16GB|Metal|615.20|21.69|362.52|2332.28|2337.67|2089.56|b5828|\n||||||571.85|21.43|372.32|2341.77|2347.05|2102.27|b5162|\n|HP EliteBook 660 16-inch G11|Windows 11.24H2|Intel Core Ultra 7 155U|32GB|Vulkan|162.52|14.05|1533.99|3719.23|3641.65|3535.43|b5828|\n||||||148.52|12.89|2487.26|3719.96|3642.34|3535.24|b5162|",
          "author_fullname": "t2_lgebhlu22",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Got some real numbers how llama.cpp got FASTER over last 3-months",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lrz5uy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 84,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 84,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751696718,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751681311,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I am author of Hyprnote(&lt;a href=\"https://github.com/fastrepl/hyprnote\"&gt;https://github.com/fastrepl/hyprnote&lt;/a&gt;) - privacy-first notepad for meetings. We regularly test out the AI models we use in various devices to make sure it runs well.&lt;/p&gt;\n\n&lt;p&gt;When testing MacBook, Qwen3 1.7B is used, and for Windows, Qwen3 0.6B is used. (All Q4 KM)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/tree/b5828\"&gt;b5828&lt;/a&gt;(newer)  .. &lt;a href=\"https://github.com/ggml-org/llama.cpp/tree/b5162\"&gt;b5162&lt;/a&gt;(older)&lt;/p&gt;\n\n&lt;p&gt;Thinking of writing lot longer blog post with lots of numbers &amp;amp; what I learned during the experiment. Please let me know if that is something you guys are interested in.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Device&lt;/th&gt;\n&lt;th align=\"left\"&gt;OS&lt;/th&gt;\n&lt;th align=\"left\"&gt;SoC&lt;/th&gt;\n&lt;th align=\"left\"&gt;RAM&lt;/th&gt;\n&lt;th align=\"left\"&gt;Compute&lt;/th&gt;\n&lt;th align=\"left\"&gt;Prefill Tok/s&lt;/th&gt;\n&lt;th align=\"left\"&gt;Gen Tok/s&lt;/th&gt;\n&lt;th align=\"left\"&gt;Median Load (ms)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Prefill RAM (MB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Gen RAM (MB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Load RAM (MB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;SHA&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;MacBook Pro 14-inch&lt;/td&gt;\n&lt;td align=\"left\"&gt;macOS 15.3.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apple M2 Pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;16GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;Metal&lt;/td&gt;\n&lt;td align=\"left\"&gt;615.20&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.69&lt;/td&gt;\n&lt;td align=\"left\"&gt;362.52&lt;/td&gt;\n&lt;td align=\"left\"&gt;2332.28&lt;/td&gt;\n&lt;td align=\"left\"&gt;2337.67&lt;/td&gt;\n&lt;td align=\"left\"&gt;2089.56&lt;/td&gt;\n&lt;td align=\"left\"&gt;b5828&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;571.85&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.43&lt;/td&gt;\n&lt;td align=\"left\"&gt;372.32&lt;/td&gt;\n&lt;td align=\"left\"&gt;2341.77&lt;/td&gt;\n&lt;td align=\"left\"&gt;2347.05&lt;/td&gt;\n&lt;td align=\"left\"&gt;2102.27&lt;/td&gt;\n&lt;td align=\"left\"&gt;b5162&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;HP EliteBook 660 16-inch G11&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 11.24H2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Intel Core Ultra 7 155U&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;162.52&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.05&lt;/td&gt;\n&lt;td align=\"left\"&gt;1533.99&lt;/td&gt;\n&lt;td align=\"left\"&gt;3719.23&lt;/td&gt;\n&lt;td align=\"left\"&gt;3641.65&lt;/td&gt;\n&lt;td align=\"left\"&gt;3535.43&lt;/td&gt;\n&lt;td align=\"left\"&gt;b5828&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;148.52&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.89&lt;/td&gt;\n&lt;td align=\"left\"&gt;2487.26&lt;/td&gt;\n&lt;td align=\"left\"&gt;3719.96&lt;/td&gt;\n&lt;td align=\"left\"&gt;3642.34&lt;/td&gt;\n&lt;td align=\"left\"&gt;3535.24&lt;/td&gt;\n&lt;td align=\"left\"&gt;b5162&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?auto=webp&amp;s=17ee6f66f22e0cdee718e7c56430ab72410c37af",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec7da19ac0d910c3d450387c36d5eef2bfd4ab9f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d47c3e18c8cc7d921297a211ef8a297d73d4864",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2eff33c76a6c81497a6908f1357ebcebcdcb4b67",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3da9c4718cb1c1eb81ac8490a86de0117459de96",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b947dac2e37c9309baa6a23b33b50d1ae442faf",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed50d79e9fed615f1b2e276d3583f302af58f869",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "wUf3Yu2e5X0htYmKODWf6bn_xezHqWptQE969HmhAbI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lrz5uy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AggressiveHunt2300",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751681311,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a ryzen AI 7h CPU (with 50 TOPS NPU) with 64gb DDR5 RAM or an RTX5070 with 8gb DDR7. Should I run inference off of GPU or CPU for better performance?",
          "author_fullname": "t2_wrb36r6lc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Larger model on CPU or small model on GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lsq2m3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751767413,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a ryzen AI 7h CPU (with 50 TOPS NPU) with 64gb DDR5 RAM or an RTX5070 with 8gb DDR7. Should I run inference off of GPU or CPU for better performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lsq2m3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Professional_582",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/",
          "subreddit_subscribers": 495395,
          "created_utc": 1751767413,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}