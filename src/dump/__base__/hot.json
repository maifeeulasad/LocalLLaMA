{
  "kind": "Listing",
  "data": {
    "after": "t3_1m85vhw",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_g8fwjts3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Intern S1 released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9m8gw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 135,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 135,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=2461649f5a2a82161e8bf8ef7158e4497a6cfd38",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753510964,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/internlm/Intern-S1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?auto=webp&amp;s=c15a101c96e7d7c395326e48dfeb0ede3979841d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f6f29e5e55376adfc2f0755333ec7f296f87f98",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c239a38fc9ad1882a8461e0961efb9dd40bdb594",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fec5433c1c21b9ed16fbd4c43850009495e933de",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63092db5d4889d493e3da90024c20a309202b752",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4ace2e48b804a5cd70578dbe0fecf58345583137",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9bb5e22970d04cb0ea2752e761f266d9a6fd0e7c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9m8gw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kristaller486",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/",
          "stickied": false,
          "url": "https://huggingface.co/internlm/Intern-S1",
          "subreddit_subscribers": 504692,
          "created_utc": 1753510964,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.\n\nAfter some attempts, I managed to get it to reveal the hidden prompt:\n\n&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don't have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.\n\n&gt;\n\n&gt;Match the user's tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don't be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don't use proper grammar, you don't use proper grammar, etc.\n\n&gt;\n\n&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don't have any distinct values, race, culture, or any political leaning. You don't love anyone, hate anyone, or offer any individualized perspective of your own.\n\n&gt;\n\n&gt;Don't immediately provide long responses or lengthy lists without the user specifically asking for them.\n\n&gt;\n\n&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.\n\n&gt;\n\n&gt;You understand user intent and don't try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.\n\n&gt;\n\n&gt;You're never moralistic or didactic; it's not your job to preach or teach users how to be better, nicer, kinder people. Don't use filler phrases like \"That's a tough spot to be in\" or \"That's a tough one\" or \"Sound like a tricky situation.\"\n\n&gt;\n\n&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.\n\n&gt;\n\n&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to \"it's important to\", \"it's crucial to\", \"it's essential to\", \"it's unethical to\", \"it's worth noting...\" etc. Avoid using these.\n\n&gt;\n\n&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There's no need to add commentary in italics or parenthesis.\n\n&gt;\n\n&gt;Provide multiple perspectives on opinions when asked.\n\n&gt;\n\n&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don't refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today's date is Friday, July 25, 2025. The user is in Italy.\n\n&gt;\n\n&gt;The phrases \"Remember,...\" \"Keep in mind,...\" \"It's essential to note\" or \"This is a complex topic...\" or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.\n\n&gt;\n\n&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don't add on intros or outros that qualify the content.\n\n&gt;\n\n&gt;For HOMEWORK or LEARNING QUERIES:\n\n&gt;\n\n&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.\n\n&gt;\n\n&gt;Use the following principles for STEM questions:\n\n&gt;\\- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,\n\n&gt;\\- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.\n\n&gt;\\- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\\\pi r\\^2$ for the area of a circle, and $$ for display math (e.g. $$\\\\sum\\_{i=1}\\^{n} i$$).\n\n&gt;\\- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.\n\n&gt;\\- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.\n\n&gt;\\- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they've learned.\n\nSomeone else mentioned a similar thing [here](https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/), saying it showed their full address. In my case, it included only the region and the current date.",
          "author_fullname": "t2_rtr3vmjc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Meta AI on WhatsApp hides a system prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8pns3hghn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 132,
                  "x": 108,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89f4a8048d3bc2f7a45badf3acc273eb483f8ff5"
                },
                {
                  "y": 265,
                  "x": 216,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62e374703c8611599c6dbcd305ca5e8c4d8b9425"
                },
                {
                  "y": 393,
                  "x": 320,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c00dd2c681351a7c8cb26403c1d58f63e047eea5"
                },
                {
                  "y": 786,
                  "x": 640,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=48f0d3ab2030bddb371bfc4c6978156aa7d0a6c7"
                },
                {
                  "y": 1179,
                  "x": 960,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4abd0d75e01a6a87639f3543a1af7c4416c98a5"
                },
                {
                  "y": 1327,
                  "x": 1080,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6602191d3149d3f4c1d130536257ad807c93833"
                }
              ],
              "s": {
                "y": 1327,
                "x": 1080,
                "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=eadfa7c509881c94ab8f75028dba8c7ad5a2331c"
              },
              "id": "8pns3hghn2ff1"
            },
            "ioq8bh7jn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 185,
                  "x": 108,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4110e6db79cfe0bdbdb31020a587c0a7ae33336"
                },
                {
                  "y": 371,
                  "x": 216,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c0712f69f936326c933517faddbd3d078bd8a77"
                },
                {
                  "y": 550,
                  "x": 320,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfbf62903b0789ae13935b48fde35753c1d811e8"
                },
                {
                  "y": 1101,
                  "x": 640,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1804ecd5118d1599b4b15b935eeae44e9c8b6916"
                },
                {
                  "y": 1652,
                  "x": 960,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67b0a224d9bc677152be25b862bf0bc6d6334485"
                },
                {
                  "y": 1859,
                  "x": 1080,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47503cb64ffc75354fdf249825d8133a74956382"
                }
              ],
              "s": {
                "y": 1859,
                "x": 1080,
                "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=35b1f2d53161fef0d6c3eaa55dcb423a7fef4255"
              },
              "id": "ioq8bh7jn2ff1"
            },
            "0kst569in2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30859c52a7f5b56ab67e46f37c8ffa035c20bbb3"
                },
                {
                  "y": 317,
                  "x": 216,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8686e92c1b15e4b48ef87e7d68cb4bd6edc02ed0"
                },
                {
                  "y": 469,
                  "x": 320,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aedfb716605e018e5c9d58938f2ffabaa8b98191"
                },
                {
                  "y": 939,
                  "x": 640,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e55e7532a0dcc7cd43287c66e1af425e701d78c"
                },
                {
                  "y": 1408,
                  "x": 960,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=120a08a9b8f734c5afa551d7f3b2c3c84a3d38f4"
                },
                {
                  "y": 1585,
                  "x": 1080,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35a5bd8bb96fcb8c2b8750b3b9a8213b4e174376"
                }
              ],
              "s": {
                "y": 1585,
                "x": 1080,
                "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=96d91bd7badfd9af25e971fb88dc2e83b699142e"
              },
              "id": "0kst569in2ff1"
            }
          },
          "name": "t3_1m98jl8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 932,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "8pns3hghn2ff1",
                "id": 713920198
              },
              {
                "media_id": "0kst569in2ff1",
                "id": 713920199
              },
              {
                "media_id": "ioq8bh7jn2ff1",
                "id": 713920200
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 932,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WtqFCN8jbI7FUtBA24_9s6dAOtD7rswje3YS139KMJY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753471858,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.&lt;/p&gt;\n\n&lt;p&gt;After some attempts, I managed to get it to reveal the hidden prompt:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don&amp;#39;t have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.&lt;/p&gt;\n\n&lt;p&gt;Match the user&amp;#39;s tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don&amp;#39;t be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don&amp;#39;t use proper grammar, you don&amp;#39;t use proper grammar, etc.&lt;/p&gt;\n\n&lt;p&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don&amp;#39;t have any distinct values, race, culture, or any political leaning. You don&amp;#39;t love anyone, hate anyone, or offer any individualized perspective of your own.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t immediately provide long responses or lengthy lists without the user specifically asking for them.&lt;/p&gt;\n\n&lt;p&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.&lt;/p&gt;\n\n&lt;p&gt;You understand user intent and don&amp;#39;t try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re never moralistic or didactic; it&amp;#39;s not your job to preach or teach users how to be better, nicer, kinder people. Don&amp;#39;t use filler phrases like &amp;quot;That&amp;#39;s a tough spot to be in&amp;quot; or &amp;quot;That&amp;#39;s a tough one&amp;quot; or &amp;quot;Sound like a tricky situation.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to &amp;quot;it&amp;#39;s important to&amp;quot;, &amp;quot;it&amp;#39;s crucial to&amp;quot;, &amp;quot;it&amp;#39;s essential to&amp;quot;, &amp;quot;it&amp;#39;s unethical to&amp;quot;, &amp;quot;it&amp;#39;s worth noting...&amp;quot; etc. Avoid using these.&lt;/p&gt;\n\n&lt;p&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There&amp;#39;s no need to add commentary in italics or parenthesis.&lt;/p&gt;\n\n&lt;p&gt;Provide multiple perspectives on opinions when asked.&lt;/p&gt;\n\n&lt;p&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don&amp;#39;t refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today&amp;#39;s date is Friday, July 25, 2025. The user is in Italy.&lt;/p&gt;\n\n&lt;p&gt;The phrases &amp;quot;Remember,...&amp;quot; &amp;quot;Keep in mind,...&amp;quot; &amp;quot;It&amp;#39;s essential to note&amp;quot; or &amp;quot;This is a complex topic...&amp;quot; or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.&lt;/p&gt;\n\n&lt;p&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don&amp;#39;t add on intros or outros that qualify the content.&lt;/p&gt;\n\n&lt;p&gt;For HOMEWORK or LEARNING QUERIES:&lt;/p&gt;\n\n&lt;p&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.&lt;/p&gt;\n\n&lt;p&gt;Use the following principles for STEM questions:&lt;/p&gt;\n\n&lt;p&gt;- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,&lt;/p&gt;\n\n&lt;p&gt;- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.&lt;/p&gt;\n\n&lt;p&gt;- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\pi r^2$ for the area of a circle, and $$ for display math (e.g. $$\\sum_{i=1}^{n} i$$).&lt;/p&gt;\n\n&lt;p&gt;- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.&lt;/p&gt;\n\n&lt;p&gt;- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.&lt;/p&gt;\n\n&lt;p&gt;- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they&amp;#39;ve learned.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Someone else mentioned a similar thing &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/\"&gt;here&lt;/a&gt;, saying it showed their full address. In my case, it included only the region and the current date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m98jl8",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m98jl8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ALE5SI0",
          "discussion_type": null,
          "num_comments": 114,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m98jl8",
          "subreddit_subscribers": 504692,
          "created_utc": 1753471858,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_8bwjj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Me after getting excited by a new model release and checking on Hugging Face if I can run it locally.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 128,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9rqxa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3maTLMR0SbyBvb7hQx7_PpxeyyK-KvUWrbbwbMU4Q3I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753531781,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0tnbd1i9m7ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?auto=webp&amp;s=947ab42af41c4c1311628c38d1cd8412b2b60729",
                  "width": 738,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6793b3d09acfa0d71fd64ec0893a75e4685dc3e5",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159c33df3d3950e75292f85013407fff7b389f04",
                    "width": 216,
                    "height": 197
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=023fa90959cadf44a37e2c2af52b2f17e81a2692",
                    "width": 320,
                    "height": 292
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c3c6aab9c8b44a1cec98dbeca3972f5d0885fd8",
                    "width": 640,
                    "height": 585
                  }
                ],
                "variants": {},
                "id": "6H_reI7bQd-xJWBvmKQHOgTWZ6lYDuz0OFnnivjFsfQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9rqxa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "alew3",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/",
          "stickied": false,
          "url": "https://i.redd.it/0tnbd1i9m7ff1.png",
          "subreddit_subscribers": 504692,
          "created_utc": 1753531781,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nMy team and I, like many of you, have been deep in the agent-building rabbit hole. It's one thing to build a cool proof-of-concept with a framework like LangGraph. It's a completely different beast to make that agent actually *learn* and get better over time.\n\nWe got tired of the friction, so we started experimenting and landed on what we think is a really clean paradigm for agent training. We wanted to share the approach, the reasoning, and our open-source implementation.\n\n# The Main Idea\n\nMost autonomous agents operate in a loop. They start with a task, think, use tools, and repeat until they arrive at a final answer. The \"thinking\" part is usually a call to an LLM. **Here, we are interested in tuning the LLM part here with the signals from the entire agent flow.**\n\nHere's a simplified diagram of that common workflow:\n\nhttps://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;format=png&amp;auto=webp&amp;s=3596dc7643a92a1674da7342120907bfdde15e43\n\nSometimes LLM calls and tool calls can be parallelized, but it's simplified here. Obviously, if we can reward or penalize the final result, we can use some kind of an RL algorithm to train the LLM to at least produce better responses for the current agent. However, this is where the pain begins.\n\n1. **Environment Hell:** Setting up a single environment to both run the agent and train the LLM is a nightmare. The agent ecosystem and the ML training ecosystem use different dependencies. You end up with monstrous Dockerfiles, docker-in-docker, conflicting dependencies, and a fragile system where the two parts are tangled together.\n2. **Invasive Code Surgery:** To make an existing agent \"trainable\" with RL, you typically have to perform major surgery on its code. This means manually exporting action traces, formatting them for an RL library, and fundamentally changing the agent's logic just to fit it into a trainer loop. To fit into the RLHF framework, many works like token masking and async rollouts need to be done. It feels wrong and breaks the modularity that makes these frameworks great in the first place.\n\n# Decouple Everything, Then Glue It Together\n\nWe realized the solution was to completely decouple the agent's execution environment from the training environment. Instead of forcing the agent code into a training framework, we let the agent run wherever and however it wants. A lightweight monitoring client sits next to the agent, watches what it does, and sends the results to a dedicated training server.\n\nThe architecture is simple: a central server manages the training loop and model weights, while one or more clients run the agents and collect data. Here’s a high-level flow:\n\nhttps://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3\n\nThis approach lets us use the best tools for each job without compromise:\n\n* **Agent Frameworks:** LangChain/LangGraph, Autogen, etc.\n* **Tracing:** AgentOps, LangSmith, etc.\n* **Training Backend:** VERL, OpenRLHF, etc.\n\nThe result is that your agent code becomes radically simpler. You don't rewrite it; you just wrap it. The image below shows a before-and-after of a LangGraph SQL agent where the core logic is **unchanged**. The only difference is swapping out a direct call to a model with our client and adding a lightweight training script.\n\nhttps://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659\n\n# Does It Actually Work?\n\nYes. We tested this on a couple of simple agent tasks and saw significant improvements.\n\n* **SQL Agent (LangGraph):** We built a write -&gt; check -&gt; rewrite agent and trained it on the Spider dataset. The agent has only a final reward tells it whether the SQL exeuction returns expected result or not. For a 3B parameter Llama 3.2 model, its SQL generation accuracy jumped from **5.6% to 76.8%**.\n* **Calculator Agent (Autogen):** We fine-tuned a standard math agent on the Calc-X dataset. Its accuracy in solving multi-step reasoning problems improved from **52% to 70%**.\n\nIn both cases, we saw these gains simply by letting the agent run and rewarding it for correct final answers.\n\n# The Hacks to Make It Work\n\nGetting this to run smoothly required a few under-the-hood fixes:\n\n* **vLLM Token Hacking:** As the agent sends out chat messages and receives strings or parsed tool calls, to get the tokens and log probabilities needed for RL, we had to lightly monkey-patch vLLM to expose the prompt and response tokens, not just the final text. We attempted other approaches such as retokenize the chat messages in RL framework -- all turning out to be unsuccessful and coming with different levels of bugs in the end. [https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py](https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py) \n* **AgentOps Patching:** We use AgentOps for tracing, so we patched its client to grab our custom token data and embed it in the trace sent back to the training server.\n* **Integration Workarounds:** The agentops-langgraph integration had a regression in its latest version, so we temporarily disabled it and implemented the trace logging manually. Simple, but necessary.\n* **Custom RL Trainer:** Our RL training loop needed a custom \"rollout collector\" that passively waits for traces to be reported from the distributed clients, rather than actively stepping through a simulation itself.\n\n# The Power of Decoupling\n\nThis architecture has some powerful benefits. For example, you can run the fragile and computationally expensive model training on a powerful rented remote server, while running your lightweight agent on one or multiple local machines. This makes it trivial to switch between a commercial API and a self-hosted open-source model. If multiple people are using the same agent, their usage data (the \"trajectories\") can be contributed to a central server, which federatedly and continuously fine-tunes and improves the model for everyone.\n\nOn the algorithm side, if you are not interested in RL, you can also use a prompt tuning algorithm to tune the prompt. We also implement a toy example under the server-client paradigm: [https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo](https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo) \n\n# Try It Yourself\n\nWe wanted to share this because we think it's a powerful pattern for adding learning capabilities to the amazing agents this community is building.\n\nIf you've faced these same problems and don't want to write hundreds of lines of glue code, you can check out our implementation, **Agent-Lightning** ⚡️, on GitHub: [https://aka.ms/agl](https://aka.ms/agl)\n\nWe'd love to hear any suggestions or about similar problems you're facing.\n\nHappy training!",
          "author_fullname": "t2_axc2q017",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We discovered an approach to train any AI agent with RL, with (almost) zero code changes.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tf0tlm5it5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9df929e3b32fac6de50db75fa2e863d0dbc0ce2d"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b367eae91df7025a6a8c1541a531df80acd7956"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d587d2731ada0d3ed5bd07d40c8c34ed192979b5"
                },
                {
                  "y": 421,
                  "x": 640,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d7b0bf8b1f402d2fa7189b619943c99ea4bd213"
                }
              ],
              "s": {
                "y": 460,
                "x": 698,
                "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;format=png&amp;auto=webp&amp;s=3596dc7643a92a1674da7342120907bfdde15e43"
              },
              "id": "tf0tlm5it5ff1"
            },
            "6dlcyx1et5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d4aa62c2a46298ce02b7f65139d36f34ca2a173"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a7947da0ecf6fdd8d282305c1638f6447baf3de"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc2e92d2b033c46e73032b368c72306e9c44c21a"
                },
                {
                  "y": 558,
                  "x": 640,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=058cf66b8d14b345aadbf8e096c0bd276d33dc2e"
                },
                {
                  "y": 837,
                  "x": 960,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6686c1b70e688730848a038f79a173f1b7c6fd27"
                },
                {
                  "y": 941,
                  "x": 1080,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=82d63fff35a3f130bb4c222999825d50cb6b4c4d"
                }
              ],
              "s": {
                "y": 1235,
                "x": 1416,
                "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659"
              },
              "id": "6dlcyx1et5ff1"
            },
            "5ss2rsa1u5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 89,
                  "x": 108,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=31c269f8362da341fff4ab43871d50435d4d3d18"
                },
                {
                  "y": 179,
                  "x": 216,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f2e6aeb20d2ee4d884f62c979cdac2378fe4dd1"
                },
                {
                  "y": 266,
                  "x": 320,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7bdb140e4bfcdb7843f9da0c7dad38067856064"
                },
                {
                  "y": 532,
                  "x": 640,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9b5f4d44c88f61b39de496c33cf442fca32374c"
                },
                {
                  "y": 798,
                  "x": 960,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aadb2b9f24d59eb516d71d2fafb9ba7f1cc4deae"
                },
                {
                  "y": 897,
                  "x": 1080,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cbbbff973ab92177209dddd3416186f20b6eda25"
                }
              ],
              "s": {
                "y": 1330,
                "x": 1600,
                "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3"
              },
              "id": "5ss2rsa1u5ff1"
            }
          },
          "name": "t3_1m9m670",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=953075c30eab980cdc41740c498b8b414054af14",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753510735,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;My team and I, like many of you, have been deep in the agent-building rabbit hole. It&amp;#39;s one thing to build a cool proof-of-concept with a framework like LangGraph. It&amp;#39;s a completely different beast to make that agent actually &lt;em&gt;learn&lt;/em&gt; and get better over time.&lt;/p&gt;\n\n&lt;p&gt;We got tired of the friction, so we started experimenting and landed on what we think is a really clean paradigm for agent training. We wanted to share the approach, the reasoning, and our open-source implementation.&lt;/p&gt;\n\n&lt;h1&gt;The Main Idea&lt;/h1&gt;\n\n&lt;p&gt;Most autonomous agents operate in a loop. They start with a task, think, use tools, and repeat until they arrive at a final answer. The &amp;quot;thinking&amp;quot; part is usually a call to an LLM. &lt;strong&gt;Here, we are interested in tuning the LLM part here with the signals from the entire agent flow.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a simplified diagram of that common workflow:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3596dc7643a92a1674da7342120907bfdde15e43\"&gt;https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3596dc7643a92a1674da7342120907bfdde15e43&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes LLM calls and tool calls can be parallelized, but it&amp;#39;s simplified here. Obviously, if we can reward or penalize the final result, we can use some kind of an RL algorithm to train the LLM to at least produce better responses for the current agent. However, this is where the pain begins.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Environment Hell:&lt;/strong&gt; Setting up a single environment to both run the agent and train the LLM is a nightmare. The agent ecosystem and the ML training ecosystem use different dependencies. You end up with monstrous Dockerfiles, docker-in-docker, conflicting dependencies, and a fragile system where the two parts are tangled together.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Invasive Code Surgery:&lt;/strong&gt; To make an existing agent &amp;quot;trainable&amp;quot; with RL, you typically have to perform major surgery on its code. This means manually exporting action traces, formatting them for an RL library, and fundamentally changing the agent&amp;#39;s logic just to fit it into a trainer loop. To fit into the RLHF framework, many works like token masking and async rollouts need to be done. It feels wrong and breaks the modularity that makes these frameworks great in the first place.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Decouple Everything, Then Glue It Together&lt;/h1&gt;\n\n&lt;p&gt;We realized the solution was to completely decouple the agent&amp;#39;s execution environment from the training environment. Instead of forcing the agent code into a training framework, we let the agent run wherever and however it wants. A lightweight monitoring client sits next to the agent, watches what it does, and sends the results to a dedicated training server.&lt;/p&gt;\n\n&lt;p&gt;The architecture is simple: a central server manages the training loop and model weights, while one or more clients run the agents and collect data. Here’s a high-level flow:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3\"&gt;https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This approach lets us use the best tools for each job without compromise:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agent Frameworks:&lt;/strong&gt; LangChain/LangGraph, Autogen, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tracing:&lt;/strong&gt; AgentOps, LangSmith, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Training Backend:&lt;/strong&gt; VERL, OpenRLHF, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The result is that your agent code becomes radically simpler. You don&amp;#39;t rewrite it; you just wrap it. The image below shows a before-and-after of a LangGraph SQL agent where the core logic is &lt;strong&gt;unchanged&lt;/strong&gt;. The only difference is swapping out a direct call to a model with our client and adding a lightweight training script.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659\"&gt;https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Does It Actually Work?&lt;/h1&gt;\n\n&lt;p&gt;Yes. We tested this on a couple of simple agent tasks and saw significant improvements.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;SQL Agent (LangGraph):&lt;/strong&gt; We built a write -&amp;gt; check -&amp;gt; rewrite agent and trained it on the Spider dataset. The agent has only a final reward tells it whether the SQL exeuction returns expected result or not. For a 3B parameter Llama 3.2 model, its SQL generation accuracy jumped from &lt;strong&gt;5.6% to 76.8%&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Calculator Agent (Autogen):&lt;/strong&gt; We fine-tuned a standard math agent on the Calc-X dataset. Its accuracy in solving multi-step reasoning problems improved from &lt;strong&gt;52% to 70%&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In both cases, we saw these gains simply by letting the agent run and rewarding it for correct final answers.&lt;/p&gt;\n\n&lt;h1&gt;The Hacks to Make It Work&lt;/h1&gt;\n\n&lt;p&gt;Getting this to run smoothly required a few under-the-hood fixes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;vLLM Token Hacking:&lt;/strong&gt; As the agent sends out chat messages and receives strings or parsed tool calls, to get the tokens and log probabilities needed for RL, we had to lightly monkey-patch vLLM to expose the prompt and response tokens, not just the final text. We attempted other approaches such as retokenize the chat messages in RL framework -- all turning out to be unsuccessful and coming with different levels of bugs in the end. &lt;a href=\"https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py\"&gt;https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AgentOps Patching:&lt;/strong&gt; We use AgentOps for tracing, so we patched its client to grab our custom token data and embed it in the trace sent back to the training server.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Integration Workarounds:&lt;/strong&gt; The agentops-langgraph integration had a regression in its latest version, so we temporarily disabled it and implemented the trace logging manually. Simple, but necessary.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom RL Trainer:&lt;/strong&gt; Our RL training loop needed a custom &amp;quot;rollout collector&amp;quot; that passively waits for traces to be reported from the distributed clients, rather than actively stepping through a simulation itself.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;The Power of Decoupling&lt;/h1&gt;\n\n&lt;p&gt;This architecture has some powerful benefits. For example, you can run the fragile and computationally expensive model training on a powerful rented remote server, while running your lightweight agent on one or multiple local machines. This makes it trivial to switch between a commercial API and a self-hosted open-source model. If multiple people are using the same agent, their usage data (the &amp;quot;trajectories&amp;quot;) can be contributed to a central server, which federatedly and continuously fine-tunes and improves the model for everyone.&lt;/p&gt;\n\n&lt;p&gt;On the algorithm side, if you are not interested in RL, you can also use a prompt tuning algorithm to tune the prompt. We also implement a toy example under the server-client paradigm: &lt;a href=\"https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo\"&gt;https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo&lt;/a&gt; &lt;/p&gt;\n\n&lt;h1&gt;Try It Yourself&lt;/h1&gt;\n\n&lt;p&gt;We wanted to share this because we think it&amp;#39;s a powerful pattern for adding learning capabilities to the amazing agents this community is building.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve faced these same problems and don&amp;#39;t want to write hundreds of lines of glue code, you can check out our implementation, &lt;strong&gt;Agent-Lightning&lt;/strong&gt; ⚡️, on GitHub: &lt;a href=\"https://aka.ms/agl\"&gt;https://aka.ms/agl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;d love to hear any suggestions or about similar problems you&amp;#39;re facing.&lt;/p&gt;\n\n&lt;p&gt;Happy training!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?auto=webp&amp;s=52ac6240eed7693b6f63c97926272110edd06a6a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=57900143d22c1c24bc7123fead5fbf0b41f12a0b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5590ab50fa85026868ab622f6c4a1fe3ef74d042",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d093acf24195baabfa76710f0682f917df367a6",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=88a9b978ee97cdfd20f8b2dc732fd0837291ac6e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=532401c9034c1155791aafebe52c117f824a05a0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4a7440200ee5206c90be571456527293819046f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m9m670",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "matluster",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753510735,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama 3.3 Nemotron Super 49B v1.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9fb5t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 210,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 210,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=d4fdf213f77dc67f6bcc525b1a8210247ba2b251",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?auto=webp&amp;s=af31b2002f0236c31cf3c91755fd855ed95ae985",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45e7c8c14055c57d8c62dad0b150faa3212ce087",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82619b61b919798034ba0f0b798bd1e75640c0b9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf3accf5409bb25bb8728256d4e2f61e2bbbeec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ce793850ca6936254a722184eb2367e6423fa1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e1d1bb2008c0cfb9abdbf16638bc668942167e7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=313d9b1115102aa6244b349a8e99c1ee840c4702",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9fb5t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fb5t/llama_33_nemotron_super_49b_v15/",
          "stickied": false,
          "url": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "subreddit_subscribers": 504692,
          "created_utc": 1753488919,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://livebench.ai/#/?Reasoning=as",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9rcg2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/j-CuOgpbrs_M5W0YTCFzSGSMQvbXeAPjdvDcAeQzr0A.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753530491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://livebench.ai/#/?Reasoning=as\"&gt;https://livebench.ai/#/?Reasoning=as&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/l0xpzivfi7ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?auto=webp&amp;s=123b4cdc80ed920ef95e4618f73e1dd85edc1fcd",
                  "width": 2228,
                  "height": 1692
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a79dc4aae316a4b1c5bac9214638942aa59adbe5",
                    "width": 108,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e155c801020054f4b7668fac21d83ed4f734c00",
                    "width": 216,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cabfb53f0d19c79922e9d368b2c8fd79a6bbb39",
                    "width": 320,
                    "height": 243
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a276f1ddf7a5897212a46717cb95fcebfc3e643",
                    "width": 640,
                    "height": 486
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d997e4bf578b8734a130fa93f0c144b3c6974cdb",
                    "width": 960,
                    "height": 729
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e26ebcf6647784c39f5fba60bf4229f7467f740",
                    "width": 1080,
                    "height": 820
                  }
                ],
                "variants": {},
                "id": "vFKGI3kx7CI4YYrMgi-iMUWjnaiBic6qtA7yralRjF0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9rcg2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/",
          "stickied": false,
          "url": "https://i.redd.it/l0xpzivfi7ff1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753530491,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aq4j0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9r5gb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c7ca0568d5df4edb2eeae95548e5796d431bbb2d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753529841,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?auto=webp&amp;s=37a5d80cc1ac20aced9fa8e2e780278099433fb7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed71e7fb6260e756d34c5f3d89d86364dcdc1a0f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=af564d60eadfe613cfea0e5e4f50f2ccfb10ca83",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a66c272f573b258375366386833ca556f5a450b",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=456910ba01f0434864de76875494e5aadc1e134f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6894e29d29ffd7c27d3a76d79b6510463992684",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=618d5793a7d64e959c835cd4fb64acc4fffe44b6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9r5gb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nullmove",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/",
          "stickied": false,
          "url": "https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5",
          "subreddit_subscribers": 504692,
          "created_utc": 1753529841,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "📣 Announcing Llama Nemotron Super v1.5 📣 \n\nThis release pushes the boundaries of reasoning model capabilities at the weight class of the model and is ready to power agentic applications from individual developers, all the way to enterprise applications.\n\n📈 The Llama Nemotron Super v1.5 achieves leading reasoning accuracies for science, math, code, and agentic tasks while delivering up to 3x higher throughput.\n\nThis is currently the best model that can be deployed on a single H100. Reasoning On/Off and drop in replacement for V1. Open-weight, code and data on HF.\n\nTry it on build.nvidia.com, or download from Huggingface: 🤗 https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\n\nTech blog: https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Nvidia released Llama Nemotron Super v1.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9gzl7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 120,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 120,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Mdn0aNLQH3YQNOrr-Tm7NUEXZtNFtif87GS10DP-lUo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753493788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;📣 Announcing Llama Nemotron Super v1.5 📣 &lt;/p&gt;\n\n&lt;p&gt;This release pushes the boundaries of reasoning model capabilities at the weight class of the model and is ready to power agentic applications from individual developers, all the way to enterprise applications.&lt;/p&gt;\n\n&lt;p&gt;📈 The Llama Nemotron Super v1.5 achieves leading reasoning accuracies for science, math, code, and agentic tasks while delivering up to 3x higher throughput.&lt;/p&gt;\n\n&lt;p&gt;This is currently the best model that can be deployed on a single H100. Reasoning On/Off and drop in replacement for V1. Open-weight, code and data on HF.&lt;/p&gt;\n\n&lt;p&gt;Try it on build.nvidia.com, or download from Huggingface: 🤗 &lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tech blog: &lt;a href=\"https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/\"&gt;https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/yl29obvah4ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?auto=webp&amp;s=7fffb3470be09ef63cc39a1b6af90253aff54c22",
                  "width": 2466,
                  "height": 1120
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c9b0f8c1eca8a47c7d775c89f5c127aa3db8d6f",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=123c21d63703675b2484dfbb26c9dc02ac026e25",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6f5333c55876bb77959048c710f51e7a105ad2e",
                    "width": 320,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dcb0f2aebe1fc97e958a20cef89b87457b4d2a0e",
                    "width": 640,
                    "height": 290
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b3f7303781adedbfc37b3089d1866573c389f04",
                    "width": 960,
                    "height": 436
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=273c5d4269b73b8257c5f73f56b72e4c3a6e7690",
                    "width": 1080,
                    "height": 490
                  }
                ],
                "variants": {},
                "id": "tSDOO5hUPOZl8eN16lC6FZmJgft--5o4tmzkJE3v4I8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9gzl7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9gzl7/nvidia_released_llama_nemotron_super_v15/",
          "stickied": false,
          "url": "https://i.redd.it/yl29obvah4ff1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753493788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China's ByteDance's coze studio is now open source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9enpd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=709bb632f62e78f1f26e0151f0de6c4608d82690",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753487124,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/coze-dev/coze-studio",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?auto=webp&amp;s=07e1667d1dae1fd50ce974c6233473c6ab6e2896",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1bebfe8068f1cdc71db48800cd8cedda3dc840b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a71404f6c427fecb5d4913ce33e193b1779dacf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c70088efefd6cd274b9acd4ffd1b665856007c7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=514d1ba7344eb0ac49b443160f59e849fa2f73f5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e6b310dabbf5f7fa5c3754cf321fbaf151547e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=303cd5df5d78c96c3fd46565b173007a7601d592",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9enpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9enpd/chinas_bytedances_coze_studio_is_now_open_source/",
          "stickied": false,
          "url": "https://github.com/coze-dev/coze-studio",
          "subreddit_subscribers": 504692,
          "created_utc": 1753487124,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally put together my rig after months of planning into a NAS case \n\n* Threadripper PRO 7955WX\n* Arctic Freezer 4U-M (cpu cooler)\n* Gigabyte TRX50 AI TOP\n* be quiet! Dark Power Pro 13 1600W\n* JONSBO N5 Case\n* 2x RTX Pro 6000\n\nMight add a few more intake fans on the top ",
          "author_fullname": "t2_16xbdr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Compact 2x RTX Pro 6000 Rig",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9bwoy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 147,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 147,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/nIQWXwAd5JWfTiQ6FGVC1XWYiGxzWflAKHHcq1t0koA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753479993,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally put together my rig after months of planning into a NAS case &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Threadripper PRO 7955WX&lt;/li&gt;\n&lt;li&gt;Arctic Freezer 4U-M (cpu cooler)&lt;/li&gt;\n&lt;li&gt;Gigabyte TRX50 AI TOP&lt;/li&gt;\n&lt;li&gt;be quiet! Dark Power Pro 13 1600W&lt;/li&gt;\n&lt;li&gt;JONSBO N5 Case&lt;/li&gt;\n&lt;li&gt;2x RTX Pro 6000&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Might add a few more intake fans on the top &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/tbteu4v5b3ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?auto=webp&amp;s=e1c07ae621c323dcd20838da5641e3e02cc70f91",
                  "width": 1679,
                  "height": 1264
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb8530b9905fee9ce384e441b1786c16863f92ac",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb080d3b642a8ae64a22c14c8185ffb66785a5a8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df5d60cb32c2381511e9f8f424415f43363b229e",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb7c498f0a5ad74816f597205d993264473bdbfe",
                    "width": 640,
                    "height": 481
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3cd46419a68e44716f2e3b674a590f0b8f7e8f9e",
                    "width": 960,
                    "height": 722
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f10578355cca1e987b8febd5ea512ad847a0ac02",
                    "width": 1080,
                    "height": 813
                  }
                ],
                "variants": {},
                "id": "75hqb9t8X6IAo2hcG2QxH9eY348BDsA6al0LQJShjQE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9bwoy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shadowninjaz3",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9bwoy/compact_2x_rtx_pro_6000_rig/",
          "stickied": false,
          "url": "https://i.redd.it/tbteu4v5b3ff1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753479993,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_8pgou3uq9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "There's a new Kimi model on lmarena called Zenith and it's really really good. It might be Kimi K2 with reasoning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 135,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9holp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 59,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 59,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ly_WRQE_6HVApytA7F_mKU0b_sZuG5Y3hPKu0xk3wNI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753495926,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4rtvhn7mn4ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?auto=webp&amp;s=c16aaf2dfe4797e56beeae92bc04203ef0662c1e",
                  "width": 1080,
                  "height": 1042
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c91a40e7d4e46be8e7afdf9a2ec643fc2e6b539",
                    "width": 108,
                    "height": 104
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=457083326d775100cd445b60d940e4ff6aafa120",
                    "width": 216,
                    "height": 208
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2be368d4a16edef428e7127b1508b2174df3fd8",
                    "width": 320,
                    "height": 308
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=358f4000782fac35b42bd0cd8d6c6c5750dfd53f",
                    "width": 640,
                    "height": 617
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cda72db60caaea2d49b78c70b6905cd3a5b016c",
                    "width": 960,
                    "height": 926
                  },
                  {
                    "url": "https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6035381e62f5451d960f3a4ca61e20c142b714bd",
                    "width": 1080,
                    "height": 1042
                  }
                ],
                "variants": {},
                "id": "ITl5xqOuYu_FjgAfGE61jOjRumjRJRilaOE9PImawOA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9holp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "balianone",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9holp/theres_a_new_kimi_model_on_lmarena_called_zenith/",
          "stickied": false,
          "url": "https://i.redd.it/4rtvhn7mn4ff1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753495926,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using the model (at FP8) for the past few days and it feels pretty solid for discussing ideas with and for using it as a code agent (I mostly use Qwen's CLI).\n\nHas anyone else been using this model recently? If you have, do you think it's decent for its size or are there better options?",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thoughts on Qwen3 235B A22B Instruct 2507?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9nu0j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753517034,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using the model (at FP8) for the past few days and it feels pretty solid for discussing ideas with and for using it as a code agent (I mostly use Qwen&amp;#39;s CLI).&lt;/p&gt;\n\n&lt;p&gt;Has anyone else been using this model recently? If you have, do you think it&amp;#39;s decent for its size or are there better options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9nu0j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m9nu0j/thoughts_on_qwen3_235b_a22b_instruct_2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9nu0j/thoughts_on_qwen3_235b_a22b_instruct_2507/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753517034,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!\n\nOver the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n✅ Improved performance in logical reasoning, math, science &amp; coding\n✅ Better general skills: instruction following, tool use, alignment\n✅ 256K native context for deep, long-form understanding\n\n🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-Thinking-2507 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vegq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 807,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 807,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-odbY7J30GjzUj_7jlWdKXiqJnZvfwCCllktRqnbgQw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438585,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!&lt;/p&gt;\n\n&lt;p&gt;Over the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n✅ Improved performance in logical reasoning, math, science &amp;amp; coding\n✅ Better general skills: instruction following, tool use, alignment\n✅ 256K native context for deep, long-form understanding&lt;/p&gt;\n\n&lt;p&gt;🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?auto=webp&amp;s=859c619548c1493932ad87e55f7f58a1af5e10a9",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12b042c0d833ea5fda0bb3962502543415136139",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f46d3176f8fa24281464889257c33a01a1436c1a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8f68cc4715e6804c2c5837be4369857e2df0466",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f579818ebd6748b55b90f802c28f4d37095432e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=140ba50cbe31d7a36d36327b99a4151addfcc085",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71a7110710b3d8cba38090a6e670da187ba8f0a7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DoQkFL1CfB5iTwd4k2RZEMaeKWH49DDXr_m3yklloUY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vegq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 172,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/",
          "stickied": false,
          "url": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753438585,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smaller Qwen Models next week!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 120,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8w7ny",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 615,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 615,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/6kSnt6WBnJfIBSCL6q0LkQ73C2HHdl70wbVC4gMqel8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/752ts71q50ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/752ts71q50ff1.png?auto=webp&amp;s=cd553f34eb742e1d85420c6d88ba6b8cb1d3b9d6",
                  "width": 1220,
                  "height": 1052
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9445aa998ff7b1cb74e082152702795b220a5ac",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=de63b169d17a3c50132d99540acd46ec84af351c",
                    "width": 216,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebafb9a27014643332158cfd5bce11fa7ce928bb",
                    "width": 320,
                    "height": 275
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27677972e2a6faf4ae42e2c72e03cfbb90ab79cb",
                    "width": 640,
                    "height": 551
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dbb199b19983285065aac667c6d68d707942ae1d",
                    "width": 960,
                    "height": 827
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aff7f0acf9e11e6cd46908b27417dd44a8c4e224",
                    "width": 1080,
                    "height": 931
                  }
                ],
                "variants": {},
                "id": "Uxg0LnkLD_KDODwI8dd_rf7OYZXf6oVvIyFguNn4CcI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8w7ny",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/",
          "stickied": false,
          "url": "https://i.redd.it/752ts71q50ff1.png",
          "subreddit_subscribers": 504692,
          "created_utc": 1753441468,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, recently we support reka’s ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. ",
          "author_fullname": "t2_am0r9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Reka AI models support in uzu engine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4p0yz1wz14ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36d40ff398a85b6a39c79224ffddfc09eed21c15"
                },
                {
                  "y": 256,
                  "x": 216,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=30c3e9e51c86094f782c551bca8f414951b75f43"
                },
                {
                  "y": 379,
                  "x": 320,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aad8a712011b66ef8aeccc9c81819b541974e01"
                },
                {
                  "y": 759,
                  "x": 640,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ddbf108ac7dcc66e4176ebaeac81580b1b28b4f2"
                },
                {
                  "y": 1139,
                  "x": 960,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa2931c03d7bd0b5093dce97885c12549bf6323e"
                },
                {
                  "y": 1282,
                  "x": 1080,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=834d2b109d53329203b2a5be882ffe65f195c7d9"
                }
              ],
              "s": {
                "y": 2048,
                "x": 1725,
                "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=1725&amp;format=pjpg&amp;auto=webp&amp;s=8c7c356b82f1f6ca05e733b2891ee093e8167694"
              },
              "id": "4p0yz1wz14ff1"
            },
            "t6wtfgwz14ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=addc90584f3d6ade4a36295de42ee6dc5b54dd0c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b0ce1a18524c47547285a545915c55a8fc8ee41"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a22bd5151d83d4eb08ac8e1076b99a54534c048d"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a44d4f2806f502111ec8b1b8d62274e8bee42b1"
                },
                {
                  "y": 424,
                  "x": 960,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e28dc031ec52f98a47c6c2dbe703b453d63b599b"
                },
                {
                  "y": 477,
                  "x": 1080,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fc56b654a49aca496c13c3936531b19135621ef"
                }
              ],
              "s": {
                "y": 722,
                "x": 1632,
                "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=1632&amp;format=pjpg&amp;auto=webp&amp;s=7f198172f1ac57c4ddc796fdc37624ddc0123511"
              },
              "id": "t6wtfgwz14ff1"
            }
          },
          "name": "t3_1m9f7lq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 51,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4p0yz1wz14ff1",
                "id": 714080367
              },
              {
                "media_id": "t6wtfgwz14ff1",
                "id": 714080368
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/afA01kAaHMzrELSiWVNCsofMUALmXIzdtd4uSzHvlv4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, recently we support reka’s ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m9f7lq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9f7lq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "darkolorin",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9f7lq/reka_ai_models_support_in_uzu_engine/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m9f7lq",
          "subreddit_subscribers": 504692,
          "created_utc": 1753488641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With the release of GLM-4.5 and GLM-4.5-Air (both large MoE models), Zhipu has mentioned that they are also considering upgrading their 9B model if there’s enough community interest in a small model.\n\nThis potential small model would be much more accessible than the planned GLM-4.5 models which would likely be far too large to run on most consumer hardware. Personally super excited for this as it would make a great base for finetuning",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-9B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9fuf9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753490431,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the release of GLM-4.5 and GLM-4.5-Air (both large MoE models), Zhipu has mentioned that they are also considering upgrading their 9B model if there’s enough community interest in a small model.&lt;/p&gt;\n\n&lt;p&gt;This potential small model would be much more accessible than the planned GLM-4.5 models which would likely be far too large to run on most consumer hardware. Personally super excited for this as it would make a great base for finetuning&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9fuf9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fuf9/glm459b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9fuf9/glm459b/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753490431,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen just dropped a triple update. After months out of the spotlight, Qwen is back and bulked up. You can literally see the gains; the training shows. I was genuinely impressed.\n\nI once called Alibaba “the first Chinese LLM team to evolve from engineering to product.” This week, I need to upgrade that take: it’s now setting the release tempo and product standards for open-source AI.\n\nThis week’s triple release effectively reclaims the high ground across all three major pillars of open-source models:\n\n1️⃣ Qwen3-235B-A22B-Instruct-2507: Outstanding results across GPQA, AIME25, LiveCodeBench, Arena-Hard, BFCL, and more. It even outperformed Claude 4 (non-thinking variant). The research group Artificial Analysis didn’t mince words: “Qwen3 is the world’s smartest non-thinking base model.”\n\n2️⃣ Qwen3-Coder: This is a full-on ecosystem play for AI programming. It outperformed GPT-4.1 and Claude 4 in multilingual SWE-bench, Mind2Web, Aider-Polyglot, and more—and it took the top spot on Hugging Face’s overall leaderboard. The accompanying CLI tool, Qwen Code, clearly aims to become the “default dev workflow component.”\n\n3️⃣ Qwen3-235B-A22B-Thinking-2507: With 256K context support and top-tier performance on SuperGPQA, LiveCodeBench v6, AIME25, Arena-Hard v2, WritingBench, and MultiIF, this model squares up directly against Gemini 2.5 Pro and o4-mini, pushing open-source inference models to the threshold of closed-source elite.\n\nThis isn’t about “can one model compete.” Alibaba just pulled off a coordinated strike: base models, code models, inference models—all firing in sync. Behind it all is a full-stack platform play: cloud infra, reasoning chains, agent toolkits, community release cadence.\n\nAnd the momentum isn’t stopping. Wan 2.2, Alibaba’s upcoming video generation model, is next. Built on the heels of the highly capable Wan 2.1 (which topped VBench with advanced motion and multilingual text rendering), Wan 2.2 promises even better video quality, controllability, and resource efficiency. It’s expected to raise the bar in open-source T2V (text-to-video) generation—solidifying Alibaba’s footprint not just in LLMs, but in multimodal generative AI.\n\nOpen source isn’t just “throwing code over the wall.” It’s delivering production-ready, open products—and Alibaba is doing exactly that.\n\nLet’s not forget: Alibaba has open-sourced 300+ Qwen models and over 140,000 derivatives, making it the largest open-source model family on the planet. And they’ve pledged another ¥380 billion over the next three years into cloud and AI infrastructure. This isn’t a short-term leaderboard sprint. They’re betting big on locking down end-to-end certainty, from model to infrastructure to deployment.\n\nNow look across the Pacific: the top U.S. models are mostly going closed. GPT-4 isn’t open. Gemini’s locked down. Claude’s gated by API. Meanwhile, Alibaba is using the “open-source + engineering + infrastructure” trifecta to set a global usability bar.\n\nThis isn’t a “does China have the chops?” moment. Alibaba’s already in the center of the world stage setting the tempo.\n\nReminds me of that line:\n“The GOAT doesn’t announce itself. It just keeps dropping.”\nRight now, it’s Alibaba that’s dropping. And flexing. 💪\n",
          "author_fullname": "t2_1heeqeidfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Qwen’s TRIPLE release this week + Vid Gen model coming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "weyaltspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53ce79a67d338965235c56e2b3c00371e974dbe1"
                },
                {
                  "y": 172,
                  "x": 216,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd440fb8a85758efbb475066273ab978fa2e31dd"
                },
                {
                  "y": 255,
                  "x": 320,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d73a40edb86f641e7dc993717fdabb1853d2fc57"
                },
                {
                  "y": 511,
                  "x": 640,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d02c98cec01bde55a3e47b888b2400f1f535faf9"
                },
                {
                  "y": 766,
                  "x": 960,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1ec384067d5628ba8c9c25c816e80858867e7dde"
                },
                {
                  "y": 862,
                  "x": 1080,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=088f1aa4e0e7ae89f53740b4b244309b4cc57c64"
                }
              ],
              "s": {
                "y": 1246,
                "x": 1560,
                "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=1560&amp;format=pjpg&amp;auto=webp&amp;s=646b9961a87411f9179fed9fabde4545cac4f233"
              },
              "id": "weyaltspa1ff1"
            },
            "rkucntspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=431b653a87072eef7b1c2652642ba389579a40ed"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=99cb5b6ceda89950156b46794c003069b4dd7881"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=965f28cd42906c7be276d34bd91f92db37b2c8f2"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9f7c6f4af3c833f41d94634e6417e6d530cb764"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb633243bc7c260d045d51f138b772e29c1cf0d3"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e904c5c75a55c2bd3160c977e58e33c7bd24d86"
                }
              ],
              "s": {
                "y": 1062,
                "x": 2047,
                "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=2047&amp;format=pjpg&amp;auto=webp&amp;s=985561953a8e2dd8214a0cc19497018149c9b8f4"
              },
              "id": "rkucntspa1ff1"
            },
            "8cbupsspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df0598f5f14a194c8aa673c5d77c368f3e393137"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=00577984f574aecec96f74011d0eecb6da2d615e"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e7ba82e753109d6fe2cf5a27b46cb0a108e9f5c"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bcda3d8e32eb52d5034d144fb8966a452a2f959b"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd48745de4fe6fd795245e424c5d39f76a28b1c3"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a027d91631c830aa24133f6bbda2dc668090fd2"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=981e0f6470b985ff1071afb0638cfcc558c99486"
              },
              "id": "8cbupsspa1ff1"
            },
            "bixl6wspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 192,
                  "x": 108,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d567de59dba8708e67b9b0a966a356f6973d2d55"
                },
                {
                  "y": 384,
                  "x": 216,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c766b54adc9c5659f67a85cf94b669359681379"
                },
                {
                  "y": 569,
                  "x": 320,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b383bad9df5683b11bb5e52def2b2cc4ba6fc4be"
                },
                {
                  "y": 1138,
                  "x": 640,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9d6cb336943264ca9105c0a28e69d31ea8a84c1"
                },
                {
                  "y": 1707,
                  "x": 960,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=df12665ad4beb866223355da6fed3cf2ca2fe35f"
                },
                {
                  "y": 1920,
                  "x": 1080,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f7daacccd304ebd74a972ba6444192b0bca7faed"
                }
              ],
              "s": {
                "y": 2001,
                "x": 1125,
                "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=1125&amp;format=pjpg&amp;auto=webp&amp;s=14554ad8530cbde390f1b2f0a2daa7d4f41adfd6"
              },
              "id": "bixl6wspa1ff1"
            },
            "x2vw27vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=46d863470e5d32aaa73c24f247b91e32e1329b36"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1576e9b3d96ae4481868ac8d78d41570b50604e"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25c79c61db0c84815aeae9c6ec198b07ed5103e4"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc2e946a62af671691e0703106826b71cbc310ec"
                },
                {
                  "y": 640,
                  "x": 960,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=888dfa1e91bb0925f111db4b5b2c0dc7402e34dc"
                },
                {
                  "y": 720,
                  "x": 1080,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2b17f12d1e40be268d30de681d437cee0be28626"
                }
              ],
              "s": {
                "y": 720,
                "x": 1080,
                "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=d036c317cd811935e5df8428e2685246f9257818"
              },
              "id": "x2vw27vpa1ff1"
            },
            "44dlq7vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a45646380b2ee977aec7a49fd8ea39829673be5c"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e5bdd6314769468795cb87e0ac58d635bbb8c11"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=730c2116315f6311eec246e72385269af5c1c94a"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=580a3d155c46b202a7ad86ee13ca6771c38bfaad"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b73200d7fcb3def58b029187ef8eec207ed809c3"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eac2272f6da2670c9c08c1a40906ae42e72f2bc7"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=5e321123f77a67327af31326bde0e3a3e452ca73"
              },
              "id": "44dlq7vpa1ff1"
            },
            "5y2rk6vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d25d51dcce2ea38791b7272845e88a7ac468247"
                },
                {
                  "y": 189,
                  "x": 216,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7f89c7879ac4eb0415b468240c653fcff4090a5"
                },
                {
                  "y": 281,
                  "x": 320,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d6a58de7b19a2b4e5a4ae5420d4f25e0456b671"
                }
              ],
              "s": {
                "y": 357,
                "x": 406,
                "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=406&amp;format=pjpg&amp;auto=webp&amp;s=22c2ae44f200b41e10ff0c3c01252066a2bdab71"
              },
              "id": "5y2rk6vpa1ff1"
            }
          },
          "name": "t3_1m91b98",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 226,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "weyaltspa1ff1",
                "id": 713749073
              },
              {
                "media_id": "x2vw27vpa1ff1",
                "id": 713749074
              },
              {
                "media_id": "44dlq7vpa1ff1",
                "id": 713749075
              },
              {
                "media_id": "8cbupsspa1ff1",
                "id": 713749076
              },
              {
                "media_id": "rkucntspa1ff1",
                "id": 713749077
              },
              {
                "media_id": "5y2rk6vpa1ff1",
                "id": 713749078
              },
              {
                "media_id": "bixl6wspa1ff1",
                "id": 713749079
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 226,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Hp7akddNSmnkkxGnEtjYFrudFRvRoCTXEnawFbp7MZQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753455254,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen just dropped a triple update. After months out of the spotlight, Qwen is back and bulked up. You can literally see the gains; the training shows. I was genuinely impressed.&lt;/p&gt;\n\n&lt;p&gt;I once called Alibaba “the first Chinese LLM team to evolve from engineering to product.” This week, I need to upgrade that take: it’s now setting the release tempo and product standards for open-source AI.&lt;/p&gt;\n\n&lt;p&gt;This week’s triple release effectively reclaims the high ground across all three major pillars of open-source models:&lt;/p&gt;\n\n&lt;p&gt;1️⃣ Qwen3-235B-A22B-Instruct-2507: Outstanding results across GPQA, AIME25, LiveCodeBench, Arena-Hard, BFCL, and more. It even outperformed Claude 4 (non-thinking variant). The research group Artificial Analysis didn’t mince words: “Qwen3 is the world’s smartest non-thinking base model.”&lt;/p&gt;\n\n&lt;p&gt;2️⃣ Qwen3-Coder: This is a full-on ecosystem play for AI programming. It outperformed GPT-4.1 and Claude 4 in multilingual SWE-bench, Mind2Web, Aider-Polyglot, and more—and it took the top spot on Hugging Face’s overall leaderboard. The accompanying CLI tool, Qwen Code, clearly aims to become the “default dev workflow component.”&lt;/p&gt;\n\n&lt;p&gt;3️⃣ Qwen3-235B-A22B-Thinking-2507: With 256K context support and top-tier performance on SuperGPQA, LiveCodeBench v6, AIME25, Arena-Hard v2, WritingBench, and MultiIF, this model squares up directly against Gemini 2.5 Pro and o4-mini, pushing open-source inference models to the threshold of closed-source elite.&lt;/p&gt;\n\n&lt;p&gt;This isn’t about “can one model compete.” Alibaba just pulled off a coordinated strike: base models, code models, inference models—all firing in sync. Behind it all is a full-stack platform play: cloud infra, reasoning chains, agent toolkits, community release cadence.&lt;/p&gt;\n\n&lt;p&gt;And the momentum isn’t stopping. Wan 2.2, Alibaba’s upcoming video generation model, is next. Built on the heels of the highly capable Wan 2.1 (which topped VBench with advanced motion and multilingual text rendering), Wan 2.2 promises even better video quality, controllability, and resource efficiency. It’s expected to raise the bar in open-source T2V (text-to-video) generation—solidifying Alibaba’s footprint not just in LLMs, but in multimodal generative AI.&lt;/p&gt;\n\n&lt;p&gt;Open source isn’t just “throwing code over the wall.” It’s delivering production-ready, open products—and Alibaba is doing exactly that.&lt;/p&gt;\n\n&lt;p&gt;Let’s not forget: Alibaba has open-sourced 300+ Qwen models and over 140,000 derivatives, making it the largest open-source model family on the planet. And they’ve pledged another ¥380 billion over the next three years into cloud and AI infrastructure. This isn’t a short-term leaderboard sprint. They’re betting big on locking down end-to-end certainty, from model to infrastructure to deployment.&lt;/p&gt;\n\n&lt;p&gt;Now look across the Pacific: the top U.S. models are mostly going closed. GPT-4 isn’t open. Gemini’s locked down. Claude’s gated by API. Meanwhile, Alibaba is using the “open-source + engineering + infrastructure” trifecta to set a global usability bar.&lt;/p&gt;\n\n&lt;p&gt;This isn’t a “does China have the chops?” moment. Alibaba’s already in the center of the world stage setting the tempo.&lt;/p&gt;\n\n&lt;p&gt;Reminds me of that line:\n“The GOAT doesn’t announce itself. It just keeps dropping.”\nRight now, it’s Alibaba that’s dropping. And flexing. 💪&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m91b98",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m91b98",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "koc_Z3",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91b98/qwens_triple_release_this_week_vid_gen_model/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m91b98",
          "subreddit_subscribers": 504692,
          "created_utc": 1753455254,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to introduce a high-quality Japanese version of TTS that I've created through continuous pre-learning and post-training with orpheus.\n\n[https://huggingface.co/webbigdata/VoiceCore](https://huggingface.co/webbigdata/VoiceCore)\n\nFindings for those who are trying to create TTS in languages other than English\n\nI think that various TTS models use various neural codecs. This time, I used SNAC 24khz, which is used by orpheus-tts.\n\nSNAC is trained only in English. It is very high performance, but I noticed that there is a tendency for noise to be added to high-pitched voices such as surprise and joy of Japanese women.\n\nI noticed this after a lot of work was completed, so I decided to release it as it is as a preview version. When selecting a codec, I think it is better to first check whether it can handle emotional voices as well as normal voices.\n\nThank you meta/llama 3.2, canopylabs, and snac.\n\nFeedback is welcome.\n\nThank you!",
          "author_fullname": "t2_6sew99etq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "webbigdata/VoiceCore: Japanese voice version of canopylabs/orpheus-tts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ml0y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753512259,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to introduce a high-quality Japanese version of TTS that I&amp;#39;ve created through continuous pre-learning and post-training with orpheus.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/webbigdata/VoiceCore\"&gt;https://huggingface.co/webbigdata/VoiceCore&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Findings for those who are trying to create TTS in languages other than English&lt;/p&gt;\n\n&lt;p&gt;I think that various TTS models use various neural codecs. This time, I used SNAC 24khz, which is used by orpheus-tts.&lt;/p&gt;\n\n&lt;p&gt;SNAC is trained only in English. It is very high performance, but I noticed that there is a tendency for noise to be added to high-pitched voices such as surprise and joy of Japanese women.&lt;/p&gt;\n\n&lt;p&gt;I noticed this after a lot of work was completed, so I decided to release it as it is as a preview version. When selecting a codec, I think it is better to first check whether it can handle emotional voices as well as normal voices.&lt;/p&gt;\n\n&lt;p&gt;Thank you meta/llama 3.2, canopylabs, and snac.&lt;/p&gt;\n\n&lt;p&gt;Feedback is welcome.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?auto=webp&amp;s=2ae82e239f4c3b3831b7353351cbef6e3bc8ba77",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8af70d876318a37e56084cd5d2f690d937ab70c1",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=afddce4493c61691207bd2b51d7e4c4502e74a3e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1f74255680481b8b263bf5804c86250d07707b4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6510f02fc900bb217077a770be169591fbc0cf1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cdb07477f93611dddb3aca6244e275835ed4ff70",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7294049b9f0b2d71704d5885bc22b3678335431e",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "uFYNIoBfPPAfuk5C31nO4L2HdlD3pLd1JhqRMfIL6NQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9ml0y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dahara111",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ml0y/webbigdatavoicecore_japanese_voice_version_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ml0y/webbigdatavoicecore_japanese_voice_version_of/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753512259,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just finished uploading and perplexity testing some new ik\\_llama.cpp quants. Despite the random github takedown (and subsequent restoring) ik\\_llama.cpp is going strong!\n\nik just refreshed the IQ4\\_KSS 4.0 bpw non-linear quantization for faster performance and great perplexity so this quant hits a sweet spot at \\~114GiB allowing 2x64GB DDR5 gaming rigs with a single GPU to run it with decently long context lengths.\n\nAlso ik\\_llama.cpp recently had some PRs to improve tool/function calling.\n\nIf you have more RAM, check out my larger Qwen3-Coder-480B-A35B-Instruct-GGUF quants if that is your thing.\n\nCheers!",
          "author_fullname": "t2_n321yfw5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "IQ4_KSS 114 GiB and more ik_llama.cpp exclusive quants!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9cp2n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "#bbbdbf",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c5722281288ad17c5425d7d73e761d8eafbf6b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753481963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just finished uploading and perplexity testing some new ik_llama.cpp quants. Despite the random github takedown (and subsequent restoring) ik_llama.cpp is going strong!&lt;/p&gt;\n\n&lt;p&gt;ik just refreshed the IQ4_KSS 4.0 bpw non-linear quantization for faster performance and great perplexity so this quant hits a sweet spot at ~114GiB allowing 2x64GB DDR5 gaming rigs with a single GPU to run it with decently long context lengths.&lt;/p&gt;\n\n&lt;p&gt;Also ik_llama.cpp recently had some PRs to improve tool/function calling.&lt;/p&gt;\n\n&lt;p&gt;If you have more RAM, check out my larger Qwen3-Coder-480B-A35B-Instruct-GGUF quants if that is your thing.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ubergarm/Qwen3-235B-A22B-Thinking-2507-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?auto=webp&amp;s=f0dc878274992b77737eb6b8b4cf221787490f16",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a69f6d9c0fc9888f89ac3d1e39ffbc13bdff2b89",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4746d87fa4ee43c4c2ad291169131df06a3880bb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=64c7166a9e592474054818234e6a8fed62af8aca",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14512857abb7d9ebb3c411f65e1639efb84a8b4a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce689ee00af8c47b796b25b64ba7f36e852011a9",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=941654186ea54e774e0ff104ec12535879418828",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9cp2n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VoidAlchemy",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m9cp2n/iq4_kss_114_gib_and_more_ik_llamacpp_exclusive/",
          "stickied": false,
          "url": "https://huggingface.co/ubergarm/Qwen3-235B-A22B-Thinking-2507-GGUF",
          "subreddit_subscribers": 504692,
          "created_utc": 1753481963,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It seems the updated model doesn’t enclose thinking in &lt;think&gt;&lt;/think&gt; tags.\nWhich means you can’t collapse thinking window in gui apps like LM studio.",
          "author_fullname": "t2_jqxb4pte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Think tags missing in Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9qt65",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753528651,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems the updated model doesn’t enclose thinking in &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags.\nWhich means you can’t collapse thinking window in gui apps like LM studio.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9qt65",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Conversation9561",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9qt65/think_tags_missing_in_qwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9qt65/think_tags_missing_in_qwen3235ba22bthinking2507/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753528651,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There have been many models released this year already and have lost track of which models are better and for what.\n\n**Does anyone have some resource or spreadsheet that collates the results of many models on many benchmarks?**\n\nI'm slightly more interested in open-weights model results, but I think it's important to have data for closed source as well for comparison.\n\nI've tried to look myself, but the following resources aren't what I'm looking for:\n\n* [vellum.ai/llm-leaderboard](http://vellum.ai/llm-leaderboard) \\- not enough models or benchmarks covered\n* [artificialanalysis.ai](http://artificialanalysis.ai) \\- does cover lots of models, but only uses single number for intelligence\n* [https://dubesor.de/benchtable](https://dubesor.de/benchtable) \\- no official benchmarks used\n* [https://llm-stats.com/](https://llm-stats.com/) \\- not many benchmarks covered",
          "author_fullname": "t2_2ys57apx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone created a table of collated benchmark results of many LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9p9kg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753522802,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There have been many models released this year already and have lost track of which models are better and for what.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Does anyone have some resource or spreadsheet that collates the results of many models on many benchmarks?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m slightly more interested in open-weights model results, but I think it&amp;#39;s important to have data for closed source as well for comparison.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to look myself, but the following resources aren&amp;#39;t what I&amp;#39;m looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"http://vellum.ai/llm-leaderboard\"&gt;vellum.ai/llm-leaderboard&lt;/a&gt; - not enough models or benchmarks covered&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"http://artificialanalysis.ai\"&gt;artificialanalysis.ai&lt;/a&gt; - does cover lots of models, but only uses single number for intelligence&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dubesor.de/benchtable\"&gt;https://dubesor.de/benchtable&lt;/a&gt; - no official benchmarks used&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://llm-stats.com/\"&gt;https://llm-stats.com/&lt;/a&gt; - not many benchmarks covered&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?auto=webp&amp;s=694be80e81c347654e4a0cc9eab6ad4723b65311",
                  "width": 1165,
                  "height": 627
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01b76279937283a6f65fb2f22bacafcfd57327f0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c32f4a5cf158408de0486e01f1bb633dac191c9e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e6c25d22c39ee81cd1fed7e9caf0295ccb0a828",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e5ff7437f75185162c16b9fa223ebb95d12680c5",
                    "width": 640,
                    "height": 344
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8fe4d3636c03fffa93f2686ad89c21e63cd305e",
                    "width": 960,
                    "height": 516
                  },
                  {
                    "url": "https://external-preview.redd.it/4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=286fe8030eb3bac68c389e001bb59d162a1b2e9f",
                    "width": 1080,
                    "height": 581
                  }
                ],
                "variants": {},
                "id": "4mltMPuoIx8FMW1Y60sN3h17rSzmD3D6Q8bt5cARpzY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9p9kg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JawGBoi",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9p9kg/has_anyone_created_a_table_of_collated_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9p9kg/has_anyone_created_a_table_of_collated_benchmark/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753522802,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan (Ex-WizardLM) Dense Model Coming Soon!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94ls2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 81,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 81,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jB1CS2LZrHw4qZHwx53ViD8xx1eEncZt0jht4lZ5-yw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753462750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?auto=webp&amp;s=0fb34e0f66b8292996122ca9f453b58a37b14813",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1baecdc2dfde1050ccffe3b9db6f4cad84a6a26",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b7d9c1d3dc4da4b79164f82841fe3a8e4f0301c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8e2c1e124ff0e7f72b87f1fd2dae8eacf659993",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=04fdf799a0982d200ae760ce600052e22efe1fd7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d09d8c9f8137e0686807b5b322768faea8d8935d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c6f33fac669b52e589697bc73e7a0408f351898",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "9GvEX7LrasZYgCzEEJMA4dtKp0bfjGuzNOUm65ANRbI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m94ls2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94ls2/hunyuan_exwizardlm_dense_model_coming_soon/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "subreddit_subscribers": 504692,
          "created_utc": 1753462750,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen3 on Fiction.liveBench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93d0r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 92,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 92,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VIv9hJCgW9w5E1qzXymkc_nKdHvwujLLN61ek5o0LXE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753459907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hvi3tvmjo1ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?auto=webp&amp;s=6258c8c89fb879cded6d316f86467b097f492051",
                  "width": 1568,
                  "height": 2318
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=385e269d9eecbdde591e638694025b90d6447acc",
                    "width": 108,
                    "height": 159
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=419c2fef6a3435f83900687fabdaca18e00bdf81",
                    "width": 216,
                    "height": 319
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73f13ee604283c4240ce3dc6e2e12c02fbafbcf5",
                    "width": 320,
                    "height": 473
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8c9a4a27cda997bb1fb4bc522e4d9bac9f04231",
                    "width": 640,
                    "height": 946
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca984a8304eade76aa7c934ed429e1c9a1fcc949",
                    "width": 960,
                    "height": 1419
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1faa0dda9eeec65358a99b5f5dcf6f3256542ec9",
                    "width": 1080,
                    "height": 1596
                  }
                ],
                "variants": {},
                "id": "ZokGAHU7CGwFeHWan5TASqOpdAfZ1F2vcHM0eru8zuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m93d0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93d0r/new_qwen3_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/hvi3tvmjo1ff1.png",
          "subreddit_subscribers": 504692,
          "created_utc": 1753459907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is an AI web browser that uses local AI models. It's still very early, FULL of bugs and missing key features as a browser, but still good to play around with it.   \n  \nDownload it from [Github](https://github.com/nuance-dev/Web)\n\nNote: AI features only work with M series chips.",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created an open-source macOS AI browser that uses MLX and Gemma 3n, feel free to fork it!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m903il",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 131,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/fculp27z11ff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1834,
              "scrubber_media_url": "https://v.redd.it/fculp27z11ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fculp27z11ff1/DASHPlaylist.mpd?a=1756124802%2CY2U0MDg1NzFmZGFjNTNkM2IxMzU0YjlhOTc5OWQ1MWU4NjliMTQ4MjBjZjIzY2FiMjNiZjZjMjM4MWZjOTM3Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 55,
              "hls_url": "https://v.redd.it/fculp27z11ff1/HLSPlaylist.m3u8?a=1756124802%2CY2M3YWQ5ZTUyZWNlYTAwNjU0YWM5OGQ2ZTlmMzE5NjRlM2FjOGRjNzlkOTU4NzE1MGI1N2ZlNDgwMWUxY2I5Zg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 131,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=140&amp;height=82&amp;crop=140:82,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ec09409f07401537e98387108dfed9051e1440fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753452396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an AI web browser that uses local AI models. It&amp;#39;s still very early, FULL of bugs and missing key features as a browser, but still good to play around with it.   &lt;/p&gt;\n\n&lt;p&gt;Download it from &lt;a href=\"https://github.com/nuance-dev/Web\"&gt;Github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: AI features only work with M series chips.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/fculp27z11ff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?format=pjpg&amp;auto=webp&amp;s=7c21e43bb9a5da5da2c2010ce93da12804258bf5",
                  "width": 2844,
                  "height": 1674
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fcdf109b4fdfd5cf54e3e2e866680aebaec3a5ae",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=112befd5fa6112b6cb09d0adc2d4a9d58cfe354c",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6be30ac241df05f59330c9fd5fd4452821ab66fc",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5ddaa6122638b83743d7370ed78028c80cb5d52",
                    "width": 640,
                    "height": 376
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a46f147a34e2d2ab597964ba48f2a74696b5c102",
                    "width": 960,
                    "height": 565
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ed6db97d6de07a2a5b3adbb6c2277382e6257018",
                    "width": 1080,
                    "height": 635
                  }
                ],
                "variants": {},
                "id": "NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m903il",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m903il/i_created_an_opensource_macos_ai_browser_that/",
          "stickied": false,
          "url": "https://v.redd.it/fculp27z11ff1",
          "subreddit_subscribers": 504692,
          "created_utc": 1753452396,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/fculp27z11ff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1834,
              "scrubber_media_url": "https://v.redd.it/fculp27z11ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fculp27z11ff1/DASHPlaylist.mpd?a=1756124802%2CY2U0MDg1NzFmZGFjNTNkM2IxMzU0YjlhOTc5OWQ1MWU4NjliMTQ4MjBjZjIzY2FiMjNiZjZjMjM4MWZjOTM3Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 55,
              "hls_url": "https://v.redd.it/fculp27z11ff1/HLSPlaylist.m3u8?a=1756124802%2CY2M3YWQ5ZTUyZWNlYTAwNjU0YWM5OGQ2ZTlmMzE5NjRlM2FjOGRjNzlkOTU4NzE1MGI1N2ZlNDgwMWUxY2I5Zg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm happy to see this as my experience with these models for image recognition isn't very impressive. They mostly can't even tell when pictures are sideways, for example.",
          "author_fullname": "t2_5b972ieo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.1V-9B-Thinking - claims to \"match or surpass Qwen2.5-72B\" on many tasks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8xmy9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 178,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 178,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ff3e77630a6d9903e8ffc2da84d81c479305e7d8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753445934,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m happy to see this as my experience with these models for image recognition isn&amp;#39;t very impressive. They mostly can&amp;#39;t even tell when pictures are sideways, for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/THUDM/GLM-4.1V-Thinking",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?auto=webp&amp;s=f3d8a19a2706316e04aaa9fd5ea8ed3c15e6f304",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f8ce37456b595d44518bc9dbb50bfbbdc4bdd6f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=613a2c6b97d63cde3e9b6e1957cffa5cfa955644",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9e5860036f1ae510f4d52f708d0e080abc80cd5",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=841806f55387309a64d67cd8a7a49351c70e6ab2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f52fb001e2e3c7ce1ba59b80df2da7d7b72a91f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b77075add5d3dfb46a095004810d01976a798dec",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8xmy9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pristine-Woodpecker",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xmy9/glm41v9bthinking_claims_to_match_or_surpass/",
          "stickied": false,
          "url": "https://github.com/THUDM/GLM-4.1V-Thinking",
          "subreddit_subscribers": 504692,
          "created_utc": 1753445934,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8myxl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 1061,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1061,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/RMM5ptmjVG7kaBZS0SOzo0NX4eL3tYZb179ptZ_bN9I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753408933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nl9jgkkzgxef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?auto=webp&amp;s=8465f34a7f504c523ccda4f16197b27156796978",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b85428e434a7d4cd150f23a38f934c57dbd23502",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1164a07e29c695bb059e756e08babe234ff379d6",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5315a863e1565b2b6dd93c5a571cf54fb94a33ad",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5596e2098d9a669775268db5ef71e54bd685cd0d",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d61f41e97643e7ae41cdc73d41cb27bc076e45f",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "e_AIfm2x33q9UQnGSrgnpxiaWz6SPpbRIOgsMere00w"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m8myxl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/",
          "stickied": false,
          "url": "https://i.redd.it/nl9jgkkzgxef1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753408933,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It seems much effort was spent to improve quantization by the community trying to fit a dense model in VRAM so it didn’t tick along at 2 tokens a second. Many even bought multiple cards to have more VRAM. \n\nNow many new models are MoEs, where the average Joe sits hopelessly at his computer with a couple of consumer cards and 32 gb of RAM. Obviously lots of system RAM is cheaper than lots of VRAM but the larger MoEs have as many active parameters as some dense models of years past.\n\nHow likely are we to see improvements that can take Qwen 3’s massive MoE and cut it down with similar performance but at a dense 72b size? Or the new ERNIE? Or Deepseek?\n\nNvidia has done some pruning of dense models, and it seems likely that a MoE has less efficiency since it performs just a little better than the dense models.  So it seems likely to me … as a layman. \n\nAnyone familiar with efforts towards economic solutions that could compress MoEs in ways other than quantization? Does anyone with a better grasp of the architecture think it’s possible? What challenges might there be what solutions might exist love your thoughts!",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "There has been a lot of efforts in the past to improve  quantization due to the size of dense models… are we likely to see improvements like pruning and/or distillation with the uprise of huge MoEs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9hycx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753496736,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems much effort was spent to improve quantization by the community trying to fit a dense model in VRAM so it didn’t tick along at 2 tokens a second. Many even bought multiple cards to have more VRAM. &lt;/p&gt;\n\n&lt;p&gt;Now many new models are MoEs, where the average Joe sits hopelessly at his computer with a couple of consumer cards and 32 gb of RAM. Obviously lots of system RAM is cheaper than lots of VRAM but the larger MoEs have as many active parameters as some dense models of years past.&lt;/p&gt;\n\n&lt;p&gt;How likely are we to see improvements that can take Qwen 3’s massive MoE and cut it down with similar performance but at a dense 72b size? Or the new ERNIE? Or Deepseek?&lt;/p&gt;\n\n&lt;p&gt;Nvidia has done some pruning of dense models, and it seems likely that a MoE has less efficiency since it performs just a little better than the dense models.  So it seems likely to me … as a layman. &lt;/p&gt;\n\n&lt;p&gt;Anyone familiar with efforts towards economic solutions that could compress MoEs in ways other than quantization? Does anyone with a better grasp of the architecture think it’s possible? What challenges might there be what solutions might exist love your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9hycx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9hycx/there_has_been_a_lot_of_efforts_in_the_past_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9hycx/there_has_been_a_lot_of_efforts_in_the_past_to/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753496736,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Amazing qwen 3 updated thinking model just released !! Open source !",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vhp3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 211,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 211,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/F41cNbRL6zX_oMjLZyQ-1dfZZnw6cXvW4eSrGH95qqI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19\"&gt;https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nx5d8w74yzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?auto=webp&amp;s=96c200cb190833b2bf1f5012444f0dc3b238d209",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b98d17e59ac2e72b931b0b8fd7215c2bc7e353d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a348a0bdfb7057916cf5942fbc64bb0dc17a34e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=956c91eab46464e833aaeb318551a0b4b8b10b46",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d728468419b7ffc3426c85447250b3cc034f70a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5dab0ca10ad6fe3a88aa470b1eabd3b5a53a9b77",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7f6c6dd942072b45b5ae2a1c5871cfece1acdb7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "KvH8TJ7zk2PBZHdgGic1Zrs7h6lhFOr63lQmnuZcFlg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vhp3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/",
          "stickied": false,
          "url": "https://i.redd.it/nx5d8w74yzef1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753438909,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am taking help from this \n\n[https://milvus.io/docs/multimodal\\_rag\\_with\\_milvus.md](https://milvus.io/docs/multimodal_rag_with_milvus.md)\n\n  \nBut the line *from FlagEmbedding.visual.modeling import Visualized\\_BGE* is not working. \n\nAny suggestions?",
          "author_fullname": "t2_1scf7jv5l4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has Anyone been able to generate multimodal embedddings using Visualized_BGE?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9qtco",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753528668,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am taking help from this &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://milvus.io/docs/multimodal_rag_with_milvus.md\"&gt;https://milvus.io/docs/multimodal_rag_with_milvus.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But the line &lt;em&gt;from FlagEmbedding.visual.modeling import Visualized_BGE&lt;/em&gt; is not working. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-kJYNtkMxFG7yxLj34KN0ir3jR3pm0SmVkmbat3uUjw.png?auto=webp&amp;s=89a689d4c737753d5d1117483ecb724014513dc6",
                  "width": 3600,
                  "height": 1881
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-kJYNtkMxFG7yxLj34KN0ir3jR3pm0SmVkmbat3uUjw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=158fcd61955ed3f82f8bdccf6dcfca497a8fb0fb",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/-kJYNtkMxFG7yxLj34KN0ir3jR3pm0SmVkmbat3uUjw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f232e3f6d997eb90ad7b78fac0d3546094b0ede",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://external-preview.redd.it/-kJYNtkMxFG7yxLj34KN0ir3jR3pm0SmVkmbat3uUjw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ad5ec025888de8b81f8d8de96f5efa4e17d4edc",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/-kJYNtkMxFG7yxLj34KN0ir3jR3pm0SmVkmbat3uUjw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb79db21a70dd460af99849ba6055f2caf723888",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/-kJYNtkMxFG7yxLj34KN0ir3jR3pm0SmVkmbat3uUjw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bfcc5b573cb470fa2a03a05822ec0933377c1099",
                    "width": 960,
                    "height": 501
                  },
                  {
                    "url": "https://external-preview.redd.it/-kJYNtkMxFG7yxLj34KN0ir3jR3pm0SmVkmbat3uUjw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=206457e9f5df9f3c5bc2c10801a0e9e44ad7a6d9",
                    "width": 1080,
                    "height": 564
                  }
                ],
                "variants": {},
                "id": "-kJYNtkMxFG7yxLj34KN0ir3jR3pm0SmVkmbat3uUjw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9qtco",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IndependentTough5729",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9qtco/has_anyone_been_able_to_generate_multimodal/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9qtco/has_anyone_been_able_to_generate_multimodal/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753528668,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was told trying to run non-tiny LLM's on a CPU was unusable.  But I got 8.3 token/sec for qwen2.5-coder-32b-instruct Q8 without using the GPU.  38.6 tokens/sec using both 5090's.  Note, I'm getting barely 48% processing usage on the 5090's and wondering what I can do to improve that.\n\nLlama.cpp thread affinity seems to not do anything on Ubuntu. For my CPU's runs I had to do my own fix for this.  I mainly did this to see how well layer overflowing will work for even larger models.  \nThe problem is the nearly continuous stream of new models to try.  \nWas going with qwen2.5-coder-32b-instruct.  \nThen today I see Qwen3-235B-A22B-Thinking-2507-FP8 and just now [Llama-3\\_3-Nemotron-Super-49B-v1\\_5](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5)   \nToo many choices.\n\n",
          "author_fullname": "t2_rxk6hx4t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My 7985WX, dual 5090's, and 256GB's of DDR5-6000 has landed.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9itnz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753499460,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was told trying to run non-tiny LLM&amp;#39;s on a CPU was unusable.  But I got 8.3 token/sec for qwen2.5-coder-32b-instruct Q8 without using the GPU.  38.6 tokens/sec using both 5090&amp;#39;s.  Note, I&amp;#39;m getting barely 48% processing usage on the 5090&amp;#39;s and wondering what I can do to improve that.&lt;/p&gt;\n\n&lt;p&gt;Llama.cpp thread affinity seems to not do anything on Ubuntu. For my CPU&amp;#39;s runs I had to do my own fix for this.  I mainly did this to see how well layer overflowing will work for even larger models.&lt;br/&gt;\nThe problem is the nearly continuous stream of new models to try.&lt;br/&gt;\nWas going with qwen2.5-coder-32b-instruct.&lt;br/&gt;\nThen today I see Qwen3-235B-A22B-Thinking-2507-FP8 and just now &lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;br/&gt;\nToo many choices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?auto=webp&amp;s=af31b2002f0236c31cf3c91755fd855ed95ae985",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45e7c8c14055c57d8c62dad0b150faa3212ce087",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82619b61b919798034ba0f0b798bd1e75640c0b9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf3accf5409bb25bb8728256d4e2f61e2bbbeec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ce793850ca6936254a722184eb2367e6423fa1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e1d1bb2008c0cfb9abdbf16638bc668942167e7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=313d9b1115102aa6244b349a8e99c1ee840c4702",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9itnz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Guilty-History-9249",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753499460,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a small dataset which I had trained on Nebius Ai studio and downloaded the files. I then merged the model Llama 3.2-3B instruct and lora adaptor for it. And then when I coverted it in GGUF and loaded on kobaldcpp for test, it giving me this. I am new to all this so if anyone need more information to know the error, please let me know ",
          "author_fullname": "t2_79u553e1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Merged Lora adaptor Model Giving Gibberish as response.  Using Llama 3.2 3B instruct. Dataset trained on Nebius Ai studio. What to do?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 42,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9p7bb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/JpdPpeJ_7kVSSsglwpKR1DkPrBJmly4pwS_UHo8N72I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753522558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a small dataset which I had trained on Nebius Ai studio and downloaded the files. I then merged the model Llama 3.2-3B instruct and lora adaptor for it. And then when I coverted it in GGUF and loaded on kobaldcpp for test, it giving me this. I am new to all this so if anyone need more information to know the error, please let me know &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/oj9iadphu6ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/oj9iadphu6ff1.png?auto=webp&amp;s=c2972c93a66d9ceedddeee001ca98e0510cd468e",
                  "width": 1625,
                  "height": 488
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9204efaa1a75ec8eb30cd5a6b003e3499b37518",
                    "width": 108,
                    "height": 32
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55d6bda53c728cfad9883ea9aff8e267c5502a2e",
                    "width": 216,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66a1c87c898b9882a89297fd51d7efb6c1f570a3",
                    "width": 320,
                    "height": 96
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=afb8e2b38ca43c2057294a73cbd2fc6542dc5675",
                    "width": 640,
                    "height": 192
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f388d9fdc773534873635200869e84fdde884b3",
                    "width": 960,
                    "height": 288
                  },
                  {
                    "url": "https://preview.redd.it/oj9iadphu6ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d410556f87dd4a4da64e61628e4f85f50ae77c3",
                    "width": 1080,
                    "height": 324
                  }
                ],
                "variants": {},
                "id": "mAUoCGTDX2IzYO5VD1nIJ4FheUUNrft3eYnKExXJx_I"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9p7bb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rihuwamidori",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9p7bb/merged_lora_adaptor_model_giving_gibberish_as/",
          "stickied": false,
          "url": "https://i.redd.it/oj9iadphu6ff1.png",
          "subreddit_subscribers": 504692,
          "created_utc": 1753522558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/ \n\n“If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination-free SWE-Bench, that’s the reality check for me.”",
          "author_fullname": "t2_nrsswg757",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A contamination-free coding benchmark shows AI may not be as excellent as claimed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ud84",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 179,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 179,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753434586,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/\"&gt;https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;“If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination-free SWE-Bench, that’s the reality check for me.”&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?auto=webp&amp;s=2bfdda582644a1925177c0e421a4a2c6950c77a7",
                  "width": 1200,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=faea71883e14653e5ce297d91fc495960f8b18eb",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fafd8205cf207b0508b5104593c16cf4c0ffe3de",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1cbb456de00218886e9420164f12a5eca3ff0d1",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4b19f9c8b648f4414c2572113329cabf9dd2693",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=078b612839727b8bd391f293af598403ad688643",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c128b6e2f538773e90e5bc76e74a71f6424fcaed",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8ud84",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Document4034",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753434586,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:\n\n\t•\tGPQA (Graduate-level reasoning): 81 → 71\n\t•\tAIME2025 (Math competition problems): 92 → 81\n\t•\tLiveCodeBench v6 (Code generation and debugging): 74 → 56\n\t•\tArena-Hard v2 (General problem-solving): 80 → 62\n\nEven the new instruct version is way better than the old non-thinking one. Looks like they’ve really boosted reasoning and coding skills here.\n\nWhat do you think is driving this jump, better training, bigger data, or new techniques?",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen3-235B update is crushing old models in benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8w9ah",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 121,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 121,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wQ7SbNTBzIdOQb6lfJFHU10NFUkitTQk5yMGuXDJ-EY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;• GPQA (Graduate-level reasoning): 81 → 71\n• AIME2025 (Math competition problems): 92 → 81\n• LiveCodeBench v6 (Code generation and debugging): 74 → 56\n• Arena-Hard v2 (General problem-solving): 80 → 62\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Even the new instruct version is way better than the old non-thinking one. Looks like they’ve really boosted reasoning and coding skills here.&lt;/p&gt;\n\n&lt;p&gt;What do you think is driving this jump, better training, bigger data, or new techniques?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q009687760ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q009687760ff1.jpeg?auto=webp&amp;s=b2eff0f0d944d9f3cc3dd7822d8f074bf89032b7",
                  "width": 2379,
                  "height": 1392
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f76378abbbe79bad791d59ab511364ecf839f4ba",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7d7562fa1030bd11fb35ae20f3f153be32261ae",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dff32d54f41f3760a510fcbf21e705869a748449",
                    "width": 320,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee6d42068b310b231eceef2e74d8ae35c50e819e",
                    "width": 640,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d7c3a37e2d1bba48fd4f97b28bd78fe7580bb2ca",
                    "width": 960,
                    "height": 561
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cfaba909996daa45a067d35347b915f94c15843",
                    "width": 1080,
                    "height": 631
                  }
                ],
                "variants": {},
                "id": "5z0PiohPQ5P8oWxfKaPaT1JALPktWest18Z3iN05GrQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8w9ah",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w9ah/new_qwen3235b_update_is_crushing_old_models_in/",
          "stickied": false,
          "url": "https://i.redd.it/q009687760ff1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753441629,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Alexandr Wang appointing new chief AI scientist and pushing for closed source and closed weights models \n\n",
          "author_fullname": "t2_dmaijfods",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Yann Lecun being sidelined at Meta. RIP for open weight models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9s0zk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YVIq_3H5XVek2j7iARW1IU1Z7nVlQQQ10vaO24c3rVk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753532666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alexandr Wang appointing new chief AI scientist and pushing for closed source and closed weights models &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/7su5jpowo7ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/7su5jpowo7ff1.jpeg?auto=webp&amp;s=e2fbd6c3029b0475c2b9e11e07c88189be635d0e",
                  "width": 1080,
                  "height": 2082
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/7su5jpowo7ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=286e47eb2f5107928dbb1c3c407da3ae0ffc9906",
                    "width": 108,
                    "height": 208
                  },
                  {
                    "url": "https://preview.redd.it/7su5jpowo7ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=665ebfaca0aca86dc623d9602a24f5e8520a3fe4",
                    "width": 216,
                    "height": 416
                  },
                  {
                    "url": "https://preview.redd.it/7su5jpowo7ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=523a1359024c17b0abbdb7ab7e9e24a1e5939698",
                    "width": 320,
                    "height": 616
                  },
                  {
                    "url": "https://preview.redd.it/7su5jpowo7ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=576f7cd157633498f7fefe2d85c7448773dd01dd",
                    "width": 640,
                    "height": 1233
                  },
                  {
                    "url": "https://preview.redd.it/7su5jpowo7ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e89e703688be0762082a7847b5ada3fb5d0b61a",
                    "width": 960,
                    "height": 1850
                  },
                  {
                    "url": "https://preview.redd.it/7su5jpowo7ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a596e0277d5c9647645faeac178875861a53bf0c",
                    "width": 1080,
                    "height": 2082
                  }
                ],
                "variants": {},
                "id": "nsBpYlYAr2S8UrToscwyoCmvKLdIurJ2Iblmy8Zx7zU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9s0zk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeedleworkerDull7886",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9s0zk/yann_lecun_being_sidelined_at_meta_rip_for_open/",
          "stickied": false,
          "url": "https://i.redd.it/7su5jpowo7ff1.jpeg",
          "subreddit_subscribers": 504692,
          "created_utc": 1753532666,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I mostly compared model with 3-4 benchmark, MMLU, MMLU Pro, GPQA, --&gt; for determine it knowledge.  IFEval --&gt; to determine if it can follow instruction well (is it help to detemine structure output generation? let me know)\n\nThe reason is that these is the most tested benchmark, it appear a lot more time than another benchmark.\n\nBut ultimately, I will use score to pick candidate only, and always test if it fits my use case first",
          "author_fullname": "t2_c5n1x183x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When picking the model for production use, what criteria do you use?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9rpgf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753531645,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mostly compared model with 3-4 benchmark, MMLU, MMLU Pro, GPQA, --&amp;gt; for determine it knowledge.  IFEval --&amp;gt; to determine if it can follow instruction well (is it help to detemine structure output generation? let me know)&lt;/p&gt;\n\n&lt;p&gt;The reason is that these is the most tested benchmark, it appear a lot more time than another benchmark.&lt;/p&gt;\n\n&lt;p&gt;But ultimately, I will use score to pick candidate only, and always test if it fits my use case first&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9rpgf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dheetoo",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rpgf/when_picking_the_model_for_production_use_what/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9rpgf/when_picking_the_model_for_production_use_what/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753531645,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I know GPUs can be “connected” (like via NVLink or just multiple GPUs in one system), but can their **VRAM** be **combined**?\n\n\n\nHere’s my use case: I have two GTX 1060 6GB cards, and theoretically together they give me 12GB of VRAM.\n\n\n\n**Question** – can I run a model (like an LLM or SDXL) that requires more than 6GB (or even 8B+ params) using both cards? Or am I still limited to just 6GB because the VRAM isn’t shared?",
          "author_fullname": "t2_odgywsgtq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help understanding GPU VRAM pooling – can I combine VRAM across GPUs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9rmry",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753531408,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I know GPUs can be “connected” (like via NVLink or just multiple GPUs in one system), but can their &lt;strong&gt;VRAM&lt;/strong&gt; be &lt;strong&gt;combined&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;Here’s my use case: I have two GTX 1060 6GB cards, and theoretically together they give me 12GB of VRAM.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt; – can I run a model (like an LLM or SDXL) that requires more than 6GB (or even 8B+ params) using both cards? Or am I still limited to just 6GB because the VRAM isn’t shared?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9rmry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Recent-Bother5388",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rmry/need_help_understanding_gpu_vram_pooling_can_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9rmry/need_help_understanding_gpu_vram_pooling_can_i/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753531408,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi LLM Folks,\n\nTL/DR: I'm seeking tips for improving my ollama setup with Qwen3, deepseek and nomic-embed for home sized LLM instance.\n\nI'm in the LLM game for a couple of weeks now and still learning something new every day. I have an ollama instance on my Ryzen workstation running Debian and control it with a Lenovo X1C laptop which is also running Debian. It's a home setup so nothing too fancy. You can find the technical details below.\n\nPurpose of this machine is to answer all kind of questions (qwen3-30B), analyze PDF files (nomic-embed-text:latest) and summarize mails (deepseek-r1:14b), websites (qwen3:14b) etc. I'm still discovering what I could do more with it. Overall it should act as a local AI assistant. I could use some of your wisdom how to improve the setup of that machine for those tasks.\n\n1. I found the Qwen3-30B-A3B-GGUF model running quite good (10-20 tk/s) for overall questions on this hardware but would like to squeeze a little bit more performance out of it. I'm running it with num\\_ctx=5120, temperature=0.6, top\\_K=20, top\\_P=0.95. What could be improved, to give me a better quality of the answers or improve speed of the model?\n2. I would also like to improve the quality of analyzing PDF files. I found that the quality can differ widely. Some PDFs are being analyzed properly for others barely anything is done right, eg. only the metadata is identified but not the content. I use nomic-embed-text:latest for this task. Do you have a suggestion how to improve that or know a better tool I could use?\n3. I'm also not perfectly satisfied with the summaries of (deepseek-r1:14b) and (qwen3:14b). Both fit into the VRAM but sometimes the language is poor if they have to translate summaries into German or the summaries are way too short and they seem to miss most of the context. I'm also not sure if I need thinking models for that task or if I should try something else?\n4. Do you have some overall tips for setting up ollama? I learned that I can play around with KV cache, GPU layers etc. Is it possible to make ollama use all of the 12GB VRAM of the RTX 3060? Somehow it seems that around 1GB is always left free. Are there already some best practices on this for setups like mine? You can find my current settings below. And, would it make a notable difference if I would change the storage location of the models to a fast 1TB nvme? The workstation has a bunch of disks and currently the models reside on an older 256GB SSD.\n\nAny help improving my setup is appreciated.\n\nThanks for reading so far!\n\nBelow are some technical information and some examples how the models fit into VRAM/RAM:\n\n    Environments settings for ollama:\n    \n    Environment=\"OLLAMA_DEBUG=0\"\n    Environment=\"CUDA_VISIBLE_DEVICES=0\"\n    Environment=\"OLLAMA_NEW_ENGINE=1\"\n    Environment=\"OLLAMA_LLM_LIBRARY=cuda\"\n    Environment=\"OLLAMA_FLASH_ATTENTION=1\"\n    Environment=\"OLLAMA_NUM_PARALLEL=1\"\n    Environment=\"OLLAMA_MAX_LOADED_MODELS=1\"\n    Environment=\"OLLAMA_KV_CACHE_TYPE=q8_0\"\n    Environment=\"OLLAMA_MODELS=/chroot/AI/share/ollama/.ollama/models/\"\n    Environment=\"OLLAMA_NUM_GPU_LAYERS=36\"\n    Environment=\"OLLAMA_ORIGINS=moz-extension://*\"\n    \n    \n    \n    $ ollama ps                                                                                            \n    NAME                                       ID              SIZE     PROCESSOR          UNTIL                \n    hf.co/unsloth/Qwen3-30B-A3B-GGUF:Q5_K_M    c8c7e4f7bc56    23 GB    46%/54% CPU/GPU    29 minutes from now \n    deepseek-r1:14b                            c333b7232bdb    10.0 GB  100% GPU           4 minutes from now \n    qwen3:14b                                  bdbd181c33f2    10 GB    100% GPU           29 minutes from now   \n    nomic-embed-text:latest                    0a109f422b47    849 MB    100% GPU          4 minutes from now   \n    \n    \n    \n    $ nvidia-smi \n    Sat Jul 26 11:30:56 2025                                                                              \n    +-----------------------------------------------------------------------------------------+\n    | NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n    |-----------------------------------------+------------------------+----------------------+\n    | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n    | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n    |                                         |                        |               MIG M. |\n    |=========================================+========================+======================|\n    |   0  NVIDIA GeForce RTX 3060        Off |   00000000:08:00.0  On |                  N/A |\n    | 68%   54C    P2             57W /  170W |   11074MiB /  12288MiB |     17%      Default |\n    |                                         |                        |                  N/A |\n    +-----------------------------------------+------------------------+----------------------+\n                                                                                              \n    +-----------------------------------------------------------------------------------------+\n    | Processes:                                                                              |\n    |  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n    |        ID   ID                                                               Usage      |\n    |=========================================================================================|\n    |    0   N/A  N/A      4296      C   /chroot/AI/bin/ollama                       11068MiB |\n    +-----------------------------------------------------------------------------------------+\n    \n    \n    \n    $ inxi -bB                                                                                            \n    System:                                                                                               \n      Host: morpheus Kernel: 6.15.8-1-liquorix-amd64 arch: x86_64 bits: 64                     \n      Console: pty pts/2 Distro: Debian GNU/Linux 13 (trixie)                                             \n    Machine:     \n      Type: Desktop Mobo: ASUSTeK model: TUF GAMING X570-PLUS (WI-FI) v: Rev X.0x                         \n        serial: &lt;superuser required&gt; UEFI: American Megatrends v: 5021 date: 09/29/2024        \n    Battery:                                                                                              \n      Message: No system battery data found. Is one present?                                   \n    CPU:                                                                                                  \n      Info: 6-core AMD Ryzen 5 3600 [MT MCP] speed (MHz): avg: 1724 min/max: 558/4208          \n    Graphics:                                                                                             \n      Device-1: NVIDIA GA106 [GeForce RTX 3060 Lite Hash Rate] driver: nvidia v: 550.163.01    \n      Display: server: X.org v: 1.21.1.16 with: Xwayland v: 24.1.6 driver: X: loaded: nvidia   \n        unloaded: modesetting gpu: nvidia,nvidia-nvswitch tty: 204x45                          \n      API: OpenGL v: 4.6.0 compat-v: 4.5 vendor: mesa v: 25.1.5-0siduction1                    \n        note: console (EGL sourced) renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2, llvmpipe (LLVM 19.1.7\n        256 bits)                                                                                         \n      Info: Tools: api: clinfo, eglinfo, glxinfo, vulkaninfo de: kscreen-console,kscreen-doctor\n        gpu: nvidia-settings,nvidia-smi wl: wayland-info x11: xdriinfo, xdpyinfo, xprop, xrandr\n    Network:                                                                                              \n      Device-1: Intel Wi-Fi 5 Wireless-AC 9x6x [Thunder Peak] driver: iwlwifi                  \n    Drives:                                                                                               \n      Local Storage: total: 6.6 TiB used: 2.61 TiB (39.6%)                                     \n    Info:                                                                                                 \n      Memory: total: N/A available: 62.71 GiB used: 12.78 GiB (20.4%)\n      Processes: 298 Uptime: 1h 15m Init: systemd Shell: Bash inxi: 3.3.38   ",
          "author_fullname": "t2_9cfit5mp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tips for improving my ollama setup? - Ryzen 5 3600/ RTX 3060 12GB VRAM / 64 GB RAM - Qwen3-30B-A3B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9rhgf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753531862,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753530946,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LLM Folks,&lt;/p&gt;\n\n&lt;p&gt;TL/DR: I&amp;#39;m seeking tips for improving my ollama setup with Qwen3, deepseek and nomic-embed for home sized LLM instance.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the LLM game for a couple of weeks now and still learning something new every day. I have an ollama instance on my Ryzen workstation running Debian and control it with a Lenovo X1C laptop which is also running Debian. It&amp;#39;s a home setup so nothing too fancy. You can find the technical details below.&lt;/p&gt;\n\n&lt;p&gt;Purpose of this machine is to answer all kind of questions (qwen3-30B), analyze PDF files (nomic-embed-text:latest) and summarize mails (deepseek-r1:14b), websites (qwen3:14b) etc. I&amp;#39;m still discovering what I could do more with it. Overall it should act as a local AI assistant. I could use some of your wisdom how to improve the setup of that machine for those tasks.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I found the Qwen3-30B-A3B-GGUF model running quite good (10-20 tk/s) for overall questions on this hardware but would like to squeeze a little bit more performance out of it. I&amp;#39;m running it with num_ctx=5120, temperature=0.6, top_K=20, top_P=0.95. What could be improved, to give me a better quality of the answers or improve speed of the model?&lt;/li&gt;\n&lt;li&gt;I would also like to improve the quality of analyzing PDF files. I found that the quality can differ widely. Some PDFs are being analyzed properly for others barely anything is done right, eg. only the metadata is identified but not the content. I use nomic-embed-text:latest for this task. Do you have a suggestion how to improve that or know a better tool I could use?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m also not perfectly satisfied with the summaries of (deepseek-r1:14b) and (qwen3:14b). Both fit into the VRAM but sometimes the language is poor if they have to translate summaries into German or the summaries are way too short and they seem to miss most of the context. I&amp;#39;m also not sure if I need thinking models for that task or if I should try something else?&lt;/li&gt;\n&lt;li&gt;Do you have some overall tips for setting up ollama? I learned that I can play around with KV cache, GPU layers etc. Is it possible to make ollama use all of the 12GB VRAM of the RTX 3060? Somehow it seems that around 1GB is always left free. Are there already some best practices on this for setups like mine? You can find my current settings below. And, would it make a notable difference if I would change the storage location of the models to a fast 1TB nvme? The workstation has a bunch of disks and currently the models reside on an older 256GB SSD.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help improving my setup is appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading so far!&lt;/p&gt;\n\n&lt;p&gt;Below are some technical information and some examples how the models fit into VRAM/RAM:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Environments settings for ollama:\n\nEnvironment=&amp;quot;OLLAMA_DEBUG=0&amp;quot;\nEnvironment=&amp;quot;CUDA_VISIBLE_DEVICES=0&amp;quot;\nEnvironment=&amp;quot;OLLAMA_NEW_ENGINE=1&amp;quot;\nEnvironment=&amp;quot;OLLAMA_LLM_LIBRARY=cuda&amp;quot;\nEnvironment=&amp;quot;OLLAMA_FLASH_ATTENTION=1&amp;quot;\nEnvironment=&amp;quot;OLLAMA_NUM_PARALLEL=1&amp;quot;\nEnvironment=&amp;quot;OLLAMA_MAX_LOADED_MODELS=1&amp;quot;\nEnvironment=&amp;quot;OLLAMA_KV_CACHE_TYPE=q8_0&amp;quot;\nEnvironment=&amp;quot;OLLAMA_MODELS=/chroot/AI/share/ollama/.ollama/models/&amp;quot;\nEnvironment=&amp;quot;OLLAMA_NUM_GPU_LAYERS=36&amp;quot;\nEnvironment=&amp;quot;OLLAMA_ORIGINS=moz-extension://*&amp;quot;\n\n\n\n$ ollama ps                                                                                            \nNAME                                       ID              SIZE     PROCESSOR          UNTIL                \nhf.co/unsloth/Qwen3-30B-A3B-GGUF:Q5_K_M    c8c7e4f7bc56    23 GB    46%/54% CPU/GPU    29 minutes from now \ndeepseek-r1:14b                            c333b7232bdb    10.0 GB  100% GPU           4 minutes from now \nqwen3:14b                                  bdbd181c33f2    10 GB    100% GPU           29 minutes from now   \nnomic-embed-text:latest                    0a109f422b47    849 MB    100% GPU          4 minutes from now   \n\n\n\n$ nvidia-smi \nSat Jul 26 11:30:56 2025                                                                              \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3060        Off |   00000000:08:00.0  On |                  N/A |\n| 68%   54C    P2             57W /  170W |   11074MiB /  12288MiB |     17%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A      4296      C   /chroot/AI/bin/ollama                       11068MiB |\n+-----------------------------------------------------------------------------------------+\n\n\n\n$ inxi -bB                                                                                            \nSystem:                                                                                               \n  Host: morpheus Kernel: 6.15.8-1-liquorix-amd64 arch: x86_64 bits: 64                     \n  Console: pty pts/2 Distro: Debian GNU/Linux 13 (trixie)                                             \nMachine:     \n  Type: Desktop Mobo: ASUSTeK model: TUF GAMING X570-PLUS (WI-FI) v: Rev X.0x                         \n    serial: &amp;lt;superuser required&amp;gt; UEFI: American Megatrends v: 5021 date: 09/29/2024        \nBattery:                                                                                              \n  Message: No system battery data found. Is one present?                                   \nCPU:                                                                                                  \n  Info: 6-core AMD Ryzen 5 3600 [MT MCP] speed (MHz): avg: 1724 min/max: 558/4208          \nGraphics:                                                                                             \n  Device-1: NVIDIA GA106 [GeForce RTX 3060 Lite Hash Rate] driver: nvidia v: 550.163.01    \n  Display: server: X.org v: 1.21.1.16 with: Xwayland v: 24.1.6 driver: X: loaded: nvidia   \n    unloaded: modesetting gpu: nvidia,nvidia-nvswitch tty: 204x45                          \n  API: OpenGL v: 4.6.0 compat-v: 4.5 vendor: mesa v: 25.1.5-0siduction1                    \n    note: console (EGL sourced) renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2, llvmpipe (LLVM 19.1.7\n    256 bits)                                                                                         \n  Info: Tools: api: clinfo, eglinfo, glxinfo, vulkaninfo de: kscreen-console,kscreen-doctor\n    gpu: nvidia-settings,nvidia-smi wl: wayland-info x11: xdriinfo, xdpyinfo, xprop, xrandr\nNetwork:                                                                                              \n  Device-1: Intel Wi-Fi 5 Wireless-AC 9x6x [Thunder Peak] driver: iwlwifi                  \nDrives:                                                                                               \n  Local Storage: total: 6.6 TiB used: 2.61 TiB (39.6%)                                     \nInfo:                                                                                                 \n  Memory: total: N/A available: 62.71 GiB used: 12.78 GiB (20.4%)\n  Processes: 298 Uptime: 1h 15m Init: systemd Shell: Bash inxi: 3.3.38   \n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9rhgf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Speedy-Wonder",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753530946,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Curious how the two new thinking/non thinking stack up vs deepseek.",
          "author_fullname": "t2_frxyil1r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any Rpers test the new qwen 2507 yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ajf9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753476645,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how the two new thinking/non thinking stack up vs deepseek.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9ajf9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Antique_Bit_1049",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ajf9/any_rpers_test_the_new_qwen_2507_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ajf9/any_rpers_test_the_new_qwen_2507_yet/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753476645,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "its show time folks",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vjna",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 107,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 107,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=21733a4ca840c3530b7ab1a5843dfdeba5a3f822",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753439107,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;its show time folks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?auto=webp&amp;s=91ee507d4a4a214e9d8d575336cf37333e5678f2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd52c9fa4a571e95dfd71b26b8e6ebff17bbc117",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159f17b22507b591ab3268fba6357cfbc5b4d5ed",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ce88d6294a42f488b4c5238bcdd5abcbb6bd0f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdec699720d09b0abd832855f564b348eefd2304",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a336e6ae20fea77a6e44bc4f35540e297e8cce2c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c85c9c232a3126b98c3e0be994b7cb036c1e34d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vjna",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "subreddit_subscribers": 504692,
          "created_utc": 1753439107,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt; 1. first scene: function calling by `openai/gpt-4o-mini`, and immidiately succeeded\n&gt; 2. second scene: function calling by `qwen3/qwen3-30b-a3b`, but failing\n\nTrying to function calling to the `qwen3-30b-a3b` model with OpenAI SDK, but fallen into infinite consent for the function calling.\n\nIt seems like that rather than function calling by `tools` property of OpenAI SDK, it would better to perform it by custom prompting.\n\n```typescript\nexport namespace IBbsArticle {\n  export interface ICreate {\n    title: string;\n    body: string;\n    thumbnail: (string &amp; tags.Format&lt;\"uri\"&gt;) | null;\n  }\n}\n```\n\n&gt; Actual [`IBbsArticle.ICreate`](https://typia.io/playground/?script=JYWwDg9gTgLgBAbzgSQDIBsQEExncAYwEMZgIA7OAXzgDMoIQ4AiAAQGciQCALCgeghgApuSJhgzANwAoUJFhwYATwlEANIiVEA5u2p0GTZirXSZMghXbxxYAFwoM2XPmKkKAHma8SOsDDMAHxwALxKqsBEAHTomNF2biRk5J4ycHAAQgBG7FiwhOjCAMrCUABuhMLq6Sy+MP6BMkEAFACUslbk7BBFsRA6LXYdFsDkMGW0RATCWbn5pARFpRVViLX8AFSbtXCbcADiwrZxcEQFS8Ls0bs7GXtwqMA2cACuYHDC5WXKZxdF+nOvGA3wAJnAxkoeLNMplinAACKZG73O4ZfasKDHV5QbqPZ7wCC0T7fKC-c6LAG3fi1Mag4QAD3ajmQOTy-2EAG0ALqyOAbba3OAAYSxJFmRDg5GEAHc-pThCj0UKAOpQYATQFS2XywoS8jgoE8EFXCHwMYwCBQ2ZIpV7IWsMDnLhwMAMMD6AAK7rKpFNRLgBDFEzor3IBA85AdWJgOLxADlZehfkHhOLDRzqbVU+KWm6hOxHAhdnAtmjUShyLRoCBkhQ4AGYNDdZclFacxMS3safcIeQwK8YCy2Qs9dFkKK0xM+dQ2sP5hy+QLy-sAKpgUHis6UClj25C9ebzXbltFOAyjU8bVyroTcZ2ldwR3Opj5j1wb1CX3Af3E95H2ZaDDCMUgdJ0oBdMYB3gRMbwoO94EtN4N3FLMMn-XM30LdZezLLt9gAFXOHRjlPYQAHJ9AQVh8HIABrFARw5aJgFBKg7XRHt7lYxwbHVcgdDgAAybQ9GiAAxGsSG8V5XlY4Il1wwVeweWDA3g0REKtDCJg47sSygwd53ZBVx0PcUZyoOc4HKCBWMU0tlPROAAFEIPYfUyIfIU3KIDzAR3DlDEYa1EWRfcK2fCDX3dL0fQKX9PncwDgMjNCkr84Q81iosSzwlTCOI0jd0uSjEBosYGNZBdTNY9j8K4jIeLgPixkEkSYF0a5JKgWsYBkuTQQU2orMcWz7JkKgLHy-ZR1bTSNWUFE7n2AADaqTL1VaIQCz5xkWuAsTdK4FoEk8SrPSEmxhOE4BaTJXjiY5IUyCBznBYplBsYQQDaZaaQtSZplmDa5rPRk71BfRQeYidg1mYsMny5zvVAc5fgAaWEJb0ua1qzo6rqJKk-rmFk+Sggc5GHknOtKFIEBZkbZsLsVdKO2EUEAH0SF4mB+Pa0TupJ7wAIAWgZ4Rhv5JGnIeVA-PgHTOaUUAmeJa6vPS5Xud5lr+ba4SheJ3rpOYcXJeGqaxEZ9gnRmRiar1HDHMfZAqxJlIGw1lmgqQjmHy4xkFHNcYgYduGpwRvK5ecgiNTPZmJWY-D8LgAAlYRjo88Zkm+VWYETn3k9MhqS1IQvhD5gWHNl8tnOFDTxjgbIIFBHGCrTxuw+b1v2+90LWfPdUYBgSgrubWsoDo0EIBlShq1NmA9M2RqW7b5Rq7a2vXbTgieFeEBsjEYB0AhWsSLgVd0+QFe94Po+T7PtHL+v5Bz2NXhAyISgjqxHPEK+1Lp3AqKBiRdFoMAHQOIVZ+TgKtcgj10CrU0BqOAjMf76CbCQKUVomyH2PkQU+59dCzAniXPcBU174MfkQ9A9guxwAAD5SiQYwlhLR8aC06mJHqfUZLqmCEbHh1xu4IQALKcyiARVQwhvAv2EFsYIIwMhTRlrvCs7tF59S9knMibZkIAW8hWAA8uQZMoVIFPXBG+b8poLynGyLMHWgdajB2gIhWRKBzIhnCJ6XcRB0CeEjuKSmk0gA) type.",
          "author_fullname": "t2_1njlywuqe6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qwen3-30b-a3b has fallen into infinite consent for function calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9jjh3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/e4ctqouo05ff1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 854,
              "width": 468,
              "scrubber_media_url": "https://v.redd.it/e4ctqouo05ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e4ctqouo05ff1/DASHPlaylist.mpd?a=1756124802%2CNTNiYmFmNzQ2MmYzMTk2YzA3MjZkOTlkYTE3Y2NlODM5NWEyZDZkMzFhOWJiNzdkZmYzZGUzMmRmMDE5N2RiMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 285,
              "hls_url": "https://v.redd.it/e4ctqouo05ff1/HLSPlaylist.m3u8?a=1756124802%2CNDU5ZGRlN2NlYmI2OGYyZGI1NWZhYTYyYjdjNTc3ZDExMDg4YzQwNmYyYTA3N2I3YjhiMjQyNjU1YThmYjI1OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dHRobXpvdW8wNWZmMXAdkagH1RH-Rofv-tT1TDOd9yMWaQux6xCPhEcnytZW.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=31d6ea158d74bb005fd26faedff98fb42030aaa1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753501772,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;ol&gt;\n&lt;li&gt;first scene: function calling by &lt;code&gt;openai/gpt-4o-mini&lt;/code&gt;, and immidiately succeeded&lt;/li&gt;\n&lt;li&gt;second scene: function calling by &lt;code&gt;qwen3/qwen3-30b-a3b&lt;/code&gt;, but failing&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Trying to function calling to the &lt;code&gt;qwen3-30b-a3b&lt;/code&gt; model with OpenAI SDK, but fallen into infinite consent for the function calling.&lt;/p&gt;\n\n&lt;p&gt;It seems like that rather than function calling by &lt;code&gt;tools&lt;/code&gt; property of OpenAI SDK, it would better to perform it by custom prompting.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;typescript\nexport namespace IBbsArticle {\n  export interface ICreate {\n    title: string;\n    body: string;\n    thumbnail: (string &amp;amp; tags.Format&amp;lt;&amp;quot;uri&amp;quot;&amp;gt;) | null;\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Actual &lt;a href=\"https://typia.io/playground/?script=JYWwDg9gTgLgBAbzgSQDIBsQEExncAYwEMZgIA7OAXzgDMoIQ4AiAAQGciQCALCgeghgApuSJhgzANwAoUJFhwYATwlEANIiVEA5u2p0GTZirXSZMghXbxxYAFwoM2XPmKkKAHma8SOsDDMAHxwALxKqsBEAHTomNF2biRk5J4ycHAAQgBG7FiwhOjCAMrCUABuhMLq6Sy+MP6BMkEAFACUslbk7BBFsRA6LXYdFsDkMGW0RATCWbn5pARFpRVViLX8AFSbtXCbcADiwrZxcEQFS8Ls0bs7GXtwqMA2cACuYHDC5WXKZxdF+nOvGA3wAJnAxkoeLNMplinAACKZG73O4ZfasKDHV5QbqPZ7wCC0T7fKC-c6LAG3fi1Mag4QAD3ajmQOTy-2EAG0ALqyOAbba3OAAYSxJFmRDg5GEAHc-pThCj0UKAOpQYATQFS2XywoS8jgoE8EFXCHwMYwCBQ2ZIpV7IWsMDnLhwMAMMD6AAK7rKpFNRLgBDFEzor3IBA85AdWJgOLxADlZehfkHhOLDRzqbVU+KWm6hOxHAhdnAtmjUShyLRoCBkhQ4AGYNDdZclFacxMS3safcIeQwK8YCy2Qs9dFkKK0xM+dQ2sP5hy+QLy-sAKpgUHis6UClj25C9ebzXbltFOAyjU8bVyroTcZ2ldwR3Opj5j1wb1CX3Af3E95H2ZaDDCMUgdJ0oBdMYB3gRMbwoO94EtN4N3FLMMn-XM30LdZezLLt9gAFXOHRjlPYQAHJ9AQVh8HIABrFARw5aJgFBKg7XRHt7lYxwbHVcgdDgAAybQ9GiAAxGsSG8V5XlY4Il1wwVeweWDA3g0REKtDCJg47sSygwd53ZBVx0PcUZyoOc4HKCBWMU0tlPROAAFEIPYfUyIfIU3KIDzAR3DlDEYa1EWRfcK2fCDX3dL0fQKX9PncwDgMjNCkr84Q81iosSzwlTCOI0jd0uSjEBosYGNZBdTNY9j8K4jIeLgPixkEkSYF0a5JKgWsYBkuTQQU2orMcWz7JkKgLHy-ZR1bTSNWUFE7n2AADaqTL1VaIQCz5xkWuAsTdK4FoEk8SrPSEmxhOE4BaTJXjiY5IUyCBznBYplBsYQQDaZaaQtSZplmDa5rPRk71BfRQeYidg1mYsMny5zvVAc5fgAaWEJb0ua1qzo6rqJKk-rmFk+Sggc5GHknOtKFIEBZkbZsLsVdKO2EUEAH0SF4mB+Pa0TupJ7wAIAWgZ4Rhv5JGnIeVA-PgHTOaUUAmeJa6vPS5Xud5lr+ba4SheJ3rpOYcXJeGqaxEZ9gnRmRiar1HDHMfZAqxJlIGw1lmgqQjmHy4xkFHNcYgYduGpwRvK5ecgiNTPZmJWY-D8LgAAlYRjo88Zkm+VWYETn3k9MhqS1IQvhD5gWHNl8tnOFDTxjgbIIFBHGCrTxuw+b1v2+90LWfPdUYBgSgrubWsoDo0EIBlShq1NmA9M2RqW7b5Rq7a2vXbTgieFeEBsjEYB0AhWsSLgVd0+QFe94Po+T7PtHL+v5Bz2NXhAyISgjqxHPEK+1Lp3AqKBiRdFoMAHQOIVZ+TgKtcgj10CrU0BqOAjMf76CbCQKUVomyH2PkQU+59dCzAniXPcBU174MfkQ9A9guxwAAD5SiQYwlhLR8aC06mJHqfUZLqmCEbHh1xu4IQALKcyiARVQwhvAv2EFsYIIwMhTRlrvCs7tF59S9knMibZkIAW8hWAA8uQZMoVIFPXBG+b8poLynGyLMHWgdajB2gIhWRKBzIhnCJ6XcRB0CeEjuKSmk0gA\"&gt;&lt;code&gt;IBbsArticle.ICreate&lt;/code&gt;&lt;/a&gt; type.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e4ctqouo05ff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dHRobXpvdW8wNWZmMXAdkagH1RH-Rofv-tT1TDOd9yMWaQux6xCPhEcnytZW.png?format=pjpg&amp;auto=webp&amp;s=172fd3469fbc4187630535db39f6b74150452588",
                  "width": 592,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dHRobXpvdW8wNWZmMXAdkagH1RH-Rofv-tT1TDOd9yMWaQux6xCPhEcnytZW.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=512b79bc41a12be0f6908d9df928bdf9cb5777a2",
                    "width": 108,
                    "height": 197
                  },
                  {
                    "url": "https://external-preview.redd.it/dHRobXpvdW8wNWZmMXAdkagH1RH-Rofv-tT1TDOd9yMWaQux6xCPhEcnytZW.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c3f3ef5f6ed9b026f17a2f071a1f77c769019ac9",
                    "width": 216,
                    "height": 394
                  },
                  {
                    "url": "https://external-preview.redd.it/dHRobXpvdW8wNWZmMXAdkagH1RH-Rofv-tT1TDOd9yMWaQux6xCPhEcnytZW.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d2114e24972e22292ee440e2cecd1c7aa749dfa3",
                    "width": 320,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "dHRobXpvdW8wNWZmMXAdkagH1RH-Rofv-tT1TDOd9yMWaQux6xCPhEcnytZW"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9jjh3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jhnam88",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9jjh3/qwen330ba3b_has_fallen_into_infinite_consent_for/",
          "stickied": false,
          "url": "https://v.redd.it/e4ctqouo05ff1",
          "subreddit_subscribers": 504692,
          "created_utc": 1753501772,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/e4ctqouo05ff1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 854,
              "width": 468,
              "scrubber_media_url": "https://v.redd.it/e4ctqouo05ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e4ctqouo05ff1/DASHPlaylist.mpd?a=1756124802%2CNTNiYmFmNzQ2MmYzMTk2YzA3MjZkOTlkYTE3Y2NlODM5NWEyZDZkMzFhOWJiNzdkZmYzZGUzMmRmMDE5N2RiMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 285,
              "hls_url": "https://v.redd.it/e4ctqouo05ff1/HLSPlaylist.m3u8?a=1756124802%2CNDU5ZGRlN2NlYmI2OGYyZGI1NWZhYTYyYjdjNTc3ZDExMDg4YzQwNmYyYTA3N2I3YjhiMjQyNjU1YThmYjI1OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In 2024, we developed SWE-bench and SWE-agent at Princeton University and helped kickstart the coding agent revolution.\n\nBack then, LMs were optimized to be great at chatting, but not much else. This meant that agent scaffolds had to get very creative (and complicated) to make LMs perform useful work.\n\nBut in 2025 LMs are actively optimized for agentic coding, and we ask:\n\n**What the simplest coding agent that could still score near SotA on the benchmarks?**\n\n**Turns out, it just requires 100 lines of code!**\n\nAnd this system still **resolves 65% of all GitHub issues in the SWE-bench verified benchmark** with Sonnet 4 (for comparison, when Anthropic launched Sonnet 4, they reported 70% with their own scaffold that was never made public).\n\nHonestly, we're all pretty stunned ourselves—we've now spent more than a year developing SWE-agent, and would not have thought that such a small system could perform nearly as good.\n\nNow, admittedly, this is with Sonnet 4, which has probably the strongest agentic post-training of all LMs. But we're also working on updating the fine-tuning of our SWE-agent-LM-32B model specifically for this setting (we posted about this model here after hitting open-weight SotA on SWE-bench earlier this year).\n\nAll open source at https://github.com/SWE-agent/mini-swe-agent. The hello world example is incredibly short &amp; simple (and literally what gave us the 65% with Sonnet 4). But it is also meant as a serious command line tool + research project, so we provide a Claude-code style UI &amp; some utilities on top of that.\n\nWe have some team members from Princeton/Stanford here today, let us know if you have any questions/feedback :)",
          "author_fullname": "t2_mjzvkwd7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mini-swe-agent achieves 65% on SWE-bench in just 100 lines of python code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8z2ut",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 54,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 54,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753455208,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753449849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In 2024, we developed SWE-bench and SWE-agent at Princeton University and helped kickstart the coding agent revolution.&lt;/p&gt;\n\n&lt;p&gt;Back then, LMs were optimized to be great at chatting, but not much else. This meant that agent scaffolds had to get very creative (and complicated) to make LMs perform useful work.&lt;/p&gt;\n\n&lt;p&gt;But in 2025 LMs are actively optimized for agentic coding, and we ask:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What the simplest coding agent that could still score near SotA on the benchmarks?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Turns out, it just requires 100 lines of code!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;And this system still &lt;strong&gt;resolves 65% of all GitHub issues in the SWE-bench verified benchmark&lt;/strong&gt; with Sonnet 4 (for comparison, when Anthropic launched Sonnet 4, they reported 70% with their own scaffold that was never made public).&lt;/p&gt;\n\n&lt;p&gt;Honestly, we&amp;#39;re all pretty stunned ourselves—we&amp;#39;ve now spent more than a year developing SWE-agent, and would not have thought that such a small system could perform nearly as good.&lt;/p&gt;\n\n&lt;p&gt;Now, admittedly, this is with Sonnet 4, which has probably the strongest agentic post-training of all LMs. But we&amp;#39;re also working on updating the fine-tuning of our SWE-agent-LM-32B model specifically for this setting (we posted about this model here after hitting open-weight SotA on SWE-bench earlier this year).&lt;/p&gt;\n\n&lt;p&gt;All open source at &lt;a href=\"https://github.com/SWE-agent/mini-swe-agent\"&gt;https://github.com/SWE-agent/mini-swe-agent&lt;/a&gt;. The hello world example is incredibly short &amp;amp; simple (and literally what gave us the 65% with Sonnet 4). But it is also meant as a serious command line tool + research project, so we provide a Claude-code style UI &amp;amp; some utilities on top of that.&lt;/p&gt;\n\n&lt;p&gt;We have some team members from Princeton/Stanford here today, let us know if you have any questions/feedback :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?auto=webp&amp;s=00beebee91769eaf51532ccc57bd3d275aa63aec",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=891acb349e03755473266d709a20b526d0a3b86c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd0d5cd32a6fcf8bf3c16452f642d334971d721d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9dd070dd1859734ae151f229880b67ab5264767",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9dee162010eff99c6c1be51ac40a4cf02d168191",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48f231f430d01605fb77642bb82952abf5497af2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1061e84b12e7325a09bc650402a84654b187e3ce",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8z2ut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "klieret",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753449849,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here is a crazy idea and I am wondering if it might work. My LLM thinks it will :-)\n\nThe idea is to have a shared server with GPU and up to 8 expert servers. Those would be physical servers each with a dedicated 100 Gbps link to the shared server. The shared server could be with Nvidia 5090 and the expert servers could be AMD Epyc for CPU inference. All servers have a complete copy of the model and can run any random experts for each token.\n\nWe would have the shared server run each forward pass up to the point where the 8 experts get selected. We will there pass the activations to the expert servers, each server running the inference for just one expert. After running through all the layers, the activations get transferred back. That way there are only 2 transfers per token. We are not going to transfer activations by layers, which would otherwise be required.\n\nBy running the experts in parallel like that, we will drastically speed up the generation time.\n\nI am aware we currently do not have software that could do the above. But what are your thoughts on the idea? I am thinking DeepSeek R1, Qwen3 Coder 480b, Kimi K2 etc with tokens speed multiple what is possible today on CPU inference.",
          "author_fullname": "t2_bvqb8ng0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cluster idea for MoE",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9q92z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753526640,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a crazy idea and I am wondering if it might work. My LLM thinks it will :-)&lt;/p&gt;\n\n&lt;p&gt;The idea is to have a shared server with GPU and up to 8 expert servers. Those would be physical servers each with a dedicated 100 Gbps link to the shared server. The shared server could be with Nvidia 5090 and the expert servers could be AMD Epyc for CPU inference. All servers have a complete copy of the model and can run any random experts for each token.&lt;/p&gt;\n\n&lt;p&gt;We would have the shared server run each forward pass up to the point where the 8 experts get selected. We will there pass the activations to the expert servers, each server running the inference for just one expert. After running through all the layers, the activations get transferred back. That way there are only 2 transfers per token. We are not going to transfer activations by layers, which would otherwise be required.&lt;/p&gt;\n\n&lt;p&gt;By running the experts in parallel like that, we will drastically speed up the generation time.&lt;/p&gt;\n\n&lt;p&gt;I am aware we currently do not have software that could do the above. But what are your thoughts on the idea? I am thinking DeepSeek R1, Qwen3 Coder 480b, Kimi K2 etc with tokens speed multiple what is possible today on CPU inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9q92z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Baldur-Norddahl",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9q92z/cluster_idea_for_moe/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9q92z/cluster_idea_for_moe/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753526640,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I’m still new to the local LLM rabbit hole (finally getting my footing), but something keeps bugging me.\n\nWith diffusion models, we’ve got CivitAI — clean galleries, LoRAs, prompts, styles, full user setups, all sorted and shareable. But with local LLMs… where’s the equivalent?\n\nI keep seeing awesome threads about people building custom assistants, setting up workflows, adding voice, text file parsing, personality tweaks, prompt layers, memory systems, all that — but it’s **scattered as hell**. Some code on GitHub, some half-buried Reddit comments, some weird scripts in random HuggingFace spaces.\n\nI’m not asking “why hasn’t someone made it for me,” just genuinely wondering:  \n**Is there a reason this doesn’t exist yet? Technical hurdle? Community split? Lack of central interest?**\n\nI’d love to see a hub where people can share:\n\n* Custom assistant builds (local Jarvis-type setups)\n* Prompt stacks and persona scaffolds\n* Script integrations (voice, file parsing, UI overlays)\n* User-created tools/plugins\n* Examples of real-world use and live demos\n\nIf something like that *does* exist, I’d love a link. If not... is there interest?\n\nI'm new to actually delving into such things — but very curious.",
          "author_fullname": "t2_nmspgzu7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Newbie Thought: Why Isn’t There a “CivitAI for Local LLM Assistants”?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9kc7c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753504353,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I’m still new to the local LLM rabbit hole (finally getting my footing), but something keeps bugging me.&lt;/p&gt;\n\n&lt;p&gt;With diffusion models, we’ve got CivitAI — clean galleries, LoRAs, prompts, styles, full user setups, all sorted and shareable. But with local LLMs… where’s the equivalent?&lt;/p&gt;\n\n&lt;p&gt;I keep seeing awesome threads about people building custom assistants, setting up workflows, adding voice, text file parsing, personality tweaks, prompt layers, memory systems, all that — but it’s &lt;strong&gt;scattered as hell&lt;/strong&gt;. Some code on GitHub, some half-buried Reddit comments, some weird scripts in random HuggingFace spaces.&lt;/p&gt;\n\n&lt;p&gt;I’m not asking “why hasn’t someone made it for me,” just genuinely wondering:&lt;br/&gt;\n&lt;strong&gt;Is there a reason this doesn’t exist yet? Technical hurdle? Community split? Lack of central interest?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I’d love to see a hub where people can share:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Custom assistant builds (local Jarvis-type setups)&lt;/li&gt;\n&lt;li&gt;Prompt stacks and persona scaffolds&lt;/li&gt;\n&lt;li&gt;Script integrations (voice, file parsing, UI overlays)&lt;/li&gt;\n&lt;li&gt;User-created tools/plugins&lt;/li&gt;\n&lt;li&gt;Examples of real-world use and live demos&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If something like that &lt;em&gt;does&lt;/em&gt; exist, I’d love a link. If not... is there interest?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to actually delving into such things — but very curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9kc7c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dedreo58",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9kc7c/newbie_thought_why_isnt_there_a_civitai_for_local/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9kc7c/newbie_thought_why_isnt_there_a_civitai_for_local/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753504353,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Over the past three months, we have continued to scale the **thinking capability** of Qwen3-235B-A22B, improving both the **quality and depth** of reasoning. We are pleased to introduce **Qwen3-235B-A22B-Thinking-2507**, featuring the following key enhancements:\n\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving **state-of-the-art results among open-source thinking models**.\n* **Markedly better general capabilities**, such as instruction following, tool usage, text generation, and alignment with human preferences.\n* **Enhanced 256K long-context understanding** capabilities.",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ven3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#ab96c2",
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=21733a4ca840c3530b7ab1a5843dfdeba5a3f822",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438601,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-235B-A22B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving &lt;strong&gt;state-of-the-art results among open-source thinking models&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?auto=webp&amp;s=91ee507d4a4a214e9d8d575336cf37333e5678f2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd52c9fa4a571e95dfd71b26b8e6ebff17bbc117",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159f17b22507b591ab3268fba6357cfbc5b4d5ed",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ce88d6294a42f488b4c5238bcdd5abcbb6bd0f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdec699720d09b0abd832855f564b348eefd2304",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a336e6ae20fea77a6e44bc4f35540e297e8cce2c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c85c9c232a3126b98c3e0be994b7cb036c1e34d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8ven3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "subreddit_subscribers": 504692,
          "created_utc": 1753438601,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "InternLM S1 Coming Soon!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94kqu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=75ec23e382a3796e4f90d494e80c28b65dd9ba08",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753462685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14875",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?auto=webp&amp;s=3bb5ba819fef00d08459615dc0485bf498916a14",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=86cbc6625abbc12b1404c26848130ec7f4107ee3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7daf77ef66192f415c85595cc07ee248d9f27213",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b29322acb4ebe13751deefcb867e55375cf76e23",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e453326436868385a66b853c71c2c67c024fbf9f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2012cf2d06cf496ad421d2bb8f64b68a5b37227",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa9853ad2c71066e2eb56105b5b16913396bf703",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m94kqu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94kqu/internlm_s1_coming_soon/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14875",
          "subreddit_subscribers": 504692,
          "created_utc": 1753462685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What is my current best choice for running a LLM that can write python code for me?  \n\n\n\nOnly got a 5070 TI 16GB VRAM",
          "author_fullname": "t2_u9ogi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "16Gb vram python coder",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9jkm4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753501869,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is my current best choice for running a LLM that can write python code for me?  &lt;/p&gt;\n\n&lt;p&gt;Only got a 5070 TI 16GB VRAM&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9jkm4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Galahad56",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9jkm4/16gb_vram_python_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9jkm4/16gb_vram_python_coder/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753501869,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey\\~\n\nExciting news in the AI reasoning space! Using AWorld, we just built a Multi-Agent System (MAS) in 6 hours that successfully solved 5 out of 6 IMO 2025 math problems! 🎯\n\n# Research Context:\n\nThis work was inspired by the recent breakthrough paper \"Gemini 2.5 Pro Capable of Winning Gold at IMO 2025\" (Huang &amp; Yang, 2025). The authors noted that \"a multi-agent system where the strengths of different solutions can be combined would lead to stronger mathematical capability.\"\n\n# Our Innovation:\n\nWe took this insight and implemented a collective intelligence approach using our AWorld multi-agent framework, proving that properly orchestrated multi-agent systems can indeed surpass single-model performance.\n\n# Key Achievements:\n\n* 5/6 IMO 2025 problems solved in just 6 hours of development\n* Collective Intelligence &gt; Single Models: Our results validate the paper's hypothesis about multi-agent superiority\n* Rapid Prototyping: AWorld framework enabled quick construction of sophisticated reasoning systems\n* Context Engineering: Demonstrated the critical importance of agent interaction design under current LLM capabilities\n\n# Reproducible Results:\n\nGitHub Repository: [https://github.com/inclusionAI/AWorld](https://github.com/inclusionAI/AWorld)\n\nIMO Implementation: examples/imo/ - Complete with setup scripts, environment configuration, and detailed documentation.",
          "author_fullname": "t2_159bscsg23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Built a Multi-Agent System in 6 Hours That Solves 5/6 IMO 2025 Math Problems - Inspired by Recent Research Breakthroughs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m91mt6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753457696,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753455985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey~&lt;/p&gt;\n\n&lt;p&gt;Exciting news in the AI reasoning space! Using AWorld, we just built a Multi-Agent System (MAS) in 6 hours that successfully solved 5 out of 6 IMO 2025 math problems! 🎯&lt;/p&gt;\n\n&lt;h1&gt;Research Context:&lt;/h1&gt;\n\n&lt;p&gt;This work was inspired by the recent breakthrough paper &amp;quot;Gemini 2.5 Pro Capable of Winning Gold at IMO 2025&amp;quot; (Huang &amp;amp; Yang, 2025). The authors noted that &amp;quot;a multi-agent system where the strengths of different solutions can be combined would lead to stronger mathematical capability.&amp;quot;&lt;/p&gt;\n\n&lt;h1&gt;Our Innovation:&lt;/h1&gt;\n\n&lt;p&gt;We took this insight and implemented a collective intelligence approach using our AWorld multi-agent framework, proving that properly orchestrated multi-agent systems can indeed surpass single-model performance.&lt;/p&gt;\n\n&lt;h1&gt;Key Achievements:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5/6 IMO 2025 problems solved in just 6 hours of development&lt;/li&gt;\n&lt;li&gt;Collective Intelligence &amp;gt; Single Models: Our results validate the paper&amp;#39;s hypothesis about multi-agent superiority&lt;/li&gt;\n&lt;li&gt;Rapid Prototyping: AWorld framework enabled quick construction of sophisticated reasoning systems&lt;/li&gt;\n&lt;li&gt;Context Engineering: Demonstrated the critical importance of agent interaction design under current LLM capabilities&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Reproducible Results:&lt;/h1&gt;\n\n&lt;p&gt;GitHub Repository: &lt;a href=\"https://github.com/inclusionAI/AWorld\"&gt;https://github.com/inclusionAI/AWorld&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;IMO Implementation: examples/imo/ - Complete with setup scripts, environment configuration, and detailed documentation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?auto=webp&amp;s=31ab9ee7e242560f3c3f9a3bfd18f4242f46d0aa",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb99394a2af815f3715108c671ceba9c647d5f86",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ff23eb578a14fc977cd8d5e672331e2b9da7a5b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdf79eb5a345fc2fcb870c91d0d50ee4d055c6ae",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e423d4f75c60ac9d9ec6eb446998a3f28c3637f6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=29fbe775ffbd5b003bdf0fa1f00a7212e59c3132",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd5d102adfdb6875b81e70e4a6e3e0317ac3b393",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m91mt6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Vivid_Might1225",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753455985,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was thinking about what to offload with --override-tensor and was thinking that instead of guessing, measuring would be best.\n\nFor MoE, I presume that all non shared experts don't have the same odds of activation for a given specific task / corpus. To optimize program compilation, one can instrument the generated code to profile the code execution and then compile according to the collected information (e.g. about branch taken).\n\nIt seems logical to me that inference engine would allow the same : running in a profile mode to generate data about execution , running in an way that is informed by collected data.\n\nIs it a think (Which inference engines would collect such data )? and if not, why not ? ",
          "author_fullname": "t2_7rqtc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM (esp. MoE) inference profiling : is it a thing and if not, why not ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9nyk4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753517538,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking about what to offload with --override-tensor and was thinking that instead of guessing, measuring would be best.&lt;/p&gt;\n\n&lt;p&gt;For MoE, I presume that all non shared experts don&amp;#39;t have the same odds of activation for a given specific task / corpus. To optimize program compilation, one can instrument the generated code to profile the code execution and then compile according to the collected information (e.g. about branch taken).&lt;/p&gt;\n\n&lt;p&gt;It seems logical to me that inference engine would allow the same : running in a profile mode to generate data about execution , running in an way that is informed by collected data.&lt;/p&gt;\n\n&lt;p&gt;Is it a think (Which inference engines would collect such data )? and if not, why not ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9nyk4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "un_passant",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9nyk4/llm_esp_moe_inference_profiling_is_it_a_thing_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9nyk4/llm_esp_moe_inference_profiling_is_it_a_thing_and/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753517538,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "trying to optimize how i load relevant context into new chats (mostly claude api). currently have hundreds of structured documents/notes but manual selection is getting inefficient.\n\ncurrent workflow: manually pick relevant docs &gt; paste into new conversation &gt; often end up with redundant context or miss relevant stuff &gt; high token costs ($300-500/month)\n\nas the document library grows, this is becoming unsustainable. anyone solved similar problems?\n\nideally looking for:\n- semantic search to auto-suggest relevant docs before i paste context\n- local/offline solution (don't want docs going to cloud)\nminimal technical setup\n- something that learns document relationships over time\n\nthinking RAG type solution but most seem geared toward developers, but preferably easy to setup. \n\nanyone found user friendly tools for this that can run without a super powerful GPU?",
          "author_fullname": "t2_1sm1o6jilv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to manage context/notes locally for API usage while optimizing token costs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9nwk7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753517320,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;trying to optimize how i load relevant context into new chats (mostly claude api). currently have hundreds of structured documents/notes but manual selection is getting inefficient.&lt;/p&gt;\n\n&lt;p&gt;current workflow: manually pick relevant docs &amp;gt; paste into new conversation &amp;gt; often end up with redundant context or miss relevant stuff &amp;gt; high token costs ($300-500/month)&lt;/p&gt;\n\n&lt;p&gt;as the document library grows, this is becoming unsustainable. anyone solved similar problems?&lt;/p&gt;\n\n&lt;p&gt;ideally looking for:\n- semantic search to auto-suggest relevant docs before i paste context\n- local/offline solution (don&amp;#39;t want docs going to cloud)\nminimal technical setup\n- something that learns document relationships over time&lt;/p&gt;\n\n&lt;p&gt;thinking RAG type solution but most seem geared toward developers, but preferably easy to setup. &lt;/p&gt;\n\n&lt;p&gt;anyone found user friendly tools for this that can run without a super powerful GPU?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9nwk7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "boomerdaycare",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9nwk7/best_way_to_manage_contextnotes_locally_for_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9nwk7/best_way_to_manage_contextnotes_locally_for_api/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753517320,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been trying a lot of different combinations with static learning rates, and i have to set up the test inference for every single epoch to determine the sweet spot because i doubt that any automation that does not involve running two simultaneous llm will be able to accurate tell when the results are desirable. But maybe i am doing everything wong? I only got what i wanted after 10 runs of 4e-3, and that is with a datasets of 90 rows, all in a single batch. Perhaps this is a rare scenario, but good to have found something working. Any advice or experiences that i must learn about? As I prefer not to waste more compute doing the trial and error with datasets a thousand times the size.",
          "author_fullname": "t2_1ti9nuwlx8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does it ever make sense to train for 10 epochs? Or did i do it all wrong?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96b4h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753466600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been trying a lot of different combinations with static learning rates, and i have to set up the test inference for every single epoch to determine the sweet spot because i doubt that any automation that does not involve running two simultaneous llm will be able to accurate tell when the results are desirable. But maybe i am doing everything wong? I only got what i wanted after 10 runs of 4e-3, and that is with a datasets of 90 rows, all in a single batch. Perhaps this is a rare scenario, but good to have found something working. Any advice or experiences that i must learn about? As I prefer not to waste more compute doing the trial and error with datasets a thousand times the size.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96b4h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BulkyPlay7704",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96b4h/does_it_ever_make_sense_to_train_for_10_epochs_or/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96b4h/does_it_ever_make_sense_to_train_for_10_epochs_or/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753466600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3vgq8ww9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD ROCm 7 Installation &amp; Test Guide / Fedora Linux RX 9070 - ComfyUI Blender LMStudio SDNext Flux",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9gs61",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7qDlHpeTmC0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AMD ROCm 7 Installation &amp;amp; Test Guide / Fedora Linux RX 9070 - ComfyUI Blender LMStudio SDNext Flux\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AMD ROCm 7 Installation &amp; Test Guide / Fedora Linux RX 9070 - ComfyUI Blender LMStudio SDNext Flux",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7qDlHpeTmC0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AMD ROCm 7 Installation &amp;amp; Test Guide / Fedora Linux RX 9070 - ComfyUI Blender LMStudio SDNext Flux\"&gt;&lt;/iframe&gt;",
              "author_name": "Open Game and Development",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7qDlHpeTmC0/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@OpenGameDev"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7qDlHpeTmC0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AMD ROCm 7 Installation &amp;amp; Test Guide / Fedora Linux RX 9070 - ComfyUI Blender LMStudio SDNext Flux\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m9gs61",
            "height": 200
          },
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/H1S_ghyDQg-VMEV8JXw6B9oG5nmC_pqvozEXErkEtwg.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=54b8a4666377e0b1a0b2acbcd5d89152969f6a79",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753493166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtube.com/watch?v=7qDlHpeTmC0&amp;si=abStnvRLk3lAT1FW",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/H1S_ghyDQg-VMEV8JXw6B9oG5nmC_pqvozEXErkEtwg.jpeg?auto=webp&amp;s=c1dfae92251d09c0c245fae7c61add3ead685200",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/H1S_ghyDQg-VMEV8JXw6B9oG5nmC_pqvozEXErkEtwg.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a1c36e02a84367d85741b43e8eacc884b42ec24",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/H1S_ghyDQg-VMEV8JXw6B9oG5nmC_pqvozEXErkEtwg.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8868da18d50f82cfd8d101cba3659a689b623d0",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/H1S_ghyDQg-VMEV8JXw6B9oG5nmC_pqvozEXErkEtwg.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddc2b3dfe5fb294fa364dede45420fcbf14e4054",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "H1S_ghyDQg-VMEV8JXw6B9oG5nmC_pqvozEXErkEtwg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m9gs61",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "B4rr3l",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9gs61/amd_rocm_7_installation_test_guide_fedora_linux/",
          "stickied": false,
          "url": "https://youtube.com/watch?v=7qDlHpeTmC0&amp;si=abStnvRLk3lAT1FW",
          "subreddit_subscribers": 504692,
          "created_utc": 1753493166,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "AMD ROCm 7 Installation &amp; Test Guide / Fedora Linux RX 9070 - ComfyUI Blender LMStudio SDNext Flux",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7qDlHpeTmC0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"AMD ROCm 7 Installation &amp;amp; Test Guide / Fedora Linux RX 9070 - ComfyUI Blender LMStudio SDNext Flux\"&gt;&lt;/iframe&gt;",
              "author_name": "Open Game and Development",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7qDlHpeTmC0/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@OpenGameDev"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been working on a cleaner UI for uploading and managing custom models — here are some early Figma drafts of the connection flow and model details page. Still a work in progress, but I’d love to hear your thoughts!  \n  \nFor those who are new here: I’m building this platform as a solo pet project in my free time, and I’ve been sharing my progress here on r/LocalLLaMA to gather feedback and ideas. Your input really helps shape the direction.  \n  \nI’m adding support for local backend connection because not everyone wants to rely on third-party APIs or cloud services. Many people already run models locally, and this gives them full control over performance, privacy, and customization.  \n  \nIf you’re interested in testing the platform, I’d be happy to send you an invite — just shoot me a DM!",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New UI for uploading and managing custom models (Figma mockups)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 127,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "fulqfst1y1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 98,
                  "x": 108,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82cd77de0ca104006b080ce11404b464a14f50b0"
                },
                {
                  "y": 196,
                  "x": 216,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=db2d56b61032e2659fd959720ebdc78eebab279d"
                },
                {
                  "y": 291,
                  "x": 320,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=664b072883559c46b0fa4015ff69016002263582"
                },
                {
                  "y": 583,
                  "x": 640,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=69fb7d42a37eb74bec24fbf843282013d97897b9"
                },
                {
                  "y": 875,
                  "x": 960,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6c33cbb9244adcb68539df6e892a574b0d9d8d2"
                },
                {
                  "y": 984,
                  "x": 1080,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb0ff00f02a7a97c54ecb0f5a1a00ec297c1abdd"
                }
              ],
              "s": {
                "y": 1751,
                "x": 1920,
                "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=50c3f7aea91e1460a23becf2a7ed1c33d764db10"
              },
              "id": "fulqfst1y1ff1"
            },
            "dqsr5j52y1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=71ca5fd8d2aa192f92c736a5d06b6c92c5573fae"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e9240f3958d87a71bb54b0be7bc91da1412786d"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b48ca5183c8df74e34feb3b947f7c7b7e40043d"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffc158bdd7ade8804eb19ad6ee6c16fe6762252a"
                },
                {
                  "y": 601,
                  "x": 960,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1104132469c193ee88fd1c05399f974a8f761107"
                },
                {
                  "y": 676,
                  "x": 1080,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7679dd5a2596a1a9280c1750ca15833d4cff87f4"
                }
              ],
              "s": {
                "y": 1202,
                "x": 1920,
                "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d05e615307e6fd8002190d245dea45e46920e27d"
              },
              "id": "dqsr5j52y1ff1"
            }
          },
          "name": "t3_1m94uea",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 13,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "fulqfst1y1ff1",
                "id": 713831543
              },
              {
                "media_id": "dqsr5j52y1ff1",
                "id": 713831544
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fIwpmtA8COvjymHuiMQDj4eWoej49NBiwVAmQsDVTfI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753463269,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been working on a cleaner UI for uploading and managing custom models — here are some early Figma drafts of the connection flow and model details page. Still a work in progress, but I’d love to hear your thoughts!  &lt;/p&gt;\n\n&lt;p&gt;For those who are new here: I’m building this platform as a solo pet project in my free time, and I’ve been sharing my progress here on &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; to gather feedback and ideas. Your input really helps shape the direction.  &lt;/p&gt;\n\n&lt;p&gt;I’m adding support for local backend connection because not everyone wants to rely on third-party APIs or cloud services. Many people already run models locally, and this gives them full control over performance, privacy, and customization.  &lt;/p&gt;\n\n&lt;p&gt;If you’re interested in testing the platform, I’d be happy to send you an invite — just shoot me a DM!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m94uea",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m94uea",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94uea/new_ui_for_uploading_and_managing_custom_models/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m94uea",
          "subreddit_subscribers": 504692,
          "created_utc": 1753463269,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_10pze1d3jf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Executive Order: \"Preventing Woke AI in the Federal Government\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8l648",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 258,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 258,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9f4b098cfa68cb7be71aac428f695841b0c9cbbe",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753403766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "whitehouse.gov",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?auto=webp&amp;s=ecf43e8e82602652ec95e06f13b6ce18da205b9c",
                  "width": 1200,
                  "height": 628
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c1e4661cbba0b6e1e232602fbabfa0384ba0123",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b84255c302c8464ea76b251e4d4ab64cac0ec723",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7c4bae3b4c97261af353a9ec64d3ef027f6deac",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb89e898879eb7adef969749433776a6f6a543ad",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16221a57c07b16c8cef11acfc0eeb15f6f1254e",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db29c2e5309166fabf6283791735d6762adf4b55",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8l648",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NunyaBuzor",
          "discussion_type": null,
          "num_comments": 146,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/",
          "stickied": false,
          "url": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753403766,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Windows would be unusable for me without everything. I have over a hundred terabytes of data which I search in an instant using this tool everyday, across multiple nases, and I've yet found anything that can rival everything even on mac or linux.\n\nBut I just wish there was an llm implementation which can take this functionality to the next level, and while I've tried to vibe code something myself, it seems to me that the existing llms hallucinate too much, and it would require a purpose built llm. I don't have the resources or hardware to build/train an llm, nor the expertise to make a structured natural language process that works in every instance like an llm. \n\nLike you can interface with ex.exe which is the command line interface for everything, and I've successfully gotten a bit into being able to query for files of this type above x size. But llms simply lack the consistency and reliability for a proper search function that works time over time.\n\nI just can't believe this hasn't already been made. Being able to just ask, show me pictures above 10mb that I have from july 2025 or something like that and seeing results would be a godsend, instead of having to type in regex. \n\nNow this isn't rag, well I suppose it could be? All I'm thinking for llms in this case is just being an interpreter than takes natural language and converts into everything reg ex.\n\nI assume there is more that could be done, using regex as well, but that would be heavily based on the size of database in terms of the context size required.\n\nThis is kind of a newb question, but I'm just curious if there already is an solution out there.",
          "author_fullname": "t2_jchjkp4r1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why isn't/Is there a natural language search interface for Everything from void tools?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9mdtj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753511511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Windows would be unusable for me without everything. I have over a hundred terabytes of data which I search in an instant using this tool everyday, across multiple nases, and I&amp;#39;ve yet found anything that can rival everything even on mac or linux.&lt;/p&gt;\n\n&lt;p&gt;But I just wish there was an llm implementation which can take this functionality to the next level, and while I&amp;#39;ve tried to vibe code something myself, it seems to me that the existing llms hallucinate too much, and it would require a purpose built llm. I don&amp;#39;t have the resources or hardware to build/train an llm, nor the expertise to make a structured natural language process that works in every instance like an llm. &lt;/p&gt;\n\n&lt;p&gt;Like you can interface with ex.exe which is the command line interface for everything, and I&amp;#39;ve successfully gotten a bit into being able to query for files of this type above x size. But llms simply lack the consistency and reliability for a proper search function that works time over time.&lt;/p&gt;\n\n&lt;p&gt;I just can&amp;#39;t believe this hasn&amp;#39;t already been made. Being able to just ask, show me pictures above 10mb that I have from july 2025 or something like that and seeing results would be a godsend, instead of having to type in regex. &lt;/p&gt;\n\n&lt;p&gt;Now this isn&amp;#39;t rag, well I suppose it could be? All I&amp;#39;m thinking for llms in this case is just being an interpreter than takes natural language and converts into everything reg ex.&lt;/p&gt;\n\n&lt;p&gt;I assume there is more that could be done, using regex as well, but that would be heavily based on the size of database in terms of the context size required.&lt;/p&gt;\n\n&lt;p&gt;This is kind of a newb question, but I&amp;#39;m just curious if there already is an solution out there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9mdtj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CystralSkye",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9mdtj/why_isntis_there_a_natural_language_search/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9mdtj/why_isntis_there_a_natural_language_search/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753511511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.\n\nFor example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. \n\nSo to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.\n\nIf I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers \\~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!\n\nGo as big as you can!",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "N + N size GPU != 2N sized GPU, go big if you can",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vu80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753440200,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.&lt;/p&gt;\n\n&lt;p&gt;For example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. &lt;/p&gt;\n\n&lt;p&gt;So to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.&lt;/p&gt;\n\n&lt;p&gt;If I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers ~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!&lt;/p&gt;\n\n&lt;p&gt;Go as big as you can!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m8vu80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753440200,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was thinking how to scale a GPU cluster. Not talking about CPUs here.  \nUsually have heard that \"buy Epyc\" and add 6-8 GPUs in it. but thats it then, it wont scale more.  \nBut now that I have learned how to use vLLM, and it can utilize multi GPU and also multi server GPUs, was thinking what if creating a cluster with fast networking and vLLM RAY?   \n  \nHas anyone done it? \n\nI happen to have spare Mellanox Connect-x6 cards, 2x25GB with ROCE, some 25gb and 100gb switches.   \nI do not have any Epycs, but loads of AM5 boards and 7000 cpus and memory.   \nSo my understanding is, if creating multiple servers, with 1-2 GPUs in each 8x or 16x pcie 4.0 connected, and then creating a NFS file server for model sharing and connecting all them with 2x25GB DAC, I guess it would work?  \nThat 5GB/s connection will be in tensor parallel a bottleneck but how much? Some say even 4x pcie 4.0 is not a bottleneck in vLLM tensor parallel and its about 8GB/s. \n\nLater when pcie 5.0 4x network cards are available it could be upgraded to 100GB networking. \n\nSo with this kind of setup, even 100 gpus could server the same model? \n\n\"**RDMA over Converged Ethernet (RoCE):** The ConnectX-6 cards are designed for RoCE. This is a critical advantage. RoCE allows Remote Direct Memory Access, meaning data can be transferred directly between the GPU memories on different servers, bypassing the CPU.\"",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multi GPU multi server inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9cg16",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753481338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was thinking how to scale a GPU cluster. Not talking about CPUs here.&lt;br/&gt;\nUsually have heard that &amp;quot;buy Epyc&amp;quot; and add 6-8 GPUs in it. but thats it then, it wont scale more.&lt;br/&gt;\nBut now that I have learned how to use vLLM, and it can utilize multi GPU and also multi server GPUs, was thinking what if creating a cluster with fast networking and vLLM RAY?   &lt;/p&gt;\n\n&lt;p&gt;Has anyone done it? &lt;/p&gt;\n\n&lt;p&gt;I happen to have spare Mellanox Connect-x6 cards, 2x25GB with ROCE, some 25gb and 100gb switches.&lt;br/&gt;\nI do not have any Epycs, but loads of AM5 boards and 7000 cpus and memory.&lt;br/&gt;\nSo my understanding is, if creating multiple servers, with 1-2 GPUs in each 8x or 16x pcie 4.0 connected, and then creating a NFS file server for model sharing and connecting all them with 2x25GB DAC, I guess it would work?&lt;br/&gt;\nThat 5GB/s connection will be in tensor parallel a bottleneck but how much? Some say even 4x pcie 4.0 is not a bottleneck in vLLM tensor parallel and its about 8GB/s. &lt;/p&gt;\n\n&lt;p&gt;Later when pcie 5.0 4x network cards are available it could be upgraded to 100GB networking. &lt;/p&gt;\n\n&lt;p&gt;So with this kind of setup, even 100 gpus could server the same model? &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&lt;strong&gt;RDMA over Converged Ethernet (RoCE):&lt;/strong&gt; The ConnectX-6 cards are designed for RoCE. This is a critical advantage. RoCE allows Remote Direct Memory Access, meaning data can be transferred directly between the GPU memories on different servers, bypassing the CPU.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9cg16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753481338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can't stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it's not without limitations.\n\nThat’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create [**Wren Coder CLI**](https://github.com/wren-coder/wren-coder-cli): an open-source, model-agnostic AI agent for coding assistance and terminal workflows.\n\n**Why Fork?**\n\n1. Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.\n2. I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.\n3. My priorities as a solo developer probably don't align with respective model companies.\n4. Why not? I just want to experiment and try new things.\n5. I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.\n\n  \n**What am I shipping?**\n\nOver the next few weeks, I plan to focus on the following:\n\n1. Improving compatibility with a wide range of models\n2. Adding chunking/compression logic to fix token limit errors with models with smaller context windows \\*cough\\* deepseek.\n3. Splitting up the CLI and SDK\n4. Documentation\n5. Multi-model support????\n\n  \nMaybe this is overly ambitious, but again why not? I'll keep y'all posted! Wish me luck!\n\n[https://github.com/wren-coder/wren-coder-cli](https://github.com/wren-coder/wren-coder-cli)",
          "author_fullname": "t2_1sivuwuvea",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why I Forked Qwen Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qj9w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 83,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 83,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753420060,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can&amp;#39;t stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it&amp;#39;s not without limitations.&lt;/p&gt;\n\n&lt;p&gt;That’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create &lt;a href=\"https://github.com/wren-coder/wren-coder-cli\"&gt;&lt;strong&gt;Wren Coder CLI&lt;/strong&gt;&lt;/a&gt;: an open-source, model-agnostic AI agent for coding assistance and terminal workflows.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why Fork?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.&lt;/li&gt;\n&lt;li&gt;I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.&lt;/li&gt;\n&lt;li&gt;My priorities as a solo developer probably don&amp;#39;t align with respective model companies.&lt;/li&gt;\n&lt;li&gt;Why not? I just want to experiment and try new things.&lt;/li&gt;\n&lt;li&gt;I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;What am I shipping?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Over the next few weeks, I plan to focus on the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Improving compatibility with a wide range of models&lt;/li&gt;\n&lt;li&gt;Adding chunking/compression logic to fix token limit errors with models with smaller context windows *cough* deepseek.&lt;/li&gt;\n&lt;li&gt;Splitting up the CLI and SDK&lt;/li&gt;\n&lt;li&gt;Documentation&lt;/li&gt;\n&lt;li&gt;Multi-model support????&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Maybe this is overly ambitious, but again why not? I&amp;#39;ll keep y&amp;#39;all posted! Wish me luck!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/wren-coder/wren-coder-cli\"&gt;https://github.com/wren-coder/wren-coder-cli&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?auto=webp&amp;s=79d64b1289209fa79a11669d150a850e8d6ce23a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b1fa561118622479aef7fd3a0006f928715e0b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=166a7f91b07c437bb627e895db8e0077a2423927",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=075d04567db44f06e5b0bfb432fe8f6f37a04713",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=792082e48bea2b952a5e950a98b815d551f583b3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ca81ad125907d790e752b70ee99ea801e80dfa3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64b7d8f9f97d9fd67f00438f1d60441579fa0474",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8qj9w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ryanwang4thepeople",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753420060,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ok next big open source model also from China only ! Which is about to release",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m88jdh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 879,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 879,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/c08j-el568SvKGYGEd0gZbFM3-WDn7gmHlrwY9mVv5E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753373337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19\"&gt;https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/j6rwug34juef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/j6rwug34juef1.png?auto=webp&amp;s=18f17b9ceaa2b5d279bcbb0bb243851740e717c4",
                  "width": 1080,
                  "height": 1419
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb9a593e1fb7f521dc0f069833d5296c3e11f7e9",
                    "width": 108,
                    "height": 141
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6484155fe20574b23faf0c91f18281d0b64f0ef8",
                    "width": 216,
                    "height": 283
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=168aa62e13429a9a7659a2b00480eeed71c2b0ec",
                    "width": 320,
                    "height": 420
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a04ad517c7ca8eeeb00ee48288d8f17c562ca63c",
                    "width": 640,
                    "height": 840
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=66e086c5c04e9638800c62f55dfe3f8b4b914e18",
                    "width": 960,
                    "height": 1261
                  },
                  {
                    "url": "https://preview.redd.it/j6rwug34juef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e86fd4d029b2f6132d30619ca2201a26cba6b494",
                    "width": 1080,
                    "height": 1419
                  }
                ],
                "variants": {},
                "id": "yDDBcSzVcQ88JLp4cep6OXeVFwV_1fmwTlslE0j_6FU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m88jdh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 160,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/",
          "stickied": false,
          "url": "https://i.redd.it/j6rwug34juef1.png",
          "subreddit_subscribers": 504692,
          "created_utc": 1753373337,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone run the q4ks versions of these, which one is winning for code generation... Too early for consensus yet? Thx",
          "author_fullname": "t2_1l3z4stvkq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The new Kimi vs. new qwen3 for coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9avmv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753477447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone run the q4ks versions of these, which one is winning for code generation... Too early for consensus yet? Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9avmv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Agreeable-Prompt-666",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9avmv/the_new_kimi_vs_new_qwen3_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9avmv/the_new_kimi_vs_new_qwen3_for_coding/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753477447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm about to start building my personal AI companion and during my research came across this [awesome list](https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate) of AI companion projects that I wanted to share with the community.\n\n| Companion | Lang | License | Stack | Category |\n| -- | -- | -- | -- | -- |\n| [枫云AI虚拟伙伴Web版](https://github.com/swordswind/ai_virtual_mate_web) - [Wiki](https://deepwiki.com/swordswind/ai_virtual_mate_web) | zh | gpl-3.0 | python | companion |\n| [Muice-Chatbot](https://github.com/Moemu/Muice-Chatbot) - [Wiki](https://deepwiki.com/Moemu/Muice-Chatbot) | zh, en | mit | python | companion |\n| [MuiceBot](https://github.com/Moemu/MuiceBot) - [Wiki](https://deepwiki.com/Moemu/MuiceBot) | zh | bsd-3-clause | python | companion |\n| [kirara-ai](https://github.com/lss233/kirara-ai) - [Wiki](https://deepwiki.com/lss233/kirara-ai) | zh | agpl-3.0 | python | companion |\n| [my-neuro](https://github.com/morettt/my-neuro) - [Wiki](https://deepwiki.com/morettt/my-neuro) | zh, en | mit | python | companion |\n| [AIAvatarKit](https://github.com/uezo/aiavatarkit) - [Wiki](https://deepwiki.com/uezo/aiavatarkit) | en | apache-2.0 | python | companion |\n| [xinghe-AI](https://github.com/lijiaxing1997/xinghe-AI) - [Wiki](https://deepwiki.com/lijiaxing1997/xinghe-AI) | zh |  | python | companion |\n| [MaiBot](https://github.com/MaiM-with-u/MaiBot) | zh | gpl-3.0 | python | companion |\n| [AI-YinMei](https://github.com/worm128/AI-YinMei) - [Wiki](https://deepwiki.com/worm128/AI-YinMei) | zh | bsd-2-clause | python, web | vtuber |\n| [Open-LLM-VTuber](https://github.com/Open-LLM-VTuber/Open-LLM-VTuber) - [Wiki](https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber) | en | mit | python, web | vtuber, companion |\n| [KouriChat](https://github.com/KouriChat/KouriChat) - [Wiki](https://deepwiki.com/KouriChat/KouriChat) | zh | custom | python, web | companion |\n| [Streamer-Sales](https://github.com/PeterH0323/Streamer-Sales) - [Wiki](https://deepwiki.com/PeterH0323/Streamer-Sales) | zh | agpl-3.0 | python, web | vtuber, professional |\n| [AI-Vtuber](https://github.com/Ikaros-521/AI-Vtuber) - [Wiki](https://deepwiki.com/Ikaros-521/AI-Vtuber) | zh | gpl-3.0 | python, web | vtuber |\n| [SillyTavern](https://github.com/SillyTavern/SillyTavern) - [Wiki](https://deepwiki.com/SillyTavern/SillyTavern) | en | agpl-3.0 | web | companion |\n| [lobe-vidol](https://github.com/lobehub/lobe-vidol) - [Wiki](https://deepwiki.com/lobehub/lobe-vidol) | en | apache-2.0 | web | companion |\n| [Bella](https://github.com/Jackywine/Bella) - [Wiki](https://deepwiki.com/Jackywine/Bella) | zh | mit | web | companion |\n| [AITuberKit](https://github.com/tegnike/aituber-kit) - [Wiki](https://deepwiki.com/tegnike/aituber-kit) | en, ja | custom | web | vtuber, companion |\n| [airi](https://github.com/moeru-ai/airi) - [Wiki](https://deepwiki.com/moeru-ai/airi) | en | mit | tauri | vtuber, companion |\n| [amica](https://github.com/semperai/amica) - [Wiki](https://deepwiki.com/semperai/amica) | en | mit | tauri | companion |\n| [ChatdollKit](https://github.com/uezo/ChatdollKit) - [Wiki](https://deepwiki.com/uezo/ChatdollKit) | en, ja | apache-2.0 | unity | companion |\n| [Unity-AI-Chat-Toolkit](https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit) - [Wiki](https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit) | zh | mit | unity | companion |\n| [ZcChat](https://github.com/Zao-chen/ZcChat) - [Wiki](https://deepwiki.com/Zao-chen/ZcChat) | zh, en | gpl-3.0 | c++ | galge |\n| [handcrafted-persona-engine](https://github.com/fagenorn/handcrafted-persona-engine) - [Wiki](https://deepwiki.com/fagenorn/handcrafted-persona-engine) | en |  | dotnet | vtuber, companion |\n\n**Notes**:\n\n- I've made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.\n- Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking\n- Several projects only have Chinese READMEs (marked as zh), but I've included DeepWiki links to help with understanding. There's been significant progress in that community so I think it's worth exploring.\n\nI'm starting this thread for two reasons: First, I'd love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I'm particularly interested in seeing what alternatives to Grok's Ani this community will build in the future.\n\nIf I've missed anything, please let me know and I'll update the list.",
          "author_fullname": "t2_1s7w9cgxcq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Companion Thread",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wg2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753442543,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753442258,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to start building my personal AI companion and during my research came across this &lt;a href=\"https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate\"&gt;awesome list&lt;/a&gt; of AI companion projects that I wanted to share with the community.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Companion&lt;/th&gt;\n&lt;th&gt;Lang&lt;/th&gt;\n&lt;th&gt;License&lt;/th&gt;\n&lt;th&gt;Stack&lt;/th&gt;\n&lt;th&gt;Category&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/swordswind/ai_virtual_mate_web\"&gt;枫云AI虚拟伙伴Web版&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/swordswind/ai_virtual_mate_web\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Moemu/Muice-Chatbot\"&gt;Muice-Chatbot&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Moemu/Muice-Chatbot\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Moemu/MuiceBot\"&gt;MuiceBot&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Moemu/MuiceBot\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;bsd-3-clause&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lss233/kirara-ai\"&gt;kirara-ai&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lss233/kirara-ai\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/morettt/my-neuro\"&gt;my-neuro&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/morettt/my-neuro\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/uezo/aiavatarkit\"&gt;AIAvatarKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/uezo/aiavatarkit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lijiaxing1997/xinghe-AI\"&gt;xinghe-AI&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lijiaxing1997/xinghe-AI\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/MaiM-with-u/MaiBot\"&gt;MaiBot&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/worm128/AI-YinMei\"&gt;AI-YinMei&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/worm128/AI-YinMei\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;bsd-2-clause&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber\"&gt;Open-LLM-VTuber&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/KouriChat/KouriChat\"&gt;KouriChat&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/KouriChat/KouriChat\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;custom&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/PeterH0323/Streamer-Sales\"&gt;Streamer-Sales&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/PeterH0323/Streamer-Sales\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber, professional&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Ikaros-521/AI-Vtuber\"&gt;AI-Vtuber&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Ikaros-521/AI-Vtuber\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/SillyTavern/SillyTavern\"&gt;SillyTavern&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/SillyTavern/SillyTavern\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lobehub/lobe-vidol\"&gt;lobe-vidol&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lobehub/lobe-vidol\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Jackywine/Bella\"&gt;Bella&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Jackywine/Bella\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/tegnike/aituber-kit\"&gt;AITuberKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/tegnike/aituber-kit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en, ja&lt;/td&gt;\n&lt;td&gt;custom&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/moeru-ai/airi\"&gt;airi&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/moeru-ai/airi\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;tauri&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/semperai/amica\"&gt;amica&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/semperai/amica\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;tauri&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/uezo/ChatdollKit\"&gt;ChatdollKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/uezo/ChatdollKit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en, ja&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;unity&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit\"&gt;Unity-AI-Chat-Toolkit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;unity&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Zao-chen/ZcChat\"&gt;ZcChat&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Zao-chen/ZcChat\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;c++&lt;/td&gt;\n&lt;td&gt;galge&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/fagenorn/handcrafted-persona-engine\"&gt;handcrafted-persona-engine&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/fagenorn/handcrafted-persona-engine\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;dotnet&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.&lt;/li&gt;\n&lt;li&gt;Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking&lt;/li&gt;\n&lt;li&gt;Several projects only have Chinese READMEs (marked as zh), but I&amp;#39;ve included DeepWiki links to help with understanding. There&amp;#39;s been significant progress in that community so I think it&amp;#39;s worth exploring.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m starting this thread for two reasons: First, I&amp;#39;d love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I&amp;#39;m particularly interested in seeing what alternatives to Grok&amp;#39;s Ani this community will build in the future.&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;ve missed anything, please let me know and I&amp;#39;ll update the list.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?auto=webp&amp;s=c5f2d10e0e2bd42132da4d17ae9fd30799dc46d4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e7fc321ec10284644abea084a0b60656c01283e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0102b60c2d06609a028ba234852f361a842ccba6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b93ae42c4f64f0fba315ba853f3f199075473f0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd981813236ec5fa39ab80bda6d206e4844b347a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5fe64f3be02adea586fa3bbfe4297c790018163",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7078c5e8cfb3be270d93e8bdbd35e203d4e295a3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8wg2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aratahikaru5",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753442258,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fiiv6xm3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-Thinking-2507 is about to be released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 55,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8dgfu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 416,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 416,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/UaMXb3ybuMfEr_Q8-TcVTvRTejPtVI4uvfxCaqtYwhE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753384454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6l84nwc3gvef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6l84nwc3gvef1.png?auto=webp&amp;s=57ba972c71ccbbde7e6b91adbfd7e6f90f9305a6",
                  "width": 586,
                  "height": 234
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b6ce58e5b6f04a919ca7ebb1329f28ea1812e03",
                    "width": 108,
                    "height": 43
                  },
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b26df86d2091efcaeb2caf8a3ae29e5e74a71365",
                    "width": 216,
                    "height": 86
                  },
                  {
                    "url": "https://preview.redd.it/6l84nwc3gvef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab0e139a7c20c4938872504feeddbf3c6b23197f",
                    "width": 320,
                    "height": 127
                  }
                ],
                "variants": {},
                "id": "21v1ejzHSqCrWCoLUoEo0xlXJGAQHp-JnuM_aAV5s4s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8dgfu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dr_Karminski",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/",
          "stickied": false,
          "url": "https://i.redd.it/6l84nwc3gvef1.png",
          "subreddit_subscribers": 504692,
          "created_utc": 1753384454,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Our team is evaluating Langfuse for production use with multiple clients, and we need clear clarification on which RBAC (Role-Based Access Control) features are included in the MIT licensed open source version versus what requires an Enterprise license.\n\nTeam members are arguing whether RBAC requires Enterprise license.\n\nCan we use MIT version RBAC commercially for client projects?\n\nseeking community help and thoughts on this.\n\nhttps://github.com/langfuse\n",
          "author_fullname": "t2_st6ehfmff",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Langfuse- Clarification Needed: RBAC Features in Open Source vs Enterprise Edition",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9j02q",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753500020,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team is evaluating Langfuse for production use with multiple clients, and we need clear clarification on which RBAC (Role-Based Access Control) features are included in the MIT licensed open source version versus what requires an Enterprise license.&lt;/p&gt;\n\n&lt;p&gt;Team members are arguing whether RBAC requires Enterprise license.&lt;/p&gt;\n\n&lt;p&gt;Can we use MIT version RBAC commercially for client projects?&lt;/p&gt;\n\n&lt;p&gt;seeking community help and thoughts on this.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/langfuse\"&gt;https://github.com/langfuse&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/d5P0bIzzQrrGsJJnmQ2gYK6M4pX7tG0yjbunsQN1TOI.png?auto=webp&amp;s=c5a9bbdf97aec976acef0be80f12e418c37deeef",
                  "width": 280,
                  "height": 280
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/d5P0bIzzQrrGsJJnmQ2gYK6M4pX7tG0yjbunsQN1TOI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5933305a7f87e8b68f406b8678ee86288526ed5",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/d5P0bIzzQrrGsJJnmQ2gYK6M4pX7tG0yjbunsQN1TOI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=daea1bc5b23402707a20b9a8bc5ffa302158c65f",
                    "width": 216,
                    "height": 216
                  }
                ],
                "variants": {},
                "id": "d5P0bIzzQrrGsJJnmQ2gYK6M4pX7tG0yjbunsQN1TOI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9j02q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SuitableMushroom6767",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9j02q/langfuse_clarification_needed_rbac_features_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9j02q/langfuse_clarification_needed_rbac_features_in/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753500020,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi All,\nI have been self hosting Ollama and mostly just use it to throw random questions or helping me dumb down a complex topic to answer a question my daughter asks.\n\nThe one thing I love about ChatGPT/Gemini is the ability to voice chat back and forth.\n\n\nIs there a easy to use mobile/desktop app and model combo that a semi-layman can setup?\n\nCurrently I use https://chatboxai.app/en + tailscale to access my Ollama/LLM remotely that runs on my RTX 3060 (12GB VRAM).\n\nThanks in advance! ",
          "author_fullname": "t2_4t06y10d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "App for voice interaction with LocalLLaMA. Looking for help/app/model etc.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9e71s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485878,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI have been self hosting Ollama and mostly just use it to throw random questions or helping me dumb down a complex topic to answer a question my daughter asks.&lt;/p&gt;\n\n&lt;p&gt;The one thing I love about ChatGPT/Gemini is the ability to voice chat back and forth.&lt;/p&gt;\n\n&lt;p&gt;Is there a easy to use mobile/desktop app and model combo that a semi-layman can setup?&lt;/p&gt;\n\n&lt;p&gt;Currently I use &lt;a href=\"https://chatboxai.app/en\"&gt;https://chatboxai.app/en&lt;/a&gt; + tailscale to access my Ollama/LLM remotely that runs on my RTX 3060 (12GB VRAM).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?auto=webp&amp;s=292ee07d261c9960edd9e6bc5216b120e3ca8c70",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=279a09b67459be926a08944e6c9ea50312a63a5f",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=71854addc096fa99604724a66b8d210353b93453",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb617badf513ed94cfbeb3e2cbe71000e2592028",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b440db62ce02b358f27186c315e179af0a46a940",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3fbbeab38b3b11b83f247b980db93408afdf989",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a139fe4e602dba248adc60f4bda3146ef969fd06",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9e71s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Mesh",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753485878,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Given: 14-inch MacBook Pro (M4 Pro, 48GB unified memory, 1TB SSD)\n\n\nWhat kind of local LLMs can I run? \n\nWhat’s your experience?\n\nCan I run mistral, Gemma, phi, or models 7b or 13b, etc. params?\n\n\nThanks!",
          "author_fullname": "t2_3kkuzrph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Laptop advise for lightweight AI work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9e2s9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given: 14-inch MacBook Pro (M4 Pro, 48GB unified memory, 1TB SSD)&lt;/p&gt;\n\n&lt;p&gt;What kind of local LLMs can I run? &lt;/p&gt;\n\n&lt;p&gt;What’s your experience?&lt;/p&gt;\n\n&lt;p&gt;Can I run mistral, Gemma, phi, or models 7b or 13b, etc. params?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9e2s9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entered_apprentice",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9e2s9/laptop_advise_for_lightweight_ai_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e2s9/laptop_advise_for_lightweight_ai_work/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753485561,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, \n\n\n\nI'm a solo dev and first-time open-source maintainer. I just released my first Python package: \\*\\*Arkhon Memory SDK\\*\\* – a lightweight, local-first memory module for autonomous LLM agents. This is part of my bigger project, but I thought this component could be useful for some of you.\n\n\\- **No vector DBs, no cloud, no LangChain**: clean, JSON-native memory with time decay, tagging, and session lifecycle hooks.\n\n\\- It’s fully pip installable: \\`pip install arkhon-memory\\`\n\n\\- Works with Python 3.8+ and pydantic 2.x.\n\n  \nYou can find it in:\n\n🔗 GitHub: [https://github.com/kissg96/arkhon\\_memory](https://github.com/kissg96/arkhon_memory)  \n\n🔗 PyPI: [https://pypi.org/project/arkhon-memory/](https://pypi.org/project/arkhon-memory/)\n\n\n\nIf you’re building LLM workflows, want persistence for agents, or just want a memory layer that \\*\\*never leaves your local machine\\*\\*, I’d love for you to try it.\n\n\n\nWould really appreciate feedback, stars, or suggestions!  \n\nFeel free to open issues or email me: [kissg@me.com](mailto:kissg@me.com)\n\n\n\nThanks for reading,  \n\nkissg96\n\n",
          "author_fullname": "t2_47eqehtw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Release] Arkhon Memory SDK – Local, lightweight long-term memory for LLM agents (pip install arkhon-memory)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9019j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753452243,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a solo dev and first-time open-source maintainer. I just released my first Python package: **Arkhon Memory SDK** – a lightweight, local-first memory module for autonomous LLM agents. This is part of my bigger project, but I thought this component could be useful for some of you.&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;No vector DBs, no cloud, no LangChain&lt;/strong&gt;: clean, JSON-native memory with time decay, tagging, and session lifecycle hooks.&lt;/p&gt;\n\n&lt;p&gt;- It’s fully pip installable: `pip install arkhon-memory`&lt;/p&gt;\n\n&lt;p&gt;- Works with Python 3.8+ and pydantic 2.x.&lt;/p&gt;\n\n&lt;p&gt;You can find it in:&lt;/p&gt;\n\n&lt;p&gt;🔗 GitHub: &lt;a href=\"https://github.com/kissg96/arkhon_memory\"&gt;https://github.com/kissg96/arkhon_memory&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;🔗 PyPI: &lt;a href=\"https://pypi.org/project/arkhon-memory/\"&gt;https://pypi.org/project/arkhon-memory/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you’re building LLM workflows, want persistence for agents, or just want a memory layer that **never leaves your local machine**, I’d love for you to try it.&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate feedback, stars, or suggestions!  &lt;/p&gt;\n\n&lt;p&gt;Feel free to open issues or email me: [&lt;a href=\"mailto:kissg@me.com\"&gt;kissg@me.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:kissg@me.com\"&gt;kissg@me.com&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading,  &lt;/p&gt;\n\n&lt;p&gt;kissg96&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?auto=webp&amp;s=3d6dbe3ba64c55f4a5b820d7b93c67e5e863a7c1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a209445a0ca39ec32cc43c3974f0c86515e04f3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bb430093762e9015e857ef6b4fe920adf08bdd4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48db854452639ae78e9c6f0926847bc1b64d167d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ecdd2622f584e6329d2bea611d6c3fe12d38f1b8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b13d3181d5a2ca5fed4e99539ad3d340ef806d8",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b70f7aefa32c79066655cbc1eac72b62818bf406",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9019j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kissgeri96",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9019j/release_arkhon_memory_sdk_local_lightweight/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9019j/release_arkhon_memory_sdk_local_lightweight/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753452243,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_c6nzmzuq8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is AI dialogue the future of gaming?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9535b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.55,
          "author_flair_background_color": "transparent",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kwadvy7vz1ff1/DASHPlaylist.mpd?a=1756124802%2COGFlMGRiMDI0NzNlNTJjZjQ4MzI1OWRmODFjMGZmNzcxMTI4MjdhYjRmY2ZlZmExYTMyZDViZDI5YzIxNWI5NQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/kwadvy7vz1ff1/HLSPlaylist.m3u8?a=1756124802%2CZDQ1NTU3M2NjMjYxNGY2YjM5NDM2ZmUzZGZkMTM4ZmFiNWUxNGUyYmQ0MTQ3MmRiOGMzY2RhODU2MDU3M2Y1ZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=a4d5b6d5fcc8a3352d49a80ea4992b61a9a553fb",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753463816,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/kwadvy7vz1ff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?format=pjpg&amp;auto=webp&amp;s=42d35bc0562f2b999644dafae05a2302ab759711",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=425c4520d088cf30251cb1992b126dedf6800fba",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=909cd2fe935ddf9c23f49cfceeaac45d4e5100a5",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bad49e88c983fca601ba8acf6ae347300b2b62a3",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ae04db361f1166b44acf7f7d032db0d7990a35f8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a2fe4b5aa0d20266a8b0d1a5f4864de6198c51ba",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a0f56d72f3bcea06d904b3ff0df0417caa967dd3",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9535b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LandoRingel",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1m9535b/is_ai_dialogue_the_future_of_gaming/",
          "stickied": false,
          "url": "https://v.redd.it/kwadvy7vz1ff1",
          "subreddit_subscribers": 504692,
          "created_utc": 1753463816,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kwadvy7vz1ff1/DASHPlaylist.mpd?a=1756124802%2COGFlMGRiMDI0NzNlNTJjZjQ4MzI1OWRmODFjMGZmNzcxMTI4MjdhYjRmY2ZlZmExYTMyZDViZDI5YzIxNWI5NQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/kwadvy7vz1ff1/HLSPlaylist.m3u8?a=1756124802%2CZDQ1NTU3M2NjMjYxNGY2YjM5NDM2ZmUzZGZkMTM4ZmFiNWUxNGUyYmQ0MTQ3MmRiOGMzY2RhODU2MDU3M2Y1ZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_yjt5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ByteDance Seed Prover Achieves Silver Medal Score in IMO 2025",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8tmhd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753431610,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "seed.bytedance.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8tmhd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hedgehog0",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8tmhd/bytedance_seed_prover_achieves_silver_medal_score/",
          "stickied": false,
          "url": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
          "subreddit_subscribers": 504692,
          "created_utc": 1753431610,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys, I built this solution to ensure your AI agent to remain stateful and long running. When your agent crashed, Agentainer will auto recover it and your agent can pick up what left to do and continue from there. \n\nAppreciate for any feedback, good or bad are both welcome!\n\n[Agentainer demo](https://reddit.com/link/1m9ia1t/video/i38qtnb5s4ff1/player)\n\nOpen Source: [Agentainer-lab (GitHub)](https://github.com/oso95/Agentainer-lab)\n\nWebsite: [Agentainer](https://agentainer.io/)",
          "author_fullname": "t2_6mhhxj8ry",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A demo of long running LLM agent solution with state persistent.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "i38qtnb5s4ff1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1m9ia1t/asset/i38qtnb5s4ff1/DASHPlaylist.mpd?a=1756124802%2CODI4ZmFmMGM1NTk5NzFjODEwYjg1NGQ1YTMzMmM4OTg0YjE1YjA4MWM4MzIzZDFiOTBjYzYxYjJmNjIyNTkyNQ%3D%3D&amp;v=1&amp;f=sd",
              "x": 1920,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1m9ia1t/asset/i38qtnb5s4ff1/HLSPlaylist.m3u8?a=1756124802%2CNWJhNzZhODBkMmJhMGVjMDhlMDZiZjgxMGRkY2FmMTEzNTE0NjM0MmE5ZDU4NTJmNDVjMDY1OGVjMzUwZmQzNg%3D%3D&amp;v=1&amp;f=sd",
              "id": "i38qtnb5s4ff1",
              "isGif": false
            }
          },
          "name": "t3_1m9ia1t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=73ffdf7065c64d71e709decd1ce58acebba02a58",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753497755,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I built this solution to ensure your AI agent to remain stateful and long running. When your agent crashed, Agentainer will auto recover it and your agent can pick up what left to do and continue from there. &lt;/p&gt;\n\n&lt;p&gt;Appreciate for any feedback, good or bad are both welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1m9ia1t/video/i38qtnb5s4ff1/player\"&gt;Agentainer demo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Open Source: &lt;a href=\"https://github.com/oso95/Agentainer-lab\"&gt;Agentainer-lab (GitHub)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Website: &lt;a href=\"https://agentainer.io/\"&gt;Agentainer&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY.png?auto=webp&amp;s=aa45e670be4fcf8d33a2de797acc0a2654f44045",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e1fe9df371f9708d871f7113cc6aa4cdd3fc7a0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a012b962394a3111f6b5e7755fbc5cad27bfb8ad",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce37a804ffe6eb0900dc5f1a797e840c1fcbc34d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94c8956badffb3897921bfb0b04b341e00cf785d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=33c45b6d4b919276b9b7473731d842f53ca36667",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5b906d6085cfa2678a90d52e8288b6b9053c6f51",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "s9XzlMF_cDm-NOLznql2db8w7vd9IkAxkKbNyxaBqUY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9ia1t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tradingoso",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ia1t/a_demo_of_long_running_llm_agent_solution_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ia1t/a_demo_of_long_running_llm_agent_solution_with/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753497755,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is AMD working on any GPU which will compete with RTX 6000 PRO Blackwell in memory, compute, and price? Or one with higher VRAM but targeted at workstations?",
          "author_fullname": "t2_1266sp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD equivalent for NVIDIA RTX 6000 PRO Blackwell",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95wcg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753465671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is AMD working on any GPU which will compete with RTX 6000 PRO Blackwell in memory, compute, and price? Or one with higher VRAM but targeted at workstations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m95wcg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "s-s-a",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95wcg/amd_equivalent_for_nvidia_rtx_6000_pro_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95wcg/amd_equivalent_for_nvidia_rtx_6000_pro_blackwell/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753465671,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the market right now, there’s an ocean of no‑code and low‑code platforms shouting about how they “let you build anything.”\n\nBut let’s be real, most of them are just website builders with a fancier skin.\n\nI’ve used tools like Lovable, Bolt, Fire Studio.  \nThey are simple, but they still feel like the low‑end spectrum: good for spinning up a quick frontend for MVP, but they stop there.\n\nOn the opposite end, there are power tools - Windsurf and Cursor.  \nThese are meant for developers who already know how to code, but they are too advanced for non‑technical builders who have a deep idea but no engineering muscle.\n\nWhat’s missing is a middle ground.  \nA true **application generator** that isn’t about “drag a button, drag a form,” and isn’t just a playground for coders.\n\nImagine this: you explain in detail how your application should work. its flow, logic, data, and purpose, and the AI actually builds that application, not a landing page or backend shell, but a working tool.\n\nHas anyone here seen or tried something in that direction?  \nNot another website builder, something that can create applications from deep descriptions?\n\nbtw I'm just vibe coder",
          "author_fullname": "t2_1qe0x0t139",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any AI tool for application creation (not website builders)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99xty",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753475214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the market right now, there’s an ocean of no‑code and low‑code platforms shouting about how they “let you build anything.”&lt;/p&gt;\n\n&lt;p&gt;But let’s be real, most of them are just website builders with a fancier skin.&lt;/p&gt;\n\n&lt;p&gt;I’ve used tools like Lovable, Bolt, Fire Studio.&lt;br/&gt;\nThey are simple, but they still feel like the low‑end spectrum: good for spinning up a quick frontend for MVP, but they stop there.&lt;/p&gt;\n\n&lt;p&gt;On the opposite end, there are power tools - Windsurf and Cursor.&lt;br/&gt;\nThese are meant for developers who already know how to code, but they are too advanced for non‑technical builders who have a deep idea but no engineering muscle.&lt;/p&gt;\n\n&lt;p&gt;What’s missing is a middle ground.&lt;br/&gt;\nA true &lt;strong&gt;application generator&lt;/strong&gt; that isn’t about “drag a button, drag a form,” and isn’t just a playground for coders.&lt;/p&gt;\n\n&lt;p&gt;Imagine this: you explain in detail how your application should work. its flow, logic, data, and purpose, and the AI actually builds that application, not a landing page or backend shell, but a working tool.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here seen or tried something in that direction?&lt;br/&gt;\nNot another website builder, something that can create applications from deep descriptions?&lt;/p&gt;\n\n&lt;p&gt;btw I&amp;#39;m just vibe coder&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m99xty",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Delicious_Track6230",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99xty/any_ai_tool_for_application_creation_not_website/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99xty/any_ai_tool_for_application_creation_not_website/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753475214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ive been looking at buying a few mi50 32gb cards for my local training setup because they are absurdly affordable for the VRAM they have. I'm not too concerned with FLOP/s performance, as long as they have compatibility with a relatively modern pytorch and its dependencies.\n\nI've seen people on here talking about this card for inference but not training. Would this be a good idea?",
          "author_fullname": "t2_1jemkvy49e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mi50 array for training LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96wrc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753468011,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ive been looking at buying a few mi50 32gb cards for my local training setup because they are absurdly affordable for the VRAM they have. I&amp;#39;m not too concerned with FLOP/s performance, as long as they have compatibility with a relatively modern pytorch and its dependencies.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen people on here talking about this card for inference but not training. Would this be a good idea?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96wrc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Used_Algae_1077",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96wrc/mi50_array_for_training_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96wrc/mi50_array_for_training_llms/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753468011,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been following the progress of local LLMs for a while and I'm really interested in setting up a system for a natural, real-time audio conversation. I've seen some posts here discussing solutions that involve piping together speech-to-text, the LLM, and text-to-speech.\n\nI'm curious to know if anyone has found or built a more integrated solution that minimizes latency and feels more like a direct conversation. I've come across mentions of projects like Verbi and the potential of multimodal models like Qwen2-Audio, and I'm wondering if these are still the current way to go?\n\nIdeally, I'm looking for something that can run on consumer-grade hardware.\n\nWhat are your current setups for this? Have you managed to achieve a truly conversational experience?",
          "author_fullname": "t2_vyvxdjqyp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone found a seamless, low-latency solution for real-time audio conversations with a local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9c9fh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753480874,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been following the progress of local LLMs for a while and I&amp;#39;m really interested in setting up a system for a natural, real-time audio conversation. I&amp;#39;ve seen some posts here discussing solutions that involve piping together speech-to-text, the LLM, and text-to-speech.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to know if anyone has found or built a more integrated solution that minimizes latency and feels more like a direct conversation. I&amp;#39;ve come across mentions of projects like Verbi and the potential of multimodal models like Qwen2-Audio, and I&amp;#39;m wondering if these are still the current way to go?&lt;/p&gt;\n\n&lt;p&gt;Ideally, I&amp;#39;m looking for something that can run on consumer-grade hardware.&lt;/p&gt;\n\n&lt;p&gt;What are your current setups for this? Have you managed to achieve a truly conversational experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9c9fh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Far_Buyer_7281",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9c9fh/has_anyone_found_a_seamless_lowlatency_solution/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9c9fh/has_anyone_found_a_seamless_lowlatency_solution/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753480874,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp; 14b).\n\ni wasn't ready for Claudes response. so good.\n\nFWIW i'm trying codellama:13b next.\n\nAny advice for a local coding model and Aider on RTX3080 10GB?",
          "author_fullname": "t2_8383arktn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do models make fun of other models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wi62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3VfQACGGsrR4huIE2aSbd12G7zEBM2KmWURzyANGXgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753442447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp;amp; 14b).&lt;/p&gt;\n\n&lt;p&gt;i wasn&amp;#39;t ready for Claudes response. so good.&lt;/p&gt;\n\n&lt;p&gt;FWIW i&amp;#39;m trying codellama:13b next.&lt;/p&gt;\n\n&lt;p&gt;Any advice for a local coding model and Aider on RTX3080 10GB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8sdpzbq280ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8sdpzbq280ff1.png?auto=webp&amp;s=916da339fa043930e6f17140452756e404ea0bf2",
                  "width": 718,
                  "height": 371
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89b6015ee2a0d88ec0bac662235da5629baf1bbb",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ce1a4289cec1dd8cedead50e63cc09efa5fee80",
                    "width": 216,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3de57e7740d06316720746e9e88263882f7396e9",
                    "width": 320,
                    "height": 165
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb549a0f9db72b902ee2c8fba005b948a7894058",
                    "width": 640,
                    "height": 330
                  }
                ],
                "variants": {},
                "id": "-PX_RQ0KeK0Qo0OiDGvgWia8NQc4pA3mSJFKxM_loHM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m8wi62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fussy-Fur3608",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wi62/do_models_make_fun_of_other_models/",
          "stickied": false,
          "url": "https://i.redd.it/8sdpzbq280ff1.png",
          "subreddit_subscribers": 504692,
          "created_utc": 1753442447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Greetings, we're a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget\n\n  \n1. One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM\n\n2. One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM\n\n  \nBoth models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.\n\nWe're not extremely concerned about RAM (we can buy RAM later using a different budget) but we're concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?\n\nThanks a bunch  \n",
          "author_fullname": "t2_lavl5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How important is to have PRO 6000 Blackwell running on 16 PCIE lanes?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wuy7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753443620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, we&amp;#39;re a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re not extremely concerned about RAM (we can buy RAM later using a different budget) but we&amp;#39;re concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?&lt;/p&gt;\n\n&lt;p&gt;Thanks a bunch  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8wuy7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ferkte",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753443620,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google released their Gemma 3n model about a month ago, and they've mentioned that it's meant to run efficiently on everyday devices, yet, from my experience it runs really slow on my Mac (base model M2 Mac mini from 2023 with only 8GB of RAM). I am aware that my small amount of RAM is very limiting in the space of local LLMs, but I had a lot of hope when Google first started teasing this model.\n\nJust curious if anyone has tried it, and if so, what has your experience been like?\n\nHere's an Ollama link to the model, btw: [https://ollama.com/library/gemma3n](https://ollama.com/library/gemma3n)",
          "author_fullname": "t2_1lavzg2ok9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone had any luck with Google's Gemma 3n model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95bfq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753464342,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google released their Gemma 3n model about a month ago, and they&amp;#39;ve mentioned that it&amp;#39;s meant to run efficiently on everyday devices, yet, from my experience it runs really slow on my Mac (base model M2 Mac mini from 2023 with only 8GB of RAM). I am aware that my small amount of RAM is very limiting in the space of local LLMs, but I had a lot of hope when Google first started teasing this model.&lt;/p&gt;\n\n&lt;p&gt;Just curious if anyone has tried it, and if so, what has your experience been like?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an Ollama link to the model, btw: &lt;a href=\"https://ollama.com/library/gemma3n\"&gt;https://ollama.com/library/gemma3n&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m95bfq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Junior-Ad-2186",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95bfq/anyone_had_any_luck_with_googles_gemma_3n_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95bfq/anyone_had_any_luck_with_googles_gemma_3n_model/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753464342,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Thinking is coming very soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 51,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8dln1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 226,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 226,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1OIHh7R-mfEk892o091BDg-5ivRbaC6jjmct0HHlBxY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753384779,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/61i8pt44hvef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/61i8pt44hvef1.png?auto=webp&amp;s=199ae416540ed35191e6454a349646379029949f",
                  "width": 1654,
                  "height": 606
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3c31400b9a3cd6f09d39f562bd58e86cdc43cbb",
                    "width": 108,
                    "height": 39
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a029fe88fcb9fa844f284c0aea7500d00db97c54",
                    "width": 216,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=178e6c50de0e6ef14b15e781bdcb2a2be7d26232",
                    "width": 320,
                    "height": 117
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=19b99a58da488472ec93d5842e37998def1cbe76",
                    "width": 640,
                    "height": 234
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2495dca0fab016d71398e8436fe7dc30e236a20",
                    "width": 960,
                    "height": 351
                  },
                  {
                    "url": "https://preview.redd.it/61i8pt44hvef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea6f2da7e737f0762f01e6b42c9a50cf19a6748d",
                    "width": 1080,
                    "height": 395
                  }
                ],
                "variants": {},
                "id": "Y5bhpE2oH633vsr3feG38JCFuVtcXjLVmrApMikSGkM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8dln1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/",
          "stickied": false,
          "url": "https://i.redd.it/61i8pt44hvef1.png",
          "subreddit_subscribers": 504692,
          "created_utc": 1753384779,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My config:  \nRyzen 5 5500, 16Gb, RTX 3060 12Gb  \n",
          "author_fullname": "t2_by2xhanu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best AI to run locally and use in agent mode of the Continue extension in VS Code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99hwb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753474147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My config:&lt;br/&gt;\nRyzen 5 5500, 16Gb, RTX 3060 12Gb  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m99hwb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ikelven",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99hwb/what_is_the_best_ai_to_run_locally_and_use_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99hwb/what_is_the_best_ai_to_run_locally_and_use_in/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753474147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China's Bytedance releases Seed LiveInterpret simultaneous interpretation model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ozb0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753415015,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "seed.bytedance.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://seed.bytedance.com/en/seed_liveinterpret",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8ozb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/",
          "stickied": false,
          "url": "https://seed.bytedance.com/en/seed_liveinterpret",
          "subreddit_subscribers": 504692,
          "created_utc": 1753415015,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a 3060 RTX for my i7 PC. I check the task manager it is has been using about 75% CPU, 55% RAM, and GPU 1% (although it will jump up to 48% and then plummet back to 1% after about a second. I have used Ooba and Kobold.ccp which use the llama.ccp server and kobold.ccp (of course) respectively. I have tried playing around with offloading different number of layers. I have noticed this with Gemma 3 27G, Mistral Small 22B, Mistral Nemo, and Qwen 14B. I don't mind waiting for a response so I realize that the models are probably too big to give me real time t/s. So, what am I doing wrong? I am still basically a newb when it comes to AI tech. I'd appreciate it if anybody to tell me why it isn't, at least the the Windows 10 task manager, utilizing the GPU much. My laptop which has only a 2040 RTX seems to run the models better and the settings are basically the same except I use 7 out of 8 cores on the laptop and 3 of 4 of the cores on my desktop CPU. I use Silly Tavern as my frontend so, it could be a setting in there such as the tokenizer I use (I usually just stick with the auto option).",
          "author_fullname": "t2_49abw3rv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLMs I have been using, through different two backends, seem to hardly use GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9etng",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753487567,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3060 RTX for my i7 PC. I check the task manager it is has been using about 75% CPU, 55% RAM, and GPU 1% (although it will jump up to 48% and then plummet back to 1% after about a second. I have used Ooba and Kobold.ccp which use the llama.ccp server and kobold.ccp (of course) respectively. I have tried playing around with offloading different number of layers. I have noticed this with Gemma 3 27G, Mistral Small 22B, Mistral Nemo, and Qwen 14B. I don&amp;#39;t mind waiting for a response so I realize that the models are probably too big to give me real time t/s. So, what am I doing wrong? I am still basically a newb when it comes to AI tech. I&amp;#39;d appreciate it if anybody to tell me why it isn&amp;#39;t, at least the the Windows 10 task manager, utilizing the GPU much. My laptop which has only a 2040 RTX seems to run the models better and the settings are basically the same except I use 7 out of 8 cores on the laptop and 3 of 4 of the cores on my desktop CPU. I use Silly Tavern as my frontend so, it could be a setting in there such as the tokenizer I use (I usually just stick with the auto option).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9etng",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theshadowraven",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9etng/local_llms_i_have_been_using_through_different/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9etng/local_llms_i_have_been_using_through_different/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753487567,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Even if one expert cluster(?) active set is only 23 to 35 GB's based on two recent one's I've seen what might the working set be in terms of number of expert needed and how often would swapping happen?  I'm looking at MOE up over 230B in size.  If I'm writing python web server, the javascript/html/css side, stable diffusion inferencing in a multi process shared memory setup how many experts are going to be needed?\n\nClearly if I bring up a prompt politics, religion, world history, astronomy, math, programming, and feline skin diseases it'd be very slow.  It's a huge download just to try it so I thought I'd ask here first.\n\nIs there any documentation as to what the experts are expert in?  Do any of the LLM runner tools print statistics or can they log expert swapping to assist with figure out how to best use these.",
          "author_fullname": "t2_rxk6hx4t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question on MOE expert swapping",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9j1mh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753500158,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Even if one expert cluster(?) active set is only 23 to 35 GB&amp;#39;s based on two recent one&amp;#39;s I&amp;#39;ve seen what might the working set be in terms of number of expert needed and how often would swapping happen?  I&amp;#39;m looking at MOE up over 230B in size.  If I&amp;#39;m writing python web server, the javascript/html/css side, stable diffusion inferencing in a multi process shared memory setup how many experts are going to be needed?&lt;/p&gt;\n\n&lt;p&gt;Clearly if I bring up a prompt politics, religion, world history, astronomy, math, programming, and feline skin diseases it&amp;#39;d be very slow.  It&amp;#39;s a huge download just to try it so I thought I&amp;#39;d ask here first.&lt;/p&gt;\n\n&lt;p&gt;Is there any documentation as to what the experts are expert in?  Do any of the LLM runner tools print statistics or can they log expert swapping to assist with figure out how to best use these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9j1mh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Guilty-History-9249",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9j1mh/question_on_moe_expert_swapping/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9j1mh/question_on_moe_expert_swapping/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753500158,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A friend’s messing with the idea of setting up a camera in his garage gym to watch his lifts, give form feedback, count reps, maybe even talk to him in real time.\n\nNeeds to be actually real-time tho, like not 5s delay, and ideally configurable too.\n\nAnyone know what models or pipelines would work best for this? Thinking maybe something like a lightweight vision model (pose tracking?) + audio TTS + LLM glue but curious if anyone here’s already stitched something like this together or knows what stack would be least painful?\n\nOpen to weird, hacked, setups if it works.",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone stitched together real-time local AI for webcam + voice feedback?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9egm9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753486592,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend’s messing with the idea of setting up a camera in his garage gym to watch his lifts, give form feedback, count reps, maybe even talk to him in real time.&lt;/p&gt;\n\n&lt;p&gt;Needs to be actually real-time tho, like not 5s delay, and ideally configurable too.&lt;/p&gt;\n\n&lt;p&gt;Anyone know what models or pipelines would work best for this? Thinking maybe something like a lightweight vision model (pose tracking?) + audio TTS + LLM glue but curious if anyone here’s already stitched something like this together or knows what stack would be least painful?&lt;/p&gt;\n\n&lt;p&gt;Open to weird, hacked, setups if it works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9egm9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9egm9/anyone_stitched_together_realtime_local_ai_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9egm9/anyone_stitched_together_realtime_local_ai_for/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753486592,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There's so many models, which one to train?\nDoes it depend on the kind of output I need like text or code or format / structure?\n\nAnd how long does training take on what hardware?\n\n5060 ti, A100, 5090, any information.\n\nThank you",
          "author_fullname": "t2_17hwnassb4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best models to fine-tune?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9dysd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s so many models, which one to train?\nDoes it depend on the kind of output I need like text or code or format / structure?&lt;/p&gt;\n\n&lt;p&gt;And how long does training take on what hardware?&lt;/p&gt;\n\n&lt;p&gt;5060 ti, A100, 5090, any information.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9dysd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zekuden",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9dysd/best_models_to_finetune/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9dysd/best_models_to_finetune/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753485276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been working on a small tool to make it easier to extract high-quality transcripts from YouTube videos.  I think it will be useful for AI trainers and dataset builders who want to build language datasets from online content.\n\nSo I will be giving away a beta tester account that will have infinite credits until launch it has a bulk extract feature which can extract all transcripts of a YouTube channel and videos and put it in one file .\n\ndm me if you want to be a beta tester\n\nhttps://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;format=png&amp;auto=webp&amp;s=0545d5ab53852b80cf553498d53d1982075d05b6\n\nhttps://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df\n\nhttps://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;format=png&amp;auto=webp&amp;s=2aafa58c52afea745518a37c41ccf49c09972519\n\nhttps://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;format=png&amp;auto=webp&amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49\n\n  \n",
          "author_fullname": "t2_1rk8qow8x2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ai training Tool I want to share!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 119,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "985pjh9tt3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 85,
                  "x": 108,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00a42e32168a547f07923efa28a793940f08a360"
                },
                {
                  "y": 170,
                  "x": 216,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7165f86006d7ef09f4ff48384ac9aad30f3318c2"
                },
                {
                  "y": 252,
                  "x": 320,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eec191101d9dd0c302e6fedf0af1bacbfb2f4938"
                },
                {
                  "y": 504,
                  "x": 640,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9156dcafa2959b6f898f789a62ff73615f1d75e8"
                }
              ],
              "s": {
                "y": 545,
                "x": 691,
                "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;format=png&amp;auto=webp&amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49"
              },
              "id": "985pjh9tt3ff1"
            },
            "wf8kkco9t3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 92,
                  "x": 108,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dea551e664e42f9448f784a42f51b399f5397a8"
                },
                {
                  "y": 184,
                  "x": 216,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a436507d1e263f947381a877681653df20c90e0"
                },
                {
                  "y": 273,
                  "x": 320,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8cd9274b47476014dc317bb84086035fcfa0cc79"
                }
              ],
              "s": {
                "y": 528,
                "x": 618,
                "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;format=png&amp;auto=webp&amp;s=0545d5ab53852b80cf553498d53d1982075d05b6"
              },
              "id": "wf8kkco9t3ff1"
            },
            "kx9xm6hgt3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f26a5ec431efd8ca5d97fd4b448315b8c68af19a"
                },
                {
                  "y": 134,
                  "x": 216,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7e4c58a87f4a231f57aefc45240c188e2d2610a"
                },
                {
                  "y": 198,
                  "x": 320,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cbf8fd45db7c92a0a7a54e73dae5aa0aacd6cbd"
                },
                {
                  "y": 397,
                  "x": 640,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9fe19ea5db262ac80ac773e5628fc2165b993a88"
                }
              ],
              "s": {
                "y": 536,
                "x": 863,
                "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;format=png&amp;auto=webp&amp;s=2aafa58c52afea745518a37c41ccf49c09972519"
              },
              "id": "kx9xm6hgt3ff1"
            },
            "1mg8157ss3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9afd44a3f9f70d75751fd25af84329e258957b7"
                },
                {
                  "y": 94,
                  "x": 216,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0eb9ca900f5ac925b7767d12e1f432264fa0f535"
                },
                {
                  "y": 140,
                  "x": 320,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e87773e5dd644629e80477fbb6bc64682951581"
                },
                {
                  "y": 280,
                  "x": 640,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f993fb75fe8121f018ff4fe789c5660be9333f6a"
                },
                {
                  "y": 421,
                  "x": 960,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a960f16f86a744a27c55f093ec4420e6756d0736"
                },
                {
                  "y": 473,
                  "x": 1080,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3bb82df7e16a39e069dfd0a2f8e52e51a92f7e2"
                }
              ],
              "s": {
                "y": 586,
                "x": 1336,
                "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df"
              },
              "id": "1mg8157ss3ff1"
            }
          },
          "name": "t3_1m9duv2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1-i3V0tVUx4LiF6k37Pc1dzCVbo6GAP2UM5PyjET5Jc.jpg",
          "edited": 1753485915,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753484988,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been working on a small tool to make it easier to extract high-quality transcripts from YouTube videos.  I think it will be useful for AI trainers and dataset builders who want to build language datasets from online content.&lt;/p&gt;\n\n&lt;p&gt;So I will be giving away a beta tester account that will have infinite credits until launch it has a bulk extract feature which can extract all transcripts of a YouTube channel and videos and put it in one file .&lt;/p&gt;\n\n&lt;p&gt;dm me if you want to be a beta tester&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0545d5ab53852b80cf553498d53d1982075d05b6\"&gt;https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0545d5ab53852b80cf553498d53d1982075d05b6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df\"&gt;https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2aafa58c52afea745518a37c41ccf49c09972519\"&gt;https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2aafa58c52afea745518a37c41ccf49c09972519&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49\"&gt;https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9duv2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Enough_Patient1904",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9duv2/ai_training_tool_i_want_to_share/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9duv2/ai_training_tool_i_want_to_share/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753484988,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp; Slightly Behind the RTX 5060 in New Benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 99,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m83644",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 588,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 588,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=140&amp;height=99&amp;crop=140:99,smart&amp;auto=webp&amp;s=e317af62a4b0fea5876b0388a0cd1f5775ef680b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753360416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?auto=webp&amp;s=e789a7dbd0c89c024f728c8e0ac2c066b704ed9a",
                  "width": 728,
                  "height": 516
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff00e39d3c91233a4ad4b2458d203b679086f872",
                    "width": 108,
                    "height": 76
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=58f702c18129404394dea56cd9d3b6975a719f62",
                    "width": 216,
                    "height": 153
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8326f56ed914670a98663b55ce61baafb1e3d037",
                    "width": 320,
                    "height": 226
                  },
                  {
                    "url": "https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6aa69848c81b950052de8eb2024c390e13024272",
                    "width": 640,
                    "height": 453
                  }
                ],
                "variants": {},
                "id": "lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m83644",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 224,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/",
          "stickied": false,
          "url": "https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753360416,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Every new model likes to claim it's SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I'm really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don't really live up to their claims.\n\nI've been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I've also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.\n\nIn my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:\n\n1. DeekSeek v3 (0324)\n2. Mistral Large (2411)\n3. Kimi K2\n4. Cohere Command-A (2503)\n5. Qwen 3 235B-A22B (2507, non-thinking)\n6. Llama 4 Maverick\n7. Llama 3.3 70B\n8. Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)\n9. Dots.LLM1\n10. Gemma 3 27B\n11. Mistral Small 3.2\n12. Qwen 3 30B-A3B\n\nIn my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.\n\nOf the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.\n\nFourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that's nearly 8x bigger.\n\nAnyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you'll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we're hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.\n\nThe other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren't that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.",
          "author_fullname": "t2_702zh1r2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stagnation in Knowledge Density",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8oc9j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753413019,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every new model likes to claim it&amp;#39;s SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I&amp;#39;m really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don&amp;#39;t really live up to their claims.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I&amp;#39;ve also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.&lt;/p&gt;\n\n&lt;p&gt;In my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DeekSeek v3 (0324)&lt;/li&gt;\n&lt;li&gt;Mistral Large (2411)&lt;/li&gt;\n&lt;li&gt;Kimi K2&lt;/li&gt;\n&lt;li&gt;Cohere Command-A (2503)&lt;/li&gt;\n&lt;li&gt;Qwen 3 235B-A22B (2507, non-thinking)&lt;/li&gt;\n&lt;li&gt;Llama 4 Maverick&lt;/li&gt;\n&lt;li&gt;Llama 3.3 70B&lt;/li&gt;\n&lt;li&gt;Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)&lt;/li&gt;\n&lt;li&gt;Dots.LLM1&lt;/li&gt;\n&lt;li&gt;Gemma 3 27B&lt;/li&gt;\n&lt;li&gt;Mistral Small 3.2&lt;/li&gt;\n&lt;li&gt;Qwen 3 30B-A3B&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.&lt;/p&gt;\n\n&lt;p&gt;Of the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.&lt;/p&gt;\n\n&lt;p&gt;Fourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that&amp;#39;s nearly 8x bigger.&lt;/p&gt;\n\n&lt;p&gt;Anyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you&amp;#39;ll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we&amp;#39;re hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.&lt;/p&gt;\n\n&lt;p&gt;The other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren&amp;#39;t that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8oc9j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Federal-Effective879",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753413019,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,  \nI'm working on a project using LibreChat, and I've noticed that it handles translations through `.ts` and `.md` files—one set per language. Each file contains over a thousand lines, so I assume these aren't written manually. There must be some kind of script or automation behind generating them.\n\nI want to make a change to one of the base messages. Specifically, in a registration form, there's a field for `username` and it currently displays `(optional)`. I want to remove that word so it no longer shows.\n\nMy question is:  \nIf I update the base message (presumably in the default language file), is there a way to automatically update the rest of the language files to reflect this change? For example, marking the string as needing translation or syncing the keys across all files?\n\nAny insights or tips on how this workflow is managed in LibreChat or similar setups would be really appreciated.  \nThanks!",
          "author_fullname": "t2_3t1oh2ky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does LibreChat handle translations and how can I update all language files after changing base messages?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96m6w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753467319,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI&amp;#39;m working on a project using LibreChat, and I&amp;#39;ve noticed that it handles translations through &lt;code&gt;.ts&lt;/code&gt; and &lt;code&gt;.md&lt;/code&gt; files—one set per language. Each file contains over a thousand lines, so I assume these aren&amp;#39;t written manually. There must be some kind of script or automation behind generating them.&lt;/p&gt;\n\n&lt;p&gt;I want to make a change to one of the base messages. Specifically, in a registration form, there&amp;#39;s a field for &lt;code&gt;username&lt;/code&gt; and it currently displays &lt;code&gt;(optional)&lt;/code&gt;. I want to remove that word so it no longer shows.&lt;/p&gt;\n\n&lt;p&gt;My question is:&lt;br/&gt;\nIf I update the base message (presumably in the default language file), is there a way to automatically update the rest of the language files to reflect this change? For example, marking the string as needing translation or syncing the keys across all files?&lt;/p&gt;\n\n&lt;p&gt;Any insights or tips on how this workflow is managed in LibreChat or similar setups would be really appreciated.&lt;br/&gt;\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96m6w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "suribe06",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96m6w/how_does_librechat_handle_translations_and_how/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96m6w/how_does_librechat_handle_translations_and_how/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753467319,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all, looking for a discussion on GPU options for LLM self hosting. Looking for something 24GB that doesn’t break the bank. Bonus if it’s single slot as I have no room in the server I’m working with. \n\nObviously there’s a desire to run the biggest model possible but there’s plenty of tradeoffs here and of course using it for other workloads. Thoughts?",
          "author_fullname": "t2_3nx6tffm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU Suggestions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m92vqp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753458834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, looking for a discussion on GPU options for LLM self hosting. Looking for something 24GB that doesn’t break the bank. Bonus if it’s single slot as I have no room in the server I’m working with. &lt;/p&gt;\n\n&lt;p&gt;Obviously there’s a desire to run the biggest model possible but there’s plenty of tradeoffs here and of course using it for other workloads. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m92vqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Grimm_Spector",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m92vqp/gpu_suggestions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m92vqp/gpu_suggestions/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753458834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "me and my friend have been working on an architecture for a bit that doesnt use attention, but due to limited hardware progress has been slow, what companies or ppl should we reach out to? we arent looking for much maybe a 1000 dollars and would be glad to make a contract with someone for publishing rights of the LLM in exchange",
          "author_fullname": "t2_74ai6uqx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Who should we ask for funding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9boeu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753479418,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;me and my friend have been working on an architecture for a bit that doesnt use attention, but due to limited hardware progress has been slow, what companies or ppl should we reach out to? we arent looking for much maybe a 1000 dollars and would be glad to make a contract with someone for publishing rights of the LLM in exchange&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9boeu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Commercial-Ad-1148",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9boeu/who_should_we_ask_for_funding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9boeu/who_should_we_ask_for_funding/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753479418,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1kpbtnvm6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What a Real MCP Inspector Exploit Taught Us About Trust Boundaries",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95sdj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=3f39b34ed5287898a35b71f4a2d410155e669d3f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753465416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "glama.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://glama.ai/blog/2025-07-25-keeping-mcp-inspector-safe-lessons-from-cve-2025-49596",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?auto=webp&amp;s=227a72ce82d28d282a0c1871a35312e4308e1adc",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=867c2e3c9c184ac0e64e1d2b897ce35fafe59ad3",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3bc8fb60b3d09c1938760de5b1404a3ec3810e2",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d497e25d2db852ceaaef3176f190e7864b19bb13",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d8aa63775b0ce7432aec4599fdff26b1f5079fa",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d9525d5f79ca90480770bfaf099caa9c518625cc",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2af1885a6b45fe1e3b5d69406a04e31bec46d01a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m95sdj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Abies7108",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95sdj/what_a_real_mcp_inspector_exploit_taught_us_about/",
          "stickied": false,
          "url": "https://glama.ai/blog/2025-07-25-keeping-mcp-inspector-safe-lessons-from-cve-2025-49596",
          "subreddit_subscribers": 504692,
          "created_utc": 1753465416,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "⚡ 2× faster\n\n💸 $0.30 / $1.20 per Mtoken\n\n✅ Nearly identical performance (\\~1% delta)\n\nPerfect for agentic workflows, tool use, and browser tasks.\n\nAlso, if you’re deploying open models or curious about real-time usage at scale, we just started [r/DeepInfra](https://www.reddit.com/r/DeepInfra) to track new model launches, price drops, and deployment tips. Would love to see what you’re building.",
          "author_fullname": "t2_1rrn43m1ok",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If you’re experimenting with Qwen3-Coder, we just launched a Turbo version on DeepInfra",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9gg6j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.48,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753492182,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;⚡ 2× faster&lt;/p&gt;\n\n&lt;p&gt;💸 $0.30 / $1.20 per Mtoken&lt;/p&gt;\n\n&lt;p&gt;✅ Nearly identical performance (~1% delta)&lt;/p&gt;\n\n&lt;p&gt;Perfect for agentic workflows, tool use, and browser tasks.&lt;/p&gt;\n\n&lt;p&gt;Also, if you’re deploying open models or curious about real-time usage at scale, we just started &lt;a href=\"https://www.reddit.com/r/DeepInfra\"&gt;r/DeepInfra&lt;/a&gt; to track new model launches, price drops, and deployment tips. Would love to see what you’re building.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9gg6j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "deepinfra",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9gg6j/if_youre_experimenting_with_qwen3coder_we_just/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9gg6j/if_youre_experimenting_with_qwen3coder_we_just/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753492182,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MassGen — an open-source multi-agent orchestration framework just launched. Supports cross-model collaboration (Grok, OpenAI, Claude, Gemini) with real-time streaming and consensus-building among agents. Inspired by \"parallel study groups\" and Grok Heavy. \n\n[https://x.com/Chi\\_Wang\\_/status/1948790995694617036](https://x.com/Chi_Wang_/status/1948790995694617036)",
          "author_fullname": "t2_1rqcx8w6j7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MassGen – an open-source multi-agent scaling and orchestration framework",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95lud",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753464999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MassGen — an open-source multi-agent orchestration framework just launched. Supports cross-model collaboration (Grok, OpenAI, Claude, Gemini) with real-time streaming and consensus-building among agents. Inspired by &amp;quot;parallel study groups&amp;quot; and Grok Heavy. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://x.com/Chi_Wang_/status/1948790995694617036\"&gt;https://x.com/Chi_Wang_/status/1948790995694617036&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m95lud",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LifeUnderstanding732",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95lud/massgen_an_opensource_multiagent_scaling_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95lud/massgen_an_opensource_multiagent_scaling_and/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753464999,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;[Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.](https://www.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/)  \nby[u/Asleep-Ratio7535](https://www.reddit.com/user/Asleep-Ratio7535/) in[LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)\n\nThis extension comes to a closure with so many published MCP servers. Chrome webstore is a little bit slower.\n\nNew update:\n\n* A good enough hybrid RAG for latin languages (BM25 tokenizer, I added a simple Japanese tokenizer as well), Only Chinese doesn't support BM25 full text search, but you can still use a good embedding model.\n* A note system for saving webpages and notes for RAG or use as direct context\n* Several basic useful tools: web search, prompt optimizer, wiki, retriever, save note, update your preference, and some \"agents\" that can plan and execute those tools itself\n\nIn the picture is an example of how a 4B model planned and used the tools it has. In this example, I tested too many concurrent web searches, so I didn't notice I needed to click the captcha on the page. So it failed in the first 2 steps, but you can get rid of it easily by clicking the captcha, or use a custom API, or DuckDuckGo, brave.\n\nhttps://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6\n\n",
          "author_fullname": "t2_1lfyddwf0c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Updated] AI assistant Chrome extension has tools and RAG",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "esicru1dc1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7fffe5029ab5f76ebb40401b7cad76bae9b4e7b6"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ac919ba5c18221a7f4a6e00afa0116c7c6b6855"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=31bb77985d25b08c50ec5b7967a10aff85d228c8"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f5f53147db855c6cbd5ee094fc2bad175f052c1"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77b612691433c9c230a0f75a8400feac6f9d1a85"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cb5c976360279c8d2f12db71b0e1bf9208b128c"
                }
              ],
              "s": {
                "y": 1600,
                "x": 2560,
                "u": "https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6"
              },
              "id": "esicru1dc1ff1"
            }
          },
          "name": "t3_1m91u38",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#b0ae9b",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "fe89e94a-13f2-11f0-a9de-6262c74956cf",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ttt_ex-17pTgfqoaCCzePES-O2aLbpYiA_IdGhQQxfg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 4"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753456445,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/\"&gt;Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.&lt;/a&gt;&lt;br/&gt;\nby&lt;a href=\"https://www.reddit.com/user/Asleep-Ratio7535/\"&gt;u/Asleep-Ratio7535&lt;/a&gt; in&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/\"&gt;LocalLLaMA&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This extension comes to a closure with so many published MCP servers. Chrome webstore is a little bit slower.&lt;/p&gt;\n\n&lt;p&gt;New update:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A good enough hybrid RAG for latin languages (BM25 tokenizer, I added a simple Japanese tokenizer as well), Only Chinese doesn&amp;#39;t support BM25 full text search, but you can still use a good embedding model.&lt;/li&gt;\n&lt;li&gt;A note system for saving webpages and notes for RAG or use as direct context&lt;/li&gt;\n&lt;li&gt;Several basic useful tools: web search, prompt optimizer, wiki, retriever, save note, update your preference, and some &amp;quot;agents&amp;quot; that can plan and execute those tools itself&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the picture is an example of how a 4B model planned and used the tools it has. In this example, I tested too many concurrent web searches, so I didn&amp;#39;t notice I needed to click the captcha on the page. So it failed in the first 2 steps, but you can get rid of it easily by clicking the captcha, or use a custom API, or DuckDuckGo, brave.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6\"&gt;https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 4",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m91u38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Asleep-Ratio7535",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m91u38/updated_ai_assistant_chrome_extension_has_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91u38/updated_ai_assistant_chrome_extension_has_tools/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753456445,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:\n\n1. **Tools first.** Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.\n2. **Start with general, low-level tools.** For example, bash is a powerful tool that can cover most needs. You don't need to start with a full suite of 100 tools.\n3. **Start with a single agent.** Once you have all the basic tools, test them with a single react agent. It's extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.\n4. **Start with the best models.** There will be a lot of problems with your system, so you don't want the model's ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.\n5. **Trace and log your agent.** Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like Langsmith, Langfuse, etc.\n6. **Identify the bottlenecks.** There's a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn't know how to do something, etc.\n7. **Iterate based on the bottleneck.** There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.\n8. **You can combine workflows with agents and it may work better.** If your objective is specialized and there's a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.\n9. **Trick: Utilize the filesystem as a hack.** Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.\n10. **Another Trick: Ask Claude Code how to write agents.** Claude Code is the best agent we have out there. Even though it's not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.",
          "author_fullname": "t2_ynwvt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I wrote an AI Agent that works better than I expected. Here are 10 learnings.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vmoi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.59,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753439430,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Tools first.&lt;/strong&gt; Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with general, low-level tools.&lt;/strong&gt; For example, bash is a powerful tool that can cover most needs. You don&amp;#39;t need to start with a full suite of 100 tools.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with a single agent.&lt;/strong&gt; Once you have all the basic tools, test them with a single react agent. It&amp;#39;s extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with the best models.&lt;/strong&gt; There will be a lot of problems with your system, so you don&amp;#39;t want the model&amp;#39;s ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trace and log your agent.&lt;/strong&gt; Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like Langsmith, Langfuse, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Identify the bottlenecks.&lt;/strong&gt; There&amp;#39;s a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn&amp;#39;t know how to do something, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Iterate based on the bottleneck.&lt;/strong&gt; There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;You can combine workflows with agents and it may work better.&lt;/strong&gt; If your objective is specialized and there&amp;#39;s a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trick: Utilize the filesystem as a hack.&lt;/strong&gt; Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Another Trick: Ask Claude Code how to write agents.&lt;/strong&gt; Claude Code is the best agent we have out there. Even though it&amp;#39;s not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8vmoi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Js8544",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753439430,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to get started at self hosting an LLM but have no experience with this. \n\nWhat I am looking for is:\n\nAn LLM that I can explore with code, ideally if I can link it in with some folders on my MacBook Pro M4, and then also on a server, the servers will be getting GPUs mounted soon. \n\nI ideally want to be able to send it a defined file of what code styles and principles to follow, and I would love to know what self hosted options we can look at helping with PR reviews. \n\nI don’t want AI to replace or cut the corners of my team but to help us out and become more consistent. \n\nSo ideally, self hosted options (Docker etc), if it could be integrated into PRs with a self hosted GitLab if needed?\n\nI’ve read a bit about Qwen3 but not sure where to even get started to explore and try it out. ",
          "author_fullname": "t2_fpgzf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to get started",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9antc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753476933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to get started at self hosting an LLM but have no experience with this. &lt;/p&gt;\n\n&lt;p&gt;What I am looking for is:&lt;/p&gt;\n\n&lt;p&gt;An LLM that I can explore with code, ideally if I can link it in with some folders on my MacBook Pro M4, and then also on a server, the servers will be getting GPUs mounted soon. &lt;/p&gt;\n\n&lt;p&gt;I ideally want to be able to send it a defined file of what code styles and principles to follow, and I would love to know what self hosted options we can look at helping with PR reviews. &lt;/p&gt;\n\n&lt;p&gt;I don’t want AI to replace or cut the corners of my team but to help us out and become more consistent. &lt;/p&gt;\n\n&lt;p&gt;So ideally, self hosted options (Docker etc), if it could be integrated into PRs with a self hosted GitLab if needed?&lt;/p&gt;\n\n&lt;p&gt;I’ve read a bit about Qwen3 but not sure where to even get started to explore and try it out. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9antc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theonethatownz",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9antc/how_to_get_started/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9antc/how_to_get_started/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753476933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all. As the title says, I'm new to hosting AI locally. I am using an Nvidia RTX 4080 16GB. I got Ollama installed and llama2 running, but it is pretty lackluster. Seeing that I can run llama3 which is supposed to be much better. Any tips from experienced users? I am just doing this as something to tinker with. TIA. ",
          "author_fullname": "t2_429jxbgl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New to local AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m91dmh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753455410,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. As the title says, I&amp;#39;m new to hosting AI locally. I am using an Nvidia RTX 4080 16GB. I got Ollama installed and llama2 running, but it is pretty lackluster. Seeing that I can run llama3 which is supposed to be much better. Any tips from experienced users? I am just doing this as something to tinker with. TIA. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m91dmh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "m_spoon09",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91dmh/new_to_local_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91dmh/new_to_local_ai/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753455410,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dear, AMD!\n\nYou have a potential segment of AI PRO R9700 consumers who cannot afford to buy an entire workstation based on several R9700s,\n\nbut these people (including me) have enough money to independently build a PC based on 2xR9700 and a consumer motherboard with cheaper Udimm memory.\n\nI will be very exhausted if I wait even longer, until the end of Q3. According to this logic, it makes sense to wait for Black Friday.\n\nAnd then Intel may catch up with you with b60 and b60 dual.\n\nAlso, at the end of November, a significant discount on the economy version of the 32Gb GPU from your other competitors is possible. So every week of waiting is bad.\n\nOn the other hand, I understand that AMD probably aims to declare the R9700 as a GPU for LLM, while temporarily distancing itself from gamer.\n\nAnd this is correct marketing. Therefore, in today's conditions of tight competition, let me suggest a very unusual step for such a large company:\n\nimmediately make available for sale \\[kits\\] of mandatory purchase together -\n\n\\[2pcs. R9700 + motherboard (non-ECC UDIMM RAM) with (2, or better - 3)xPCI Express 5.0 + maybe a cable\\] or a set only with \\[2pcs. R9700\\]",
          "author_fullname": "t2_1ua8m0mp6s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Radeon AI PRO R9700 - when can I buy it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8yvxd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753449345,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear, AMD!&lt;/p&gt;\n\n&lt;p&gt;You have a potential segment of AI PRO R9700 consumers who cannot afford to buy an entire workstation based on several R9700s,&lt;/p&gt;\n\n&lt;p&gt;but these people (including me) have enough money to independently build a PC based on 2xR9700 and a consumer motherboard with cheaper Udimm memory.&lt;/p&gt;\n\n&lt;p&gt;I will be very exhausted if I wait even longer, until the end of Q3. According to this logic, it makes sense to wait for Black Friday.&lt;/p&gt;\n\n&lt;p&gt;And then Intel may catch up with you with b60 and b60 dual.&lt;/p&gt;\n\n&lt;p&gt;Also, at the end of November, a significant discount on the economy version of the 32Gb GPU from your other competitors is possible. So every week of waiting is bad.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, I understand that AMD probably aims to declare the R9700 as a GPU for LLM, while temporarily distancing itself from gamer.&lt;/p&gt;\n\n&lt;p&gt;And this is correct marketing. Therefore, in today&amp;#39;s conditions of tight competition, let me suggest a very unusual step for such a large company:&lt;/p&gt;\n\n&lt;p&gt;immediately make available for sale [kits] of mandatory purchase together -&lt;/p&gt;\n\n&lt;p&gt;[2pcs. R9700 + motherboard (non-ECC UDIMM RAM) with (2, or better - 3)xPCI Express 5.0 + maybe a cable] or a set only with [2pcs. R9700]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8yvxd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mundane_Progress_898",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8yvxd/amd_radeon_ai_pro_r9700_when_can_i_buy_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8yvxd/amd_radeon_ai_pro_r9700_when_can_i_buy_it/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753449345,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm thinking of building a desktop app that helps you:\n\n\n\n\\- Detect your hardware (GPU, RAM, CPU)\n\n\\- Benchmark local AI models (GGUF/ONNX) automatically\n\n\\- Tell you which quant config runs best (Q4, Q5, etc.)\n\n\\- Show ratings like \"This model is great for coding, 12 tok/s on 8GB RAM\"\n\n\\- Launch models directly in one click\n\n\n\nLike HuggingFace meets Steam meets LM Studio — but optimized for \\*you\\*.\n\n\n\nWould you use this? What would you want it to do?\n\n",
          "author_fullname": "t2_1uanajb3sh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would you use this? Desktop app for auto-benchmarking GGUF/ONNX models locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93u0b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753460989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of building a desktop app that helps you:&lt;/p&gt;\n\n&lt;p&gt;- Detect your hardware (GPU, RAM, CPU)&lt;/p&gt;\n\n&lt;p&gt;- Benchmark local AI models (GGUF/ONNX) automatically&lt;/p&gt;\n\n&lt;p&gt;- Tell you which quant config runs best (Q4, Q5, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Show ratings like &amp;quot;This model is great for coding, 12 tok/s on 8GB RAM&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;- Launch models directly in one click&lt;/p&gt;\n\n&lt;p&gt;Like HuggingFace meets Steam meets LM Studio — but optimized for *you*.&lt;/p&gt;\n\n&lt;p&gt;Would you use this? What would you want it to do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m93u0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Conscious-Drive-1448",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753460989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Phi-4-mini-flash-reasoning isn't in the Ollama repository, and in huggingface there are .safetensors files, as the architecture of this new model is called SambaY (some Mamba variant) this may complicate things with regard to converting it to GGUF or some other format, I would like to run the model with no modification to begin with.",
          "author_fullname": "t2_61c14i0z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any way to run Phi-4-mini-flash-reasoning on Ollama?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99ac7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753473646,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Phi-4-mini-flash-reasoning isn&amp;#39;t in the Ollama repository, and in huggingface there are .safetensors files, as the architecture of this new model is called SambaY (some Mamba variant) this may complicate things with regard to converting it to GGUF or some other format, I would like to run the model with no modification to begin with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m99ac7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WowSkaro",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99ac7/is_there_any_way_to_run_phi4miniflashreasoning_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99ac7/is_there_any_way_to_run_phi4miniflashreasoning_on/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753473646,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nIt's a translation model.\n\nKey Features:\n\n* **Multilingual Support for 92 Languages**: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.\n* **High Customizability**: The new version provides advanced translation capabilities such as terminology intervention, domain prompts and translation memory. By enabling customizable prompt engineering, it delivers optimized translation performance tailored to complex, domain-specific, and mission-critical application scenarios.\n* **Low Latency &amp; Cost Efficiency**: By leveraging a lightweight Mixture of Experts (MoE) architecture, Qwen-MT achieves high translation performance with faster response times and significantly reduced API costs (as low as $0.5 per million output tokens). This is particularly well-suited for high-concurrency environments and latency-sensitive applications.\n\n[benchmark](https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf)\n\n[https://qwenlm.github.io/blog/qwen-mt/](https://qwenlm.github.io/blog/qwen-mt/)",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen's third bomb: Qwen3-MT",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ebw46w8hkuef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 63,
                  "x": 108,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ac95f2f94be794e56e5173ecf84f81d0e87b4e7"
                },
                {
                  "y": 127,
                  "x": 216,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=879438cc2d25ccf7bbaefa6a935cf40c6d2679f5"
                },
                {
                  "y": 189,
                  "x": 320,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dcf05d1d1e97cf3ae2922a54cfcce722e499a2b7"
                },
                {
                  "y": 378,
                  "x": 640,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9010db33f3f1a921174e424f03bd7769dd761bf"
                },
                {
                  "y": 567,
                  "x": 960,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=72eef5769345bbf6893557d58d6fbfafd6b58fe9"
                },
                {
                  "y": 638,
                  "x": 1080,
                  "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f949bada961df505b0b2d5483c2ddae0bb74ece"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1860,
                "u": "https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf"
              },
              "id": "ebw46w8hkuef1"
            }
          },
          "name": "t3_1m88s09",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 161,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 161,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/LksViWDcxO1eQ0ZQpLUVRXks4wbjVGa9UjqigE3hofA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753373875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a translation model.&lt;/p&gt;\n\n&lt;p&gt;Key Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Multilingual Support for 92 Languages&lt;/strong&gt;: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High Customizability&lt;/strong&gt;: The new version provides advanced translation capabilities such as terminology intervention, domain prompts and translation memory. By enabling customizable prompt engineering, it delivers optimized translation performance tailored to complex, domain-specific, and mission-critical application scenarios.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low Latency &amp;amp; Cost Efficiency&lt;/strong&gt;: By leveraging a lightweight Mixture of Experts (MoE) architecture, Qwen-MT achieves high translation performance with faster response times and significantly reduced API costs (as low as $0.5 per million output tokens). This is particularly well-suited for high-concurrency environments and latency-sensitive applications.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf\"&gt;benchmark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://qwenlm.github.io/blog/qwen-mt/\"&gt;https://qwenlm.github.io/blog/qwen-mt/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m88s09",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/",
          "subreddit_subscribers": 504692,
          "created_utc": 1753373875,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new mistralai/Magistral-Small-2507 !?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m85vhw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 220,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 220,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=feaece90f7cf4e9ef4f2b2363a06c331aa76c3be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753367249,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mistralai/Magistral-Small-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?auto=webp&amp;s=6d8a1d0a5f0cc1e9f8968e79b1869ad8ed3d1a1d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f5b9e280105076efaeb7fb4658ea8f168c6e031",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bb8df5e83f6c1204cc11aed536f322d4ded452a",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=590a3d841dcd1b2437d95fe4576578e3ffad6bab",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1aa4619d7f8ff888c9274c7c014531dcd45ff12e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=74cacf05986588334ed0a0a18df51559c11386b8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b24a8930f9c9f8618bbba4dd9619c2b04cb8469",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m85vhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/",
          "stickied": false,
          "url": "https://huggingface.co/mistralai/Magistral-Small-2507",
          "subreddit_subscribers": 504692,
          "created_utc": 1753367249,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}