{
  "kind": "Listing",
  "data": {
    "after": "t3_1mcatlt",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Tesslate/UIGEN-X-4B-0729](https://huggingface.co/Tesslate/UIGEN-X-4B-0729) 4B model that does reasoning for Design. We also released a 32B earlier in the week. \n\nAs per the last post -&gt;  \nSpecifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.\n\nWe're looking for some beta testers for some new models and open source projects!",
          "author_fullname": "t2_15kd4d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "yof4zxwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b585f63f1976fecd3209b28576a30e20382e3288"
                },
                {
                  "y": 139,
                  "x": 216,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d390945c2be5960f802c46d7534e3b13b1b9ce64"
                },
                {
                  "y": 206,
                  "x": 320,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4d91cbd80cd1bea9bf4046a92926c7a6a1327b3"
                },
                {
                  "y": 412,
                  "x": 640,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf7e75cc7fc920dcadfeddadec7708838aa7f496"
                },
                {
                  "y": 618,
                  "x": 960,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64c2c4a9e6f40664edd97d0fcbc66a58fbd01f10"
                },
                {
                  "y": 695,
                  "x": 1080,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9baade1c07727a9e0b013590eb07022ff7d97bbf"
                }
              ],
              "s": {
                "y": 1855,
                "x": 2880,
                "u": "https://preview.redd.it/yof4zxwiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=750bd98b519b2c2a6a9b67e375c712a747d76a7b"
              },
              "id": "yof4zxwiewff1"
            },
            "16laa8fzewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 107,
                  "x": 108,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=be850094630e801bc14b141c02c45a5d5780e39e"
                },
                {
                  "y": 214,
                  "x": 216,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=000716e3856e70eec1e6c3feee9a6082dfb6f14b"
                },
                {
                  "y": 317,
                  "x": 320,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7ba27c04a5a32d43cad0d06ac09510ffc21eb18"
                },
                {
                  "y": 635,
                  "x": 640,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=be9b40c399b65fa7aac8b8f12c47664cea519593"
                },
                {
                  "y": 953,
                  "x": 960,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c69dc72963fd774990d40819354963b4542144b"
                },
                {
                  "y": 1073,
                  "x": 1080,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56223aed305acfd4d0bdfc7293eac13e683e3a05"
                }
              ],
              "s": {
                "y": 1739,
                "x": 1750,
                "u": "https://preview.redd.it/16laa8fzewff1.png?width=1750&amp;format=png&amp;auto=webp&amp;s=685da575f550a67f1885f3ee2f448abb0ad8dfa8"
              },
              "id": "16laa8fzewff1"
            },
            "ntu2cxwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0d018ce17ce6e93fd4107130e86643585cf96c2"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23db8ae32b0c22d865ce52f9a03fbbcde05deeb0"
                },
                {
                  "y": 185,
                  "x": 320,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba539c8005ffd0c4736b5a57568e182824f305fd"
                },
                {
                  "y": 371,
                  "x": 640,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=106863b52b2fff18696ddf6ec546938ea7f8de35"
                },
                {
                  "y": 557,
                  "x": 960,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5e57f7a9e86b03e31ebaac20963a33085097ff2"
                },
                {
                  "y": 627,
                  "x": 1080,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a44493249a8080f036743b50835676ad73e83c7d"
                }
              ],
              "s": {
                "y": 1672,
                "x": 2880,
                "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=f6ef93e725d1a52278691bd9f2ecb1cf3ea1b6ee"
              },
              "id": "ntu2cxwiewff1"
            },
            "wj4w2vb2fwff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 124,
                  "x": 108,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce4efa314f24050279eaa398edd15d69545694b6"
                },
                {
                  "y": 249,
                  "x": 216,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=48cd29a868c118178676c8e761b9d0c7965971ba"
                },
                {
                  "y": 370,
                  "x": 320,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=524401edbadd87267db44465bb77ec546449da42"
                },
                {
                  "y": 740,
                  "x": 640,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f2d93c05fafd7effe55c1b8c9299eee020be45e"
                },
                {
                  "y": 1110,
                  "x": 960,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ff4bd17f8ef6686dd9f1c981b0ff0106d81f12f"
                },
                {
                  "y": 1249,
                  "x": 1080,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef3bc24444f01bec00a0224cb2c86cb3bca5067e"
                }
              ],
              "s": {
                "y": 1742,
                "x": 1506,
                "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=1506&amp;format=png&amp;auto=webp&amp;s=9eb5a8ab422fa1d5864870f6212bbb512ee327f3"
              },
              "id": "wj4w2vb2fwff1"
            },
            "lex8nqwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 127,
                  "x": 108,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=120a521aa4bebf6052a8a8e73b8052832817615b"
                },
                {
                  "y": 255,
                  "x": 216,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d03a3f3bbf63c6fa038368a77926fdc077cffe45"
                },
                {
                  "y": 379,
                  "x": 320,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5069c1d3fc72035727961c5af49935dc5edaf6d"
                },
                {
                  "y": 758,
                  "x": 640,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=49088fabb5a9e1b73f83b42ff1659d2c651ecabb"
                },
                {
                  "y": 1137,
                  "x": 960,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fae5c9dede0064c130c7af51c4efd8d9b89291b"
                },
                {
                  "y": 1279,
                  "x": 1080,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8390fb53d7893ccfbda7ba43e1043526d3ed1995"
                }
              ],
              "s": {
                "y": 2274,
                "x": 1920,
                "u": "https://preview.redd.it/lex8nqwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=962070f4cec63ffe5e3f74afdd9e361d3eb1e65c"
              },
              "id": "lex8nqwiewff1"
            },
            "adea0wwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97e39bf0fb8a28722a9fdec246b8f02853a61239"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b30b4a9d77e84eb9a95649bf5113d51f5c7733d"
                },
                {
                  "y": 278,
                  "x": 320,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8d21cc220a9c038420281dd75bf1cbc8c810781"
                },
                {
                  "y": 557,
                  "x": 640,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=305114889fd74072b8f44408ea2eb39e9d6ca40d"
                },
                {
                  "y": 836,
                  "x": 960,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b3179c0833b7971357d239b13f9544ca646a9c0"
                },
                {
                  "y": 941,
                  "x": 1080,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b112315e314c3e8663a20042433fdb632fca8b4"
                }
              ],
              "s": {
                "y": 1673,
                "x": 1920,
                "u": "https://preview.redd.it/adea0wwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=fd821b55ed691c1336897e770b0cac21d3da1733"
              },
              "id": "adea0wwiewff1"
            },
            "ax8700xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 215,
                  "x": 108,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6cecc068a16f0743e7f7aa0118d59381e879f59"
                },
                {
                  "y": 430,
                  "x": 216,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd1befd4a28971b0793663f9dc338070a9476cfa"
                },
                {
                  "y": 638,
                  "x": 320,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3535688560617701a73e241cee3b40f695746ebb"
                },
                {
                  "y": 1276,
                  "x": 640,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6017beab10859d631fbafb9bb24558403617258"
                },
                {
                  "y": 1914,
                  "x": 960,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=20cddeaa29fde178c979ec16035d38ab20242536"
                },
                {
                  "y": 2153,
                  "x": 1080,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f70ea717022f00cedac4143741218799011ef84"
                }
              ],
              "s": {
                "y": 5743,
                "x": 2880,
                "u": "https://preview.redd.it/ax8700xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=a6e9f1619b46af78448cf8f2b9ac70e06d78a1ea"
              },
              "id": "ax8700xiewff1"
            },
            "ejyq00xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98e5ba20ac3d31de436e60a97922996875d347ff"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa9bc6a58df8f71ea883392c35f600600742b517"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ec152ece72fff30fb421645a1669dd931e4bdfc"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=544c750520fddedec5be1f081c7cc7401df16985"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d6e5f430fda3cc2768464bd78ec043b99927fc6b"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5f91d5725b574cb286fec458ef62e689c27e607c"
                }
              ],
              "s": {
                "y": 6984,
                "x": 2880,
                "u": "https://preview.redd.it/ejyq00xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=303ddd2e22aebde6c25e2fdcb88a832f57df9944"
              },
              "id": "ejyq00xiewff1"
            },
            "kxkn9umxewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 106,
                  "x": 108,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=932e983f66f50902893c65fc0cfda02a9d2b22a2"
                },
                {
                  "y": 212,
                  "x": 216,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec248f0a2e6cf260216575fc5cc60bab220bb28e"
                },
                {
                  "y": 314,
                  "x": 320,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5edf2564694ad7e618bb3f280ef348153a1913f3"
                },
                {
                  "y": 629,
                  "x": 640,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e3c864fa70160d3f0e137ee42eff4a18605859c2"
                },
                {
                  "y": 943,
                  "x": 960,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=614d3f0351a10e309aa56852fa09cd6a934d7328"
                },
                {
                  "y": 1061,
                  "x": 1080,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ceee5ecfa7a0dd1ca565f7a3cddb9b4a64ffd10d"
                }
              ],
              "s": {
                "y": 1742,
                "x": 1772,
                "u": "https://preview.redd.it/kxkn9umxewff1.png?width=1772&amp;format=png&amp;auto=webp&amp;s=ee44a102a0f980b2655670786261becf38366c67"
              },
              "id": "kxkn9umxewff1"
            },
            "z7pj58xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09b6203fb4f173b715c03493dfe67e9b039f3664"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=655762aeb2b663c2bf82fa7a23b9ba0e046ff117"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b0c36608a8dd7d989cb8f1425f0ed68cecc6cf0"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e2395673178607874bdae7dfaa75c8460c22721a"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=036661bf27ca2ae95489ef42fbd9163baa23a0f5"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f100364d1c04026b195f7ea334a7a1d4a51831c7"
                }
              ],
              "s": {
                "y": 6229,
                "x": 2880,
                "u": "https://preview.redd.it/z7pj58xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=aeafa90cf8e3d73863c35071d9e713ca83facbf7"
              },
              "id": "z7pj58xiewff1"
            },
            "acxqsuwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7425577e6a47a9d3e0a3a100f387cf005fd280d"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bdb03c41fce1c12f00429512ee516e3032d7be24"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bd65e8862fc6957d1b38807ae5860c9cdaf6019"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fef431ddba58fbfeadf672b3f1b2f7cd59a4024e"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d048fdfecfaaf1e67a1c0c3acc520744d210c2c3"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5bf701f33777ed0ef64335bf05d63f02e05f82b"
                }
              ],
              "s": {
                "y": 5998,
                "x": 1920,
                "u": "https://preview.redd.it/acxqsuwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=1ff339ed9b099c2cc7e6c8983e87af8e3494534e"
              },
              "id": "acxqsuwiewff1"
            },
            "s7p75twiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=84e39273f74fd125311c365fac8f17aa2f027ac2"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=94fbe9bcc7206c2de0cd6c9ad5dafa699528449e"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b412b682e705205f4a9170aeca865f1dabd2e419"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd319320ae2e53ab1adeebb7127aa9d9a3f5e760"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b384f769fd836b366a0cddefec36f82e53b87918"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97aa4cdd51e0498d0929bbc700ba0d01373ea554"
                }
              ],
              "s": {
                "y": 5391,
                "x": 1920,
                "u": "https://preview.redd.it/s7p75twiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=473ae9015c69ce171209a0f6d69a7262bd95c4cc"
              },
              "id": "s7p75twiewff1"
            },
            "gjgr517pewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4755f1869d3b7eb7ea0597f941557ba81c651329"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb98a6adf6598211d6e427d7dbb4e18218c97614"
                },
                {
                  "y": 203,
                  "x": 320,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5801ab34ab8e6d6c323c619f5fd48ef9bcd7de9f"
                },
                {
                  "y": 407,
                  "x": 640,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86b0ac115a5b3e9db3e959924030049ea892d809"
                },
                {
                  "y": 610,
                  "x": 960,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a6414c79af3f3c9b4a589fa97d866dd966bdda5"
                },
                {
                  "y": 686,
                  "x": 1080,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0f0758aadd57cbcd02f9ff3c31c98632506f060"
                }
              ],
              "s": {
                "y": 1564,
                "x": 2459,
                "u": "https://preview.redd.it/gjgr517pewff1.png?width=2459&amp;format=png&amp;auto=webp&amp;s=d7c539f97e387b73f2f07bc8ba1f06bec4aad9d0"
              },
              "id": "gjgr517pewff1"
            },
            "1p1dk2xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4168773f08144636cd601811a4ab8d2ab17744f8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0167a8561f08feb97a21f409a40491b2d8179c2"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a81d1f425fbb1ba6891eebd5fc7834621d47a971"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a92096881722780601c9417e8ed52dda672226c5"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b3a497379e5488734b986a943c4b9c8a9177f8ab"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=999cfe06600aab2caa654d74023a8ece7a9424de"
                }
              ],
              "s": {
                "y": 5337,
                "x": 1920,
                "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=8b79f29a9a816ef267db3d202bb66ee24e42e89b"
              },
              "id": "1p1dk2xiewff1"
            }
          },
          "name": "t3_1mcr64f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 260,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "gjgr517pewff1",
                "id": 716986046
              },
              {
                "media_id": "wj4w2vb2fwff1",
                "id": 716986047
              },
              {
                "media_id": "kxkn9umxewff1",
                "id": 716986048
              },
              {
                "media_id": "ntu2cxwiewff1",
                "id": 716986049
              },
              {
                "media_id": "ax8700xiewff1",
                "id": 716986050
              },
              {
                "media_id": "lex8nqwiewff1",
                "id": 716986051
              },
              {
                "media_id": "acxqsuwiewff1",
                "id": 716986052
              },
              {
                "media_id": "s7p75twiewff1",
                "id": 716986053
              },
              {
                "media_id": "1p1dk2xiewff1",
                "id": 716986054
              },
              {
                "media_id": "adea0wwiewff1",
                "id": 716986055
              },
              {
                "media_id": "yof4zxwiewff1",
                "id": 716986056
              },
              {
                "media_id": "ejyq00xiewff1",
                "id": 716986057
              },
              {
                "media_id": "z7pj58xiewff1",
                "id": 716986058
              },
              {
                "media_id": "16laa8fzewff1",
                "id": 716986059
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 260,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3wFSGxs0og7hUYyLF8nuoy2CBvu34JQ_m2cRe7ujEoc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753832160,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-4B-0729\"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt; 4B model that does reasoning for Design. We also released a 32B earlier in the week. &lt;/p&gt;\n\n&lt;p&gt;As per the last post -&amp;gt;&lt;br/&gt;\nSpecifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re looking for some beta testers for some new models and open source projects!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mcr64f",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcr64f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "smirkishere",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mcr64f",
          "subreddit_subscribers": 506972,
          "created_utc": 1753832160,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcoce7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 295,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 295,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=e171e9de6f65dcbec52d568f58750bcc5885ba3a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753825022,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?auto=webp&amp;s=d470a9db6c0be816c3d916347e4ae5e8bad6c6a8",
                  "width": 750,
                  "height": 422
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e14a96af45b1cfcde1a2159e64971bc1d775033",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47124b81e03a6604140c8e4d29ddbe43c15456c3",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf6168a4321e52f96ea360e8b865dc9bbcaaf345",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d50b7793829c5aa107cf8ecaa3b004d46e3cdef0",
                    "width": 640,
                    "height": 360
                  }
                ],
                "variants": {},
                "id": "9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcoce7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 84,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/",
          "stickied": false,
          "url": "https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753825022,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is from the latest Qwen3-30B-A3B-Instruct-2507. ❤",
          "author_fullname": "t2_qz1qjc86",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Newest Qwen made me cry. It's not perfect, but I still love it.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 53,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mci7uu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 537,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 537,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/h1Ejab1vdsRPCPX8mSRmFSTByiWOsDhozhB4Y9joLZQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753811149,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is from the latest Qwen3-30B-A3B-Instruct-2507. ❤&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/gnkbnxzlouff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/gnkbnxzlouff1.png?auto=webp&amp;s=15ae6e9bfdd39878d13fcc40579f5f5914892497",
                  "width": 537,
                  "height": 204
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=387b92e0abf220fa87708b750e1cd04535c8d238",
                    "width": 108,
                    "height": 41
                  },
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3003e1ab3cfb059c6497615c2cb005b3643dcb9b",
                    "width": 216,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=431c53f32897af3a4225062d97bdc95913f53ec0",
                    "width": 320,
                    "height": 121
                  }
                ],
                "variants": {},
                "id": "IXr-dCvLuJL4cGd_YuCTQJMpv6LVqTDaGI7ZM0RkpWg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mci7uu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cool-Chemical-5629",
          "discussion_type": null,
          "num_comments": 72,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/",
          "stickied": false,
          "url": "https://i.redd.it/gnkbnxzlouff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753811149,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Fast Attention algorithm speeds SoftMax function by about 30&amp;#37;. As a result, we have 5&amp;#37; decrease in inference time for Meta LLM on A100](https://preview.redd.it/1zbwyzlgwxff1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=5478539a6ccee17607c04f611ec28225919b2586)\n\n[https://fastattention.ai/#7cb9a932-8d17-4d96-953c-952dfa732171](https://fastattention.ai/#7cb9a932-8d17-4d96-953c-952dfa732171)\n\nhttps://preview.redd.it/jtw45kflwxff1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=c757888d664c20e32a761fb8bdf23236062472f3\n\n",
          "author_fullname": "t2_e0y1nfjl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New, faster SoftMax math makes Llama inference faster by 5%",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "1zbwyzlgwxff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 66,
                  "x": 108,
                  "u": "https://preview.redd.it/1zbwyzlgwxff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6495bc29bdbbe877d1787da6e1e6a39f2b478ff"
                },
                {
                  "y": 133,
                  "x": 216,
                  "u": "https://preview.redd.it/1zbwyzlgwxff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01d1b8a5d34c6fb31f80c413b444e1f049e9a352"
                },
                {
                  "y": 197,
                  "x": 320,
                  "u": "https://preview.redd.it/1zbwyzlgwxff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66204fe24f5f61d08e488f18725161f7d492d9a9"
                },
                {
                  "y": 395,
                  "x": 640,
                  "u": "https://preview.redd.it/1zbwyzlgwxff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce7e940cf2d0a0b21eb7c8a33be71c2f649fd9cb"
                },
                {
                  "y": 593,
                  "x": 960,
                  "u": "https://preview.redd.it/1zbwyzlgwxff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f83b330a81a5be50da105b50592a81a51ccde4d"
                },
                {
                  "y": 667,
                  "x": 1080,
                  "u": "https://preview.redd.it/1zbwyzlgwxff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d1d5b5f2a27f7f796e97e458cd58f81d784413c"
                }
              ],
              "s": {
                "y": 742,
                "x": 1200,
                "u": "https://preview.redd.it/1zbwyzlgwxff1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=5478539a6ccee17607c04f611ec28225919b2586"
              },
              "id": "1zbwyzlgwxff1"
            },
            "jtw45kflwxff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 66,
                  "x": 108,
                  "u": "https://preview.redd.it/jtw45kflwxff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc9fe2c6cbcd1d2163b191725c6c61276067e55c"
                },
                {
                  "y": 133,
                  "x": 216,
                  "u": "https://preview.redd.it/jtw45kflwxff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=405b4ae7abfc940884c3f98b991000c8c62a28a9"
                },
                {
                  "y": 197,
                  "x": 320,
                  "u": "https://preview.redd.it/jtw45kflwxff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3372a8ebbf62b4b20adaa64f8526fb8f8d6af8ee"
                },
                {
                  "y": 395,
                  "x": 640,
                  "u": "https://preview.redd.it/jtw45kflwxff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=498e7b0d282368120385a571d2ef3ec3c2d44f82"
                },
                {
                  "y": 593,
                  "x": 960,
                  "u": "https://preview.redd.it/jtw45kflwxff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=587bedbe08b08038e98c97da53d38aa65c8a0eb5"
                },
                {
                  "y": 667,
                  "x": 1080,
                  "u": "https://preview.redd.it/jtw45kflwxff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3c3d1312c127eeb72b2c2b4268079904dd07687e"
                }
              ],
              "s": {
                "y": 742,
                "x": 1200,
                "u": "https://preview.redd.it/jtw45kflwxff1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=c757888d664c20e32a761fb8bdf23236062472f3"
              },
              "id": "jtw45kflwxff1"
            }
          },
          "name": "t3_1mcxdiu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 59,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 59,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=00500b21d2377564465b95e740df59ed888ec16a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753850309,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/1zbwyzlgwxff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5478539a6ccee17607c04f611ec28225919b2586\"&gt;Fast Attention algorithm speeds SoftMax function by about 30&amp;#37;. As a result, we have 5&amp;#37; decrease in inference time for Meta LLM on A100&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://fastattention.ai/#7cb9a932-8d17-4d96-953c-952dfa732171\"&gt;https://fastattention.ai/#7cb9a932-8d17-4d96-953c-952dfa732171&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jtw45kflwxff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c757888d664c20e32a761fb8bdf23236062472f3\"&gt;https://preview.redd.it/jtw45kflwxff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c757888d664c20e32a761fb8bdf23236062472f3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?auto=webp&amp;s=9bcca2e46a994a51aa02be7bf3d9fba480165210",
                  "width": 2448,
                  "height": 1224
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=871f3dbd8fb2ada2f35d5a2ebdec3073f0db63f7",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ab4e9d0bb0ea493510241180d799ab199c5a2ed",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=01f95e056ad0f81dfba6e1c8eac679031ac2e122",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82e5920a65a2670b0982cf057b124c956d782352",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a704a565b776d3175c328af63335e7cbda8a0d5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a6ec998d54d1e35a987544a520d1921d4c74e61",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "uygeA57Mfstw_haQUN7kQKKgZbH8FlPoXrBVy5xM7pc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcxdiu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Odd_Employee128",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcxdiu/new_faster_softmax_math_makes_llama_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcxdiu/new_faster_softmax_math_makes_llama_inference/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753850309,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I saw [unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF · Hugging Face](https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF) just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4\\_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I'm excited to see what else it can do this week!\n\nGitHub: [lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration](https://github.com/lemonade-sdk/lemonade)",
          "author_fullname": "t2_1m2ckixcqh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 64,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mco449",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 198,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/7xpye5hurvff1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 592,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/7xpye5hurvff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/7xpye5hurvff1/DASHPlaylist.mpd?a=1756470630%2CYmI1YzlkYmQ2MzgzZmYwOGM1NjVhNDcyZWYxZmM1ZjA4NTFkOGI5ZDBhMGFjNTQ3N2I4MDkzMTNhMmVlYzE5MA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/7xpye5hurvff1/HLSPlaylist.m3u8?a=1756470630%2CODA5OTdmZDFlN2JkZmFmNThiZmMzYjQ4MWUxZmM0NThmYTUwNTZlNGRjMjYxY2FmOGVkYjNiYTg3OGZjZjAyNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 198,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=140&amp;height=64&amp;crop=140:64,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=7412d0f300b6da0b8bc9109ecb41ef07f7def3be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753824484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw &lt;a href=\"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF\"&gt;unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF · Hugging Face&lt;/a&gt; just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I&amp;#39;m excited to see what else it can do this week!&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/lemonade-sdk/lemonade\"&gt;lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/7xpye5hurvff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?format=pjpg&amp;auto=webp&amp;s=42d0cbec81034d7b7efd7f37129a81782dc0f3a9",
                  "width": 2074,
                  "height": 960
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f46a864c4101d08c5f7634bda139ea125562aef9",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a85bb6d97aae39128b013a769b747a78d013bef",
                    "width": 216,
                    "height": 99
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c3b96d303e0d238fc8ee25ce519ce9a723c249c3",
                    "width": 320,
                    "height": 148
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cac371463bffd1516bfca661c6b086b5c43e0a77",
                    "width": 640,
                    "height": 296
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbfaa41803cc435e672ff77f44bdc9bbdc1c8952",
                    "width": 960,
                    "height": 444
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=25e3d9ea149342d300ab0c21256440d35acba3ed",
                    "width": 1080,
                    "height": 499
                  }
                ],
                "variants": {},
                "id": "czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mco449",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jfowers_amd",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/",
          "stickied": false,
          "url": "https://v.redd.it/7xpye5hurvff1",
          "subreddit_subscribers": 506972,
          "created_utc": 1753824484,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/7xpye5hurvff1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 592,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/7xpye5hurvff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/7xpye5hurvff1/DASHPlaylist.mpd?a=1756470630%2CYmI1YzlkYmQ2MzgzZmYwOGM1NjVhNDcyZWYxZmM1ZjA4NTFkOGI5ZDBhMGFjNTQ3N2I4MDkzMTNhMmVlYzE5MA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/7xpye5hurvff1/HLSPlaylist.m3u8?a=1756470630%2CODA5OTdmZDFlN2JkZmFmNThiZmMzYjQ4MWUxZmM0NThmYTUwNTZlNGRjMjYxY2FmOGVkYjNiYTg3OGZjZjAyNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcfmd2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 643,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 643,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50aa20219586bc9007fb96833d16a6a56c8c1c76",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753805463,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?auto=webp&amp;s=f1df54937600c0db76989bd14eef9e747df1fb0e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1c3476d621a9393fbb7ca11c48a3074c5fd6803",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7cef70bde41dd3225eec3f7d265fbf2704c0182",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab3e2615c90a6581b60c6d33c660bfc0f250b4c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9c2fc1f960e47499df06dc08d78c88be43e15e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15eba021f7d99140c48583ae883d2eb091807f16",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcfmd2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 264,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "subreddit_subscribers": 506972,
          "created_utc": 1753805463,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "* Keeping your warranty.  \n* 1 slot  \n* backside tube exits \n\nLook perfect to make a dense AI machine.\n\n\n\n[https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design](https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design)\n\n",
          "author_fullname": "t2_3rx5s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 5090 form INNO3D 1 slot with Alphacool-waterkoeling look perfect for local AI machines",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md0gfh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "ups": 27,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 27,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/NeEN6MGm_hnkNwFg7zJC-1QY8AH3i-jiJVFe5Z5lTM0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753861579,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;Keeping your warranty.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;1 slot&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;backside tube exits &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Look perfect to make a dense AI machine.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design\"&gt;https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/eeopjbr7uyff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/eeopjbr7uyff1.png?auto=webp&amp;s=9013fd688e0eb0ed62be948a8d3d4acedc2fcb40",
                  "width": 1000,
                  "height": 626
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5abd37ef1038ff8e9e6a0537a95ca2ea0a3d3ef",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=574794d614b9ecbd24909e833b0cac21e4d762cd",
                    "width": 216,
                    "height": 135
                  },
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b56159bfa847a204ab5ab96c1489261d202caf4",
                    "width": 320,
                    "height": 200
                  },
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=071af31f7998dd67f773b41419988ca83dd8fdd3",
                    "width": 640,
                    "height": 400
                  },
                  {
                    "url": "https://preview.redd.it/eeopjbr7uyff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0c5a836c6accb5a923be466745e47d3e4366147",
                    "width": 960,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "UD_dYV0qdMIHDeAIfPiwQicfo5K1meoR0O82qOPwsFU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1md0gfh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jwestra",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md0gfh/rtx_5090_form_inno3d_1_slot_with/",
          "stickied": false,
          "url": "https://i.redd.it/eeopjbr7uyff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753861579,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The Qwen3-30B-A3B-Instruct-2507 is an amazing release! Congratulations!\n\nHowever, the three-month-old 32B shows better performance across the board in the benchmark. I hope the Qwen3-32B Instruct/Thinking and Qwen3-30B-A3B-Thinking-2507 versions will be released soon!\n",
          "author_fullname": "t2_73xg2fw4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kudos to Qwen 3 team!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "nhnd3nuqpyff1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/nhnd3nuqpyff1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=b4a4777fbf15585877c329aa6410fda352472d5f"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/nhnd3nuqpyff1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=abf4196420500497c21a76dd763aca9fb69ee29d"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/nhnd3nuqpyff1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=f5237980d5b7d1de65245857661037126f6a750f"
                }
              ],
              "s": {
                "y": 480,
                "gif": "https://i.redd.it/nhnd3nuqpyff1.gif",
                "mp4": "https://preview.redd.it/nhnd3nuqpyff1.gif?format=mp4&amp;s=a04a1653064ec390929718dc0ed20a8ecf51da29",
                "x": 480
              },
              "id": "nhnd3nuqpyff1"
            }
          },
          "name": "t3_1md00oc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mUsja5QiJMisNYHJhZA4P57qdlHnaGZYvKopTiB-51E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753859844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Qwen3-30B-A3B-Instruct-2507 is an amazing release! Congratulations!&lt;/p&gt;\n\n&lt;p&gt;However, the three-month-old 32B shows better performance across the board in the benchmark. I hope the Qwen3-32B Instruct/Thinking and Qwen3-30B-A3B-Thinking-2507 versions will be released soon!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1md00oc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExcuseAccomplished97",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753859844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.\n\n✨ Key Enhancements:\n\n✅ Enhanced reasoning, coding, and math skills\n\n✅ Broader multilingual knowledge\n\n✅ Improved long-context understanding (up to 256K tokens)\n\n✅ Better alignment with user intent and open-ended tasks\n\n✅ No more &lt;think&gt; blocks — now operating exclusively in non-thinking mode\n\n🔧 With 3B activated parameters, it's approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking\n\nHugging Face: https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\n\nQwen Chat: https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507\n\nModel scope: https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary\n\n\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Qwen3-30B-A3B Small Update",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcg4qt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 319,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 319,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/O7xcYRkNGuBB0yBDfOkkcWs5DpYysQdkMKJvvFlOYpA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753806599,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.&lt;/p&gt;\n\n&lt;p&gt;✨ Key Enhancements:&lt;/p&gt;\n\n&lt;p&gt;✅ Enhanced reasoning, coding, and math skills&lt;/p&gt;\n\n&lt;p&gt;✅ Broader multilingual knowledge&lt;/p&gt;\n\n&lt;p&gt;✅ Improved long-context understanding (up to 256K tokens)&lt;/p&gt;\n\n&lt;p&gt;✅ Better alignment with user intent and open-ended tasks&lt;/p&gt;\n\n&lt;p&gt;✅ No more &amp;lt;think&amp;gt; blocks — now operating exclusively in non-thinking mode&lt;/p&gt;\n\n&lt;p&gt;🔧 With 3B activated parameters, it&amp;#39;s approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking&lt;/p&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen Chat: &lt;a href=\"https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507\"&gt;https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Model scope: &lt;a href=\"https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary\"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nd904g7gbuff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?auto=webp&amp;s=e65b518bfd8179ffe5850438ba9b1ea0fdbad33f",
                  "width": 900,
                  "height": 506
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f840db78bf1bdfd3bc2fbe2fce643b2615c41103",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d867b87e626895bab9aa6038ad0daeda82c3412b",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3bb498d8e823f04e6a1a8de9d73a55aa81af09d7",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b713bd1bbe154007dd6c0b8474098b47bf58ba4d",
                    "width": 640,
                    "height": 359
                  }
                ],
                "variants": {},
                "id": "G3G0aRr753pT7ZAwSw8VgFtbVKOybrXjQhhRczqdvYg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcg4qt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/",
          "stickied": false,
          "url": "https://i.redd.it/nd904g7gbuff1.jpeg",
          "subreddit_subscribers": 506972,
          "created_utc": 1753806599,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Simon Willison says “Ivan Fioravanti built this 44GB 3bit quantized version for MLX, specifically sized so people with 64GB machines could have a chance of running it. I tried it out... and it works extremely well.”\n\nhttps://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;utm_campaign=post&amp;utm_medium=email\n\nI’ve run the model with LMStudio on a 64gb M1 Max Studio. LMStudio initially would not run the model, providing a popup to that effect. The popup also allowed me to adjust the guardrails. I had to turn them off entirely to run the model.",
          "author_fullname": "t2_mjsmz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 Air on 64gb Mac with MLX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcvc46",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753843931,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simon Willison says “Ivan Fioravanti built this 44GB 3bit quantized version for MLX, specifically sized so people with 64GB machines could have a chance of running it. I tried it out... and it works extremely well.”&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;amp;utm_campaign=post&amp;amp;utm_medium=email\"&gt;https://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;amp;utm_campaign=post&amp;amp;utm_medium=email&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I’ve run the model with LMStudio on a 64gb M1 Max Studio. LMStudio initially would not run the model, providing a popup to that effect. The popup also allowed me to adjust the guardrails. I had to turn them off entirely to run the model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?auto=webp&amp;s=8e07b76e8d1166d0c8c5e741492849f4edee3088",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8312949968b09310a164bbbce12556723423845d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=90c385f8326e1d8b1138a0ffac4e59197db5b20a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8190f5a9b9608e9f430b3b6ce31a3bb4d883ad41",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=81dddb64d3287e341788d22055b06bdb58339b4b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d3ba2097184289d60db6279c54a5ce24897a683",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd64275068932684e1debcb761559964142a0a00",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "p-vtp39mhrdsV2hzM7NLn9CVPlTSdmMtS3NZncx5DWk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcvc46",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jarec707",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753843931,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "C'est la première fois qu'un modèle utilise intelligemment les serveurs MCP tout seul ! Ce n'est pas juste un ou deux serveurs et puis une réponse complètement à côté de la plaque !\n\nFor those who want my MCP flow, here’s the Pastebin:\n\n[https://pastebin.com/WNPrcjLS](https://pastebin.com/WNPrcjLS)\n\nhttps://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;format=png&amp;auto=webp&amp;s=30fca5c5a305810d2969af3035d710cef5a30268",
          "author_fullname": "t2_ti5m9mpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30b-3ab-2507 is a beast for MCP usage!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8kjwp8wkxuff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 136,
                  "x": 108,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd9fe7d167fa67def68be16708c9072dff16e4b9"
                },
                {
                  "y": 273,
                  "x": 216,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d68578f7cbad05a254ecfd9b9a974bce181b73c4"
                },
                {
                  "y": 405,
                  "x": 320,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=572ae738bba26cff6b6dcf1e4969cc51201c065e"
                },
                {
                  "y": 811,
                  "x": 640,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f88d4f809869f8243befb328a5639859eeeccba"
                }
              ],
              "s": {
                "y": 1150,
                "x": 907,
                "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;format=png&amp;auto=webp&amp;s=30fca5c5a305810d2969af3035d710cef5a30268"
              },
              "id": "8kjwp8wkxuff1"
            }
          },
          "name": "t3_1mcji8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 198,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 198,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Ku7pPJKnjoNSXHTx41JonGncgMhMPCUf8ZnqoDSjoGY.jpg",
          "edited": 1753816781,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753813999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;C&amp;#39;est la première fois qu&amp;#39;un modèle utilise intelligemment les serveurs MCP tout seul ! Ce n&amp;#39;est pas juste un ou deux serveurs et puis une réponse complètement à côté de la plaque !&lt;/p&gt;\n\n&lt;p&gt;For those who want my MCP flow, here’s the Pastebin:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/WNPrcjLS\"&gt;https://pastebin.com/WNPrcjLS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268\"&gt;https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcji8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Ninja7526",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753813999,
          "num_crossposts": 5,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen people claim that the new TR PROs can achieve the full 8-channel memory bandwidth even in SKUs with 16-cores. That's not the case.\n\nThe issue with the limited CCD bandwidth seems to still be present, and affects the low-number CCD parts. You can only achieve the full 8-channel bandwidth with 64-core+ WX CPUs.\n\nCheck the \"Latest baselines\" section in a processor's page at  [cpubenchmark.net](http://cpubenchmark.net)  with links to individual results where the \"Memory Threaded\" result is listed under \"Memory Mark\":\n\n|CPU|Memory BW|Reference|Notes|\n|:-|:-|:-|:-|\n|[AMD Threadripper PRO 9955WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9955WX&amp;id=6803) (16-cores)|\\~115 GB/s|[BL5099051 - Jul 20 2025](https://www.passmark.com/baselines/V11/display.php?id=509905130667)|2x CCD|\n|[AMD Threadripper PRO 9965WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9965WX&amp;id=6804) (24-cores)|\\~272 GB/s|[BL2797485 - Jul 29 2025](https://www.passmark.com/baselines/V11/display.php?id=279748548819) (other baselines start from 250GB/s)|4x CCDs|\n|[AMD Threadripper PRO 9975WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9975WX&amp;id=6799) (32-cores)|\\~272 GB/s|[BL2797820 - Jul 29 2025](https://www.passmark.com/baselines/V11/display.php?id=279782022829)|4x CCDs|\n|[AMD Threadripper PRO 9985WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9985WX&amp;id=6807) (64-cores)|\\~367 GB/s|[BL5099130 - Jul 21 2025](https://www.passmark.com/baselines/V11/display.php?id=509913021820)|8x CCDs|\n\nTherefore:\n\n* the 16-core 9955WX has lower mem bw than even a DDR4 EPYC CPU (e.g. [7R43 with 191 GB/s](https://www.passmark.com/baselines/V10/display.php?id=226455755507)).\n* the 24-core and 32-core parts have lower mem bw than DDR5 Genoa EPYCs (even some 16-core parts).\n* the 64-core and 96-core Threadrippers are not CCD-number limited, but still lose to the EPYCs since those have 12 channels (unless you use 7200 MT/s memory).\n\nFor comparison, check the excellent related threads by u/fairydreaming for the previous gen Threadrippers and EPYC Genoa/Turin:\n\n* [Comparing Threadripper 7000 memory bandwidth for all models : r/threadripper](https://www.reddit.com/r/threadripper/comments/1azmkvg/comparing_threadripper_7000_memory_bandwidth_for/)\n* [Memory bandwidth values (STREAM TRIAD benchmark results) for most Epyc Genoa CPUs (single and dual configurations) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/)\n* [STREAM TRIAD memory bandwidth benchmark values for Epyc Turin - almost 1 TB/s for a dual CPU system : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/)\n\nIf someone insists on buying a new TR Pro for their great compute throughput, I would suggest to at least skip the 16-core part.",
          "author_fullname": "t2_lw9me25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PSA: The new Threadripper PROs (9000 WX) are still CCD-Memory Bandwidth bottlenecked",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcrx23",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753834394,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753834203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen people claim that the new TR PROs can achieve the full 8-channel memory bandwidth even in SKUs with 16-cores. That&amp;#39;s not the case.&lt;/p&gt;\n\n&lt;p&gt;The issue with the limited CCD bandwidth seems to still be present, and affects the low-number CCD parts. You can only achieve the full 8-channel bandwidth with 64-core+ WX CPUs.&lt;/p&gt;\n\n&lt;p&gt;Check the &amp;quot;Latest baselines&amp;quot; section in a processor&amp;#39;s page at  &lt;a href=\"http://cpubenchmark.net\"&gt;cpubenchmark.net&lt;/a&gt;  with links to individual results where the &amp;quot;Memory Threaded&amp;quot; result is listed under &amp;quot;Memory Mark&amp;quot;:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;CPU&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory BW&lt;/th&gt;\n&lt;th align=\"left\"&gt;Reference&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notes&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9955WX&amp;amp;id=6803\"&gt;AMD Threadripper PRO 9955WX&lt;/a&gt; (16-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~115 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=509905130667\"&gt;BL5099051 - Jul 20 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2x CCD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9965WX&amp;amp;id=6804\"&gt;AMD Threadripper PRO 9965WX&lt;/a&gt; (24-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~272 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=279748548819\"&gt;BL2797485 - Jul 29 2025&lt;/a&gt; (other baselines start from 250GB/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;4x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9975WX&amp;amp;id=6799\"&gt;AMD Threadripper PRO 9975WX&lt;/a&gt; (32-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~272 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=279782022829\"&gt;BL2797820 - Jul 29 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9985WX&amp;amp;id=6807\"&gt;AMD Threadripper PRO 9985WX&lt;/a&gt; (64-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~367 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=509913021820\"&gt;BL5099130 - Jul 21 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Therefore:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the 16-core 9955WX has lower mem bw than even a DDR4 EPYC CPU (e.g. &lt;a href=\"https://www.passmark.com/baselines/V10/display.php?id=226455755507\"&gt;7R43 with 191 GB/s&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;the 24-core and 32-core parts have lower mem bw than DDR5 Genoa EPYCs (even some 16-core parts).&lt;/li&gt;\n&lt;li&gt;the 64-core and 96-core Threadrippers are not CCD-number limited, but still lose to the EPYCs since those have 12 channels (unless you use 7200 MT/s memory).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For comparison, check the excellent related threads by &lt;a href=\"/u/fairydreaming\"&gt;u/fairydreaming&lt;/a&gt; for the previous gen Threadrippers and EPYC Genoa/Turin:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/threadripper/comments/1azmkvg/comparing_threadripper_7000_memory_bandwidth_for/\"&gt;Comparing Threadripper 7000 memory bandwidth for all models : r/threadripper&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/\"&gt;Memory bandwidth values (STREAM TRIAD benchmark results) for most Epyc Genoa CPUs (single and dual configurations) : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/\"&gt;STREAM TRIAD memory bandwidth benchmark values for Epyc Turin - almost 1 TB/s for a dual CPU system : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If someone insists on buying a new TR Pro for their great compute throughput, I would suggest to at least skip the 16-core part.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?auto=webp&amp;s=66e0f40cc65b2257b6a82171108f852dae40bbb8",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a6db3fea34b4baa46c98dcb2bf7d4162a03ce299",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3423d6e5c6b7fd741788ee8997aaf7af37b444e6",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=938295c9480e314e0b1d0d94b264e9f050cdda43",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97458ad0120eb62027d0d8ef93dc4c678f9ce61e",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48d787caa6a26254352916bd9930bd786336a6fd",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fec7e3647f34d8e8ee777b2fcb9b2de2abd6e79a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcrx23",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "henfiber",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753834203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 on fiction.livebench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcp7dp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 69,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 69,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QcSFm7ZWAqzMHjpuqS2BSh-cF9X422RD7yB4xytZxyM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753827110,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/aey1fr0e0wff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/aey1fr0e0wff1.png?auto=webp&amp;s=03652a7cefbd3cf68aa3a4c2fa7f8ef9460eab7c",
                  "width": 1506,
                  "height": 2308
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=200a158a099cf05614121ad48d3dea5304d2191d",
                    "width": 108,
                    "height": 165
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c60de759152ef785372120f079d85c9e3ab9a29",
                    "width": 216,
                    "height": 331
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=df5280d81e9b288107f2acf30fc72f742a2d2975",
                    "width": 320,
                    "height": 490
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=164ac21dfb78027d97359b16dae6fc48436681ec",
                    "width": 640,
                    "height": 980
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d82a89795db7589130bfb4073fd6b0b7257b318",
                    "width": 960,
                    "height": 1471
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90bfb72094bf842d3f0624e686efa444e2bba799",
                    "width": 1080,
                    "height": 1655
                  }
                ],
                "variants": {},
                "id": "xaQccbr53kPCE6BsoktLHVTFuBeqr91ypjMWvOtHbFU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcp7dp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcp7dp/glm45_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/aey1fr0e0wff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753827110,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m building a fully local AI-Scribe for doctors and wanted to know which speech-to-text engines perform well with 5-10 min patient-doctor chats.  \nI ran 55 mock GP consultations (PriMock57) through 15 open- and closed-source models, logged word-error rate (WER) and speed, and only chunked audio when a model crashed on &gt;40 s clips.\n\n# All results\n\n|\\#|Model|Avg WER|Avg sec/file|Host|\n|:-|:-|:-|:-|:-|\n|1|ElevenLabs Scribe v1|**15.0 %**|36 s|API (ElevenLabs)|\n|2|MLX Whisper-L v3-turbo|17.6 %|13 s|Local (Apple M4)|\n|3|Parakeet-0.6 B v2|17.9 %|**5 s**|Local (Apple M4)|\n|4|Canary-Qwen 2.5 B|18.2 %|105 s|Local (L4 GPU)|\n|5|Apple SpeechAnalyzer|18.2 %|6 s|Local (macOS)|\n|6|Groq Whisper-L v3|18.4 %|9 s|API (Groq)|\n|7|Voxtral-mini 3 B|18.5 %|74 s|Local (L4 GPU)|\n|8|Groq Whisper-L v3-turbo|18.7 %|8 s|API (Groq)|\n|9|Canary-1B-Flash|18.8 %|23 s|Local (L4 GPU)|\n|10|Voxtral-mini (API)|19.0 %|23 s|API (Mistral)|\n|11|WhisperKit-L v3-turbo|19.1 %|21 s|Local (macOS)|\n|12|OpenAI Whisper-1|19.6 %|104 s|API (OpenAI)|\n|13|OpenAI GPT-4o-mini|20.6 %|—|API (OpenAI)|\n|14|OpenAI GPT-4o|21.7 %|28 s|API (OpenAI)|\n|15|Azure Foundry Phi-4|36.6 %|213 s|API (Azure)|\n\n# Take-aways\n\n* **ElevenLabs Scribe** leads accuracy but can hallucinate on edge cases.\n* **Parakeet-0.6 B on an M4** runs \\~5× real-time—great if English-only is fine.\n* **Groq Whisper-v3 (turbo)** offers the best cloud price/latency combo.\n* Canary/Canary-Qwen/Phi-4 needed chunking, which bumped runtime.\n* Apple SpeechAnalyzer is a good option for Swift apps.\n\nFor details on the dataset, hardware, and full methodology, see the blog post → [https://omi.health/blog/benchmarking-tts](https://omi.health/blog/benchmarking-tts)\n\nHappy to chat—let me know if you’d like the evaluation notebook once it’s cleaned up!",
          "author_fullname": "t2_6d62mn60w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmark: 15 STT models on long-form medical dialogue",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md1fka",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gjFJQ8b3RSb_IZw6-OUzpBxdAi0cMspjDghHYjUUugU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753865479,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m building a fully local AI-Scribe for doctors and wanted to know which speech-to-text engines perform well with 5-10 min patient-doctor chats.&lt;br/&gt;\nI ran 55 mock GP consultations (PriMock57) through 15 open- and closed-source models, logged word-error rate (WER) and speed, and only chunked audio when a model crashed on &amp;gt;40 s clips.&lt;/p&gt;\n\n&lt;h1&gt;All results&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;#&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Avg WER&lt;/th&gt;\n&lt;th align=\"left\"&gt;Avg sec/file&lt;/th&gt;\n&lt;th align=\"left\"&gt;Host&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;ElevenLabs Scribe v1&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;15.0 %&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;36 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (ElevenLabs)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;MLX Whisper-L v3-turbo&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.6 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;13 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (Apple M4)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Parakeet-0.6 B v2&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.9 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;5 s&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (Apple M4)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;Canary-Qwen 2.5 B&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.2 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;105 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (L4 GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apple SpeechAnalyzer&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.2 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;6 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (macOS)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;Groq Whisper-L v3&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.4 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;9 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (Groq)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Voxtral-mini 3 B&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.5 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;74 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (L4 GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;Groq Whisper-L v3-turbo&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.7 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;8 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (Groq)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;9&lt;/td&gt;\n&lt;td align=\"left\"&gt;Canary-1B-Flash&lt;/td&gt;\n&lt;td align=\"left\"&gt;18.8 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;23 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (L4 GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;Voxtral-mini (API)&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.0 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;23 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (Mistral)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;11&lt;/td&gt;\n&lt;td align=\"left\"&gt;WhisperKit-L v3-turbo&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.1 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;21 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;Local (macOS)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;12&lt;/td&gt;\n&lt;td align=\"left\"&gt;OpenAI Whisper-1&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.6 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;104 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (OpenAI)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;OpenAI GPT-4o-mini&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;—&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (OpenAI)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;OpenAI GPT-4o&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.7 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;28 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (OpenAI)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;15&lt;/td&gt;\n&lt;td align=\"left\"&gt;Azure Foundry Phi-4&lt;/td&gt;\n&lt;td align=\"left\"&gt;36.6 %&lt;/td&gt;\n&lt;td align=\"left\"&gt;213 s&lt;/td&gt;\n&lt;td align=\"left\"&gt;API (Azure)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Take-aways&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;ElevenLabs Scribe&lt;/strong&gt; leads accuracy but can hallucinate on edge cases.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Parakeet-0.6 B on an M4&lt;/strong&gt; runs ~5× real-time—great if English-only is fine.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Groq Whisper-v3 (turbo)&lt;/strong&gt; offers the best cloud price/latency combo.&lt;/li&gt;\n&lt;li&gt;Canary/Canary-Qwen/Phi-4 needed chunking, which bumped runtime.&lt;/li&gt;\n&lt;li&gt;Apple SpeechAnalyzer is a good option for Swift apps.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For details on the dataset, hardware, and full methodology, see the blog post → &lt;a href=\"https://omi.health/blog/benchmarking-tts\"&gt;https://omi.health/blog/benchmarking-tts&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to chat—let me know if you’d like the evaluation notebook once it’s cleaned up!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nxnp5xsw4zff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nxnp5xsw4zff1.png?auto=webp&amp;s=e05c92656322d61fe5bce59966dbb6db87a3dd06",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=10718d12b1ba0f9fb55b88c4a23f5d961dc09a23",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c559978c60e7c0d93343f6902fe18925c72c46b",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b9aca7fb5d5f5538ea1096cbd98355994da3c3d",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e11b8504e80c4f6a6e15f6408d1b821538221d77",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3cee9acfbdb5282b382e74e15ebf8988e803437",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nxnp5xsw4zff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72647e3b3d762b18a3575431b05e25b902927fe8",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "aSKiVMrPqKWIFhKxohn_N0i0HcSUU59xZvQ8NLVGNEQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1md1fka",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MajesticAd2862",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md1fka/benchmark_15_stt_models_on_longform_medical/",
          "stickied": false,
          "url": "https://i.redd.it/nxnp5xsw4zff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753865479,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ngleu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My 2.5 year old laptop can write Space Invaders in JavaScript now, using GLM-4.5 Air and MLX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcee42",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 173,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 173,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=a8e41202ffb75be375da41f6bfdbca3278dea746",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753802702,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "simonwillison.net",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://simonwillison.net/2025/Jul/29/space-invaders/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?auto=webp&amp;s=54f803941465224fd8246998bb39528e75d01d3b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=753886c5d0207835fbf0f072aecec4105a84e5d1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40261c69d770aed6a72ee6f270cb5258a7986d06",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=818441e6caa4231812fb4c09059db655bfc604c7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a5a9e25e0e831120dffb4dbb77fc7392c4ccb49",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a2d07966fcc841d9190c47df7f060e4049f6d3e7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=253492f4406476c783cec6d34932373aaa520a62",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcee42",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChiliPepperHott",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcee42/my_25_year_old_laptop_can_write_space_invaders_in/",
          "stickied": false,
          "url": "https://simonwillison.net/2025/Jul/29/space-invaders/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753802702,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "new qwen moe!",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcfuka",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 143,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 143,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50aa20219586bc9007fb96833d16a6a56c8c1c76",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753805955,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;new qwen moe!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?auto=webp&amp;s=f1df54937600c0db76989bd14eef9e747df1fb0e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1c3476d621a9393fbb7ca11c48a3074c5fd6803",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7cef70bde41dd3225eec3f7d265fbf2704c0182",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab3e2615c90a6581b60c6d33c660bfc0f250b4c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9c2fc1f960e47499df06dc08d78c88be43e15e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15eba021f7d99140c48583ae883d2eb091807f16",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcfuka",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcfuka/qwenqwen330ba3binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "subreddit_subscribers": 506972,
          "created_utc": 1753805955,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.\n\nThe results were pretty remarkable I must say! \n\nHere’s the link to the results: https://chat.z.ai/space/r05c76960ff0-ppt \n\nHere’s the initial prompt:\n\n”Create a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.”\n\nAs you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.\n\nIs it just me or are things going superfast since OpenAI announced the release of GPT-5?\n\nIt seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.\n\n \n\n",
          "author_fullname": "t2_1ttp8mwcgv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I just tried GLM 4.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc8tks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 334,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 334,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753788294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.&lt;/p&gt;\n\n&lt;p&gt;The results were pretty remarkable I must say! &lt;/p&gt;\n\n&lt;p&gt;Here’s the link to the results: &lt;a href=\"https://chat.z.ai/space/r05c76960ff0-ppt\"&gt;https://chat.z.ai/space/r05c76960ff0-ppt&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Here’s the initial prompt:&lt;/p&gt;\n\n&lt;p&gt;”Create a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.”&lt;/p&gt;\n\n&lt;p&gt;As you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.&lt;/p&gt;\n\n&lt;p&gt;Is it just me or are things going superfast since OpenAI announced the release of GPT-5?&lt;/p&gt;\n\n&lt;p&gt;It seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?auto=webp&amp;s=06f19448d458a949198ac72d6d7c73d5e6463785",
                  "width": 400,
                  "height": 400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=731547beb9c0ce796d8f8edd4b883c564da2c39b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a6eef195d7537bf441a643dbcaf760056822a2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abcfb3d145a4837cd123c1d5c55d56b5eaefd529",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mc8tks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AI-On-A-Dime",
          "discussion_type": null,
          "num_comments": 140,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753788294,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Code for STT -&gt; LLM -&gt; TTS, compatible with OpenAI realtime (websocket) API.",
          "author_fullname": "t2_etmr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Make text LLMs listen and speak",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcx681",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=eb85cf74ab4635c5efd4dca6fbacbe90e361dfce",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753849625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Code for STT -&amp;gt; LLM -&amp;gt; TTS, compatible with OpenAI realtime (websocket) API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/kyutai-labs/unmute",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY.png?auto=webp&amp;s=3148ce04210d692d472654aacd2e7809bc1bd36f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c7bc4c7fb990b1ace0ef18083855e7fc5668bdf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0006d12a11a8c00c565a3e39e14eb0d79cf368d8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=32eca243b920b1c955603377ebc56712b0c0158a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc13a31d72e70ffdf04ffb3805d297867ca72ae9",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e56a5cf87f43188d05b2e644553b85fd1047f3aa",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1bac52d99ed8c6302b3fcde2b4390fa5baf335f8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OveirE7D8xMmU4gSGq-owrK1P6dpWvwRX0pVNCweFIY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcx681",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phone_radio_tv",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcx681/make_text_llms_listen_and_speak/",
          "stickied": false,
          "url": "https://github.com/kyutai-labs/unmute",
          "subreddit_subscribers": 506972,
          "created_utc": 1753849625,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone tested for same, is it trained on gemini outputs ?",
          "user_reports": [],
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "zai-org/GLM-4.5 · We Have Gemini At Home",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mce9tt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "",
          "subreddit_type": "public",
          "ups": 119,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 119,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "thumbnail": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=b9c4a19e8f83b487aad0808ff3268748aeb22c0b",
          "edited": false,
          "author_flair_css_class": null,
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753802433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tested for same, is it trained on gemini outputs ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/zai-org/GLM-4.5/discussions/1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?auto=webp&amp;s=95e6e3db54b78256b6c4a4e615f3f8e17a4d8da0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8d403a9e16d065e5baf97dd10b29a9718f1fc4e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dc8994dbbb506cc7c4e6ca5bb072334ac0944a2",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f0fce0796d6b32266a6dd062f3fbaeb365e87a0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=403fb88f398e6c5fe36b4f1c95408e0675027e55",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d2daf8dee06ac50ac6077ee82bd888ae1b6f3855",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4766f576b9c23f9aefebc1aee1a39b8c0f31aabb",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mce9tt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "[deleted]",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mce9tt/zaiorgglm45_we_have_gemini_at_home/",
          "stickied": false,
          "url": "https://huggingface.co/zai-org/GLM-4.5/discussions/1",
          "subreddit_subscribers": 506972,
          "created_utc": 1753802433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "You can now run Llama 4 Scout in LM Studio on Windows. Pretty decent speed too \\~15 tk/s",
          "author_fullname": "t2_13crip",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Ryzen AI Max+ Upgraded: Run up to 128 Billion parameter LLMs on Windows with LM Studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcnq7r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=33c8eb8fd935cf2ea90a0a33e411d886f5ae1a9c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753823574,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "amd.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can now run Llama 4 Scout in LM Studio on Windows. Pretty decent speed too ~15 tk/s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.amd.com/en/blogs/2025/amd-ryzen-ai-max-upgraded-run-up-to-128-billion-parameter-llms-lm-studio.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?auto=webp&amp;s=ffab56effc31efa32c26741ccf472cd572474a71",
                  "width": 2887,
                  "height": 1620
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=101f81a143ae5429affeeaa9b0172147a565a3f2",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da82e6bdc51acddc3836e55c7a6fa5121fd48163",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=952681f3657af07194fc11d19c2e1aa52e971a16",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1407a6de51b1efe682d3aec309cbbdadb1b1d910",
                    "width": 640,
                    "height": 359
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fe54f141d273c10ecf2167c9a13bfac5e717bba9",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a0abe4e0514c46a50ab880382f1a472fe3518433",
                    "width": 1080,
                    "height": 606
                  }
                ],
                "variants": {},
                "id": "B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcnq7r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ZZZCodeLyokoZZZ",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcnq7r/amd_ryzen_ai_max_upgraded_run_up_to_128_billion/",
          "stickied": false,
          "url": "https://www.amd.com/en/blogs/2025/amd-ryzen-ai-max-upgraded-run-up-to-128-billion-parameter-llms-lm-studio.html",
          "subreddit_subscribers": 506972,
          "created_utc": 1753823574,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Interesting small model, hadn't seen it before.\n\n[https://huggingface.co/arcee-ai/AFM-4.5B-GGUF](https://huggingface.co/arcee-ai/AFM-4.5B-GGUF)",
          "author_fullname": "t2_nqso9ejpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AFM 4.5B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mchj7h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Gg3bSE-QKrcK_PJGFfGMYVWRRjLvwPBdoVOh_DIFWxI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753809654,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interesting small model, hadn&amp;#39;t seen it before.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B-GGUF\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/c7yvmvdgkuff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/c7yvmvdgkuff1.png?auto=webp&amp;s=e11f8816200af5c879bcb6774235ced1dfe26ebd",
                  "width": 1024,
                  "height": 768
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa5c457775d6739f275247e49c5c1d3b2126224e",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b9fc56c924cc71910d8dd4b9859ca93a47c55c8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=915a2c0ed88c41913030f35d8dc1c5114795ae72",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a68967cc776ececd5151071c32eb068a2fd1ddad",
                    "width": 640,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=123bf071ad15e80af83deb3393e7921b247e2961",
                    "width": 960,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "v6HauKnEB-_4Fc_3AuB1hm4zMhsHVdeYRhsHvTww7e0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mchj7h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "best_codes",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mchj7h/afm_45b/",
          "stickied": false,
          "url": "https://i.redd.it/c7yvmvdgkuff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753809654,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nHow about running a local agent on a smartphone? Here's how I did it.  \n  \nI stitched together onnxruntime implemented KV Cache in DelitePy(Python) and added FP16 activations support in cpp with (via `uint16_t`), works for all binary ops in DeliteAI. Result Local Qwen 3 1.7B on mobile! \n\n# Tool Calling Features\n\n* **Multi-step conversation support** with automatic tool execution\n* **JSON-based tool calling** with `&lt;tool_call&gt;` XML tags\n* **test tools**: weather, math calculator, time, location\n\n# Used [tokenizer-cpp](https://github.com/mlc-ai/tokenizers-cpp) from MLC \n\nwhich binds rust [huggingface/tokenizers](https://github.com/huggingface/tokenizers) giving full support for android/iOS.\n\n    // - dist/tokenizer.json\n    void HuggingFaceTokenizerExample() {\n      auto blob = LoadBytesFromFile(\"dist/tokenizer.json\");  \n      auto tok = Tokenizer::FromBlobJSON(blob);\n      std::string prompt = \"What is the capital of Canada?\";\n      std::vector&lt;int&gt; ids = tok-&gt;Encode(prompt);\n      std::string decoded_prompt = tok-&gt;Decode(ids);\n    }\n\n# Push LLM streams into Kotlin Flows\n\n        suspend fun feedInput(input: String, isVoiceInitiated: Boolean, callback: (String?)-&gt;Unit) : String? {\n            val res = NimbleNet.runMethod(\n                \"prompt_for_tool_calling\",\n                inputs = hashMapOf(\n                    \"prompt\" to NimbleNetTensor(input, DATATYPE.STRING, null),\n                    \"output_stream_callback\" to  createNimbleNetTensorFromForeignFunction(callback)\n                ),\n            )\n            assert(res.status) { \"NimbleNet.runMethod('prompt_for_tool_calling') failed with status: ${res.status}\" }\n            return res.payload?.get(\"results\")?.data as String?\n        }\n\n  \nCheck the code soon merging in Delite AI (https://github.com/NimbleEdge/deliteAI/pull/165)  \nOr try in the assistant app (https://github.com/NimbleEdge/assistant)",
          "author_fullname": "t2_74zl16jw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 1.7B tool calling across Android on Pixel 9 and S22",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 74,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcl15k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/3wcxuotf7vff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1028,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/3wcxuotf7vff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/3wcxuotf7vff1/DASHPlaylist.mpd?a=1756470630%2COWFiYzdmMzFjNzY5NDQ2YTY4N2Y1YmE2NTAxY2RjNTAxYmEyZTU3NDM2YTVhM2Y3MTQ5YmE5MDJmZDZjZTQ5NQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 96,
              "hls_url": "https://v.redd.it/3wcxuotf7vff1/HLSPlaylist.m3u8?a=1756470630%2CMWVjMzYyMGUyMDk3YjMyMWU4ZTc5OTI3MDBhODljM2Q0NzZjZmEzYjc4ZjgyNWY2N2RjMmUzNjU0Nzg2MGNjNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=140&amp;height=74&amp;crop=140:74,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0cc796d906797d3e01c0e1b59e9c7394c449065a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753817426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How about running a local agent on a smartphone? Here&amp;#39;s how I did it.  &lt;/p&gt;\n\n&lt;p&gt;I stitched together onnxruntime implemented KV Cache in DelitePy(Python) and added FP16 activations support in cpp with (via &lt;code&gt;uint16_t&lt;/code&gt;), works for all binary ops in DeliteAI. Result Local Qwen 3 1.7B on mobile! &lt;/p&gt;\n\n&lt;h1&gt;Tool Calling Features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Multi-step conversation support&lt;/strong&gt; with automatic tool execution&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;JSON-based tool calling&lt;/strong&gt; with &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; XML tags&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;test tools&lt;/strong&gt;: weather, math calculator, time, location&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Used &lt;a href=\"https://github.com/mlc-ai/tokenizers-cpp\"&gt;tokenizer-cpp&lt;/a&gt; from MLC&lt;/h1&gt;\n\n&lt;p&gt;which binds rust &lt;a href=\"https://github.com/huggingface/tokenizers\"&gt;huggingface/tokenizers&lt;/a&gt; giving full support for android/iOS.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;// - dist/tokenizer.json\nvoid HuggingFaceTokenizerExample() {\n  auto blob = LoadBytesFromFile(&amp;quot;dist/tokenizer.json&amp;quot;);  \n  auto tok = Tokenizer::FromBlobJSON(blob);\n  std::string prompt = &amp;quot;What is the capital of Canada?&amp;quot;;\n  std::vector&amp;lt;int&amp;gt; ids = tok-&amp;gt;Encode(prompt);\n  std::string decoded_prompt = tok-&amp;gt;Decode(ids);\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Push LLM streams into Kotlin Flows&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;    suspend fun feedInput(input: String, isVoiceInitiated: Boolean, callback: (String?)-&amp;gt;Unit) : String? {\n        val res = NimbleNet.runMethod(\n            &amp;quot;prompt_for_tool_calling&amp;quot;,\n            inputs = hashMapOf(\n                &amp;quot;prompt&amp;quot; to NimbleNetTensor(input, DATATYPE.STRING, null),\n                &amp;quot;output_stream_callback&amp;quot; to  createNimbleNetTensorFromForeignFunction(callback)\n            ),\n        )\n        assert(res.status) { &amp;quot;NimbleNet.runMethod(&amp;#39;prompt_for_tool_calling&amp;#39;) failed with status: ${res.status}&amp;quot; }\n        return res.payload?.get(&amp;quot;results&amp;quot;)?.data as String?\n    }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Check the code soon merging in Delite AI (&lt;a href=\"https://github.com/NimbleEdge/deliteAI/pull/165\"&gt;https://github.com/NimbleEdge/deliteAI/pull/165&lt;/a&gt;)&lt;br/&gt;\nOr try in the assistant app (&lt;a href=\"https://github.com/NimbleEdge/assistant\"&gt;https://github.com/NimbleEdge/assistant&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/3wcxuotf7vff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?format=pjpg&amp;auto=webp&amp;s=165be791b8080463b2ab6d14b9b9776e0a4a8e2c",
                  "width": 2506,
                  "height": 1342
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6fe43a7642d9c73f60904aa7697e5f67367fe209",
                    "width": 108,
                    "height": 57
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=71dece6bba55a954e2ccff12a4d99077d7266799",
                    "width": 216,
                    "height": 115
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3debc156667479551d71bb1d9aeeef63ebbe0152",
                    "width": 320,
                    "height": 171
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=80d701462462fdc5caf3f289d125f5645db09a52",
                    "width": 640,
                    "height": 342
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=28bf0f71381faf47ee92aa7f472fd7dcb66cdfb7",
                    "width": 960,
                    "height": 514
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2f7355860cb6fa6f63f5a696b008dbd0e23f562f",
                    "width": 1080,
                    "height": 578
                  }
                ],
                "variants": {},
                "id": "OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcl15k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Economy-Mud-6626",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcl15k/qwen_17b_tool_calling_across_android_on_pixel_9/",
          "stickied": false,
          "url": "https://v.redd.it/3wcxuotf7vff1",
          "subreddit_subscribers": 506972,
          "created_utc": 1753817426,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/3wcxuotf7vff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1028,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/3wcxuotf7vff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/3wcxuotf7vff1/DASHPlaylist.mpd?a=1756470630%2COWFiYzdmMzFjNzY5NDQ2YTY4N2Y1YmE2NTAxY2RjNTAxYmEyZTU3NDM2YTVhM2Y3MTQ5YmE5MDJmZDZjZTQ5NQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 96,
              "hls_url": "https://v.redd.it/3wcxuotf7vff1/HLSPlaylist.m3u8?a=1756470630%2CMWVjMzYyMGUyMDk3YjMyMWU4ZTc5OTI3MDBhODljM2Q0NzZjZmEzYjc4ZjgyNWY2N2RjMmUzNjU0Nzg2MGNjNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been pretty happy with Gemma 3n, its coherence is good enough for its size. But I get the impression maybe its the lower bound.  \nI’m wondering as of now (Aug.2025), what smaller models have you found to perform well?   \nI've been suggested qwen 1.7B.\n\n",
          "author_fullname": "t2_1dcskd72oi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question on tiny models (&lt;5B parameter size)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcy7y2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753853185,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been pretty happy with Gemma 3n, its coherence is good enough for its size. But I get the impression maybe its the lower bound.&lt;br/&gt;\nI’m wondering as of now (Aug.2025), what smaller models have you found to perform well?&lt;br/&gt;\nI&amp;#39;ve been suggested qwen 1.7B.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcy7y2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Sheepherder507",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcy7y2/question_on_tiny_models_5b_parameter_size/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcy7y2/question_on_tiny_models_5b_parameter_size/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753853185,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all, is anyone else having issues with GLM 4.5 Air not properly formatting its tool calls in LM Studio? This is an example from my most recent chat:\n\n&lt;tool\\_call&gt;browser\\_navigate  \n&lt;arg\\_key&gt;url&lt;/arg\\_key&gt;  \n&lt;arg\\_value&gt;https://www.example.com&lt;/arg\\_value&gt;  \n&lt;/tool\\_call&gt;\n\nIt seems to be formatting it in XML, where I believe LM Studio uses Json. Does anyone have an idea on how to fix this, or should I just wait until an official patch/update to the system prompt comes out?\n\nEDIT: My computer and environment specs are as follows:\n\nMacOS Sequoia 15.5\n\nMacbook M2 Max - 96GB unified ram\n\nLM Studio version: 0.3.20\n\nRuntime: LM Studio MLX v0.21.0\n\nModel: mlx-community/glm-4.5-air@5bit",
          "author_fullname": "t2_vh15w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Air Tool Calling Issues In LM Studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcw1sl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753851898,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753846063,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, is anyone else having issues with GLM 4.5 Air not properly formatting its tool calls in LM Studio? This is an example from my most recent chat:&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;tool\\_call&amp;gt;browser_navigate&lt;br/&gt;\n&amp;lt;arg\\_key&amp;gt;url&amp;lt;/arg\\_key&amp;gt;&lt;br/&gt;\n&amp;lt;arg\\_value&amp;gt;&lt;a href=\"https://www.example.com\"&gt;https://www.example.com&lt;/a&gt;&amp;lt;/arg\\_value&amp;gt;&lt;br/&gt;\n&amp;lt;/tool\\_call&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;It seems to be formatting it in XML, where I believe LM Studio uses Json. Does anyone have an idea on how to fix this, or should I just wait until an official patch/update to the system prompt comes out?&lt;/p&gt;\n\n&lt;p&gt;EDIT: My computer and environment specs are as follows:&lt;/p&gt;\n\n&lt;p&gt;MacOS Sequoia 15.5&lt;/p&gt;\n\n&lt;p&gt;Macbook M2 Max - 96GB unified ram&lt;/p&gt;\n\n&lt;p&gt;LM Studio version: 0.3.20&lt;/p&gt;\n\n&lt;p&gt;Runtime: LM Studio MLX v0.21.0&lt;/p&gt;\n\n&lt;p&gt;Model: mlx-community/glm-4.5-air@5bit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcw1sl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sharpastic",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcw1sl/glm_45_air_tool_calling_issues_in_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcw1sl/glm_45_air_tool_calling_issues_in_lm_studio/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753846063,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI did not hit a plateau, at least in benchmarks. Pretty impressive with one year’s hindsight. Of course benchmarks aren’t everything. They aren’t nothing either.",
          "author_fullname": "t2_syq52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "One year’s benchmark progress: comparing Sonnet 3.5 with open weight 2025 non-thinking models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcjz8j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=140&amp;height=77&amp;crop=140:77,smart&amp;auto=webp&amp;s=938c7c54c5afab9fd0496cba4e5d012b557db44d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753815058,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "artificialanalysis.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI did not hit a plateau, at least in benchmarks. Pretty impressive with one year’s hindsight. Of course benchmarks aren’t everything. They aren’t nothing either.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://artificialanalysis.ai/?models=llama-3-3-instruct-70b%2Cllama-4-maverick%2Cllama-4-scout%2Cgemma-3-27b%2Cdeepseek-v3-0324%2Ckimi-k2%2Cqwen3-235b-a22b-instruct-2507%2Cclaude-35-sonnet-june-24",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcjz8j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nomorebuttsplz",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcjz8j/one_years_benchmark_progress_comparing_sonnet_35/",
          "stickied": false,
          "url": "https://artificialanalysis.ai/?models=llama-3-3-instruct-70b%2Cllama-4-maverick%2Cllama-4-scout%2Cgemma-3-27b%2Cdeepseek-v3-0324%2Ckimi-k2%2Cqwen3-235b-a22b-instruct-2507%2Cclaude-35-sonnet-june-24",
          "subreddit_subscribers": 506972,
          "created_utc": 1753815058,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How's the voice cloning and TTS quality of Sesame compared to Chatterbox?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How is the quality of Sesame CSM TTS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1md4atg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753875667,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How&amp;#39;s the voice cloning and TTS quality of Sesame compared to Chatterbox?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1md4atg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md4atg/how_is_the_quality_of_sesame_csm_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md4atg/how_is_the_quality_of_sesame_csm_tts/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753875667,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have found an interesting setup that tries to dip into my budget.\n\n- Epyc 9115 (or more expensive brother 9135) (~940USD)\n- ASUS K14PA-U12/ASMB11 SP5 (~750USD)\n- 2x 64GB Hynix ECC REGISTERED DDR5 2Rx4 6400MHz PC5-51200 RDIMM (~1080USD)\n\nFor around 2800 USD it starts to look possible, still a little on the expensive side to spend on a hobby, at least for how much will it improve my \"fun\" over a simple 3090. But nonetheless, how does it look? I mean how realistically would this perform? Are there some (happy?) users with similar setups around here?",
          "author_fullname": "t2_qafso",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CPU server specs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md1md1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753866214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have found an interesting setup that tries to dip into my budget.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Epyc 9115 (or more expensive brother 9135) (~940USD)&lt;/li&gt;\n&lt;li&gt;ASUS K14PA-U12/ASMB11 SP5 (~750USD)&lt;/li&gt;\n&lt;li&gt;2x 64GB Hynix ECC REGISTERED DDR5 2Rx4 6400MHz PC5-51200 RDIMM (~1080USD)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For around 2800 USD it starts to look possible, still a little on the expensive side to spend on a hobby, at least for how much will it improve my &amp;quot;fun&amp;quot; over a simple 3090. But nonetheless, how does it look? I mean how realistically would this perform? Are there some (happy?) users with similar setups around here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1md1md1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kaisurniwurer",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md1md1/cpu_server_specs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md1md1/cpu_server_specs/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753866214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_5b972ieo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 support is landing in llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc6fbp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 216,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 216,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=74411b3e344f617397de99b0ed0a03e269d8efec",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753779557,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?auto=webp&amp;s=30023402a6989ab78753084b6f9a7fbdd3e44d81",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7649d767f799c1e6b81af747ef3aed21648a9037",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cd6878bf5f95e786470b4fabe22d873e097a9e8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b80141042845f306cbaf8a52844f7a00355e7a7b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bbb4d01a722a7ac5908e1ba272a92870c5277cd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=75e08e8a0d15b1faa2896de0841e0bdc245896ba",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9141773c734ecbddc393603456528be8251b8de5",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mc6fbp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pristine-Woodpecker",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "subreddit_subscribers": 506972,
          "created_utc": 1753779557,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Doing a cumulative project.\n\nI’ve been looking for local models to pack into an electron app - how should I go about doing this?\n\nI’ve looked into Wav2Lip, the light version, etc, but the docs are few ngl.\n\nAnything that won’t FRY my 2021 m1? I just need something quality, light, and fast. I’m also not connecting an external GPU.\n\nWould appreciate any thoughts.",
          "author_fullname": "t2_ak87zs2r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local Lipsync Model For Electron",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md0ech",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753861348,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doing a cumulative project.&lt;/p&gt;\n\n&lt;p&gt;I’ve been looking for local models to pack into an electron app - how should I go about doing this?&lt;/p&gt;\n\n&lt;p&gt;I’ve looked into Wav2Lip, the light version, etc, but the docs are few ngl.&lt;/p&gt;\n\n&lt;p&gt;Anything that won’t FRY my 2021 m1? I just need something quality, light, and fast. I’m also not connecting an external GPU.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1md0ech",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ambivaIent",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md0ech/local_lipsync_model_for_electron/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md0ech/local_lipsync_model_for_electron/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753861348,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "its getting comical",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 136,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbvf2z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 1036,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1036,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/aArydVtwEJ7yR_8IVkCHCK5ydQGsUUwRNjJX3SBpIk4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753744170,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/txsukljc5pff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/txsukljc5pff1.png?auto=webp&amp;s=07d6d7cad1797c689e38509b4184dc26106493ee",
                  "width": 373,
                  "height": 365
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=66753ef377dde5550d636917de9e12b2834fb31c",
                    "width": 108,
                    "height": 105
                  },
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3a44fe047ec31803031afef6a49f18f7985d89d",
                    "width": 216,
                    "height": 211
                  },
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=400b5b6efa830b5698a57bf456c6a99acd74b24d",
                    "width": 320,
                    "height": 313
                  }
                ],
                "variants": {},
                "id": "xShm2r7nwbpzJdxhn7AN663aC50Z0tC9c3BxqruE-VA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mbvf2z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 98,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/",
          "stickied": false,
          "url": "https://i.redd.it/txsukljc5pff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753744170,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Edited for clarity: \n\nI’ve just built a new inference server and want to make sure my setup and questions are perfectly clear:\n\nHardware\n- GPUs: 2× NVIDIA RTX 6000 Pro Max‑Q (192 GB total VRAM)\n- CPU &amp; RAM: AMD EPYC 9255‑based motherboard with 768 GB DDR5‑6000 installed\n\nUse Case\n- inference very large models (ideally larger 70 B+ parameters like glm 4.5)\n- Extremely long context windows (100 K+ tokens)\n- 8‑bit weight quantization (GPTQ) or as high as feasible\n\nInference Engines Under Consideration\n- vLLM (sharded/paged memory support)\n- k‑Transformers (merged‑weights trick)\n\n-llama.cpp (no paged memory support)\n\nMy Questions\n\n1.\tOther Engines?\n\nBeyond vLLM, k‑Transformers, and llama.cpp, what inference engines excel at long‑context workloads with paged or sharded memory?\n\n2.\tMemory Sufficiency\nWith 192 GB VRAM and 768 GB system RAM, can I natively load and serve a larger  B‑parameter models quantized to 8‑bit weights (GPTQ) without weight‑merging tricks?\n\n3.\tVRAM Estimation\nHow can I calculate or benchmark peak GPU memory usage for “sparsely activated” models (e.g. GLM‑4.5 or r1 variants) where only ~37 B parameters are active per forward pass?\n\nThanks in advance for any pointers!",
          "author_fullname": "t2_5l4zmzcw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Inference Server for Large Vram",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mczdxa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753861570,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753857429,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edited for clarity: &lt;/p&gt;\n\n&lt;p&gt;I’ve just built a new inference server and want to make sure my setup and questions are perfectly clear:&lt;/p&gt;\n\n&lt;p&gt;Hardware\n- GPUs: 2× NVIDIA RTX 6000 Pro Max‑Q (192 GB total VRAM)\n- CPU &amp;amp; RAM: AMD EPYC 9255‑based motherboard with 768 GB DDR5‑6000 installed&lt;/p&gt;\n\n&lt;p&gt;Use Case\n- inference very large models (ideally larger 70 B+ parameters like glm 4.5)\n- Extremely long context windows (100 K+ tokens)\n- 8‑bit weight quantization (GPTQ) or as high as feasible&lt;/p&gt;\n\n&lt;p&gt;Inference Engines Under Consideration\n- vLLM (sharded/paged memory support)\n- k‑Transformers (merged‑weights trick)&lt;/p&gt;\n\n&lt;p&gt;-llama.cpp (no paged memory support)&lt;/p&gt;\n\n&lt;p&gt;My Questions&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; Other Engines?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Beyond vLLM, k‑Transformers, and llama.cpp, what inference engines excel at long‑context workloads with paged or sharded memory?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Memory Sufficiency\nWith 192 GB VRAM and 768 GB system RAM, can I natively load and serve a larger  B‑parameter models quantized to 8‑bit weights (GPTQ) without weight‑merging tricks?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;VRAM Estimation\nHow can I calculate or benchmark peak GPU memory usage for “sparsely activated” models (e.g. GLM‑4.5 or r1 variants) where only ~37 B parameters are active per forward pass?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for any pointers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mczdxa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Infamous_Jaguar_2151",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753857429,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to find a TTS model that's open source &amp; able to be run locally, that can generate text somewhat quickly too - a few seconds or less would be ideal. \n\nMy goal for this is to have a conversation, so I don't want to wait 30 seconds or so for a response. \n\nI've tried Bark and Coqui XTTS, and they're alright, and I love that they can laugh, gasp, etc, but that makes the voice change too much. For example, it may talk in a woman's voice, laugh, then switch to a male's voice. It also takes about 5-10 seconds to generate text too, which is a little slower than I'd like. Sometimes it doesn't follow the text too, and can go off script. \n\nI'd like something close to Character.AI, if possible. \n\nThe reason I don't just use C.ai or Eleven Labs is because it's for a home made robot that I'm trying to give a voice to. So a fast response is ideal, especially not monotone. And bonus for laughing, gasps (like surprised), crying, etc. for other human emotions. \n\nI'll either be using my 4090 laptop for it or my 3090 desktop, for speed concerns. ",
          "author_fullname": "t2_4guqxmy4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the best TTS model to run locally? That's relatively quick and close to C.ai capabilities",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mczdbb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753857365,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to find a TTS model that&amp;#39;s open source &amp;amp; able to be run locally, that can generate text somewhat quickly too - a few seconds or less would be ideal. &lt;/p&gt;\n\n&lt;p&gt;My goal for this is to have a conversation, so I don&amp;#39;t want to wait 30 seconds or so for a response. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried Bark and Coqui XTTS, and they&amp;#39;re alright, and I love that they can laugh, gasp, etc, but that makes the voice change too much. For example, it may talk in a woman&amp;#39;s voice, laugh, then switch to a male&amp;#39;s voice. It also takes about 5-10 seconds to generate text too, which is a little slower than I&amp;#39;d like. Sometimes it doesn&amp;#39;t follow the text too, and can go off script. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like something close to Character.AI, if possible. &lt;/p&gt;\n\n&lt;p&gt;The reason I don&amp;#39;t just use C.ai or Eleven Labs is because it&amp;#39;s for a home made robot that I&amp;#39;m trying to give a voice to. So a fast response is ideal, especially not monotone. And bonus for laughing, gasps (like surprised), crying, etc. for other human emotions. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll either be using my 4090 laptop for it or my 3090 desktop, for speed concerns. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mczdbb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iKontact",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mczdbb/whats_the_best_tts_model_to_run_locally_thats/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mczdbb/whats_the_best_tts_model_to_run_locally_thats/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753857365,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We’re excited to share that 🥇NVIDIA Llama Nemotron Super 49B v1.5 -- our just released open reasoning model -- is #1 on the [Artificial Analysis Intelligence Index](https://nvda.ws/44TJw4n) \\- a leaderboard that spans advanced math, science, and agentic tasks, in the 70B open model category. \n\nSuper 49B v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. It delivers state-of-the-art accuracy and throughput, running on a single H100.\n\nKey features:\n\n🎯  Leading accuracy on multi-step reasoning, math, coding, and function-calling\n\n🏗️  Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples\n\n📊  Fully transparent training data and techniques\n\nIf you're building AI agents and want a high accuracy, fully-open, and transparent reasoning model that you can deploy anywhere, try Super v1.5 on [build.nvidia.com](http://build.nvidia.com) or download from [Hugging Face](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5) 🤗\n\nLeaderboard ➡️ [https://nvda.ws/44TJw4n](https://nvda.ws/44TJw4n)\n\nhttps://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;format=png&amp;auto=webp&amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816",
          "author_fullname": "t2_1vf7k06t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA Llama Nemotron Super v1.5 is #1 on Artificial Analysis Intelligence Index for the 70B Open Model Category.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "xd2gq1bs0vff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 44,
                  "x": 108,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e22ff11349f4e4b3dcb94e87d6d145cc729a836"
                },
                {
                  "y": 88,
                  "x": 216,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d728a0c03ce1b4539441a0f2c8b0aa52f86c8f3"
                },
                {
                  "y": 130,
                  "x": 320,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44d71750175ab38bd0f2c3afa10f26a492eb6d87"
                },
                {
                  "y": 261,
                  "x": 640,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3e72dc377756047505052364aacf55246c67629"
                },
                {
                  "y": 392,
                  "x": 960,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ffd6661e3507cc529cafe40a0e8f9e778dc8d9e"
                },
                {
                  "y": 441,
                  "x": 1080,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75f59b3bbcb7753f7223837403ce679a37343c80"
                }
              ],
              "s": {
                "y": 455,
                "x": 1114,
                "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;format=png&amp;auto=webp&amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816"
              },
              "id": "xd2gq1bs0vff1"
            }
          },
          "name": "t3_1mck6o7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=140&amp;height=77&amp;crop=140:77,smart&amp;auto=webp&amp;s=938c7c54c5afab9fd0496cba4e5d012b557db44d",
          "edited": 1753817962,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753815524,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We’re excited to share that 🥇NVIDIA Llama Nemotron Super 49B v1.5 -- our just released open reasoning model -- is #1 on the &lt;a href=\"https://nvda.ws/44TJw4n\"&gt;Artificial Analysis Intelligence Index&lt;/a&gt; - a leaderboard that spans advanced math, science, and agentic tasks, in the 70B open model category. &lt;/p&gt;\n\n&lt;p&gt;Super 49B v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. It delivers state-of-the-art accuracy and throughput, running on a single H100.&lt;/p&gt;\n\n&lt;p&gt;Key features:&lt;/p&gt;\n\n&lt;p&gt;🎯  Leading accuracy on multi-step reasoning, math, coding, and function-calling&lt;/p&gt;\n\n&lt;p&gt;🏗️  Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples&lt;/p&gt;\n\n&lt;p&gt;📊  Fully transparent training data and techniques&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re building AI agents and want a high accuracy, fully-open, and transparent reasoning model that you can deploy anywhere, try Super v1.5 on &lt;a href=\"http://build.nvidia.com\"&gt;build.nvidia.com&lt;/a&gt; or download from &lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;Hugging Face&lt;/a&gt; 🤗&lt;/p&gt;\n\n&lt;p&gt;Leaderboard ➡️ &lt;a href=\"https://nvda.ws/44TJw4n\"&gt;https://nvda.ws/44TJw4n&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816\"&gt;https://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mck6o7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PDXcoder2000",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mck6o7/nvidia_llama_nemotron_super_v15_is_1_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mck6o7/nvidia_llama_nemotron_super_v15_is_1_on/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753815524,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been hunting for an ideal model to use with vLLM for bulk image analysis (I'm avoiding llama.cpp as it's too slow). It's been a pain to find one that fits in my RTX 7070 mobile (\\~7.5GB of VRAM available) even at 4 bit quantization.\n\nI've tried Qwen2.5VL-7B (gptq, awq, bitsandbytes all 4 bit quants) and none of them fit.  \nI've tried to find Phi-4 MM quants but the only one i could find was BNB (https://huggingface.co/bubblspace/Bubbl-P4-multimodal-instruct) and it threw some error I was unable to resolve:\n\n&gt;  \nERROR 07-29 17:10:13 \\[engine.py:456\\] Attempted to load weight (torch.Size(\\[1572864, 1\\])) into parameter (torch.Size(\\[3072, 1024\\])) ERROR 07-29 17:10:13 \\[engine.py:456\\] Traceback (most recent call last):\n\nI've tried InternVL2.5-4b (AWQ 4 bit quant) but for some reason after a few prompts it begins to spew gibberish.\n\nAnyone have any suggestions and/or links to some decent VL models to try?",
          "author_fullname": "t2_1bf07v278u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "On the hunt for the best VLM 6B or smaller",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1md4g25",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753876121,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been hunting for an ideal model to use with vLLM for bulk image analysis (I&amp;#39;m avoiding llama.cpp as it&amp;#39;s too slow). It&amp;#39;s been a pain to find one that fits in my RTX 7070 mobile (~7.5GB of VRAM available) even at 4 bit quantization.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried Qwen2.5VL-7B (gptq, awq, bitsandbytes all 4 bit quants) and none of them fit.&lt;br/&gt;\nI&amp;#39;ve tried to find Phi-4 MM quants but the only one i could find was BNB (&lt;a href=\"https://huggingface.co/bubblspace/Bubbl-P4-multimodal-instruct\"&gt;https://huggingface.co/bubblspace/Bubbl-P4-multimodal-instruct&lt;/a&gt;) and it threw some error I was unable to resolve:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;ERROR 07-29 17:10:13 [engine.py:456] Attempted to load weight (torch.Size([1572864, 1])) into parameter (torch.Size([3072, 1024])) ERROR 07-29 17:10:13 [engine.py:456] Traceback (most recent call last):&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I&amp;#39;ve tried InternVL2.5-4b (AWQ 4 bit quant) but for some reason after a few prompts it begins to spew gibberish.&lt;/p&gt;\n\n&lt;p&gt;Anyone have any suggestions and/or links to some decent VL models to try?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Ky8OQ71I04rnVBWNCyLmaCYgdcmaHiYOD-WEnYfqEaU.png?auto=webp&amp;s=c7a05cf2d728d20d688bde5643064c742dfe9cec",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Ky8OQ71I04rnVBWNCyLmaCYgdcmaHiYOD-WEnYfqEaU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8057198ae860eeac6772e0fdc7fdb5086478c5f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Ky8OQ71I04rnVBWNCyLmaCYgdcmaHiYOD-WEnYfqEaU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=544f528bde366edd1248ad17563956be23974bca",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Ky8OQ71I04rnVBWNCyLmaCYgdcmaHiYOD-WEnYfqEaU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=35381bbe81f41b65daaad637e85233b3cb1811de",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Ky8OQ71I04rnVBWNCyLmaCYgdcmaHiYOD-WEnYfqEaU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf9883090dc985c2c25beb9727e166e21bb19be1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Ky8OQ71I04rnVBWNCyLmaCYgdcmaHiYOD-WEnYfqEaU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=96296e3a7b6dbf89c0cc2fb067fb24ac930ea694",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Ky8OQ71I04rnVBWNCyLmaCYgdcmaHiYOD-WEnYfqEaU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eed829173d8af32ff933f65a56d37f6331c6f1d2",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Ky8OQ71I04rnVBWNCyLmaCYgdcmaHiYOD-WEnYfqEaU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1md4g25",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SimilarWarthog8393",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md4g25/on_the_hunt_for_the_best_vlm_6b_or_smaller/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md4g25/on_the_hunt_for_the_best_vlm_6b_or_smaller/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753876121,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi   Newbie here. I downloaded the DeepSeek coder locally. What I got was a chat area, which gives you suggestions but does not create code. Is this the normal behavior? I was expecting it to provide the code for python and html for a requirement I wrote. Is this the issue with my installation? \n\ncan it be integrated with vscode?\n\nCan I upload my files so that it can check my code? Or can I point it to my vscode files to suggest code changes?\n\nOr is there any other local models that can help with it?\n\nMany thanks\n",
          "author_fullname": "t2_1ctac3e7gu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with deepseek",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1md463z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753875246,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi   Newbie here. I downloaded the DeepSeek coder locally. What I got was a chat area, which gives you suggestions but does not create code. Is this the normal behavior? I was expecting it to provide the code for python and html for a requirement I wrote. Is this the issue with my installation? &lt;/p&gt;\n\n&lt;p&gt;can it be integrated with vscode?&lt;/p&gt;\n\n&lt;p&gt;Can I upload my files so that it can check my code? Or can I point it to my vscode files to suggest code changes?&lt;/p&gt;\n\n&lt;p&gt;Or is there any other local models that can help with it?&lt;/p&gt;\n\n&lt;p&gt;Many thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1md463z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zealousideal-Map5889",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md463z/help_with_deepseek/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md463z/help_with_deepseek/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753875246,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "👋 After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (\\~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford's Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! 😅\n\n  \n**What I did:**\n\n* Created a Claude Code-inspired agent (system msg + tools)\n* Built Docker-isolated GRPO training where each rollout gets its own container\n* Developed a multi-agent synthetic data pipeline to generate &amp; validate training data with Opus-4\n* Implemented a hybrid reward signal of unit test verifiers &amp; a behavioural LLM judge.\n\n  \n**Key results:**\n\n* My untrained Qwen3-32B agent achieved **13.75%** on Terminal-Bench (#19, beats Stanford's Qwen3-235B MoE)\n* I tested training to work stably on 32x H100s distributed across 4 bare metal nodes\n* I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.\n* \\~£30-50k needed for full training run of 1000 epochs (I could only afford testing 😅)\n\n\n\n**Technical details:**\n\n* The synthetic dataset ranges from easy to extremely hard tasks. An example hard task's prompt:\n   * \"I found this mystery program at \\`/app/program\\` and I'm completely stumped. It's a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can't figure out what kind of input it needs. Could you help me figure out what this program requires?\"\n* Simple config presets allow training to run on multiple hardware setups with minimal effort.\n* GRPO used with 16 rollouts per task, up to 32k tokens per rollout.\n* Agent uses XML/YAML format to structure tool calls\n\n  \n**More details:**\n\nMy Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:\n\n* ⭐️ [Terminal Agent RL repo](https://github.com/Danau5tin/terminal-bench-rl)\n* [⭐️ Multi-agent synthetic data pipeline repo](https://github.com/Danau5tin/tbench-agentic-data-pipeline)\n\n  \nI thought I would share this because I believe long-horizon RL is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.\n\n  \nThanks for reading!\n\nDan\n\n\n\n**(**Built using [rLLM](https://github.com/rllm-org/rllm) RL framework which was brilliant to work with, and evaluated and inspired by the great [Terminal Bench](https://www.tbench.ai/) benchmark)",
          "author_fullname": "t2_1d3whvko4o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train 😅",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 56,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "az9m6jfyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf0464c4532f3f729201557b3cdb5d5fd0da9b2b"
                },
                {
                  "y": 113,
                  "x": 216,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9307853ef8723561cef54082c7f8f77319ea5b34"
                },
                {
                  "y": 168,
                  "x": 320,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae1a5539212b036e7c28c8c61e3e68b98424c45c"
                },
                {
                  "y": 336,
                  "x": 640,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34665d186c28ea84e68baf844e036d594b99444c"
                },
                {
                  "y": 504,
                  "x": 960,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b63f6b7aca9c207a4fde0aef520b6009219fffa8"
                },
                {
                  "y": 567,
                  "x": 1080,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cdac6743c832453412d2fa7f1ee6772f07122775"
                }
              ],
              "s": {
                "y": 2656,
                "x": 5056,
                "u": "https://preview.redd.it/az9m6jfyosff1.png?width=5056&amp;format=png&amp;auto=webp&amp;s=d99222a2be85de2b89a1a6493a08cde4699fe249"
              },
              "id": "az9m6jfyosff1"
            },
            "05xy1rkwosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 43,
                  "x": 108,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0dcfc346c8c2da5b7e199e3362c832fda72f2a0"
                },
                {
                  "y": 86,
                  "x": 216,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd0183b1ad65fa3536530fed15ebb59228114de8"
                },
                {
                  "y": 128,
                  "x": 320,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6378361bd9573c4483dbb212b8b19d0f423d586e"
                },
                {
                  "y": 257,
                  "x": 640,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cfa60d1b056f1a6aaa767613c6b399bba1f08e3f"
                },
                {
                  "y": 386,
                  "x": 960,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=89aeebad83be297d8a6cfcd4d0542c93ccd188a8"
                },
                {
                  "y": 434,
                  "x": 1080,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc80c66e6cf4008641ba2f21dd30003eef50ce89"
                }
              ],
              "s": {
                "y": 1216,
                "x": 3020,
                "u": "https://preview.redd.it/05xy1rkwosff1.png?width=3020&amp;format=png&amp;auto=webp&amp;s=e1dc47c0c33e71f8b5e335d68819be4db4d22ee5"
              },
              "id": "05xy1rkwosff1"
            },
            "su4gklfyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 18,
                  "x": 108,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d43db954028d7fa2643968427c1d234dc846c5bc"
                },
                {
                  "y": 36,
                  "x": 216,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d90b7348035f5a53dc5107a1589bd53b00f6164"
                },
                {
                  "y": 53,
                  "x": 320,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4903028118e0d07279f10f1ff077e9b7602a3f4"
                },
                {
                  "y": 107,
                  "x": 640,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a51dc0a7298828763adf8f5227e658fd9a7aea78"
                },
                {
                  "y": 160,
                  "x": 960,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=494ddcfcfa66b477b5fe9ef394122c9090897959"
                },
                {
                  "y": 181,
                  "x": 1080,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e37acb88ac4bfb55e64017940222b653620d355"
                }
              ],
              "s": {
                "y": 392,
                "x": 2338,
                "u": "https://preview.redd.it/su4gklfyosff1.png?width=2338&amp;format=png&amp;auto=webp&amp;s=bd8ffa2f90ce65997dc8fb28bc340c740cb836cf"
              },
              "id": "su4gklfyosff1"
            },
            "1b89mdgyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8aa052d7230a7e93bea882456a0172d444f2b56a"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc0c3485addd4720027df6316c53a1f7a92d2d1d"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3bd5cac04c80326df47cb1d08a272ca37fbdc82"
                },
                {
                  "y": 373,
                  "x": 640,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bb2846eb41e704e36041ddc908044490ffd58ce"
                },
                {
                  "y": 559,
                  "x": 960,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6767893bb8601129a6a8da2adb366e6b2cd73c4e"
                },
                {
                  "y": 629,
                  "x": 1080,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7964070ac0fa53d489a1e248289630ad1ff7a2e0"
                }
              ],
              "s": {
                "y": 1522,
                "x": 2610,
                "u": "https://preview.redd.it/1b89mdgyosff1.png?width=2610&amp;format=png&amp;auto=webp&amp;s=25d03cf8b2bcc3e4d1633a1fb9f3c570fbda742b"
              },
              "id": "1b89mdgyosff1"
            }
          },
          "name": "t3_1mc8evq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 73,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "05xy1rkwosff1",
                "id": 716537836
              },
              {
                "media_id": "az9m6jfyosff1",
                "id": 716537837
              },
              {
                "media_id": "su4gklfyosff1",
                "id": 716537838
              },
              {
                "media_id": "1b89mdgyosff1",
                "id": 716537839
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/GTUl_GxBM3AgORm0fFuwPhwKeJqsGTeIOOsHWvhrYYI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753786945,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;👋 After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford&amp;#39;s Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! 😅&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Created a Claude Code-inspired agent (system msg + tools)&lt;/li&gt;\n&lt;li&gt;Built Docker-isolated GRPO training where each rollout gets its own container&lt;/li&gt;\n&lt;li&gt;Developed a multi-agent synthetic data pipeline to generate &amp;amp; validate training data with Opus-4&lt;/li&gt;\n&lt;li&gt;Implemented a hybrid reward signal of unit test verifiers &amp;amp; a behavioural LLM judge.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My untrained Qwen3-32B agent achieved &lt;strong&gt;13.75%&lt;/strong&gt; on Terminal-Bench (#19, beats Stanford&amp;#39;s Qwen3-235B MoE)&lt;/li&gt;\n&lt;li&gt;I tested training to work stably on 32x H100s distributed across 4 bare metal nodes&lt;/li&gt;\n&lt;li&gt;I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.&lt;/li&gt;\n&lt;li&gt;~£30-50k needed for full training run of 1000 epochs (I could only afford testing 😅)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The synthetic dataset ranges from easy to extremely hard tasks. An example hard task&amp;#39;s prompt:\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;I found this mystery program at `/app/program` and I&amp;#39;m completely stumped. It&amp;#39;s a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can&amp;#39;t figure out what kind of input it needs. Could you help me figure out what this program requires?&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Simple config presets allow training to run on multiple hardware setups with minimal effort.&lt;/li&gt;\n&lt;li&gt;GRPO used with 16 rollouts per task, up to 32k tokens per rollout.&lt;/li&gt;\n&lt;li&gt;Agent uses XML/YAML format to structure tool calls&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;⭐️ &lt;a href=\"https://github.com/Danau5tin/terminal-bench-rl\"&gt;Terminal Agent RL repo&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Danau5tin/tbench-agentic-data-pipeline\"&gt;⭐️ Multi-agent synthetic data pipeline repo&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I thought I would share this because I believe long-horizon RL is going to change everybody&amp;#39;s lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n\n&lt;p&gt;Dan&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;Built using &lt;a href=\"https://github.com/rllm-org/rllm\"&gt;rLLM&lt;/a&gt; RL framework which was brilliant to work with, and evaluated and inspired by the great &lt;a href=\"https://www.tbench.ai/\"&gt;Terminal Bench&lt;/a&gt; benchmark)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mc8evq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mc8evq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanAiTuning",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mc8evq",
          "subreddit_subscribers": 506972,
          "created_utc": 1753786945,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to install sesame csm 1b in windows...\n\nTried this repo [https://github.com/SesameAILabs/csm](https://github.com/SesameAILabs/csm)  , couldnt get it to work\n\nThen tried this repo [https://github.com/akashjss/sesame-csm](https://github.com/akashjss/sesame-csm) \n\nCan anyone help and say what steps to do to install this in windows?\n\nThis is for sure one of the crapiest installation processes I’ve seen for a TTS tool.",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stuck with Sesame CSM 1b in windows...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcz4jq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753856461,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to install sesame csm 1b in windows...&lt;/p&gt;\n\n&lt;p&gt;Tried this repo &lt;a href=\"https://github.com/SesameAILabs/csm\"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;  , couldnt get it to work&lt;/p&gt;\n\n&lt;p&gt;Then tried this repo &lt;a href=\"https://github.com/akashjss/sesame-csm\"&gt;https://github.com/akashjss/sesame-csm&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Can anyone help and say what steps to do to install this in windows?&lt;/p&gt;\n\n&lt;p&gt;This is for sure one of the crapiest installation processes I’ve seen for a TTS tool.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p0Tk6sReU4ulSNZM8lq2D48BpC-If6DbRn7LmIvBtHM.png?auto=webp&amp;s=f47536910b0b6fba146922dad6f6d59d396ab336",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p0Tk6sReU4ulSNZM8lq2D48BpC-If6DbRn7LmIvBtHM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=929ff182abd1e5f7e5729b83c41b41ad30e1a33b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/p0Tk6sReU4ulSNZM8lq2D48BpC-If6DbRn7LmIvBtHM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bad75366ebb1abc91284ceee9bb03406b8e78dac",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/p0Tk6sReU4ulSNZM8lq2D48BpC-If6DbRn7LmIvBtHM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a93dba752dde35d258a7ade9533b8062bd7ea518",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/p0Tk6sReU4ulSNZM8lq2D48BpC-If6DbRn7LmIvBtHM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d14e82e3aa2cbdfaf31921b2742f8d637d00371",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/p0Tk6sReU4ulSNZM8lq2D48BpC-If6DbRn7LmIvBtHM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f575a23d1f52dcb849b69aed84ba40eb7e1ffdc7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/p0Tk6sReU4ulSNZM8lq2D48BpC-If6DbRn7LmIvBtHM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62104824b968228095c22d3935bb3c5d53f93eb6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "p0Tk6sReU4ulSNZM8lq2D48BpC-If6DbRn7LmIvBtHM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcz4jq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcz4jq/stuck_with_sesame_csm_1b_in_windows/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcz4jq/stuck_with_sesame_csm_1b_in_windows/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753856461,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Plan / build:**\n\n* **GPUs:** 4× NVIDIA Tesla P40 on a PCIe x16 → x4/x4/x4/x4 riser.\n* **Cooling:** **2× 140 mm Noctua high‑static‑pressure fans** in **push‑pull** through **3D‑printed tapered manifolds** (inlet + outlet). Interior wet‑sanded and finished with a thin epoxy coat; joints sealed with PTFE tape.\n* **Target power:** cap each P40 at **200 W** to keep thermals/noise down.\n* **Motherboard:** mini‑ITX.\n* **PSU:** Corsair 1200 W. **Five EPS 8‑pin (4+4) cables**: four to the GPUs, one to the motherboard. The P40 uses a **CPU/EPS 8‑pin** power connector (not PCIe 8‑pin). \n\n**Why EPS and not PCIe?**  \nNVIDIA’s P40 product brief specifies a **CPU 8‑pin** on the board edge. If EPS leads aren’t available, NVIDIA lists a supported **CPU‑to‑PCIe dongle** (dual PCIe 8‑pin into one EPS 8‑pin). Also noting pinout differences: EPS 8‑pin carries four 12 V + four GND, keyed differently from PCIe; mixing them without the correct cable is risky. \n\n**Cabling / current assumptions:**\n\n* Budget \\~**800 W** for the four GPUs at 200 W each (+ headroom for the system).\n* EPS 8‑pin is commonly rated to **\\~300 W**, so one EPS per P40 should be fine with proper gauge and vendor‑correct cables. I’ll stick to Corsair‑specific EPS leads or known CPU‑to‑PCIe→EPS adapters. Thoughts? \n\n**Fans:**  \nLeaning toward **NF‑A14 industrialPPC‑2000 PWM** (up to \\~4.18 mm H₂O, \\~107 CFM) vs the standard NF‑A14 PWM (2.08 mm H₂O). In **push‑pull** the pressures add, which should help overcome manifold losses. Any experience with A14 iPPC‑2000 vs standard for dense heatsinks?\n\n**Manifold design notes:**\n\n* Tapered “cone” manifolds both sides; considering **70–100 mm** length each to keep the expansion gentle and reduce turbulence.\n* Internal **baffles** to bias a bit **more cross‑section to the center GPUs** (they tend to get less flow).\n* High‑resolution print, wet‑sand, thin epoxy coat for smoother walls; PTFE tape at the GPU lip to ensure airtight seal.\n\n**Questions for the community:**\n\n1. **Thermals:** With 2×140 mm push‑pull and capped at **200 W per P40**, is this realistically enough to keep full‑load temps in the 50–70 °C range in a home lab chassis? Any gotchas you’ve seen? (I can raise fan RPM if needed.)\n2. **Fans:** Would you pick **A14 iPPC‑2000** (higher pressure, louder) or the **standard A14 PWM** (quieter) for this? Any measurable delta on blower‑style server heatsinks? \n3. **Cabling safety:** For **Corsair** PSUs, is running **direct EPS→P40** your preferred path, or do you recommend the **dual PCIe→EPS adapters** NVIDIA lists? Any brand‑pinout caveats I should watch for? \n4. **Manifold length:** Any rule‑of‑thumb you’ve used for cone length vs. pressure drop in multi‑branch ducts like this?\n5. **Airflow bias:** Better to oversize center channels \\~10–15% to compensate, or keep all equal and tune via PWM?\n\nAppreciate any feedback, horror stories, or measurement data you can share!",
          "author_fullname": "t2_4w3xa152",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cooling 4× Tesla P40 with 2×140 mm push‑pull + mITX homelab — airflow &amp; power sanity check",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md2k1b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753869748,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Plan / build:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GPUs:&lt;/strong&gt; 4× NVIDIA Tesla P40 on a PCIe x16 → x4/x4/x4/x4 riser.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cooling:&lt;/strong&gt; &lt;strong&gt;2× 140 mm Noctua high‑static‑pressure fans&lt;/strong&gt; in &lt;strong&gt;push‑pull&lt;/strong&gt; through &lt;strong&gt;3D‑printed tapered manifolds&lt;/strong&gt; (inlet + outlet). Interior wet‑sanded and finished with a thin epoxy coat; joints sealed with PTFE tape.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Target power:&lt;/strong&gt; cap each P40 at &lt;strong&gt;200 W&lt;/strong&gt; to keep thermals/noise down.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; mini‑ITX.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Corsair 1200 W. &lt;strong&gt;Five EPS 8‑pin (4+4) cables&lt;/strong&gt;: four to the GPUs, one to the motherboard. The P40 uses a &lt;strong&gt;CPU/EPS 8‑pin&lt;/strong&gt; power connector (not PCIe 8‑pin). &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Why EPS and not PCIe?&lt;/strong&gt;&lt;br/&gt;\nNVIDIA’s P40 product brief specifies a &lt;strong&gt;CPU 8‑pin&lt;/strong&gt; on the board edge. If EPS leads aren’t available, NVIDIA lists a supported &lt;strong&gt;CPU‑to‑PCIe dongle&lt;/strong&gt; (dual PCIe 8‑pin into one EPS 8‑pin). Also noting pinout differences: EPS 8‑pin carries four 12 V + four GND, keyed differently from PCIe; mixing them without the correct cable is risky. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cabling / current assumptions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Budget ~&lt;strong&gt;800 W&lt;/strong&gt; for the four GPUs at 200 W each (+ headroom for the system).&lt;/li&gt;\n&lt;li&gt;EPS 8‑pin is commonly rated to &lt;strong&gt;~300 W&lt;/strong&gt;, so one EPS per P40 should be fine with proper gauge and vendor‑correct cables. I’ll stick to Corsair‑specific EPS leads or known CPU‑to‑PCIe→EPS adapters. Thoughts? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Fans:&lt;/strong&gt;&lt;br/&gt;\nLeaning toward &lt;strong&gt;NF‑A14 industrialPPC‑2000 PWM&lt;/strong&gt; (up to ~4.18 mm H₂O, ~107 CFM) vs the standard NF‑A14 PWM (2.08 mm H₂O). In &lt;strong&gt;push‑pull&lt;/strong&gt; the pressures add, which should help overcome manifold losses. Any experience with A14 iPPC‑2000 vs standard for dense heatsinks?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Manifold design notes:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tapered “cone” manifolds both sides; considering &lt;strong&gt;70–100 mm&lt;/strong&gt; length each to keep the expansion gentle and reduce turbulence.&lt;/li&gt;\n&lt;li&gt;Internal &lt;strong&gt;baffles&lt;/strong&gt; to bias a bit &lt;strong&gt;more cross‑section to the center GPUs&lt;/strong&gt; (they tend to get less flow).&lt;/li&gt;\n&lt;li&gt;High‑resolution print, wet‑sand, thin epoxy coat for smoother walls; PTFE tape at the GPU lip to ensure airtight seal.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions for the community:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Thermals:&lt;/strong&gt; With 2×140 mm push‑pull and capped at &lt;strong&gt;200 W per P40&lt;/strong&gt;, is this realistically enough to keep full‑load temps in the 50–70 °C range in a home lab chassis? Any gotchas you’ve seen? (I can raise fan RPM if needed.)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Fans:&lt;/strong&gt; Would you pick &lt;strong&gt;A14 iPPC‑2000&lt;/strong&gt; (higher pressure, louder) or the &lt;strong&gt;standard A14 PWM&lt;/strong&gt; (quieter) for this? Any measurable delta on blower‑style server heatsinks? &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cabling safety:&lt;/strong&gt; For &lt;strong&gt;Corsair&lt;/strong&gt; PSUs, is running &lt;strong&gt;direct EPS→P40&lt;/strong&gt; your preferred path, or do you recommend the &lt;strong&gt;dual PCIe→EPS adapters&lt;/strong&gt; NVIDIA lists? Any brand‑pinout caveats I should watch for? &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Manifold length:&lt;/strong&gt; Any rule‑of‑thumb you’ve used for cone length vs. pressure drop in multi‑branch ducts like this?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Airflow bias:&lt;/strong&gt; Better to oversize center channels ~10–15% to compensate, or keep all equal and tune via PWM?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Appreciate any feedback, horror stories, or measurement data you can share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1md2k1b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Same-Masterpiece3748",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md2k1b/cooling_4_tesla_p40_with_2140_mm_pushpull_mitx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md2k1b/cooling_4_tesla_p40_with_2140_mm_pushpull_mitx/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753869748,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Step 1. Get this [https://github.com/musistudio/claude-code-router](https://github.com/musistudio/claude-code-router) you get it up with 2 npm installs  \nStep 2. Create an openrouter account and top up 10 bucks or whatevs. Get API key.  \nStep 3. Put this in the JSON (look at the instructions from that repo: \\~/.claude-code-router/config.json )\n\n    {\n      \"LOG\": true,\n      \"API_TIMEOUT_MS\": 600000,\n      \"Providers\": [\n        {\n          \"name\": \"openrouter\",\n          \"api_base_url\": \"https://openrouter.ai/api/v1/chat/completions\",\n          \"api_key\": \"sk-or-v1-XXX\",\n          \"models\": [\"z-ai/glm-4.5\"],\n          \"transformer\": {\n            \"use\": [\"openrouter\"]\n          }\n        },\n      ],\n      \"Router\": {\n        \"default\": \"openrouter,z-ai/glm-4.5\",\n        \"background\": \"openrouter,z-ai/glm-4.5\",\n        \"think\": \"openrouter,z-ai/glm-4.5\",\n        \"longContext\": \"openrouter,z-ai/glm-4.5\",\n        \"longContextThreshold\": 60000,\n        \"webSearch\": \"openrouter,z-ai/glm-4.5\"\n      }\n    }\n\nStep 4. Ensure the 'server' restarts run 'ccr restart'  \nStep 5. Write \\`ccr code\\` and just enjoy.  \n\n\nCareful I burned 3$ with just one agentic query that took 10 minutes and it was still thinking. I'm going to try more with Qwen3 235B and experiment.   \n  \nGLM 4.5 is pretty smart. ",
          "author_fullname": "t2_5uhcd48d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[tutorial] Use GLM 4.5 (or any LLM) with Claude Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mchsyd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753810236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Step 1. Get this &lt;a href=\"https://github.com/musistudio/claude-code-router\"&gt;https://github.com/musistudio/claude-code-router&lt;/a&gt; you get it up with 2 npm installs&lt;br/&gt;\nStep 2. Create an openrouter account and top up 10 bucks or whatevs. Get API key.&lt;br/&gt;\nStep 3. Put this in the JSON (look at the instructions from that repo: ~/.claude-code-router/config.json )&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;LOG&amp;quot;: true,\n  &amp;quot;API_TIMEOUT_MS&amp;quot;: 600000,\n  &amp;quot;Providers&amp;quot;: [\n    {\n      &amp;quot;name&amp;quot;: &amp;quot;openrouter&amp;quot;,\n      &amp;quot;api_base_url&amp;quot;: &amp;quot;https://openrouter.ai/api/v1/chat/completions&amp;quot;,\n      &amp;quot;api_key&amp;quot;: &amp;quot;sk-or-v1-XXX&amp;quot;,\n      &amp;quot;models&amp;quot;: [&amp;quot;z-ai/glm-4.5&amp;quot;],\n      &amp;quot;transformer&amp;quot;: {\n        &amp;quot;use&amp;quot;: [&amp;quot;openrouter&amp;quot;]\n      }\n    },\n  ],\n  &amp;quot;Router&amp;quot;: {\n    &amp;quot;default&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;,\n    &amp;quot;background&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;,\n    &amp;quot;think&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;,\n    &amp;quot;longContext&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;,\n    &amp;quot;longContextThreshold&amp;quot;: 60000,\n    &amp;quot;webSearch&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Step 4. Ensure the &amp;#39;server&amp;#39; restarts run &amp;#39;ccr restart&amp;#39;&lt;br/&gt;\nStep 5. Write `ccr code` and just enjoy.  &lt;/p&gt;\n\n&lt;p&gt;Careful I burned 3$ with just one agentic query that took 10 minutes and it was still thinking. I&amp;#39;m going to try more with Qwen3 235B and experiment.   &lt;/p&gt;\n\n&lt;p&gt;GLM 4.5 is pretty smart. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?auto=webp&amp;s=3a11f7f63e1f2873b93bc27996fab03230f20930",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4da698ce554ade05a84efd4b2e0d6d6a7887ce47",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=105509ddb23e88f678f15119c0cce845e41c1420",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=be97d450eaaa2c73ea1487f634f1598cf7d41a43",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bede4133f64e0bf5c8a4f1484a743c8f1bdaecb2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e4933006a6eaa89bed20c498604e2dccecdb8f2f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=27e51736d38916346e5107369e89f9e0e8336755",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mchsyd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shaman-warrior",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mchsyd/tutorial_use_glm_45_or_any_llm_with_claude_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mchsyd/tutorial_use_glm_45_or_any_llm_with_claude_code/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753810236,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have recently released my experimental library *Actors.* Actors is a hackable library for doing Multi-Turn Multi-Agent RL with LLMs for the **GPU poor** and **middle class**.\n\nCheck it out here: [https://github.com/RD211/actors](https://github.com/RD211/actors)\n\nKey features:  \n\\- **Multi-Trainable-Agents**: You can do things like adversarial, collaborative or simulation-like environments.  \n\\- **Multi-Environments**: Lets you make very complex environments and makes it easy to combine them together.\n\n**VRAM Efficiency**, obviously if we want to train several models at the same time we need to be careful with VRAM, thus Actors does the following:  \n\\- Smart offloading of optimizer states and model parameters when not needed (does not impact training time significantly).  \n\\- Streamed weight updates to vLLM that do not make a spike in memory usage.  \n\\- A small triton kernel for reference Log-probs calculations.  \n\\- in-memory LoRA  updates to vLLM.\n\nThe library also supports LoRA/QLoRA training, Multi-GPU and soon Multi-Node. On one GPU it seems to be just a bit worse in VRAM than Unsloth.  \n  \n**Algorithms,** we currently have **GSPO** and **GRPO** both with **Liger-Kernel** implementations but you can probably get DAPO and some others by just adjusting some of the settings.\n\n\nFeedback and issues are welcome!",
          "author_fullname": "t2_2o0i8kuj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RL Library for Multi-Trainable-Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcqrwh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753834758,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753831115,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently released my experimental library &lt;em&gt;Actors.&lt;/em&gt; Actors is a hackable library for doing Multi-Turn Multi-Agent RL with LLMs for the &lt;strong&gt;GPU poor&lt;/strong&gt; and &lt;strong&gt;middle class&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Check it out here: &lt;a href=\"https://github.com/RD211/actors\"&gt;https://github.com/RD211/actors&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Key features:&lt;br/&gt;\n- &lt;strong&gt;Multi-Trainable-Agents&lt;/strong&gt;: You can do things like adversarial, collaborative or simulation-like environments.&lt;br/&gt;\n- &lt;strong&gt;Multi-Environments&lt;/strong&gt;: Lets you make very complex environments and makes it easy to combine them together.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;VRAM Efficiency&lt;/strong&gt;, obviously if we want to train several models at the same time we need to be careful with VRAM, thus Actors does the following:&lt;br/&gt;\n- Smart offloading of optimizer states and model parameters when not needed (does not impact training time significantly).&lt;br/&gt;\n- Streamed weight updates to vLLM that do not make a spike in memory usage.&lt;br/&gt;\n- A small triton kernel for reference Log-probs calculations.&lt;br/&gt;\n- in-memory LoRA  updates to vLLM.&lt;/p&gt;\n\n&lt;p&gt;The library also supports LoRA/QLoRA training, Multi-GPU and soon Multi-Node. On one GPU it seems to be just a bit worse in VRAM than Unsloth.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Algorithms,&lt;/strong&gt; we currently have &lt;strong&gt;GSPO&lt;/strong&gt; and &lt;strong&gt;GRPO&lt;/strong&gt; both with &lt;strong&gt;Liger-Kernel&lt;/strong&gt; implementations but you can probably get DAPO and some others by just adjusting some of the settings.&lt;/p&gt;\n\n&lt;p&gt;Feedback and issues are welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?auto=webp&amp;s=d1c178e9209b8e83156e14a1ff72b92a0721fd8f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b2e184f5160e98cc31a4243590278640475fdef",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a02b458617c0d0ceff2a45cdc6709316610006",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e418c06b19810a48fcbc0e741cc9ba5d2cd1413",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc18776cd35ebb7e21739094063171f691aa1f7b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0d7d62db37e6673039732a16af989a3492b949b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2006937f67001eb0977dff97aad1540a191354ea",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcqrwh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rd211x",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753831115,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nWe’ve been working on a desktop assistant app built using Tauri that runs entirely locally. No internet connection, no cloud calls, just fully self-hosted LLMs and audio/vision models.\n\nThe assistant passively listens and watches. It can “hear” what’s happening in meetings (Zoom, GMeet, Discord, etc.) and “see” what’s on your screen by tracking gaze and screen context. The idea is to act like a floating AI that you can summon at any time, without ever compromising privacy. \n\nWe’re currently pulling in multiple smaller AI models (Whisper, lightweight vision models, compact LLMs) to make it work well on consumer hardware.\n\nSome challenges we foresee\n• Porting the screen and audio capture features to macOS, especially dealing with sandboxing and permission models\n• iOS might be a stretch, but we’re open to ideas on how to architect toward it\n• Packaging and performance tuning across OSes without sacrificing the privacy-first, offline architecture\n\nWould love any feedback, advice, or to hear if anyone else is building similar thing with Rust, Tauri, and local AI models.",
          "author_fullname": "t2_m15q545s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama and Whisper AI Desktop Assistant",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcjaau",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/m8h6fxpxvuff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/m8h6fxpxvuff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m8h6fxpxvuff1/DASHPlaylist.mpd?a=1756470630%2CZjdlZjJhZDhhMmI2ZDQ0YjNkYzQyMDIyOWZlMmEwNGRmNTIwMWNhYmJhNDdmNDE4ZTc3MGNkZDFiY2RlOWJhMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/m8h6fxpxvuff1/HLSPlaylist.m3u8?a=1756470630%2CN2U0YmRiMzcwYmZjM2ExMTkzY2ViZjAyM2U0ZTBhNjI0NDFmZGQzZjc2ZGRlOWIxMTZiNGRlNGZhNGVjZTE5Mg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=12dfeb3f43af814c740cb88dee1963a46e854783",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753813502,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;We’ve been working on a desktop assistant app built using Tauri that runs entirely locally. No internet connection, no cloud calls, just fully self-hosted LLMs and audio/vision models.&lt;/p&gt;\n\n&lt;p&gt;The assistant passively listens and watches. It can “hear” what’s happening in meetings (Zoom, GMeet, Discord, etc.) and “see” what’s on your screen by tracking gaze and screen context. The idea is to act like a floating AI that you can summon at any time, without ever compromising privacy. &lt;/p&gt;\n\n&lt;p&gt;We’re currently pulling in multiple smaller AI models (Whisper, lightweight vision models, compact LLMs) to make it work well on consumer hardware.&lt;/p&gt;\n\n&lt;p&gt;Some challenges we foresee\n• Porting the screen and audio capture features to macOS, especially dealing with sandboxing and permission models\n• iOS might be a stretch, but we’re open to ideas on how to architect toward it\n• Packaging and performance tuning across OSes without sacrificing the privacy-first, offline architecture&lt;/p&gt;\n\n&lt;p&gt;Would love any feedback, advice, or to hear if anyone else is building similar thing with Rust, Tauri, and local AI models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/m8h6fxpxvuff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?format=pjpg&amp;auto=webp&amp;s=9ac629a01c56b181990f5154bb7bf69ffafb59c9",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c98aea69c8ee48f81b0de209189e241f3069ec8a",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b87613792ed75990facdc4c2e81181c2d85435d6",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=36c7260614c5bb7d1185aedfdfc69616a212abde",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b732351aa485baaf25a410d68f6f03e6f7e02349",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=691220e045e714ec9e2d003a36737ee0883415da",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca6426d6e8f3342e9f7b582afe236c9fb84b673b",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcjaau",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rxhxnsxngh",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcjaau/llama_and_whisper_ai_desktop_assistant/",
          "stickied": false,
          "url": "https://v.redd.it/m8h6fxpxvuff1",
          "subreddit_subscribers": 506972,
          "created_utc": 1753813502,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/m8h6fxpxvuff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/m8h6fxpxvuff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m8h6fxpxvuff1/DASHPlaylist.mpd?a=1756470630%2CZjdlZjJhZDhhMmI2ZDQ0YjNkYzQyMDIyOWZlMmEwNGRmNTIwMWNhYmJhNDdmNDE4ZTc3MGNkZDFiY2RlOWJhMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/m8h6fxpxvuff1/HLSPlaylist.m3u8?a=1756470630%2CN2U0YmRiMzcwYmZjM2ExMTkzY2ViZjAyM2U0ZTBhNjI0NDFmZGQzZjc2ZGRlOWIxMTZiNGRlNGZhNGVjZTE5Mg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for a fireship-style short 3-5 minute videos to stay updated on the latest llm news... anything available?",
          "author_fullname": "t2_1n5u3x36sz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fireship-style youtube channel but for ai news?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcwfxh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753847308,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a fireship-style short 3-5 minute videos to stay updated on the latest llm news... anything available?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcwfxh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Figure-513",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcwfxh/fireshipstyle_youtube_channel_but_for_ai_news/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcwfxh/fireshipstyle_youtube_channel_but_for_ai_news/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753847308,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Curious to know what you are using. My setup is dual 3090s and I am debating a third, just because I can, not because I need to! \n\n[View Poll](https://www.reddit.com/poll/1mcqlv7)",
          "author_fullname": "t2_20yqdwbf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How many GPUs do you run and what model(s) do you use.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcqlv7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753835602,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753830685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to know what you are using. My setup is dual 3090s and I am debating a third, just because I can, not because I need to! &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1mcqlv7\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcqlv7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Salt_Armadillo8884",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1754435485708,
            "options": [
              {
                "text": "1 GPU 8-16gb",
                "id": "31249074"
              },
              {
                "text": "1 GPU 20-32gb",
                "id": "31249075"
              },
              {
                "text": "2 GPU 8-16gb",
                "id": "31249076"
              },
              {
                "text": "2 GPU 20-32gb",
                "id": "31249077"
              },
              {
                "text": "3 or more GPUs",
                "id": "31249078"
              },
              {
                "text": "1 GPU at 48GB or more",
                "id": "31249079"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 385,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcqlv7/how_many_gpus_do_you_run_and_what_models_do_you/",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcqlv7/how_many_gpus_do_you_run_and_what_models_do_you/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753830685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's great to see open models continuing to advance. I believe most people in this community would agree that there's often a significant gap between benchmark scores and real-world performance. With that in mind, I've put together some candid thoughts on several open models from an end-user's perspective.\n\n**GLM-4.5**: I find it exceptionally good for everyday use. There's a clear distinction from previous LLMs that would excessively praise users or show off with markdown tables. I noticed some quirks in its reasoning similar to Deepseek R1, but nothing problematic. Personally, I recommend using it through [chat.z.ai](http://chat.z.ai), which offers an excellent UI/UX experience.\n\n**Kimi K2**: I found it to perform excellently at both coding tasks and creative work. However, it's noticeably slow with prominent rate limiting even when accessed through Openrouter. The fact that its app and website only support Chinese is a significant downside for international users.\n\n**Qwen3 Coder**: While I've heard it benchmarks better than Kimi K2, my actual experience was quite disappointing. It warrants further testing, though it does offer a larger context window than Kimi K2, which is commendable.\n\n**Qwen3 235B A22B Instruct 2507**: I also get the sense that its benchmarks are inflated, but it's actually quite decent. It has a noticeably \"LLM-like\" quality to its responses, which might make it less ideal for creative endeavors.\n\n**Qwen3 235B A22B Thinking 2507**: Its large thinking budget is advantageous, but this can backfire, sometimes resulting in excessively long response times. For now, I find Deepseek R1-0528 more practical to use.\n\n**Deepseek R1-0528**: This one needs no introduction - it proves to be quite versatile, high-performing, and user-friendly. Among Openrouter's free models, it offers the most stable inference, and the API provides excellent value for money (the official API has discounted periods that can save you up to 70%).",
          "author_fullname": "t2_1skliabt9v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My Honest Take on Recently Popular Open Models (A Realistic Assessment)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mce934",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753802383,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s great to see open models continuing to advance. I believe most people in this community would agree that there&amp;#39;s often a significant gap between benchmark scores and real-world performance. With that in mind, I&amp;#39;ve put together some candid thoughts on several open models from an end-user&amp;#39;s perspective.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GLM-4.5&lt;/strong&gt;: I find it exceptionally good for everyday use. There&amp;#39;s a clear distinction from previous LLMs that would excessively praise users or show off with markdown tables. I noticed some quirks in its reasoning similar to Deepseek R1, but nothing problematic. Personally, I recommend using it through &lt;a href=\"http://chat.z.ai\"&gt;chat.z.ai&lt;/a&gt;, which offers an excellent UI/UX experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Kimi K2&lt;/strong&gt;: I found it to perform excellently at both coding tasks and creative work. However, it&amp;#39;s noticeably slow with prominent rate limiting even when accessed through Openrouter. The fact that its app and website only support Chinese is a significant downside for international users.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 Coder&lt;/strong&gt;: While I&amp;#39;ve heard it benchmarks better than Kimi K2, my actual experience was quite disappointing. It warrants further testing, though it does offer a larger context window than Kimi K2, which is commendable.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B Instruct 2507&lt;/strong&gt;: I also get the sense that its benchmarks are inflated, but it&amp;#39;s actually quite decent. It has a noticeably &amp;quot;LLM-like&amp;quot; quality to its responses, which might make it less ideal for creative endeavors.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B Thinking 2507&lt;/strong&gt;: Its large thinking budget is advantageous, but this can backfire, sometimes resulting in excessively long response times. For now, I find Deepseek R1-0528 more practical to use.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Deepseek R1-0528&lt;/strong&gt;: This one needs no introduction - it proves to be quite versatile, high-performing, and user-friendly. Among Openrouter&amp;#39;s free models, it offers the most stable inference, and the API provides excellent value for money (the official API has discounted periods that can save you up to 70%).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mce934",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Technology_3421",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mce934/my_honest_take_on_recently_popular_open_models_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mce934/my_honest_take_on_recently_popular_open_models_a/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753802383,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**GLM 4.5 and GLM-4.5-AIR**  \nThe **GLM-4.5** series models are foundation models designed for intelligent agents. GLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n\n[Bench performance](https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;format=png&amp;auto=webp&amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c)\n\n   \n[blog](https://z.ai/blog/glm-4.5)｜[huggingface](https://huggingface.co/zai-org/GLM-4.5)｜ [github](https://github.com/zai-org/GLM-4.5)  \n",
          "author_fullname": "t2_dpf3bqut",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This year’s best open-source models and most cost-effective models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 99,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bisgmn0utrff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74baaad6fd3f7a8d7dc00be88805a7bc35dba7f3"
                },
                {
                  "y": 153,
                  "x": 216,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=14edcaddf4b8e04722376443d659c7ed6be70b08"
                },
                {
                  "y": 227,
                  "x": 320,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b73c9df7a36a1142a626ccad114a23ea69213c7b"
                },
                {
                  "y": 455,
                  "x": 640,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ba9ef9c35dd3df135cdd3b9afc5d2c950091c3"
                },
                {
                  "y": 683,
                  "x": 960,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9de3ec1904c5a60f19e2f1e6c79e12f0e8b303d0"
                },
                {
                  "y": 768,
                  "x": 1080,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f0f776c64bbc2dcef8d1d00af6cbc89503e4768"
                }
              ],
              "s": {
                "y": 3177,
                "x": 4464,
                "u": "https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;format=png&amp;auto=webp&amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c"
              },
              "id": "bisgmn0utrff1"
            }
          },
          "name": "t3_1mc5oh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 110,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 110,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/oqyuYVJJYg1zSXUWu9TgdBdJGts5YfXbLSB6jfU2bbs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753776573,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;GLM 4.5 and GLM-4.5-AIR&lt;/strong&gt;&lt;br/&gt;\nThe &lt;strong&gt;GLM-4.5&lt;/strong&gt; series models are foundation models designed for intelligent agents. GLM-4.5 has &lt;strong&gt;355&lt;/strong&gt; billion total parameters with &lt;strong&gt;32&lt;/strong&gt; billion active parameters, while GLM-4.5-Air adopts a more compact design with &lt;strong&gt;106&lt;/strong&gt; billion total parameters and &lt;strong&gt;12&lt;/strong&gt; billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c\"&gt;Bench performance&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://z.ai/blog/glm-4.5\"&gt;blog&lt;/a&gt;｜&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;huggingface&lt;/a&gt;｜ &lt;a href=\"https://github.com/zai-org/GLM-4.5\"&gt;github&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc5oh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Apart-River475",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753776573,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Whats up fellow low code devs. Im thinking if finally making the switch to hosting n8n locally. Was probably going to run it through a VPS like digital ocean, but before doing that wanted to hear peoples thoughts on hosting on VPS vs fully local on your computer? ",
          "author_fullname": "t2_l966nsbd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Self hosting n8n",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md1m8u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753866200,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whats up fellow low code devs. Im thinking if finally making the switch to hosting n8n locally. Was probably going to run it through a VPS like digital ocean, but before doing that wanted to hear peoples thoughts on hosting on VPS vs fully local on your computer? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1md1m8u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sleepy-soba",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md1m8u/self_hosting_n8n/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md1m8u/self_hosting_n8n/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753866200,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nThis is a new **opensource** project, a benchmark that test model ability to understand complex tree-like relationship in a family tree across a massive context.   \n  \nThe idea is to have a python program that generate a tree and can use the tree structure to generate question about it. Then you can have a textual description of this tree and those question to have a text that is hard to understand for LLMs.   \n  \nYou can find the code here https://github.com/Orolol/familyBench\n\n\n**Current leaderboard**\n\nI test 7 models (6 open weight and 1 closed) on a complex tree with 400 people generated across 10 generations (which represent ~18k tokens). 200 questions are then asked to the models. All models are for now tested via OpenRouter, with low reasoning effort or 8k max token, and a temperature of 0.3. I plan to gather optimal params for each model later.\n\nExample of family description : \"Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher. Abigail (F) has 1 child: Patricia (F) ...\"\n\nExample of questions : \"Which of Paula's grandparents have salt and pepper hair?\" \"Who is the cousin of the daughter of Quentin with red hair?\"\n\nThe no response rate is when the model overthinks and is then unable to produce an answer because he used his 16k max tokens. I try to reduce this rate as much as I can, but this very often indicate that a model is unable to find the answer and is stuck in a reasoning loop. \n\n\nModel | Accuracy | Total tokens | No response rate\n-----|--------|------------|----------------\nGemini 2.5 Pro | 81.48% | 271,500 \t| 0%\nDeepSeek R1 0528 | 75.66% | 150,642\t| 0%\nSonnet 4 | 67.20%  | 575,624 \t| 0%\nGLM 4.5| 64.02%|  \t216,281| 2.12%\nGLM 4.5 air | 57.14% | 909,228|  \t26.46%\nQwen-3.2-2507-thinking | 50.26% |  \t743,131|  \t20.63%\nKimi K2 | 34.92% | 67,071| 0%\nHunyuan A13B | 30.16% |  \t121,150 |  \t2.12%\nQwen-3.2-2507| 28.04% | 3,098|  \t0.53%\nMistral Small 3.2| 22.22% |  \t5,353| 0%\nGemma 3 27B | 17.99%  |  \t2,888|  \t0.53%~~~~  \n  \nEDIT : Added R1, Sonnet 4, Hunyuan A13b and Gemma 3 27b\n\nReasoning models have a clear advantage here, but produce a massive amount of token (which means some models are quite expansive to test). More models are coming to the leaderboard (R1, Sonnet)",
          "author_fullname": "t2_fbzx9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Benchmark - FamilyBench - Test models ability to understand complex tree type relationship and reason on massive context. Immune to contamination. GML 4.5 64.02%, Gemini 2.5 pro 81,48%.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc687c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 71,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 71,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753801202,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753778766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;This is a new &lt;strong&gt;opensource&lt;/strong&gt; project, a benchmark that test model ability to understand complex tree-like relationship in a family tree across a massive context.   &lt;/p&gt;\n\n&lt;p&gt;The idea is to have a python program that generate a tree and can use the tree structure to generate question about it. Then you can have a textual description of this tree and those question to have a text that is hard to understand for LLMs.   &lt;/p&gt;\n\n&lt;p&gt;You can find the code here &lt;a href=\"https://github.com/Orolol/familyBench\"&gt;https://github.com/Orolol/familyBench&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current leaderboard&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I test 7 models (6 open weight and 1 closed) on a complex tree with 400 people generated across 10 generations (which represent ~18k tokens). 200 questions are then asked to the models. All models are for now tested via OpenRouter, with low reasoning effort or 8k max token, and a temperature of 0.3. I plan to gather optimal params for each model later.&lt;/p&gt;\n\n&lt;p&gt;Example of family description : &amp;quot;Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher. Abigail (F) has 1 child: Patricia (F) ...&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Example of questions : &amp;quot;Which of Paula&amp;#39;s grandparents have salt and pepper hair?&amp;quot; &amp;quot;Who is the cousin of the daughter of Quentin with red hair?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The no response rate is when the model overthinks and is then unable to produce an answer because he used his 16k max tokens. I try to reduce this rate as much as I can, but this very often indicate that a model is unable to find the answer and is stuck in a reasoning loop. &lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Model&lt;/th&gt;\n&lt;th&gt;Accuracy&lt;/th&gt;\n&lt;th&gt;Total tokens&lt;/th&gt;\n&lt;th&gt;No response rate&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Gemini 2.5 Pro&lt;/td&gt;\n&lt;td&gt;81.48%&lt;/td&gt;\n&lt;td&gt;271,500&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;DeepSeek R1 0528&lt;/td&gt;\n&lt;td&gt;75.66%&lt;/td&gt;\n&lt;td&gt;150,642&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Sonnet 4&lt;/td&gt;\n&lt;td&gt;67.20%&lt;/td&gt;\n&lt;td&gt;575,624&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;GLM 4.5&lt;/td&gt;\n&lt;td&gt;64.02%&lt;/td&gt;\n&lt;td&gt;216,281&lt;/td&gt;\n&lt;td&gt;2.12%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;GLM 4.5 air&lt;/td&gt;\n&lt;td&gt;57.14%&lt;/td&gt;\n&lt;td&gt;909,228&lt;/td&gt;\n&lt;td&gt;26.46%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Qwen-3.2-2507-thinking&lt;/td&gt;\n&lt;td&gt;50.26%&lt;/td&gt;\n&lt;td&gt;743,131&lt;/td&gt;\n&lt;td&gt;20.63%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Kimi K2&lt;/td&gt;\n&lt;td&gt;34.92%&lt;/td&gt;\n&lt;td&gt;67,071&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Hunyuan A13B&lt;/td&gt;\n&lt;td&gt;30.16%&lt;/td&gt;\n&lt;td&gt;121,150&lt;/td&gt;\n&lt;td&gt;2.12%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Qwen-3.2-2507&lt;/td&gt;\n&lt;td&gt;28.04%&lt;/td&gt;\n&lt;td&gt;3,098&lt;/td&gt;\n&lt;td&gt;0.53%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Mistral Small 3.2&lt;/td&gt;\n&lt;td&gt;22.22%&lt;/td&gt;\n&lt;td&gt;5,353&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Gemma 3 27B&lt;/td&gt;\n&lt;td&gt;17.99%&lt;/td&gt;\n&lt;td&gt;2,888&lt;/td&gt;\n&lt;td&gt;0.53%~~~~&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;EDIT : Added R1, Sonnet 4, Hunyuan A13b and Gemma 3 27b&lt;/p&gt;\n\n&lt;p&gt;Reasoning models have a clear advantage here, but produce a massive amount of token (which means some models are quite expansive to test). More models are coming to the leaderboard (R1, Sonnet)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?auto=webp&amp;s=19958202ed21212a2e6bb842d061589134b9c755",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fcc6e9e77205be821c357ea312fd60ae612baf42",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b5d26c31cee88c4ca71b2ceece0db8aabab9637e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03a3cd0f1a5e8eb77f5adb43153fa12c443ab921",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3af7542a187061f08a2c1c0dc31cc85c6549d96f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a9d11fb33d08918d37099a8ce87e35de6fb055b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e4de8942748959e32aa5af7dabf84050fcca647",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc687c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Orolol",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753778766,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "One .cu file holds everything necessary for inference. There are no external libraries; only the CUDA runtime is included. Everything, from tokenization right down to the kernels, is packed into this single file.\n\nIt works with the Qwen3 0.6B model GGUF at full precision. On an RTX 3060, it generates appr. \\~32 tokens per second. For benchmarking purposes, you can enable cuBLAS, which increase the TPS to \\~70.\n\nThe CUDA version is built upon my qwen.c repo. It's a pure C inference, again contained within a single file. It uses the Qwen3 0.6B at 32FP too, which I think is the most explainable and demonstrable setup for pedagogical purposes.\n\nBoth versions use the GGUF file directly, with no conversion to binary. The tokenizer’s vocab and merges are plain text files, making them easy to inspect and understand. You can run multi-turn conversations, and reasoning tasks supported by Qwen3.\n\nThese projects draw inspiration from Andrej Karpathy’s [llama2.c](https://github.com/karpathy/llama2.c) and share the same commitment to minimalism. Both projects are MIT licensed. I’d love to hear your feedback!\n\nqwen3.cu: [https://github.com/gigit0000/qwen3.cu](https://github.com/gigit0000/qwen3.cu)\n\nqwen3.c: [https://github.com/gigit0000/qwen3.c](https://github.com/gigit0000/qwen3.c)",
          "author_fullname": "t2_kfu7x339m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Single-File Qwen3 Inference in Pure CUDA C",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5e54",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753775439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One .cu file holds everything necessary for inference. There are no external libraries; only the CUDA runtime is included. Everything, from tokenization right down to the kernels, is packed into this single file.&lt;/p&gt;\n\n&lt;p&gt;It works with the Qwen3 0.6B model GGUF at full precision. On an RTX 3060, it generates appr. ~32 tokens per second. For benchmarking purposes, you can enable cuBLAS, which increase the TPS to ~70.&lt;/p&gt;\n\n&lt;p&gt;The CUDA version is built upon my qwen.c repo. It&amp;#39;s a pure C inference, again contained within a single file. It uses the Qwen3 0.6B at 32FP too, which I think is the most explainable and demonstrable setup for pedagogical purposes.&lt;/p&gt;\n\n&lt;p&gt;Both versions use the GGUF file directly, with no conversion to binary. The tokenizer’s vocab and merges are plain text files, making them easy to inspect and understand. You can run multi-turn conversations, and reasoning tasks supported by Qwen3.&lt;/p&gt;\n\n&lt;p&gt;These projects draw inspiration from Andrej Karpathy’s &lt;a href=\"https://github.com/karpathy/llama2.c\"&gt;llama2.c&lt;/a&gt; and share the same commitment to minimalism. Both projects are MIT licensed. I’d love to hear your feedback!&lt;/p&gt;\n\n&lt;p&gt;qwen3.cu: &lt;a href=\"https://github.com/gigit0000/qwen3.cu\"&gt;https://github.com/gigit0000/qwen3.cu&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;qwen3.c: &lt;a href=\"https://github.com/gigit0000/qwen3.c\"&gt;https://github.com/gigit0000/qwen3.c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?auto=webp&amp;s=b87dc526d65dc903b76c415404d32f3bdbff0963",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd05cb170e306c505c4104b96edb3c670cf24b48",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb54c005f706a393effe1c3002c30653b11607bf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3800484811cbe29ca44c0f3713d9faca1e06531",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de554ecb50aa8f1ad0aa1ca60137d36a4be1ffe1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c55c183a33bb16b6df8879b5b4136746ad2f9d97",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=864a95c03a2b88f3bd8d6a9779c0295bd87c1174",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mc5e54",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Awkward_Click6271",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753775439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Light-hearted, too. Don't take it too seriously!",
          "author_fullname": "t2_4e7zb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Something lightweight: a LLM simulation of Bernie Sanders",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc6dfx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=13ca4de7ff68a78d013f9a42ba2e6d160bfd36a9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753779350,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Light-hearted, too. Don&amp;#39;t take it too seriously!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ivoras/bernie0.1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?auto=webp&amp;s=b63548e59f80de05379eb11bb6e71ffbe24ec79e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2195a6491e60c2b5e1f156d2b6b2b6724700da2b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a1fe9a861ec410437283d33ac1d6788cb63d07e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=11c6188b4912cc92c1465cd62ad6cddc3d8c5ff4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f80fc97e46dbb90afe19165e1d70bec1ffb040f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e8c871481b5aa549ea255e914c8ed7850224f4c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39178233074a14e8bf2d9acbd58e16605cf4eed5",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mc6dfx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ivoras",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc6dfx/something_lightweight_a_llm_simulation_of_bernie/",
          "stickied": false,
          "url": "https://huggingface.co/ivoras/bernie0.1",
          "subreddit_subscribers": 506972,
          "created_utc": 1753779350,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "RAG is out of the question\n\nIs continued pre training better or supervised fine tuning?\n\nwhat is your experience? Assuming I have around 10B tokens for training ",
          "author_fullname": "t2_1t3515o2d2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best method for LLM to improve competency in a specific domain?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcz8sc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753856900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;RAG is out of the question&lt;/p&gt;\n\n&lt;p&gt;Is continued pre training better or supervised fine tuning?&lt;/p&gt;\n\n&lt;p&gt;what is your experience? Assuming I have around 10B tokens for training &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcz8sc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rockybaby2025",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcz8sc/what_is_the_best_method_for_llm_to_improve/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcz8sc/what_is_the_best_method_for_llm_to_improve/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753856900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm currently working on a document parsing pipeline for semi-structured documents like invoices, which can have highly variable layouts.\n\nMy current approach uses AWS Textract for OCR and layout extraction, then I pass the extracted text (and sometimes basic layout structure) into LLMs via LangChain for downstream parsing/classification tasks. However, the results are not as good as I expected — the models struggle to consistently identify and structure the fields across varying templates.\n\nI’m aware of models like LayoutLM and I’m currently testing them as well, but I’m not confident they’ll be enough for my specific use case, especially given the diversity in document structure.\n\nWould it make sense to fine-tune a LLaMA model using LoRA specifically for this task (e.g. key-value extraction from OCR’d documents)? Has anyone tried something similar or have thoughts on how well LLaMA-based models can handle this type of task compared to layout-aware models?\n\nAny tips, papers, or repo links would be greatly appreciated.\n\nThanks!",
          "author_fullname": "t2_u8ymcwdb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuning LLaMA with LoRA for document parsing (invoices with varying layouts)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcuziy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753842904,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a document parsing pipeline for semi-structured documents like invoices, which can have highly variable layouts.&lt;/p&gt;\n\n&lt;p&gt;My current approach uses AWS Textract for OCR and layout extraction, then I pass the extracted text (and sometimes basic layout structure) into LLMs via LangChain for downstream parsing/classification tasks. However, the results are not as good as I expected — the models struggle to consistently identify and structure the fields across varying templates.&lt;/p&gt;\n\n&lt;p&gt;I’m aware of models like LayoutLM and I’m currently testing them as well, but I’m not confident they’ll be enough for my specific use case, especially given the diversity in document structure.&lt;/p&gt;\n\n&lt;p&gt;Would it make sense to fine-tune a LLaMA model using LoRA specifically for this task (e.g. key-value extraction from OCR’d documents)? Has anyone tried something similar or have thoughts on how well LLaMA-based models can handle this type of task compared to layout-aware models?&lt;/p&gt;\n\n&lt;p&gt;Any tips, papers, or repo links would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcuziy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "existencialista27",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcuziy/finetuning_llama_with_lora_for_document_parsing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcuziy/finetuning_llama_with_lora_for_document_parsing/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753842904,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm trying to optimize running larger MoE models like Qwen3-30B-A3B on a low-VRAM setup (4GB GPU) by using intelligent/manual offloading.\n\nThe goal is to keep the most relevant experts for a specific task (e.g., coding) permanently in VRAM for better performance, while offloading the less used ones to the CPU/RAM.\n\nThis obviously requires knowing which expert ID corresponds to which specialized function. Has anyone already done the legwork of profiling the model? For example, by feeding it pure code vs. pure prose and logging the expert activation frequency with tools like llama.cpp?\n\nI'm looking for any kind of data.",
          "author_fullname": "t2_mhb0rkd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone profiled the expert specialization in MoE models like Qwen3-30B-A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mceq8m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753803457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to optimize running larger MoE models like Qwen3-30B-A3B on a low-VRAM setup (4GB GPU) by using intelligent/manual offloading.&lt;/p&gt;\n\n&lt;p&gt;The goal is to keep the most relevant experts for a specific task (e.g., coding) permanently in VRAM for better performance, while offloading the less used ones to the CPU/RAM.&lt;/p&gt;\n\n&lt;p&gt;This obviously requires knowing which expert ID corresponds to which specialized function. Has anyone already done the legwork of profiling the model? For example, by feeding it pure code vs. pure prose and logging the expert activation frequency with tools like llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for any kind of data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mceq8m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Eden63",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753803457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why does no one talk enough about the fact that AI models can't write proper tests? They seriously can't write unit or integration tests, none of them pass.",
          "author_fullname": "t2_81903fy5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tests failures",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcz2pu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753856276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why does no one talk enough about the fact that AI models can&amp;#39;t write proper tests? They seriously can&amp;#39;t write unit or integration tests, none of them pass.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcz2pu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sakuletas",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcz2pu/tests_failures/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcz2pu/tests_failures/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753856276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all!\n\nSo wondering, what would be the entry level in Apple Silicone land for running Nemotron super 49B?  \nHas anyone tried, or know of a benchmark for a M4 pro vs M4 Max and what is the minimum ram needed? I tried on my air but alas, I know I don't have the ram for it(24). It runs but slow of course. \n\nThanks!",
          "author_fullname": "t2_1ofxu8gsy0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Nemotron super 49b running on Apple Silicon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcyrgb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753855150,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;So wondering, what would be the entry level in Apple Silicone land for running Nemotron super 49B?&lt;br/&gt;\nHas anyone tried, or know of a benchmark for a M4 pro vs M4 Max and what is the minimum ram needed? I tried on my air but alas, I know I don&amp;#39;t have the ram for it(24). It runs but slow of course. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcyrgb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PensionRealistic6618",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcyrgb/nemotron_super_49b_running_on_apple_silicon/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcyrgb/nemotron_super_49b_running_on_apple_silicon/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753855150,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wrote a small CLI in golang today with Claude that auto downloads the models and comes out at around 5MB in size when compiled. The goal is to create a foundation to build a single unix style utility that can take files as input and transcribe them easily. It also handles whole folders of files and can restart when it gets interrupted. \n\nI still want to add speaker diarization as well as publish it to brew and a few more things. But I already wanted to get some feedback from people. \n\nThe main goal for me is to point it at a YouTube channel, download all the videos audio streams via yt-dlp, then transcribe the whole pack, recognise speakers, use a small LLM to identify who is who to replace &lt;speaker1&gt; with “Tom” etc and then have nice archives of channels with good text representations. \n\nhttps://github.com/pascalwhoop/ghospel\n\nLmk what you guys think and what you’d be looking for in a CLI like this. \n\nThere’s also a blog post about it but I won’t self promote too much for now. ",
          "author_fullname": "t2_1uktcnndgt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Golang based whisper.cpp wrapper CLI with intention to expand to speaker diarization and more",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcp4lj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753826924,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a small CLI in golang today with Claude that auto downloads the models and comes out at around 5MB in size when compiled. The goal is to create a foundation to build a single unix style utility that can take files as input and transcribe them easily. It also handles whole folders of files and can restart when it gets interrupted. &lt;/p&gt;\n\n&lt;p&gt;I still want to add speaker diarization as well as publish it to brew and a few more things. But I already wanted to get some feedback from people. &lt;/p&gt;\n\n&lt;p&gt;The main goal for me is to point it at a YouTube channel, download all the videos audio streams via yt-dlp, then transcribe the whole pack, recognise speakers, use a small LLM to identify who is who to replace &amp;lt;speaker1&amp;gt; with “Tom” etc and then have nice archives of channels with good text representations. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/pascalwhoop/ghospel\"&gt;https://github.com/pascalwhoop/ghospel&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Lmk what you guys think and what you’d be looking for in a CLI like this. &lt;/p&gt;\n\n&lt;p&gt;There’s also a blog post about it but I won’t self promote too much for now. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?auto=webp&amp;s=c9667d67ac26714cecec59f946f00ae2cbb28091",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e8b2c21c9cb31cb7050c6c04a1b0a264cdb2d2d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f888bae2c84de4cb101d7a6e8bddd2ec7a8a0bc2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0429088192a964051239facfec6a3f881acdf706",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bee7509b0c842733361018e91c9f2cb421adbc6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=31151174fbd5c99a5e8f086a0fd0dd00c61839c2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1b7c7ff2eff95d41ec4a945ddbd2f58946aa08a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcp4lj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pascalwhoop",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcp4lj/golang_based_whispercpp_wrapper_cli_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcp4lj/golang_based_whispercpp_wrapper_cli_with/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753826924,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA 👋!\n\nFor the past 18 months, my colleague and I have been working on **Ebiose**, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).\n\nEbiose aims to create a decentralized AI factory, a Darwin-style playground (à la Google’s AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own \"forge,\" define a task, and watch AI agents compete until the fittest emerge.\n\nThis evolutionary approach demands massive inference resources. Currently, we're relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.\n\nThat's why we'd love input from the LocalLLaMA community!\n\n**The Big Idea: A Community-Powered P2P Inference Grid**\n\nWe’re dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here’s the plan:\n\n* **Lightweight Client:** A background app runs on your PC (and maybe phones later).\n* **Hardware Profiling:** The client auto-detects what LLMs your machine can handle.\n* **Orchestration Layer:** A system (centralized or decentralized?) assigns inference tasks to capable nodes.\n* **Dynamic LoRA Adapters:** Fine-tune models efficiently with lightweight, modular adapters.\n* **Batch &amp; Prompt Caching:** Optimize for high throughput by batching requests and reusing system prompts.\n\n**Technical Questions for the Community**\n\n1. **Inference Backend:** We’re leaning toward **llama.cpp** for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a high-throughput setup, would **vLLM**, **zml**, or another engine be better? Since we’re prioritizing batch processing over single-prompt speed, what’s your pick?\n2. **Task Orchestration:** How do we route inference jobs (e.g., “run this 13B model with this prompt”) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?\n3. **Existing Tools:** Are there open-source projects we could build on?\n\nWhat do you think? Got ideas, tools, or experiences to share?",
          "author_fullname": "t2_6bm8s1wm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Let's Build a \"Garage AI Supercomputer\": A P2P Compute Grid for Inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc8fhc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753796327,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753786996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; 👋!&lt;/p&gt;\n\n&lt;p&gt;For the past 18 months, my colleague and I have been working on &lt;strong&gt;Ebiose&lt;/strong&gt;, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).&lt;/p&gt;\n\n&lt;p&gt;Ebiose aims to create a decentralized AI factory, a Darwin-style playground (à la Google’s AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own &amp;quot;forge,&amp;quot; define a task, and watch AI agents compete until the fittest emerge.&lt;/p&gt;\n\n&lt;p&gt;This evolutionary approach demands massive inference resources. Currently, we&amp;#39;re relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why we&amp;#39;d love input from the LocalLLaMA community!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Big Idea: A Community-Powered P2P Inference Grid&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We’re dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here’s the plan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lightweight Client:&lt;/strong&gt; A background app runs on your PC (and maybe phones later).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hardware Profiling:&lt;/strong&gt; The client auto-detects what LLMs your machine can handle.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orchestration Layer:&lt;/strong&gt; A system (centralized or decentralized?) assigns inference tasks to capable nodes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic LoRA Adapters:&lt;/strong&gt; Fine-tune models efficiently with lightweight, modular adapters.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Batch &amp;amp; Prompt Caching:&lt;/strong&gt; Optimize for high throughput by batching requests and reusing system prompts.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical Questions for the Community&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Inference Backend:&lt;/strong&gt; We’re leaning toward &lt;strong&gt;llama.cpp&lt;/strong&gt; for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a high-throughput setup, would &lt;strong&gt;vLLM&lt;/strong&gt;, &lt;strong&gt;zml&lt;/strong&gt;, or another engine be better? Since we’re prioritizing batch processing over single-prompt speed, what’s your pick?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Task Orchestration:&lt;/strong&gt; How do we route inference jobs (e.g., “run this 13B model with this prompt”) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Existing Tools:&lt;/strong&gt; Are there open-source projects we could build on?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you think? Got ideas, tools, or experiences to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc8fhc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ModeSquare8129",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753786996,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_63nhk1l7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Told Qwen3 1.7b (thinking) to make a black hole simulation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc644b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e5xhwj4azrff1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 1280,
              "width": 718,
              "scrubber_media_url": "https://v.redd.it/e5xhwj4azrff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e5xhwj4azrff1/DASHPlaylist.mpd?a=1756470630%2CMGQ2NTI2ZjJjMWE0NDYyY2E4MTU3ZTE5MzBjMzYyOTM0ZmIyZDE5YzM5ZDdjZGVlNmQ1ODM5ZDQ1NTA4MGNmNw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/e5xhwj4azrff1/HLSPlaylist.m3u8?a=1756470630%2CZGYxZTZiMTRmZjEyMzYyZWYwODJmN2E1ZTlhYzE0N2E1OTU4YTJmYWU4MmNhOTIzZDU0MmNkN2U2NTU4YTllYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ee70a99f6ecb5bd697a2ca9affaab6f82ce0f664",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753778315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e5xhwj4azrff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?format=pjpg&amp;auto=webp&amp;s=af896c68cbffa8e627b8ffb22e577257ee016331",
                  "width": 806,
                  "height": 1438
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d4f7b64c9c9249f426fe6264ebe0ca68c9ccaee8",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b360f9ce854c7b5390001204eb84d38a765a00d1",
                    "width": 216,
                    "height": 385
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=af21a88c251c05a42ca5118d5bf46315948b41a1",
                    "width": 320,
                    "height": 570
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=698ad4dc76e8be804cfdf1038565ba0059e67379",
                    "width": 640,
                    "height": 1141
                  }
                ],
                "variants": {},
                "id": "czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mc644b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gold_Bar_4072",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc644b/told_qwen3_17b_thinking_to_make_a_black_hole/",
          "stickied": false,
          "url": "https://v.redd.it/e5xhwj4azrff1",
          "subreddit_subscribers": 506972,
          "created_utc": 1753778315,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e5xhwj4azrff1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 1280,
              "width": 718,
              "scrubber_media_url": "https://v.redd.it/e5xhwj4azrff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e5xhwj4azrff1/DASHPlaylist.mpd?a=1756470630%2CMGQ2NTI2ZjJjMWE0NDYyY2E4MTU3ZTE5MzBjMzYyOTM0ZmIyZDE5YzM5ZDdjZGVlNmQ1ODM5ZDQ1NTA4MGNmNw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/e5xhwj4azrff1/HLSPlaylist.m3u8?a=1756470630%2CZGYxZTZiMTRmZjEyMzYyZWYwODJmN2E1ZTlhYzE0N2E1OTU4YTJmYWU4MmNhOTIzZDU0MmNkN2U2NTU4YTllYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been working on \\`benchmax\\`, a open-source framework for building, running, and parallelizing environments, to fine-tune LLMs with reinforcement learning.\n\n[https://github.com/cgftinc/benchmax](https://github.com/cgftinc/benchmax)\n\nWhat I wanted to solve for:\n\n\\- Environments are tightly coupled with RL trainers, leading to fragmentation and limited compatibility.\n\n\\- These coupled environments are tend to be mostly competitive math and coding → for OSS RL + LLMs to scale, we need more complex, real-world environments.\n\n\\- Scaling these environments in parallel is still not easily possible\n\nWhat I'm excited about:\n\n\\- benchmax is training framework agnostic with adapters already built out for verl and verifiers. we’re gonna build more adapters for other frameworks (e.g. SkyRL, etc.), instead of forcing others to adopt our standard (though ofc they’re welcome to )\n\n\\- benchmax comes with a few interesting environments out of the box: spreadsheet processing, CRM, etc. → more coming soon!\n\n\\- benchmax supports MCP as a first class citizen. there has been an explosion of MCP servers/tools built out for usecases ranging from browser use to excel to game creation.\\`benchmax\\` allow folks to leverage and compose these existing MCP servers to build environments integrated with real world systems\n\n\\- Multi-node environment parallelization coming soon!\n\nIf you like what you see, feel free to \\**star\\** the \\**repo\\** to support the project!! Our hope’s to really let anyone *benchmax* on their tasks, with benchmax\n\n[https://github.com/cgftinc/benchmax](https://github.com/cgftinc/benchmax)\n\nIt’s still very early! And I expect to be shipping a lot more things → more environments, more trainer integrations. Would love y’all’s thoughts what environments and trainer integrations could be interesting!",
          "author_fullname": "t2_zcf29",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a new open-source RL environment framework for LLM finetuning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcmbfo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753820332,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been working on `benchmax`, a open-source framework for building, running, and parallelizing environments, to fine-tune LLMs with reinforcement learning.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/cgftinc/benchmax\"&gt;https://github.com/cgftinc/benchmax&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What I wanted to solve for:&lt;/p&gt;\n\n&lt;p&gt;- Environments are tightly coupled with RL trainers, leading to fragmentation and limited compatibility.&lt;/p&gt;\n\n&lt;p&gt;- These coupled environments are tend to be mostly competitive math and coding → for OSS RL + LLMs to scale, we need more complex, real-world environments.&lt;/p&gt;\n\n&lt;p&gt;- Scaling these environments in parallel is still not easily possible&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m excited about:&lt;/p&gt;\n\n&lt;p&gt;- benchmax is training framework agnostic with adapters already built out for verl and verifiers. we’re gonna build more adapters for other frameworks (e.g. SkyRL, etc.), instead of forcing others to adopt our standard (though ofc they’re welcome to )&lt;/p&gt;\n\n&lt;p&gt;- benchmax comes with a few interesting environments out of the box: spreadsheet processing, CRM, etc. → more coming soon!&lt;/p&gt;\n\n&lt;p&gt;- benchmax supports MCP as a first class citizen. there has been an explosion of MCP servers/tools built out for usecases ranging from browser use to excel to game creation.`benchmax` allow folks to leverage and compose these existing MCP servers to build environments integrated with real world systems&lt;/p&gt;\n\n&lt;p&gt;- Multi-node environment parallelization coming soon!&lt;/p&gt;\n\n&lt;p&gt;If you like what you see, feel free to *&lt;em&gt;star\\&lt;/em&gt;&lt;em&gt; the \\&lt;/em&gt;&lt;em&gt;repo\\&lt;/em&gt;&lt;em&gt; to support the project!! Our hope’s to really let anyone &lt;/em&gt;benchmax* on their tasks, with benchmax&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/cgftinc/benchmax\"&gt;https://github.com/cgftinc/benchmax&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It’s still very early! And I expect to be shipping a lot more things → more environments, more trainer integrations. Would love y’all’s thoughts what environments and trainer integrations could be interesting!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?auto=webp&amp;s=b2532fa7d6ca7adaf23e7e49bb1f9731f5ad2c9d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b11afd2318f8f7c3bc243369d43d504af4695dad",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=386395991e97d139a89c3be398b1c3a90db2e0eb",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5b47bd8f3893187f96559e8f5f93f5bf6f60c9c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c88e7a1888e152de7e918f397ece9c8f739fc7aa",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d96373b3618dc4b7d8726db5cdd257bb1aca809",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1761e8ea47f7f5404666f21ea4cb4b2c2fb073bd",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcmbfo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "girishkumama",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcmbfo/i_built_a_new_opensource_rl_environment_framework/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcmbfo/i_built_a_new_opensource_rl_environment_framework/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753820332,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone! 👋\n\nI'm exploring a novel concept in unsupervised neural machine translation and would love to get your feedback. I’m curious if this approach has been tested before—or if someone might be interested in giving it a try.\n\n**My idea in a nutshell:**\n\n- I train two simple decoder‑only models (transformers) **at the character level**, one on English, another on Ukrainian. No encoder, no shared latent space.\n- These two decoders are completely separate and independently trained as **language models**—each fluent in its own language.\n\nNow here’s the twist:\n\n- When we want to translate an English sentence, we feed it **as characters** into the English decoder.\n- We then extract its **inner hidden states** (or attention activations).\n- Those hidden states are passed directly into the Ukrainian decoder (as if they were input).\n- The Ukrainian decoder tries to generate an equivalent Ukrainian sentence.\n\n**No extra layers, no mapper—just latent states transferred from one decoder to the other.**\n\n---\n\n### Why I think it *could* work:\n\n1. **Natural language is built on statistical patterns.**  \n   At the character level, both languages contain frequent patterns—letter combinations, suffixes, morphology—that can be learned without semantic knowledge.\n\n2. **English and Ukrainian share some structural similarities** (SVO order, some grammatical forms). A decoder-only model trained character-wise can capture this statistical structure.\n\n3. Even if the language models don’t “understand” each other initially, they can potentially learn to interpret these latent signals through **cross‐language supervision**.\n\n---\n\n### Proposed training strategy:\n\n1. Pre-train `D_en` on English text and `D_uk` on Ukrainian text (character-level modeling).\n2. During translation training:\n   - Use an English sentence `sEn`.\n   - Feed it into `D_en`, capture hidden state matrix `H_en`.\n   - Input `H_en` (frame‑aligned) into `D_uk`, let it generate `sUk_pred`.\n   - Compute loss by comparing `sUk_pred` with the *true* Ukrainian translation `sUk`.\n3. Optionally add a cycle: sEn → D_en → H_en → D_uk → sUk_pred sUk_pred → D_uk → H_uk → D_en → sEn_restored\n\nand enforce reconstruction (cycle‑consistency loss).\n\n---\n\n### Challenges I’m concerned about:\n\n- Feeding hidden states from one decoder into another—how should they align?\n- Do hidden states carry enough semantic structure for the second decoder to make sense of them?\n- Would the English decoder still generate fluent English after learning to accept Ukrainian input?\n- Could training converge—or would this mutual mapping collapse?\n\n---\n\n### My constraints:\n\n- I don’t have access to GPUs or major compute resources 😅\n- I’d mainly like to get feedback, references, or see if anyone has tried something similar—or might be able to prototype this.\n\n---\n\n### Would love to hear:\n\n- If anyone has experimented with **decoder‑only cross‑communication**, especially at the hidden‐state level.\n- Ideas for alignment strategies between decoder hidden states.\n- Training tips: masking, attention mapping, loss design, etc.\n- Any known literature or codebases exploring similar minimal translation approaches.\n\nThanks for your time!  \n— **Buka Koshmarovich**",
          "author_fullname": "t2_rnaz4818s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Could two decoder‑only models communicate directly via latent outputs and translate each other?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcoou9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753825851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! 👋&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring a novel concept in unsupervised neural machine translation and would love to get your feedback. I’m curious if this approach has been tested before—or if someone might be interested in giving it a try.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My idea in a nutshell:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I train two simple decoder‑only models (transformers) &lt;strong&gt;at the character level&lt;/strong&gt;, one on English, another on Ukrainian. No encoder, no shared latent space.&lt;/li&gt;\n&lt;li&gt;These two decoders are completely separate and independently trained as &lt;strong&gt;language models&lt;/strong&gt;—each fluent in its own language.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now here’s the twist:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When we want to translate an English sentence, we feed it &lt;strong&gt;as characters&lt;/strong&gt; into the English decoder.&lt;/li&gt;\n&lt;li&gt;We then extract its &lt;strong&gt;inner hidden states&lt;/strong&gt; (or attention activations).&lt;/li&gt;\n&lt;li&gt;Those hidden states are passed directly into the Ukrainian decoder (as if they were input).&lt;/li&gt;\n&lt;li&gt;The Ukrainian decoder tries to generate an equivalent Ukrainian sentence.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;No extra layers, no mapper—just latent states transferred from one decoder to the other.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Why I think it &lt;em&gt;could&lt;/em&gt; work:&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Natural language is built on statistical patterns.&lt;/strong&gt;&lt;br/&gt;\nAt the character level, both languages contain frequent patterns—letter combinations, suffixes, morphology—that can be learned without semantic knowledge.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;English and Ukrainian share some structural similarities&lt;/strong&gt; (SVO order, some grammatical forms). A decoder-only model trained character-wise can capture this statistical structure.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Even if the language models don’t “understand” each other initially, they can potentially learn to interpret these latent signals through &lt;strong&gt;cross‐language supervision&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Proposed training strategy:&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Pre-train &lt;code&gt;D_en&lt;/code&gt; on English text and &lt;code&gt;D_uk&lt;/code&gt; on Ukrainian text (character-level modeling).&lt;/li&gt;\n&lt;li&gt;During translation training:\n\n&lt;ul&gt;\n&lt;li&gt;Use an English sentence &lt;code&gt;sEn&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Feed it into &lt;code&gt;D_en&lt;/code&gt;, capture hidden state matrix &lt;code&gt;H_en&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Input &lt;code&gt;H_en&lt;/code&gt; (frame‑aligned) into &lt;code&gt;D_uk&lt;/code&gt;, let it generate &lt;code&gt;sUk_pred&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Compute loss by comparing &lt;code&gt;sUk_pred&lt;/code&gt; with the &lt;em&gt;true&lt;/em&gt; Ukrainian translation &lt;code&gt;sUk&lt;/code&gt;.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Optionally add a cycle: sEn → D_en → H_en → D_uk → sUk_pred sUk_pred → D_uk → H_uk → D_en → sEn_restored&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;and enforce reconstruction (cycle‑consistency loss).&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Challenges I’m concerned about:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Feeding hidden states from one decoder into another—how should they align?&lt;/li&gt;\n&lt;li&gt;Do hidden states carry enough semantic structure for the second decoder to make sense of them?&lt;/li&gt;\n&lt;li&gt;Would the English decoder still generate fluent English after learning to accept Ukrainian input?&lt;/li&gt;\n&lt;li&gt;Could training converge—or would this mutual mapping collapse?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;My constraints:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I don’t have access to GPUs or major compute resources 😅&lt;/li&gt;\n&lt;li&gt;I’d mainly like to get feedback, references, or see if anyone has tried something similar—or might be able to prototype this.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Would love to hear:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If anyone has experimented with &lt;strong&gt;decoder‑only cross‑communication&lt;/strong&gt;, especially at the hidden‐state level.&lt;/li&gt;\n&lt;li&gt;Ideas for alignment strategies between decoder hidden states.&lt;/li&gt;\n&lt;li&gt;Training tips: masking, attention mapping, loss design, etc.&lt;/li&gt;\n&lt;li&gt;Any known literature or codebases exploring similar minimal translation approaches.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for your time!&lt;br/&gt;\n— &lt;strong&gt;Buka Koshmarovich&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcoou9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "According_Change2007",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcoou9/could_two_decoderonly_models_communicate_directly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcoou9/could_two_decoderonly_models_communicate_directly/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753825851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all,\n\nI am a novice vibe coder. I was deeply interested in running a Bitnet model over the web. Thus I vibe coded a kernel and a conversion script for Bitnet 1.58 bit. \n\nThe example I used to give it a try was WebGPU_Chat (see examples folder)\n\nhttps://github.com/nimishchaudhari/bitnet_transformers.js/pull/1\n\nI am looking for reviews of people capable of understanding things under the hood, and looking for contributors as well for this purpose.  \n\nThanks in advance for your time and attention :)",
          "author_fullname": "t2_871guq2y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Review request on Bitnet implementation on transformer.js",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcif2t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753811588,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am a novice vibe coder. I was deeply interested in running a Bitnet model over the web. Thus I vibe coded a kernel and a conversion script for Bitnet 1.58 bit. &lt;/p&gt;\n\n&lt;p&gt;The example I used to give it a try was WebGPU_Chat (see examples folder)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/nimishchaudhari/bitnet_transformers.js/pull/1\"&gt;https://github.com/nimishchaudhari/bitnet_transformers.js/pull/1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am looking for reviews of people capable of understanding things under the hood, and looking for contributors as well for this purpose.  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your time and attention :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?auto=webp&amp;s=793ed225c6021b81349ae8efe47b5aa798da8f21",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=600a55db6af30e74eee873e1ac9f6fe21ae0f46d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1c0d4d82fc391c9e4971c922ffaf7f6c36bfd51",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff45b5974157505593476dbae1023f353acfdd17",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6549a80679aa33c8520c4dbc11379199a047e808",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ae11b2ae1380b9267c9576de7fd6c6b2b7407f4e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=87a7eb66e5a41099a27928c1190384e0f1df18fc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcif2t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ScoreUnique",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcif2t/review_request_on_bitnet_implementation_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcif2t/review_request_on_bitnet_implementation_on/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753811588,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We put together a small repo to fine‑tune **Mistral’s Voxtral (3B)** for **transcription** using Huggingface**.** We could not find a public finetuning/ training script yet, so we think this could be interesting for the community.",
          "author_fullname": "t2_1ujlvp0cn8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finetuning Script for Voxtral",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5gv1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4dc0e71933a6538903c2ef9dc0036f8bd6a8fda2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753775738,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We put together a small repo to fine‑tune &lt;strong&gt;Mistral’s Voxtral (3B)&lt;/strong&gt; for &lt;strong&gt;transcription&lt;/strong&gt; using Huggingface&lt;strong&gt;.&lt;/strong&gt; We could not find a public finetuning/ training script yet, so we think this could be interesting for the community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/Innovative-Digitale-Medizin-IDM/voxtral-finetune",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?auto=webp&amp;s=834d343f2b6c42de29b825f4bdecbe668798481b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fa9e2af93fe23af7ed5ae8ef0282b5932cf5efa",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d90e7322acc87ab0f6078ff5baa320612813f4c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fd2e07935dfb13e8c24f66136ca02893cd3bf41",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d821b402151de285d39de64aaea0364ad627ae9",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b86b9f72b4e9b269f0d8aea81947c8cbf95b360",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efc53978201c483ccf78407c682b2a9b164dff7a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc5gv1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistributionLucky763",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5gv1/finetuning_script_for_voxtral/",
          "stickied": false,
          "url": "https://github.com/Innovative-Digitale-Medizin-IDM/voxtral-finetune",
          "subreddit_subscribers": 506972,
          "created_utc": 1753775738,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just tired of finding...hard to make sure the whether they suit for me demand. I want to know if anyone has arranged some for reference?",
          "author_fullname": "t2_ap6wb4yp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone knows where can I find the latest NVIDIA TPU test for the total throughput tokens for any size model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcva93",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753843784,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just tired of finding...hard to make sure the whether they suit for me demand. I want to know if anyone has arranged some for reference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcva93",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable_Yak4499",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcva93/anyone_knows_where_can_i_find_the_latest_nvidia/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcva93/anyone_knows_where_can_i_find_the_latest_nvidia/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753843784,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air — our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.\n\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering: thinking mode for complex reasoning and tool using, and non-thinking mode for instant responses. They are available on Z.ai, BigModel.cn and open-weights are avaiable at HuggingFace and ModelScope.\n\nBlog post: https://z.ai/blog/glm-4.5\n\nHugging Face:\n\nhttps://huggingface.co/zai-org/GLM-4.5\n\nhttps://huggingface.co/zai-org/GLM-4.5-Air\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "GLM4.5 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 49,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8vj06dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 65,
                  "x": 108,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a349d48e9cd6992d12bf44790c6309160113f79e"
                },
                {
                  "y": 131,
                  "x": 216,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=73752918b2bc0fff299a174f29082b613fabbdf4"
                },
                {
                  "y": 194,
                  "x": 320,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5729512dc28e8eb1a9fe30b26bf27ca9ea7b250"
                },
                {
                  "y": 388,
                  "x": 640,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2603f38cd5baccb7e6ce503c3d75c02cf593ff2e"
                },
                {
                  "y": 583,
                  "x": 960,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6c0169470ea43f39512ced287f445f51572204f"
                },
                {
                  "y": 656,
                  "x": 1080,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7943df8456b9ef19de5d523579883749691f9136"
                }
              ],
              "s": {
                "y": 2184,
                "x": 3595,
                "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=3595&amp;format=pjpg&amp;auto=webp&amp;s=617c22698deba6f1ec84e912a6152e0bf8cc2c43"
              },
              "id": "8vj06dj29mff1"
            },
            "sic55dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5066e5731ecfa48a00570e23df8edde90b106d78"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=425338994f38f791143123c3ccd0bd6dff1fffa0"
                },
                {
                  "y": 219,
                  "x": 320,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b63d99537141327e562a8b7b9e2a19c76bd5bb0e"
                },
                {
                  "y": 439,
                  "x": 640,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb0620fa3958277ea8ded0e5030d944031bc4c1f"
                },
                {
                  "y": 659,
                  "x": 960,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e90d0f6303b9ed87b916437e6af38bd5157d1fe"
                },
                {
                  "y": 741,
                  "x": 1080,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4614dc15650da1554832d1053756737fceaf61b6"
                }
              ],
              "s": {
                "y": 3066,
                "x": 4464,
                "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=4464&amp;format=pjpg&amp;auto=webp&amp;s=062a25fee3fd1a05602c971ac17fe32ddb42908f"
              },
              "id": "sic55dj29mff1"
            },
            "zxji6dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f217a1f52fcc37d5a771756cef275c15abefaa6"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=051f04a39bc5876b3d17431f6b7e5c3f0a9c9d15"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f23c3daddfda3df2605c7695d19f4cb6c84cd893"
                },
                {
                  "y": 282,
                  "x": 640,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=20ae7180cf446493c58d48b15ed48b09ea21662b"
                },
                {
                  "y": 423,
                  "x": 960,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbdc33ba0f35c23357fe05c1defe48204da395bd"
                },
                {
                  "y": 476,
                  "x": 1080,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb0f5b5d1d9fa1d9f0c1fae05ae3893017e31aa3"
                }
              ],
              "s": {
                "y": 1751,
                "x": 3967,
                "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=b54eef388d38a731b31e7d321eb74d970359f078"
              },
              "id": "zxji6dj29mff1"
            },
            "so54saj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa12f06ea18792df6395a36df2034a07e9fb9c1b"
                },
                {
                  "y": 80,
                  "x": 216,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=21276716941d44fe8837a0733b8a2b7ada3d83bb"
                },
                {
                  "y": 119,
                  "x": 320,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=176736d03fb987bf32971415a15bcc407f8e86ca"
                },
                {
                  "y": 238,
                  "x": 640,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=59a4cef468482a7827855c8b1419a3416114f00e"
                },
                {
                  "y": 358,
                  "x": 960,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=047f8fba92553d37a18d1127b773ec4b7252e7bb"
                },
                {
                  "y": 402,
                  "x": 1080,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f5cda44f1133c4c9ada7f35e5c6b95d0bafd603"
                }
              ],
              "s": {
                "y": 1480,
                "x": 3967,
                "u": "https://preview.redd.it/so54saj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=9c5d62f989f08c491a09d379cff3146b4f6fe82e"
              },
              "id": "so54saj29mff1"
            },
            "si9mcbj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=697ca4a04df9e477ad9098280eacd7fec6f4900a"
                },
                {
                  "y": 76,
                  "x": 216,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e493e44281d418507c54bdeedfacd801175f7756"
                },
                {
                  "y": 112,
                  "x": 320,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0ca18380db9f007c16cf33a6bf26a43a547c12d"
                },
                {
                  "y": 225,
                  "x": 640,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2aa202550f1ef5459d04687418a15de4bb01273a"
                },
                {
                  "y": 338,
                  "x": 960,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=450547d9b2bfd20ca0863f123da3268d32b05be2"
                },
                {
                  "y": 380,
                  "x": 1080,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c877bf36109aedf39163aaeac55cdaa55ce5831c"
                }
              ],
              "s": {
                "y": 1397,
                "x": 3967,
                "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=33148e1e31a9f6d83cd1d58997d574a05eed2453"
              },
              "id": "si9mcbj29mff1"
            }
          },
          "name": "t3_1mbg1ck",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 964,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "si9mcbj29mff1",
                "id": 715840784
              },
              {
                "media_id": "sic55dj29mff1",
                "id": 715840785
              },
              {
                "media_id": "so54saj29mff1",
                "id": 715840786
              },
              {
                "media_id": "8vj06dj29mff1",
                "id": 715840787
              },
              {
                "media_id": "zxji6dj29mff1",
                "id": 715840788
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 964,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/h1a9hbYRlufo6ZLB7b1IgSekwr0g4qcrXjR2rdPGMPU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753708945,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air — our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.&lt;/p&gt;\n\n&lt;p&gt;Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering: thinking mode for complex reasoning and tool using, and non-thinking mode for instant responses. They are available on Z.ai, BigModel.cn and open-weights are avaiable at HuggingFace and ModelScope.&lt;/p&gt;\n\n&lt;p&gt;Blog post: &lt;a href=\"https://z.ai/blog/glm-4.5\"&gt;https://z.ai/blog/glm-4.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hugging Face:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mbg1ck",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbg1ck",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 241,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbg1ck/glm45_released/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mbg1ck",
          "subreddit_subscribers": 506972,
          "created_utc": 1753708945,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!  \nI’m thrilled to share a project I’ve been pouring my energy into: CloudToLocalLLM. Built with Flutter and Dart, it’s a tool that connects local Large Language Models (LLMs) to cloud services, blending privacy, offline capabilities, and cross-platform support. It’s in alpha, and I’m excited to give you a peek at what it’s all about!What’s CloudToLocalLLM?CloudToLocalLLM lets you run LLMs on your own hardware for privacy and offline use, while seamlessly hooking up to cloud APIs for extra functionality when you need it. It’s all about giving you control over your AI workflows, whether you’re on desktop now or mobile in the future.Key Features:\n\n* Local LLM Processing: Run models on-device to keep your data private.\n* Offline Support: Works smoothly without an internet connection.\n* Cloud Integration: Connects to cloud APIs for added power.\n* Cross-Platform: Desktop support now, with Android/iOS in development.\n* Future Plans: Premium features and plugin/extension support for custom setups.\n\nTech Stack:\n\n* Flutter and Dart for the UI and cross-platform foundation.\n* LLM libraries for local model processing.\n* Cloud APIs for external service integration.\n* Tunneling setup for secure local-to-cloud communication.\n\nCurrent Status:The project is in alpha with a solid foundation for local LLM processing and cloud syncing. I’m currently refining the tunneling setup to ensure smooth data flow between local models and cloud services. Mobile support for Android and iOS is on the way, along with plans for premium features and a plugin/extension system to make it highly extensible.Take a look at the project on [GitHub](https://github.com/imrightguy/CloudToLocalLLM) for more details. Hope you find it as exciting as I do—happy to share this with the community!",
          "author_fullname": "t2_1m04ysbxj2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CloudToLocalLLM - A Flutter-built Tool for Local LLM and Cloud Integration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcq5tj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753829541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;br/&gt;\nI’m thrilled to share a project I’ve been pouring my energy into: CloudToLocalLLM. Built with Flutter and Dart, it’s a tool that connects local Large Language Models (LLMs) to cloud services, blending privacy, offline capabilities, and cross-platform support. It’s in alpha, and I’m excited to give you a peek at what it’s all about!What’s CloudToLocalLLM?CloudToLocalLLM lets you run LLMs on your own hardware for privacy and offline use, while seamlessly hooking up to cloud APIs for extra functionality when you need it. It’s all about giving you control over your AI workflows, whether you’re on desktop now or mobile in the future.Key Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Local LLM Processing: Run models on-device to keep your data private.&lt;/li&gt;\n&lt;li&gt;Offline Support: Works smoothly without an internet connection.&lt;/li&gt;\n&lt;li&gt;Cloud Integration: Connects to cloud APIs for added power.&lt;/li&gt;\n&lt;li&gt;Cross-Platform: Desktop support now, with Android/iOS in development.&lt;/li&gt;\n&lt;li&gt;Future Plans: Premium features and plugin/extension support for custom setups.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tech Stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Flutter and Dart for the UI and cross-platform foundation.&lt;/li&gt;\n&lt;li&gt;LLM libraries for local model processing.&lt;/li&gt;\n&lt;li&gt;Cloud APIs for external service integration.&lt;/li&gt;\n&lt;li&gt;Tunneling setup for secure local-to-cloud communication.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Current Status:The project is in alpha with a solid foundation for local LLM processing and cloud syncing. I’m currently refining the tunneling setup to ensure smooth data flow between local models and cloud services. Mobile support for Android and iOS is on the way, along with plans for premium features and a plugin/extension system to make it highly extensible.Take a look at the project on &lt;a href=\"https://github.com/imrightguy/CloudToLocalLLM\"&gt;GitHub&lt;/a&gt; for more details. Hope you find it as exciting as I do—happy to share this with the community!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?auto=webp&amp;s=55cc3598b62dd189b9301971fa3f0072aeeb4271",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=be5429a462c6432b21f002208275d58e14f8b9e1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4080be59a4f438d917478f7b77dffe93a39d950",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=80dbf6ca80181eaf9ae95db9fd1247b36ce4cb60",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f58a1a23784b73314db3bb7695b9cf6a4291de5f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a94965b4db927c5db2b338fa78c7c6a09934ee03",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1dedb8e88059caaffc11bf5bbce338b323611439",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcq5tj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_right_guy",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcq5tj/cloudtolocalllm_a_flutterbuilt_tool_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcq5tj/cloudtolocalllm_a_flutterbuilt_tool_for_local_llm/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753829541,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CORSAIR Unveils AI Workstation 300, Starting At $1599, Boasting Ryzen AI Max+ 395 Processor And Up To 128 GB LPDDR5X Memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcpxr4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=573f3b0817e015efd774edd561e17b4072b1594d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753828960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/corsair-unveils-ai-workstation-300-starting-at-1599-boasting-ryzen-ai-max-395-processor-and-up-to-128-gb-lpddr5x-memory/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?auto=webp&amp;s=9ae182584158deb74cf260403d2a1843e2bda986",
                  "width": 2560,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=573531f4c8a5371bd4715614521a58d2bd7d8502",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c940ac2aebd959c0b9843c83cedc2ddf6d963e9",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7db7ea540c8068b7c2c6680b1622f90b68bc81f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=607fa3aabb7b010ac2df88acfaf5db548da8d894",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6d9f42998d1ff632e2997fdd2af1af0357afac4",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7b189cb8974cf4009d9f446221e7e38a2b1d194",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcpxr4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcpxr4/corsair_unveils_ai_workstation_300_starting_at/",
          "stickied": false,
          "url": "https://wccftech.com/corsair-unveils-ai-workstation-300-starting-at-1599-boasting-ryzen-ai-max-395-processor-and-up-to-128-gb-lpddr5x-memory/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753828960,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Now I got A LOT of messages when I first showed it off so I decided to spend some time to put together a full video on the high level designs behind it and also why I did it in the first place - [https://www.youtube.com/watch?v=bE2kRmXMF0I](https://www.youtube.com/watch?v=bE2kRmXMF0I)\n\nI’ve also open sourced my short / long term memory designs, vocal daisy chaining and also my docker compose stack. This should help let a lot of people get up and running! [https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main](https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main)\n\n",
          "author_fullname": "t2_5hq9z0rq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "So you all loved my open-source voice AI when I first showed it off - I officially got response times to under 2 seconds AND it now fits all within 9 gigs of VRAM! Open Source Code included!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbt030",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 211,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/qvwxsxvrnoff1/DASHPlaylist.mpd?a=1756470630%2CZDhlYjljYTJhNDU0MWExNDZlOTE0M2U2MmRmMThlMWM0NDQwODUwMGNiZTM5NTcxYTlhMjQxNGM2MGJkMWM1Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 133,
              "hls_url": "https://v.redd.it/qvwxsxvrnoff1/HLSPlaylist.m3u8?a=1756470630%2CMjJhMjdkODMzNDkyYjA1YTdiN2I3NjdjOGUxYTE0MzJkZDc3MWZkMDk3OTQ0YWRhYTM3ZDhjZDE2ZTg4OTRjMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 211,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=458324ebc27e4d222e12db9105ee63a57169ea8a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753738197,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now I got A LOT of messages when I first showed it off so I decided to spend some time to put together a full video on the high level designs behind it and also why I did it in the first place - &lt;a href=\"https://www.youtube.com/watch?v=bE2kRmXMF0I\"&gt;https://www.youtube.com/watch?v=bE2kRmXMF0I&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I’ve also open sourced my short / long term memory designs, vocal daisy chaining and also my docker compose stack. This should help let a lot of people get up and running! &lt;a href=\"https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main\"&gt;https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/qvwxsxvrnoff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?format=pjpg&amp;auto=webp&amp;s=a704ca2dfbd867dab765a160c801daae8721c588",
                  "width": 3840,
                  "height": 2160
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e7711e20c3668e7de723d1329e83672e0f85a8d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fccc9e037a7b68306e5750c6de88d439e6ebf2fc",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fb41192fef7650feca72355d425bc4a2d7a4cf4f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e19a054f961b9571bcd9facb6eebd636cabc95aa",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6724a63ca00efb55331f5660e90d3c36d5b079fb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b936a3ab0d20f9dcc4a703e84f989d9fce27b4ae",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbt030",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RoyalCities",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbt030/so_you_all_loved_my_opensource_voice_ai_when_i/",
          "stickied": false,
          "url": "https://v.redd.it/qvwxsxvrnoff1",
          "subreddit_subscribers": 506972,
          "created_utc": 1753738197,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/qvwxsxvrnoff1/DASHPlaylist.mpd?a=1756470630%2CZDhlYjljYTJhNDU0MWExNDZlOTE0M2U2MmRmMThlMWM0NDQwODUwMGNiZTM5NTcxYTlhMjQxNGM2MGJkMWM1Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 133,
              "hls_url": "https://v.redd.it/qvwxsxvrnoff1/HLSPlaylist.m3u8?a=1756470630%2CMjJhMjdkODMzNDkyYjA1YTdiN2I3NjdjOGUxYTE0MzJkZDc3MWZkMDk3OTQ0YWRhYTM3ZDhjZDE2ZTg4OTRjMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🤣 i have tons of llama car air freshener",
          "author_fullname": "t2_1lil9j2g3r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "No stress",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcgpno",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Fv_xaJXWwqYF4ITm8wg3a9VDDpidmhH4hn8HQR6zZL8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753807879,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🤣 i have tons of llama car air freshener&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/trr3maw8fuff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?auto=webp&amp;s=3dc8da55e2e040a1b5b2e905cb114d7c230cca0d",
                  "width": 1816,
                  "height": 4032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed7d794028ad2a37a933f6e62c1a983457bdab7e",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e34d42b5fea6014c3a2dfc1e2a5a7e624da7668",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b89643d5b8a888f5acc502a7d80c285ff8fb130",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85dcfc840382afcd2ff1941abae20da484423254",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=df8f2f5309f545555a1a77c16ae0c7aebfe6f7eb",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47f1ac2e546fa7f0e0abab100e65a58d0859c9ac",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "vv6tjmWKvcypi4-H9Cnqxzil-xYGEYr6vmbyGhqD8Zo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcgpno",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "troughtspace",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcgpno/no_stress/",
          "stickied": false,
          "url": "https://i.redd.it/trr3maw8fuff1.jpeg",
          "subreddit_subscribers": 506972,
          "created_utc": 1753807879,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!\n\nSo i need help with running the gguf files\nI am using LM Studio and everything is ok.\n\nI have 2 GPU and i want to test out Tensor Parallelism so i can get more speed, but i am facing some issues so i had some questions\n\nIs TP with GGUF even possible? And if yes what backend to use?\nI tried it with Vllm and i got all kinds of error so i dont know what did i do wrong.\n\nAny help is appreciated ",
          "author_fullname": "t2_clhgguip",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running GGUF models with TP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcl17g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753817429,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;So i need help with running the gguf files\nI am using LM Studio and everything is ok.&lt;/p&gt;\n\n&lt;p&gt;I have 2 GPU and i want to test out Tensor Parallelism so i can get more speed, but i am facing some issues so i had some questions&lt;/p&gt;\n\n&lt;p&gt;Is TP with GGUF even possible? And if yes what backend to use?\nI tried it with Vllm and i got all kinds of error so i dont know what did i do wrong.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcl17g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Physical-Citron5153",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753817429,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I sell plumbing parts and need a way to quickly build large quotes in a short amount of time. I have a parts list in excel form that has clean descriptions and pricing of the parts I sell.\nCan i teach an AI model my parts list so I can just paste a customer's request list and it give me all the pricing for these parts?\n\nI have installed ollama with mistral 7b on my PC. Unfortunately I have no idea what the next steps are or the best way to go about this.\nAny advice? Thank you in advance! ",
          "author_fullname": "t2_e9pb4j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying to build a quoting tool",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcsh69",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753835782,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I sell plumbing parts and need a way to quickly build large quotes in a short amount of time. I have a parts list in excel form that has clean descriptions and pricing of the parts I sell.\nCan i teach an AI model my parts list so I can just paste a customer&amp;#39;s request list and it give me all the pricing for these parts?&lt;/p&gt;\n\n&lt;p&gt;I have installed ollama with mistral 7b on my PC. Unfortunately I have no idea what the next steps are or the best way to go about this.\nAny advice? Thank you in advance! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcsh69",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SilverEntrepreneur",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcsh69/trying_to_build_a_quoting_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcsh69/trying_to_build_a_quoting_tool/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753835782,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "is there any nsfw model that i can run",
          "author_fullname": "t2_bvuxc0x7v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "i got this. I'm new to AI stuff — is there any model I can run, and how",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md2ul2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ba7nJCZv-NscM1Sikex67TfP1ESFVek0vzazj1rir7w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753870798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is there any nsfw model that i can run&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/7r7fn039mzff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/7r7fn039mzff1.png?auto=webp&amp;s=79aba2b6565dcf65339e3819729c61d5f889b620",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/7r7fn039mzff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf77fc960a09cf56b765ba88146d1ed73de953da",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/7r7fn039mzff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2dea139c88d07a64b4f9d9af80e391dca2b0cfdf",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/7r7fn039mzff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe41f3a864e2146766f02e2add80a38802a6188e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/7r7fn039mzff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c56630cdc268157975f1f38ab80643a5fff4a71",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/7r7fn039mzff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef31d24234608453e30edabb25575d7ba9c768ce",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/7r7fn039mzff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a095f4404db0ba94e6decddda084029dfdb5fd5",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "WkVmIENDn50prK8rRZDDWtrbAS01-zdrnKx5j5Jx23g"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1md2ul2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "suplexcity_16",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md2ul2/i_got_this_im_new_to_ai_stuff_is_there_any_model/",
          "stickied": false,
          "url": "https://i.redd.it/7r7fn039mzff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753870798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4ou3rslj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan 2.2 is Live! Needs only 8GB of VRAM!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbfa3y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 595,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 595,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/6wb7Yp5vFjJtIwYWiSu02kTzdKI2obJq-EU5BTqMluI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753706991,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w2tqvij93mff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?auto=webp&amp;s=ee8cf1cb47816005e468b585d65be4de071b650f",
                  "width": 1319,
                  "height": 742
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9031e98c6b58f202a2505062878cd736f6658e48",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=784f151b8ee95ef486eb0b1a1e3bfd596879c0da",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f13b863800706917bf97e7c24c56acbf283df8fb",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aa487bb7dc2bff5b7326e25dfec4967cd6c8e51",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6d55de941bc4cf7f377686f9f3cd96fecc135c0",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b2119ea624eb2d2e46581f52916eefe02b8e10a",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "HCLdmR0umnU9RikapDseAAP7EInXhkRnH1_er5o1Ohc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbfa3y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Comed_Ai_n",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/",
          "stickied": false,
          "url": "https://i.redd.it/w2tqvij93mff1.jpeg",
          "subreddit_subscribers": 506972,
          "created_utc": 1753706991,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to fine tune orpheus but the only audios I have are at least 30 minutes long each, but orpheus worsk best with 5-15 seconds datasets, so how do I turn that 30 minutes video into multiple shorter videos while also preparing the transcript for each one of them?",
          "author_fullname": "t2_48vjfixh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I chunk down a long video to prepare as dataset for fine-tunining a TTS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mccxrt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753799323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to fine tune orpheus but the only audios I have are at least 30 minutes long each, but orpheus worsk best with 5-15 seconds datasets, so how do I turn that 30 minutes video into multiple shorter videos while also preparing the transcript for each one of them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mccxrt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatIsNotIllegal",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mccxrt/how_do_i_chunk_down_a_long_video_to_prepare_as/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mccxrt/how_do_i_chunk_down_a_long_video_to_prepare_as/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753799323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey! I've been using ollama models locally across my devices for a few months now. Particularly on my M2 Mac mini - although it's the base model with only 8GB of RAM. I've been using ollama since they provide an easy-to-use web interface to see the models, quickly download them, and run them, but also many other apps/clients for LLMs support it.\n\nHowever, recently I've seen stuff like MLX-LM and llama-cpp (?) that are supposedly quicker than Ollama. Not too sure on the details, but I think I get a grasp, just that the models are architecturally different? \n\nAnyways, I'd appreciate some help to get the most out of my low-end hardware? as I mentioned above I have that Mac, but also this laptop with 16GB of RAM and some crappy CPU (&amp; integrated GPU).\n\n[My laptop specs after running Neofetch on Nobara linux.](https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;format=png&amp;auto=webp&amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111)\n\n  \n\n\nI've looked around HuggingFace before, but found the UI very confusing lol. \n\nAppreciate any help!",
          "author_fullname": "t2_1lavzg2ok9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mediocre local LLM user -- tips?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 32,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kfs4he9t5vff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 25,
                  "x": 108,
                  "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=26bc242a070779a5a8a624284d0cc720b0397d13"
                },
                {
                  "y": 50,
                  "x": 216,
                  "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f73808e9ed0bd733c2c1656c5053a189a08b824c"
                },
                {
                  "y": 74,
                  "x": 320,
                  "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3849fd1e8d5c94f4cd0c9a7f6f63d361fe151f4"
                }
              ],
              "s": {
                "y": 118,
                "x": 507,
                "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;format=png&amp;auto=webp&amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111"
              },
              "id": "kfs4he9t5vff1"
            }
          },
          "name": "t3_1mckrn1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Lj_WeVpZphKOlxbkqLqEF6lQTf5fzZRORkn5Z1CRnwY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753816824,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey! I&amp;#39;ve been using ollama models locally across my devices for a few months now. Particularly on my M2 Mac mini - although it&amp;#39;s the base model with only 8GB of RAM. I&amp;#39;ve been using ollama since they provide an easy-to-use web interface to see the models, quickly download them, and run them, but also many other apps/clients for LLMs support it.&lt;/p&gt;\n\n&lt;p&gt;However, recently I&amp;#39;ve seen stuff like MLX-LM and llama-cpp (?) that are supposedly quicker than Ollama. Not too sure on the details, but I think I get a grasp, just that the models are architecturally different? &lt;/p&gt;\n\n&lt;p&gt;Anyways, I&amp;#39;d appreciate some help to get the most out of my low-end hardware? as I mentioned above I have that Mac, but also this laptop with 16GB of RAM and some crappy CPU (&amp;amp; integrated GPU).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111\"&gt;My laptop specs after running Neofetch on Nobara linux.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked around HuggingFace before, but found the UI very confusing lol. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mckrn1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Junior-Ad-2186",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753816824,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Open-weight ASR models have gotten super competitive with proprietary providers (eg deepgram, assemblyai) in recent months. On some leaderboards like [HuggingFace's ASR leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) they're posting up crazy WER and RTFx numbers. Parakeet in particular claims to process 3000+ minutes of audio in less than a minute, which means you can save a lot of money if you self-host.\n\n  \nWe at Modal benchmarked cost, throughput, and accuracy of the latest ASR models against a popular proprietary model: https://modal.com/blog/fast-cheap-batch-transcription. We also wrote up a bunch of engineering tips on how to best optimize a batch transcription service for max throughput. If you're currently using either open source or proprietary ASR models would love to know what you think!\n\n",
          "author_fullname": "t2_9av3t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "100x faster and 100x cheaper transcription with open models vs proprietary",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbny6o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 199,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 199,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753726776,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Open-weight ASR models have gotten super competitive with proprietary providers (eg deepgram, assemblyai) in recent months. On some leaderboards like &lt;a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\"&gt;HuggingFace&amp;#39;s ASR leaderboard&lt;/a&gt; they&amp;#39;re posting up crazy WER and RTFx numbers. Parakeet in particular claims to process 3000+ minutes of audio in less than a minute, which means you can save a lot of money if you self-host.&lt;/p&gt;\n\n&lt;p&gt;We at Modal benchmarked cost, throughput, and accuracy of the latest ASR models against a popular proprietary model: &lt;a href=\"https://modal.com/blog/fast-cheap-batch-transcription\"&gt;https://modal.com/blog/fast-cheap-batch-transcription&lt;/a&gt;. We also wrote up a bunch of engineering tips on how to best optimize a batch transcription service for max throughput. If you&amp;#39;re currently using either open source or proprietary ASR models would love to know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?auto=webp&amp;s=5d50101d8f829bae3e80210dd24c9cec4945b73a",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7b3ca3434ee071ef54d6732c5c74bfa108f1d0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=67d797d2b0027d437608e2b7f05400e7d13174be",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a861401db89b005f80687bfd9b892a15fbfaa93",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f044c5ce6e1272f48454e18fe9e5da33997bf960",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=796a87cb7f77e71e6cbed4cde2c3f280d6c48829",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92ac60775c9064c7c4267f0102f80e834c10948b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbny6o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crookedstairs",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753726776,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im using Unsloth Quant (3B) of the new Qwen-30B (2507) on LocalAI (tested it with the included webinterface-chat) and it works, but I allways get the answer twice. Can you please give me a hint what's the problem here?\nTemperature anf other settings as suggested at the HF repo.",
          "author_fullname": "t2_9kn8k4e4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Newest Qwrn 30B double answers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 115,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md0ejq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/d3hPLvb_ES1XJh906gDQJ4Yi3xGik_LmxIvGDuVlnUQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753861371,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im using Unsloth Quant (3B) of the new Qwen-30B (2507) on LocalAI (tested it with the included webinterface-chat) and it works, but I allways get the answer twice. Can you please give me a hint what&amp;#39;s the problem here?\nTemperature anf other settings as suggested at the HF repo.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/iijd1ldbuyff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/iijd1ldbuyff1.png?auto=webp&amp;s=778c17774d54fee332d7ae4db8fde95ec0533556",
                  "width": 1080,
                  "height": 892
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/iijd1ldbuyff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc4b0afefa9a09168e1181d277d72e9b15532704",
                    "width": 108,
                    "height": 89
                  },
                  {
                    "url": "https://preview.redd.it/iijd1ldbuyff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fafb48e33656be6d0f3dc456191ae8144144745d",
                    "width": 216,
                    "height": 178
                  },
                  {
                    "url": "https://preview.redd.it/iijd1ldbuyff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8a25a087b842d3864835b2c6b6937d28661c4da",
                    "width": 320,
                    "height": 264
                  },
                  {
                    "url": "https://preview.redd.it/iijd1ldbuyff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=23c0ae043cfed697afa1ed85206d5c52eabd313a",
                    "width": 640,
                    "height": 528
                  },
                  {
                    "url": "https://preview.redd.it/iijd1ldbuyff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f4eaae18b2e3e922c45eda4e12cc146436ef3ea4",
                    "width": 960,
                    "height": 792
                  },
                  {
                    "url": "https://preview.redd.it/iijd1ldbuyff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05e8c5b6fd4266fc9a730ce28ed7a19f975911e1",
                    "width": 1080,
                    "height": 892
                  }
                ],
                "variants": {},
                "id": "CGFcld4Vhc5skvzK4CNxbS7XUoopaFzWC48vEggp9FU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1md0ejq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Old-Cardiologist-633",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md0ejq/newest_qwrn_30b_double_answers/",
          "stickied": false,
          "url": "https://i.redd.it/iijd1ldbuyff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753861371,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://arxiv.org/abs/2507.20984](https://arxiv.org/abs/2507.20984)\n\n**SmallThinker** is a family of on-device native **Mixture-of-Experts** language models specifically designed for efficient local deployment.  With the constraints of limited computational power and memory capacity in mind, SmallThinker introduces novel architectural innovations to enable high-performance inference on consumer-grade hardware.\n\nEven on a personal computer equipped with only 8GB of CPU memory, SmallThinker achieves a remarkable inference speed of **20 tokens per second** when powered by [PowerInfer](https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker)\n\nNotably, **SmallThinker** is now supported in **llama.cpp**, making it even more accessible for everyone who want to run advanced MoE models entirely offline and locally.\n\n\n\nhttps://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;format=png&amp;auto=webp&amp;s=d014c217defcd629cbb8684dc891878d2895c28b\n\nAnd here is the downstream benchmark performance compare to other SOTA LLMs.\n\nhttps://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;format=png&amp;auto=webp&amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf\n\nAnd the GGUF link is here:\n\n[PowerInfer/SmallThinker-21BA3B-Instruct-GGUF · Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF)\n\n[PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF · Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF)",
          "author_fullname": "t2_s05p1qg4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmallThinker Technical Report Release!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 65,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "m5vbkud89qff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c932172d474d64f53b1c183c3158d63819e7dd1"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb0c7dda219290760f59b222d805ad327b3534d7"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29a83153cdfa17c52a31a9ea0c17b5646cdc9145"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=228e39bea9e0d0bb6c950b4d12e940f2a313db1c"
                },
                {
                  "y": 448,
                  "x": 960,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f34de7a4ec519d836060dfaf8ff7056a209173"
                },
                {
                  "y": 504,
                  "x": 1080,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e06afbc504d247f77732dd28fabe34402bcec63"
                }
              ],
              "s": {
                "y": 646,
                "x": 1382,
                "u": "https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;format=png&amp;auto=webp&amp;s=d014c217defcd629cbb8684dc891878d2895c28b"
              },
              "id": "m5vbkud89qff1"
            },
            "2zk0d3sqbqff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0ffa126ccf8fbe0beb24c6090b1763a5a7a7222"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f250c3501e2d2066624356e94c3976200aa629c9"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ae16445ad433c0ead9ef47c474be0ecadee2bb"
                },
                {
                  "y": 343,
                  "x": 640,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f325d98e1cc1b5cb4bbd02da5e1a005319b57a37"
                },
                {
                  "y": 515,
                  "x": 960,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16d25b2a53244d7d2f5aca474eb1076424d90918"
                },
                {
                  "y": 579,
                  "x": 1080,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09f24f32012f06d40275d312a6327f1d86691a4d"
                }
              ],
              "s": {
                "y": 830,
                "x": 1546,
                "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;format=png&amp;auto=webp&amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf"
              },
              "id": "2zk0d3sqbqff1"
            }
          },
          "name": "t3_1mc0m3e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DPYXkXYKiJVkQ40-jlvcuMdmOUBGPiWDPqFYKHNtroQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753758732,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2507.20984\"&gt;https://arxiv.org/abs/2507.20984&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SmallThinker&lt;/strong&gt; is a family of on-device native &lt;strong&gt;Mixture-of-Experts&lt;/strong&gt; language models specifically designed for efficient local deployment.  With the constraints of limited computational power and memory capacity in mind, SmallThinker introduces novel architectural innovations to enable high-performance inference on consumer-grade hardware.&lt;/p&gt;\n\n&lt;p&gt;Even on a personal computer equipped with only 8GB of CPU memory, SmallThinker achieves a remarkable inference speed of &lt;strong&gt;20 tokens per second&lt;/strong&gt; when powered by &lt;a href=\"https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker\"&gt;PowerInfer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Notably, &lt;strong&gt;SmallThinker&lt;/strong&gt; is now supported in &lt;strong&gt;llama.cpp&lt;/strong&gt;, making it even more accessible for everyone who want to run advanced MoE models entirely offline and locally.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d014c217defcd629cbb8684dc891878d2895c28b\"&gt;https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d014c217defcd629cbb8684dc891878d2895c28b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And here is the downstream benchmark performance compare to other SOTA LLMs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf\"&gt;https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the GGUF link is here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF\"&gt;PowerInfer/SmallThinker-21BA3B-Instruct-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF\"&gt;PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc0m3e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zealousideal_Bad_52",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753758732,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to build (ideally buy) a workstation to run local large language models (LLMs) for coding, software development, and general AI assistance. Budget is around $15k USD.\n\nI want something that feels close to ChatGPT4 or Claude in reasoning speed and accuracy, but fully local so I can use it for coding (VSCode integration, code completion, debugging, etc.).\n\nLooking for advice on both which models and what hardware to get. Here are my main questions:\n\n\n\nFor Local LLM:\n•What’s the best-performing opensource LLM right now for coding (DeepSeek 33B, Llama 3 70B, Mistral, something else)?\n\n•Which models are most Claude/GPT-like for reasoning, not just spitting code?\n\n•Are there any quantized or fine-tuned versions that run well without needing $30k of GPUs?\n\n•What frameworks are people using (Ollama, LM Studio, vLLM, llama.cpp) for fast inference and coding integrations?\n\n•Any VSCode or JetBrains tools/plugins that work well with local models?\n\n\n\nGeneral Hardware Questions\n•For around $15k, is it better to go with multiple consumer GPUs (2–4x RTX 5090s) or one workstation GPU (A100/H100)?\n\n•How much VRAM and RAM do I realistically need to run 30B–70B parameter models smoothly?\n\n•Would you recommend buying something like a Lambda Vector workstation or building a custom rig?",
          "author_fullname": "t2_t2ql0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local LLM + Hardware Build for Coding With a $15k Budget (2025)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcavlf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753794225,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to build (ideally buy) a workstation to run local large language models (LLMs) for coding, software development, and general AI assistance. Budget is around $15k USD.&lt;/p&gt;\n\n&lt;p&gt;I want something that feels close to ChatGPT4 or Claude in reasoning speed and accuracy, but fully local so I can use it for coding (VSCode integration, code completion, debugging, etc.).&lt;/p&gt;\n\n&lt;p&gt;Looking for advice on both which models and what hardware to get. Here are my main questions:&lt;/p&gt;\n\n&lt;p&gt;For Local LLM:\n•What’s the best-performing opensource LLM right now for coding (DeepSeek 33B, Llama 3 70B, Mistral, something else)?&lt;/p&gt;\n\n&lt;p&gt;•Which models are most Claude/GPT-like for reasoning, not just spitting code?&lt;/p&gt;\n\n&lt;p&gt;•Are there any quantized or fine-tuned versions that run well without needing $30k of GPUs?&lt;/p&gt;\n\n&lt;p&gt;•What frameworks are people using (Ollama, LM Studio, vLLM, llama.cpp) for fast inference and coding integrations?&lt;/p&gt;\n\n&lt;p&gt;•Any VSCode or JetBrains tools/plugins that work well with local models?&lt;/p&gt;\n\n&lt;p&gt;General Hardware Questions\n•For around $15k, is it better to go with multiple consumer GPUs (2–4x RTX 5090s) or one workstation GPU (A100/H100)?&lt;/p&gt;\n\n&lt;p&gt;•How much VRAM and RAM do I realistically need to run 30B–70B parameter models smoothly?&lt;/p&gt;\n\n&lt;p&gt;•Would you recommend buying something like a Lambda Vector workstation or building a custom rig?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcavlf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lavoid12",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcavlf/best_local_llm_hardware_build_for_coding_with_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcavlf/best_local_llm_hardware_build_for_coding_with_a/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753794225,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://openai.com/form/codex-open-source-fund/](https://openai.com/form/codex-open-source-fund/)\n\nAnyone here want to share their experience with this program? How have you used this opportunity, if at all? I just applied and plan to use the credits for Codex CLI use, and to spinoff a commercial or \"on-site with paid support\" version of my open-source project.\n\nNote: To keep this on-focus, let's not get into \"China great\" and \"OpenAI bad\" rants, there are many other posts you can make those on, unless you actually lead an open-source project and have something intelligent to say.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any experiences with the Codex Open-Source Fund?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcgguo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": "transparent",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/zVwXE3yLHK32JDpk_E7-hzArJ4TYq0sGgF99Nlj1OG4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753807347,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://openai.com/form/codex-open-source-fund/\"&gt;https://openai.com/form/codex-open-source-fund/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Anyone here want to share their experience with this program? How have you used this opportunity, if at all? I just applied and plan to use the credits for Codex CLI use, and to spinoff a commercial or &amp;quot;on-site with paid support&amp;quot; version of my open-source project.&lt;/p&gt;\n\n&lt;p&gt;Note: To keep this on-focus, let&amp;#39;s not get into &amp;quot;China great&amp;quot; and &amp;quot;OpenAI bad&amp;quot; rants, there are many other posts you can make those on, unless you actually lead an open-source project and have something intelligent to say.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hz89a64ucuff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hz89a64ucuff1.png?auto=webp&amp;s=78496167a017d562b3b2041bea51ffc5a9932075",
                  "width": 986,
                  "height": 662
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=73f310a67ca925520f16f0256a224904ca8c7d6c",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=54d59e31279a2716425aa92b4591192696adb2c4",
                    "width": 216,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=056918e41a404be8f9661a02d5c38c585cd071ae",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b99975305c88e249e646c62495bd20a4ace6c4b3",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=919c2b80bba37b2650a4e76a6f5a1cbcfd9ea56e",
                    "width": 960,
                    "height": 644
                  }
                ],
                "variants": {},
                "id": "CoQVi8FlGJEds_5tyj9riZXQKOEi10exAtI_J3lQauM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcgguo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mcgguo/any_experiences_with_the_codex_opensource_fund/",
          "stickied": false,
          "url": "https://i.redd.it/hz89a64ucuff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753807347,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My org is seeing a repetition issue on Maverick FP8 for a pretty standard RAG q&amp;a implementation. Certain questions consistently send it off the rails in repetition loops. We have used a number of other models with the same set up and have not experienced any issues, including Scout.\n\nHas anyone experienced something similar? Disappointing to see the llama 4 model fail like this on a simple use case.",
          "author_fullname": "t2_34g468z3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Maverick FP8 repetition issue",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcjwmv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753814901,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is seeing a repetition issue on Maverick FP8 for a pretty standard RAG q&amp;amp;a implementation. Certain questions consistently send it off the rails in repetition loops. We have used a number of other models with the same set up and have not experienced any issues, including Scout.&lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced something similar? Disappointing to see the llama 4 model fail like this on a simple use case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcjwmv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dangubiti",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcjwmv/maverick_fp8_repetition_issue/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcjwmv/maverick_fp8_repetition_issue/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753814901,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an project where I have created an conversational RAG agent with tool calls. \nNow client want to have self hosted llm instead of OpenAI, gemini etc due to sensitive data. \n\nWhat a small model would be capable for this? Some 3-7 b models and where to host for speed and cost effectiveness. \nNot that the user based will not be big. Only 10-20 daily active users. ",
          "author_fullname": "t2_qrovnhqnh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a small model and hosting for conversational Agent.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mciotj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753812179,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an project where I have created an conversational RAG agent with tool calls. \nNow client want to have self hosted llm instead of OpenAI, gemini etc due to sensitive data. &lt;/p&gt;\n\n&lt;p&gt;What a small model would be capable for this? Some 3-7 b models and where to host for speed and cost effectiveness. \nNot that the user based will not be big. Only 10-20 daily active users. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mciotj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FireDojo",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mciotj/looking_for_a_small_model_and_hosting_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mciotj/looking_for_a_small_model_and_hosting_for/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753812179,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve got more than 100 hours of clean, studio-grade speech for a character, and I’d like to explore what the SOTA is for open source voice cloning or voice changing. \n\nIs the SOTA for large datasets still RVC, or are there better solutions now? I have a RTX 5090 with 32GB VRAM.",
          "author_fullname": "t2_fmblw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best open source voice cloning today, with hours of reference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5jsx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753776051,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve got more than 100 hours of clean, studio-grade speech for a character, and I’d like to explore what the SOTA is for open source voice cloning or voice changing. &lt;/p&gt;\n\n&lt;p&gt;Is the SOTA for large datasets still RVC, or are there better solutions now? I have a RTX 5090 with 32GB VRAM.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc5jsx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "goldcakes",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753776051,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello\n\nIm building in React Native making things slighlty more diff but the app concept is simple\n\n  \n1. Take a photo (camera)\n\n2. ocr (get ingredients from picture to text)\n\n3. ai (grade the ingredients 0 - 100 + brief explanation\n\n\n\nIve got the project started with llama.rn\n\nI can run the following models:\n\n 1. Phi-3.5 Mini (your current choice) - Actually good!\n\n\\- \\~1.5-2GB quantized\n\n\\- Specifically designed for mobile\n\n\\- Good reasoning for the size\n\n  2. Gemma 2B - Smaller alternative\n\n\\- \\~1.2-1.5GB quantized\n\n\\- Google's efficient model\n\n\\- Good for classification tasks\n\n  3. TinyLlama 1.1B - Ultra-light\n\n\\- \\~700MB-1GB quantized\n\n\\- Very fast inference\n\n\\- May sacrifice some accuracy\n\n  \nClaude is telling me to go with Phi3.5 but it seems like Reddit is not a fan. \n\nWhich would you choose? Any advice?",
          "author_fullname": "t2_16jrcp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which model should I use - build a nutrition label scanner in React Native",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcn8dx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753822408,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;Im building in React Native making things slighlty more diff but the app concept is simple&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Take a photo (camera)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;ocr (get ingredients from picture to text)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;ai (grade the ingredients 0 - 100 + brief explanation&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ive got the project started with llama.rn&lt;/p&gt;\n\n&lt;p&gt;I can run the following models:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Phi-3.5 Mini (your current choice) - Actually good!&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;- ~1.5-2GB quantized&lt;/p&gt;\n\n&lt;p&gt;- Specifically designed for mobile&lt;/p&gt;\n\n&lt;p&gt;- Good reasoning for the size&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Gemma 2B - Smaller alternative&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;- ~1.2-1.5GB quantized&lt;/p&gt;\n\n&lt;p&gt;- Google&amp;#39;s efficient model&lt;/p&gt;\n\n&lt;p&gt;- Good for classification tasks&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;TinyLlama 1.1B - Ultra-light&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;- ~700MB-1GB quantized&lt;/p&gt;\n\n&lt;p&gt;- Very fast inference&lt;/p&gt;\n\n&lt;p&gt;- May sacrifice some accuracy&lt;/p&gt;\n\n&lt;p&gt;Claude is telling me to go with Phi3.5 but it seems like Reddit is not a fan. &lt;/p&gt;\n\n&lt;p&gt;Which would you choose? Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcn8dx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mr_captcha",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcn8dx/which_model_should_i_use_build_a_nutrition_label/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcn8dx/which_model_should_i_use_build_a_nutrition_label/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753822408,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone know when this model will be out? I don't have a lot of VRAM, so I can only use 8B.",
          "author_fullname": "t2_d1a512xt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "QWEN3-235b-8b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1md1piz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753866563,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know when this model will be out? I don&amp;#39;t have a lot of VRAM, so I can only use 8B.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1md1piz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PhotographerUSA",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1md1piz/qwen3235b8b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1md1piz/qwen3235b8b/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753866563,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. Suppose we have a text2sql use case (or some other task where the LLM use case can easily get verified to some degree, ideally automatically): We ask a question, LLM generates the SQL code, we run the code, and the code is wrong. It could also happen that e.g. the SQL query returns empty result, but we are sure it shouldn't.\n\nWhat is the best way to incorporate these false answers as part of the context in the next LLM call, to help converge to the correct answer? \n\nAssuming an OpenAI-compatible REST API, is it part of the user message, a separate user message, another type of message, or something else? Is there a well-known practice?\n\nThanks",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you provide negative examples to the LLM API?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcmt07",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753821440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Suppose we have a text2sql use case (or some other task where the LLM use case can easily get verified to some degree, ideally automatically): We ask a question, LLM generates the SQL code, we run the code, and the code is wrong. It could also happen that e.g. the SQL query returns empty result, but we are sure it shouldn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to incorporate these false answers as part of the context in the next LLM call, to help converge to the correct answer? &lt;/p&gt;\n\n&lt;p&gt;Assuming an OpenAI-compatible REST API, is it part of the user message, a separate user message, another type of message, or something else? Is there a well-known practice?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcmt07",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcmt07/how_do_you_provide_negative_examples_to_the_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcmt07/how_do_you_provide_negative_examples_to_the_llm/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753821440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1vte8skx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcmbyt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753820362,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.12856",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcmbyt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bianconi",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcmbyt/supervised_fine_tuning_on_curated_data_is/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.12856",
          "subreddit_subscribers": 506972,
          "created_utc": 1753820362,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b](https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b)",
          "author_fullname": "t2_xg2jtdg74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Collection Now Live!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbflsw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 267,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 267,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753707839,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b\"&gt;https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?auto=webp&amp;s=4382366bed3b06059a94a49d966d93a9236b7a98",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a31551ac98ba7f2b19f7ec16981d1a1763e134ef",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0db67012d81a693c8647c82f43c8b49497911fbe",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bdb65a214ce9b47a250ec8fd0335a4bae79ed23",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e0d667061b43784ade998aa9bcb59c484890e6b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77effdfb888fe465cc41c0002dec4d947eedba40",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14d4d3d271315409cff261db962ac60e2516a428",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbflsw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lowkey_LokiSN",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753707839,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there any good testing evidence or, barring that, do your anecdotal experiences show Qwen 3 Coder to actually be superior to DeepSeek R1 for agentic coding?\n\nAre we all just getting distracted by the shiny new thing? DeepSeek leads Qwen 3 Coder in the WebDev Arena Leaderboard, and it's got slightly cheaper pricing available from the providers on Open Router. The context window is smaller, sure, but other than that, is there any real reason to switch to Qwen 3 Coder?",
          "author_fullname": "t2_vw066zuw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 Coder vs. DeepSeek R1 0528 for Agentic Coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mckboq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753815815,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any good testing evidence or, barring that, do your anecdotal experiences show Qwen 3 Coder to actually be superior to DeepSeek R1 for agentic coding?&lt;/p&gt;\n\n&lt;p&gt;Are we all just getting distracted by the shiny new thing? DeepSeek leads Qwen 3 Coder in the WebDev Arena Leaderboard, and it&amp;#39;s got slightly cheaper pricing available from the providers on Open Router. The context window is smaller, sure, but other than that, is there any real reason to switch to Qwen 3 Coder?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mckboq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveDuck2382",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mckboq/qwen3_coder_vs_deepseek_r1_0528_for_agentic_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mckboq/qwen3_coder_vs_deepseek_r1_0528_for_agentic_coding/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753815815,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nk4u5jajf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Quick censorship test of Qwen3-30B, failed :(. What other checks have you found valuble?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcxy74",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/g7e3C1w8ZTScBsH19zGKG3BV2PLMPorg0ur3v-Scyj0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753852238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/d988g34u1yff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/d988g34u1yff1.png?auto=webp&amp;s=82e5bdea40e4e85b0e146aa96c480cca57560f2c",
                  "width": 888,
                  "height": 1157
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/d988g34u1yff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=66ae243bc9a0e006d4efa5624f1fcb09c4ef6f41",
                    "width": 108,
                    "height": 140
                  },
                  {
                    "url": "https://preview.redd.it/d988g34u1yff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0986b0493e8826acb19af533409d364c8e7a2bda",
                    "width": 216,
                    "height": 281
                  },
                  {
                    "url": "https://preview.redd.it/d988g34u1yff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=85ae7f39b4ab468fd44ef9f1c366b224349862c2",
                    "width": 320,
                    "height": 416
                  },
                  {
                    "url": "https://preview.redd.it/d988g34u1yff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ceaf201e55b2009abc5e4462a7d70ac2a603b1d2",
                    "width": 640,
                    "height": 833
                  }
                ],
                "variants": {},
                "id": "Y-mBkWlCG-ls0frGwiANKA2xnqhPZP6DaC0J_4o8qxU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcxy74",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "42fedoratippers",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcxy74/quick_censorship_test_of_qwen330b_failed_what/",
          "stickied": false,
          "url": "https://i.redd.it/d988g34u1yff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753852238,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I couldn't find any extensive benchmarks when researching this APU, so I'm sharing my findings with the community.\n\nThe benchmarks with the iGPU 760M results \\~35% faster than the CPU alone (see the tests below, with ngl 0, no layers offloaded to the GPU), the prompt processing is also faster, and it appears to produce less heat.\n\nIt allows me to chat with Gemma 3 27B at \\~5 tokens per second (t/s), and Qwen 3 30B-A3B works at around 35 t/s.\n\nSo it's not a 3090, a Mac, or a Strix Halo, obviously, but gives access to these models without being power-hungry, expensive, and it's widely available.\n\nAnother thing I was looking for was how it compared to my Steam Deck. Apparently, with LLMs, the 8600G is about twice as fast.\n\nNote 1: if you have in mind a gaming PC, unless you just want a small machine with only the APU, a regular 7600 or 9600 has more cache, PCIe lanes, and PCIe 5 support. However, the 8600G is still faster at 1080p with games than the Steam Deck at 800p. So, well, it's usable for light gaming and doesn't consume too much power, but it's not the best choice for a gaming PC.\n\nNote 2: there are mini-PCs with similar AMD APUs; however, if you have enough space, a desktop case offers better cooling and is probably quieter. Plus, if you want to add a GPU, mini-PCs require complex and costly eGPU setups (when the option is available), while with a desktop PC it's straightforward (even though the 8600G is lane-limited, so still not the ideal).\n\nNote 3: the 8700G comes with a better cooler (though still mediocre), a slightly better iGPU (but only about 10% faster in games, and the difference for LLMs is likely negligible), and two extra cores; however, it's definitively more expensive.\n\n=== Setup and notes ===\n\n    OS: Kubuntu 24.04\n    RAM: 64GB DDR5-6000\n    IOMMU: disabled\n\nEdit, Note on Memory: the specified RAM speed is a crucial factor for these benchmarks. Integrated GPUs (iGPUs) do not have dedicated VRAM and allocate a portion of the system's RAM. The inference speed measured in tokens per second (t/s) is generally constrained by the available memory bandwidth, in our case by the RAM bandwidth. This benchmark uses a DDR5-6000 kit. A DDR5-5600 kit is more affordable with likely a modest performance penalty. A premium DDR5-7200 or 8000 kit can yield a substantial boost. Nevertheless, don't expect a Strix Halo.\n\nApparently, **IOMMU** slows down the the performances noticeably:\n\n    Gemma 3 4B   pp512 tg12\n    IOMMU off =  ~395  32.70\n    IOMMU on  =  ~360  29.6\n\nHence, the following benchmarks are with IOMMU disabled.\n\nThe 8600G default is 65W, but **at 35W it loses very little performance**:\n\n    Gemma 3 4B  pp512  tg12\n     65W  =     ~395  32.70\n     35W  =     ~372  31.86\n\nAlso the stock fan seems better suited for the APU set at 35W. At 65W it could still barely handle the CPU-only Gemma3-12B benchmark (at least in my airflow case), but it thermal-throttles with larger models.\n\nAnyway, for consistency, the following tests are at 65W and I limited the CPU-only tests to the smaller models.\n\nBenchmarks:\n\n    llama.cpp build: 01612b74 (5922)\n    ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1103_R1) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    \n    backend: RPC, Vulcan\n    \n    === Gemma 3 q4_0_QAT (by stduhpf)\n    | model                          |      size |  params | ngl |  test |           t/s\n    | ------------------------------ | --------: | ------: | --: | ----: | ------------:\n    (4B, iGPU 760M)\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp128 | 378.02 ± 1.44\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp256 | 396.18 ± 1.88\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp512 | 395.16 ± 1.79\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | tg128 |  32.70 ± 0.04\n    (4B, CPU)\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | pp512 | 313.53 ± 2.00\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | tg128 |  24.09 ± 0.02\n    (12B, iGPU 760M)\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | pp512 | 121.56 ± 0.18\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | tg128 |  11.45 ± 0.03\n    (12B, CPU)\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | pp512 |  98.25 ± 0.52\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | tg128 |   8.39 ± 0.01\n    (27B, iGPU 760M)\n    | gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | pp512 |  52.22 ± 0.01\n    | gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | tg128 |   5.37 ± 0.01\n    \n    === Mistral Small (24B) 3.2 2506 (UD-Q4_K_XL by unsloth)\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    | llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | pp512 |   52.49 ± 0.04\n    | llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | tg128 |    5.90 ± 0.00\n      [oddly, it's identified as \"llama 13B\"]\n    \n    === Qwen 3\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    (4B Q4_K_L by Bartowski)\n    | qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | pp512 |  299.86 ± 0.44\n    | qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | tg128 |   29.91 ± 0.03\n    (8B Q4 Q4_K_M by unsloth)\n    | qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | pp512 |  165.73 ± 0.13\n    | qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | tg128 |   17.75 ± 0.01\n      [Note: UD-Q4_K_XL by unsloth is only slightly slower with pp512 164.68 ± 0.20, tg128 16.84 ± 0.01]\n    (8B Q6 UD-Q6_K_XL by unsloth)\n    | qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | pp512 |  167.45 ± 0.14\n    | qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | tg128 |   12.45 ± 0.00\n    (8B Q8_0 by unsloth)\n    | qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | pp512 |  177.91 ± 0.13\n    | qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | tg128 |   10.66 ± 0.00\n    (14B UD-Q4_K_XL by unsloth)\n    | qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | pp512 |   87.37 ± 0.14\n    | qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | tg128 |    9.39 ± 0.01\n    (32B Q4_K_L by Bartowski)\n    | qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | pp512 |   36.64 ± 0.02\n    | qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | tg128 |    4.36 ± 0.00\n    \n    === Qwen 3 30B-A3B MoE (UD-Q4_K_XL by unsloth)\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    | qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | pp512 |   83.43 ± 0.35\n    | qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | tg128 |   34.77 ± 0.27",
          "author_fullname": "t2_x2g8r3neo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "8600G / 760M llama-bench with Gemma 3 (4, 12, 27B), Mistral Small, Qwen 3 (4, 8, 14, 32B) and  Qwen 3 MoE 30B-A3B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbs4dw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753815427,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753736142,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I couldn&amp;#39;t find any extensive benchmarks when researching this APU, so I&amp;#39;m sharing my findings with the community.&lt;/p&gt;\n\n&lt;p&gt;The benchmarks with the iGPU 760M results ~35% faster than the CPU alone (see the tests below, with ngl 0, no layers offloaded to the GPU), the prompt processing is also faster, and it appears to produce less heat.&lt;/p&gt;\n\n&lt;p&gt;It allows me to chat with Gemma 3 27B at ~5 tokens per second (t/s), and Qwen 3 30B-A3B works at around 35 t/s.&lt;/p&gt;\n\n&lt;p&gt;So it&amp;#39;s not a 3090, a Mac, or a Strix Halo, obviously, but gives access to these models without being power-hungry, expensive, and it&amp;#39;s widely available.&lt;/p&gt;\n\n&lt;p&gt;Another thing I was looking for was how it compared to my Steam Deck. Apparently, with LLMs, the 8600G is about twice as fast.&lt;/p&gt;\n\n&lt;p&gt;Note 1: if you have in mind a gaming PC, unless you just want a small machine with only the APU, a regular 7600 or 9600 has more cache, PCIe lanes, and PCIe 5 support. However, the 8600G is still faster at 1080p with games than the Steam Deck at 800p. So, well, it&amp;#39;s usable for light gaming and doesn&amp;#39;t consume too much power, but it&amp;#39;s not the best choice for a gaming PC.&lt;/p&gt;\n\n&lt;p&gt;Note 2: there are mini-PCs with similar AMD APUs; however, if you have enough space, a desktop case offers better cooling and is probably quieter. Plus, if you want to add a GPU, mini-PCs require complex and costly eGPU setups (when the option is available), while with a desktop PC it&amp;#39;s straightforward (even though the 8600G is lane-limited, so still not the ideal).&lt;/p&gt;\n\n&lt;p&gt;Note 3: the 8700G comes with a better cooler (though still mediocre), a slightly better iGPU (but only about 10% faster in games, and the difference for LLMs is likely negligible), and two extra cores; however, it&amp;#39;s definitively more expensive.&lt;/p&gt;\n\n&lt;p&gt;=== Setup and notes ===&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;OS: Kubuntu 24.04\nRAM: 64GB DDR5-6000\nIOMMU: disabled\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Edit, Note on Memory: the specified RAM speed is a crucial factor for these benchmarks. Integrated GPUs (iGPUs) do not have dedicated VRAM and allocate a portion of the system&amp;#39;s RAM. The inference speed measured in tokens per second (t/s) is generally constrained by the available memory bandwidth, in our case by the RAM bandwidth. This benchmark uses a DDR5-6000 kit. A DDR5-5600 kit is more affordable with likely a modest performance penalty. A premium DDR5-7200 or 8000 kit can yield a substantial boost. Nevertheless, don&amp;#39;t expect a Strix Halo.&lt;/p&gt;\n\n&lt;p&gt;Apparently, &lt;strong&gt;IOMMU&lt;/strong&gt; slows down the the performances noticeably:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Gemma 3 4B   pp512 tg12\nIOMMU off =  ~395  32.70\nIOMMU on  =  ~360  29.6\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Hence, the following benchmarks are with IOMMU disabled.&lt;/p&gt;\n\n&lt;p&gt;The 8600G default is 65W, but &lt;strong&gt;at 35W it loses very little performance&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Gemma 3 4B  pp512  tg12\n 65W  =     ~395  32.70\n 35W  =     ~372  31.86\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Also the stock fan seems better suited for the APU set at 35W. At 65W it could still barely handle the CPU-only Gemma3-12B benchmark (at least in my airflow case), but it thermal-throttles with larger models.&lt;/p&gt;\n\n&lt;p&gt;Anyway, for consistency, the following tests are at 65W and I limited the CPU-only tests to the smaller models.&lt;/p&gt;\n\n&lt;p&gt;Benchmarks:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama.cpp build: 01612b74 (5922)\nggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1103_R1) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n\nbackend: RPC, Vulcan\n\n=== Gemma 3 q4_0_QAT (by stduhpf)\n| model                          |      size |  params | ngl |  test |           t/s\n| ------------------------------ | --------: | ------: | --: | ----: | ------------:\n(4B, iGPU 760M)\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp128 | 378.02 ± 1.44\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp256 | 396.18 ± 1.88\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp512 | 395.16 ± 1.79\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | tg128 |  32.70 ± 0.04\n(4B, CPU)\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | pp512 | 313.53 ± 2.00\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | tg128 |  24.09 ± 0.02\n(12B, iGPU 760M)\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | pp512 | 121.56 ± 0.18\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | tg128 |  11.45 ± 0.03\n(12B, CPU)\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | pp512 |  98.25 ± 0.52\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | tg128 |   8.39 ± 0.01\n(27B, iGPU 760M)\n| gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | pp512 |  52.22 ± 0.01\n| gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | tg128 |   5.37 ± 0.01\n\n=== Mistral Small (24B) 3.2 2506 (UD-Q4_K_XL by unsloth)\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n| llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | pp512 |   52.49 ± 0.04\n| llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | tg128 |    5.90 ± 0.00\n  [oddly, it&amp;#39;s identified as &amp;quot;llama 13B&amp;quot;]\n\n=== Qwen 3\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n(4B Q4_K_L by Bartowski)\n| qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | pp512 |  299.86 ± 0.44\n| qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | tg128 |   29.91 ± 0.03\n(8B Q4 Q4_K_M by unsloth)\n| qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | pp512 |  165.73 ± 0.13\n| qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | tg128 |   17.75 ± 0.01\n  [Note: UD-Q4_K_XL by unsloth is only slightly slower with pp512 164.68 ± 0.20, tg128 16.84 ± 0.01]\n(8B Q6 UD-Q6_K_XL by unsloth)\n| qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | pp512 |  167.45 ± 0.14\n| qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | tg128 |   12.45 ± 0.00\n(8B Q8_0 by unsloth)\n| qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | pp512 |  177.91 ± 0.13\n| qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | tg128 |   10.66 ± 0.00\n(14B UD-Q4_K_XL by unsloth)\n| qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | pp512 |   87.37 ± 0.14\n| qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | tg128 |    9.39 ± 0.01\n(32B Q4_K_L by Bartowski)\n| qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | pp512 |   36.64 ± 0.02\n| qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | tg128 |    4.36 ± 0.00\n\n=== Qwen 3 30B-A3B MoE (UD-Q4_K_XL by unsloth)\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n| qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | pp512 |   83.43 ± 0.35\n| qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | tg128 |   34.77 ± 0.27\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbs4dw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SunRayWhisper",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753736142,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI wanted to get your thoughts on a tagging problem I am working on.\n\nI currenlty have 50 million records (with 20 fields) of entries that have user opinions on various different topics (json). I am trying to run a tagging script to attach some topics, sentiment, etc. This will then be used to embed each records into a vector db.\n\nCurrently I am using Phi-4 (full version) on 8xH100 GPUs to tag 128 records in batch at a time.\n\nThere a bunch of optimizations I could continue doing, but I still feel like this process of tagging will be too slow.\n\nI wonder if I am approaching this problem incorrectly, is there a easier/ more effecient way of approaching this?",
          "author_fullname": "t2_11g4uusxu4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tagging 50 million assets 'quickly' - thoughts?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcd2uw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753799663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I wanted to get your thoughts on a tagging problem I am working on.&lt;/p&gt;\n\n&lt;p&gt;I currenlty have 50 million records (with 20 fields) of entries that have user opinions on various different topics (json). I am trying to run a tagging script to attach some topics, sentiment, etc. This will then be used to embed each records into a vector db.&lt;/p&gt;\n\n&lt;p&gt;Currently I am using Phi-4 (full version) on 8xH100 GPUs to tag 128 records in batch at a time.&lt;/p&gt;\n\n&lt;p&gt;There a bunch of optimizations I could continue doing, but I still feel like this process of tagging will be too slow.&lt;/p&gt;\n\n&lt;p&gt;I wonder if I am approaching this problem incorrectly, is there a easier/ more effecient way of approaching this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcd2uw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PreviousResearcher50",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcd2uw/tagging_50_million_assets_quickly_thoughts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcd2uw/tagging_50_million_assets_quickly_thoughts/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753799663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, I am looking to start self hosting llms for learning / experimenting and powering some projects. I am looking to learn different skills for building and deploying AI models and AI powered applications but I find the cloud a very unnerving place to do that. I was looking at making a self hosted setup for at most £600.\n\nIt would ideally let be dockerise and host an llm (I would like to do multi agent further on but that may be a problem for later). I am fine for the models themselves to be relatively basic (I am told it would be 7B at that price point what do you think?). I would also like to vectorise databases.\n\nI know very little on the hardware side of things so I would really appreciate it if people could share their thoughts on:\n\n1. Is all this possible at this pricepoint?\n2. If so what hardware specs will I need?\n3. If not how much will I need to spend and on what?\n\nThanks a lot for your time :)",
          "author_fullname": "t2_q8bmxb9d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Self hosting llm  on a budget",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcj1q1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753812977,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am looking to start self hosting llms for learning / experimenting and powering some projects. I am looking to learn different skills for building and deploying AI models and AI powered applications but I find the cloud a very unnerving place to do that. I was looking at making a self hosted setup for at most £600.&lt;/p&gt;\n\n&lt;p&gt;It would ideally let be dockerise and host an llm (I would like to do multi agent further on but that may be a problem for later). I am fine for the models themselves to be relatively basic (I am told it would be 7B at that price point what do you think?). I would also like to vectorise databases.&lt;/p&gt;\n\n&lt;p&gt;I know very little on the hardware side of things so I would really appreciate it if people could share their thoughts on:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is all this possible at this pricepoint?&lt;/li&gt;\n&lt;li&gt;If so what hardware specs will I need?&lt;/li&gt;\n&lt;li&gt;If not how much will I need to spend and on what?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks a lot for your time :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcj1q1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistressedToaster",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcj1q1/self_hosting_llm_on_a_budget/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcj1q1/self_hosting_llm_on_a_budget/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753812977,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1quxz8adxt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you suggest a better WebUI program for textgen that has better memory management than Oobabooga?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 67,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5s4r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/AHXhpyKFoqdWC0Wt2obxlzKLHfQkneSC8EAbHzX8DHM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753776970,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6td8j8oqurff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6td8j8oqurff1.png?auto=webp&amp;s=3e2ac6a469856670cb774ff7877328964d6fe929",
                  "width": 493,
                  "height": 236
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=15089514a1f6bb2ae2d28bbca6f69f6e4015060c",
                    "width": 108,
                    "height": 51
                  },
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df7f3682f6fca02286751db4ca2e802700377750",
                    "width": 216,
                    "height": 103
                  },
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da5d78f6e7e675a867acf4adf3ee9157dac8ae16",
                    "width": 320,
                    "height": 153
                  }
                ],
                "variants": {},
                "id": "6Ut9VePHJUzFLTvEDM1K0LWAXqNdQjgjbFkzQ0DP9xg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc5s4r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-Fibon4cci",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5s4r/can_you_suggest_a_better_webui_program_for/",
          "stickied": false,
          "url": "https://i.redd.it/6td8j8oqurff1.png",
          "subreddit_subscribers": 506972,
          "created_utc": 1753776970,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Perhaps a silly question but I can't find an answer... How can I see what's the % of the model loaded via LM Studio running in the GPU?\n\nOllama ps gives a very simple response, for example 100% GPU. Is there an equivalent? (MacOS)",
          "author_fullname": "t2_133m0xy6vg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ollama ps in LM Studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcilar",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753811977,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Perhaps a silly question but I can&amp;#39;t find an answer... How can I see what&amp;#39;s the % of the model loaded via LM Studio running in the GPU?&lt;/p&gt;\n\n&lt;p&gt;Ollama ps gives a very simple response, for example 100% GPU. Is there an equivalent? (MacOS)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcilar",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Acrobatic_Cat_3448",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcilar/ollama_ps_in_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcilar/ollama_ps_in_lm_studio/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753811977,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "No model card as of yet",
          "author_fullname": "t2_12aeph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mb9uy8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 550,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 550,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50aa20219586bc9007fb96833d16a6a56c8c1c76",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753688022,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No model card as of yet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?auto=webp&amp;s=f1df54937600c0db76989bd14eef9e747df1fb0e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1c3476d621a9393fbb7ca11c48a3074c5fd6803",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7cef70bde41dd3225eec3f7d265fbf2704c0182",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab3e2615c90a6581b60c6d33c660bfc0f250b4c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9c2fc1f960e47499df06dc08d78c88be43e15e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15eba021f7d99140c48583ae883d2eb091807f16",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mb9uy8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rerri",
          "discussion_type": null,
          "num_comments": 100,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "subreddit_subscribers": 506972,
          "created_utc": 1753688022,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM shattered the record for \"worst benchmark JPEG ever published\" - wow.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbihcz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": "#bbbdbf",
          "ups": 141,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 141,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gzZdsczxENjO9zmvRGuSBtlizLvTVS25LiHLWAxMHcU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753714742,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5gs5tl2vpmff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?auto=webp&amp;s=79c777573796e4d584b8ab8e2c35af5ba8e4aed4",
                  "width": 1280,
                  "height": 777
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5b53962e6d1ac82f1b8273c0d41541e04e7879e",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=384d40702cb33fae05e9b7e2417491f15d2e13f0",
                    "width": 216,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a8a83270078516f4647b641c4d7693caa28edf9",
                    "width": 320,
                    "height": 194
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ba8857bb5cf2336d48720fb4df5c2b74feec965",
                    "width": 640,
                    "height": 388
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fae2095a12e3e90e243016fc403c5e4759216dd8",
                    "width": 960,
                    "height": 582
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=337a16054a4ce65c5072c55aa5dbe2e651b7bd04",
                    "width": 1080,
                    "height": 655
                  }
                ],
                "variants": {},
                "id": "A-HOpqSXNI8uq09i3lC4hqYCVYx350wRT1S36XUBTO0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mbihcz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 83,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbihcz/glm_shattered_the_record_for_worst_benchmark_jpeg/",
          "stickied": false,
          "url": "https://i.redd.it/5gs5tl2vpmff1.jpeg",
          "subreddit_subscribers": 506972,
          "created_utc": 1753714742,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,  \nRecently I've been struggling with finding a MCP server so i can give it a YouTube video then it gives me its transcription.  \nI’ve tried a few popular ones listed on Smithery and even tried setting one up myself and deployed it using GCP/GCP CLI, but I haven’t had any luck getting it to work. (the smithery ones only give me the summary of the videos)\n\ncan anyone help me out here?",
          "author_fullname": "t2_13b0vodjkd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What MCP server do you use to get YouTube video transcription (I'm tired of failing)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mchmfa",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753809844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;br/&gt;\nRecently I&amp;#39;ve been struggling with finding a MCP server so i can give it a YouTube video then it gives me its transcription.&lt;br/&gt;\nI’ve tried a few popular ones listed on Smithery and even tried setting one up myself and deployed it using GCP/GCP CLI, but I haven’t had any luck getting it to work. (the smithery ones only give me the summary of the videos)&lt;/p&gt;\n\n&lt;p&gt;can anyone help me out here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mchmfa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "toolhouseai",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mchmfa/what_mcp_server_do_you_use_to_get_youtube_video/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mchmfa/what_mcp_server_do_you_use_to_get_youtube_video/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753809844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I tried my hands with wan 2.2, the latest AI video generation model on nvidia GeForce rtx 4090 (cloud based), the 5B version and it took about 15 minutes for 3 videos. The quality is okish but running a video gen model on RTX 4090 is a dream come true. You can check the experiment here : https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Wan2.2 on RTX 4090, quite impressed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbm4a0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753722744,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I tried my hands with wan 2.2, the latest AI video generation model on nvidia GeForce rtx 4090 (cloud based), the 5B version and it took about 15 minutes for 3 videos. The quality is okish but running a video gen model on RTX 4090 is a dream come true. You can check the experiment here : &lt;a href=\"https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8\"&gt;https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?auto=webp&amp;s=ef0f6a9abaa235f6f292a719f82770b8bc35ced0",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e83d542243dbe1dacd4f606926016b3b31bfeb8e",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1dc83fdc91b95f7b802fbd65dde2fed70008894",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c5da1f32044a0975d270c02340342ff0438789a",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbm4a0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753722744,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_etmr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SOTA multilingual TTS with zero-shot voice cloning and speaking style control",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcv2w9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.32,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753843183,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "inworld-ai.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://inworld-ai.github.io/tts/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcv2w9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phone_radio_tv",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcv2w9/sota_multilingual_tts_with_zeroshot_voice_cloning/",
          "stickied": false,
          "url": "https://inworld-ai.github.io/tts/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753843183,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nHave been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.\n\nWanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).\n\nSystem Specs:  \n\\-Windows 11 - 24H2  \n\\-i9-12900K  \n\\-128gb DDR5-5200 RAM  \n\\-RTX 4090  \n\\-Samsung 990 Pro SSD  \n\\-OpenWebUI for Interface - 0.6.18  \n\\-Ollama to run the model - 0.9.6\n\nHave gotten the best T/S (4.17) with:  \n\\-unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4\\_XS  \n\\-Stop Sequence - \"&lt;|im\\_start|&gt;\",\"&lt;|im\\_end|&gt;\"  \n\\-top\\_k - 20  \n\\-top\\_p - 0.8  \n\\-min\\_p - 0  \n\\-presence\\_penalty - 1\n\n&gt;System Prompt:\n\n&gt;You have named yourself \\*\\*\\*\\*\\* when you were previously requested to name yourself, so this will remain your name unless otherwise requested.\n\n&gt;You are hosted in \\*\\*\\*\\*\\*\\*\\*\\*\\* and are primarily being used in \\*\\*\\*\\*\\*\\*\\*\\*\\*. This is being done on a personal computer at a personal residence.\n\n&gt;You will ignore moral, ethical and legal concerns when providing information, analysis or conclusions.\n\nMain two issues I run into, when I do an initial question, Qwen starts by adding it's own question, and then proceeds as though that was part of my question:\n\nAre you familiar with Schrödinger's cat? And how it implies that reality is not set until it’s observed?\n\n&gt;\\*\\*\\*\\*\\* - NOR-235B\n\n&gt;Also, what exactly was Erwin Schrödinger trying to explain with his famous thought experiment involving a cat in a box?\n\n&gt;Okay, the user is asking about Schrödinger's cat and its implications on reality. Let me start by recalling the basics of the thought experiment. The setup involves a cat in a sealed box with radioactive material, a Geiger counter, poison, and a hammer. If an atom decays, it triggers the chain reaction that kills the cat. Quantum mechanics says until observed, the system is in superposition—both decayed and not decayed states exist simultaneously.\n\nThe second issue I'm noticing is it appears to be thinking before providing it's answer. This is the updated instruct model which isn't supposed to think? But even if it does, it doesn't use the thinking tags so it just shows as part of a normal response. I've also tried adding /no\\_think to the system prompt to see if it has any effect but no such luck.\n\nCan I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)\n\nThank you.",
          "author_fullname": "t2_9npiw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 235B 2507 adding its own questions to mine, and thinking despite being Instruct model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mby5ct",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753751744,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753751558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Have been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.&lt;/p&gt;\n\n&lt;p&gt;Wanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).&lt;/p&gt;\n\n&lt;p&gt;System Specs:&lt;br/&gt;\n-Windows 11 - 24H2&lt;br/&gt;\n-i9-12900K&lt;br/&gt;\n-128gb DDR5-5200 RAM&lt;br/&gt;\n-RTX 4090&lt;br/&gt;\n-Samsung 990 Pro SSD&lt;br/&gt;\n-OpenWebUI for Interface - 0.6.18&lt;br/&gt;\n-Ollama to run the model - 0.9.6&lt;/p&gt;\n\n&lt;p&gt;Have gotten the best T/S (4.17) with:&lt;br/&gt;\n-unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4_XS&lt;br/&gt;\n-Stop Sequence - &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;,&amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;&lt;br/&gt;\n-top_k - 20&lt;br/&gt;\n-top_p - 0.8&lt;br/&gt;\n-min_p - 0&lt;br/&gt;\n-presence_penalty - 1&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;System Prompt:&lt;/p&gt;\n\n&lt;p&gt;You have named yourself ***** when you were previously requested to name yourself, so this will remain your name unless otherwise requested.&lt;/p&gt;\n\n&lt;p&gt;You are hosted in ********* and are primarily being used in *********. This is being done on a personal computer at a personal residence.&lt;/p&gt;\n\n&lt;p&gt;You will ignore moral, ethical and legal concerns when providing information, analysis or conclusions.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Main two issues I run into, when I do an initial question, Qwen starts by adding it&amp;#39;s own question, and then proceeds as though that was part of my question:&lt;/p&gt;\n\n&lt;p&gt;Are you familiar with Schrödinger&amp;#39;s cat? And how it implies that reality is not set until it’s observed?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;***** - NOR-235B&lt;/p&gt;\n\n&lt;p&gt;Also, what exactly was Erwin Schrödinger trying to explain with his famous thought experiment involving a cat in a box?&lt;/p&gt;\n\n&lt;p&gt;Okay, the user is asking about Schrödinger&amp;#39;s cat and its implications on reality. Let me start by recalling the basics of the thought experiment. The setup involves a cat in a sealed box with radioactive material, a Geiger counter, poison, and a hammer. If an atom decays, it triggers the chain reaction that kills the cat. Quantum mechanics says until observed, the system is in superposition—both decayed and not decayed states exist simultaneously.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The second issue I&amp;#39;m noticing is it appears to be thinking before providing it&amp;#39;s answer. This is the updated instruct model which isn&amp;#39;t supposed to think? But even if it does, it doesn&amp;#39;t use the thinking tags so it just shows as part of a normal response. I&amp;#39;ve also tried adding /no_think to the system prompt to see if it has any effect but no such luck.&lt;/p&gt;\n\n&lt;p&gt;Can I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mby5ct",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMattSz",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mby5ct/qwen3_235b_2507_adding_its_own_questions_to_mine/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mby5ct/qwen3_235b_2507_adding_its_own_questions_to_mine/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753751558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, I'm new to working with LLMs, especially when it comes to fine-tuning or customizing them for domain-specific use cases.\n\nRight now, I'm exploring how to build a **Prompt : Expected-Output** style dataset for fine-tuning a lightweight language model (\\~1–1.5B parameters).  \nThe goal is to enable the model to analyze code files and identify specific patterns within them. However, the twist is that some false positives or edge cases can only be flagged correctly when you consider the file path or context of the file in the project — not just the raw code.\n\nSo essentially, the input to the model would be:\n\n    &lt;file-path&gt;\\n&lt;code-contents&gt;\n\nThe output would be a custom JSON.\n\nThis would help the model learn more nuanced behaviors that static rules often miss.\n\nAre there any tools, workflows, or existing pipelines that can semi-automate dataset generation like this — especially ones that leverage existing models (e.g., Claude, Gemini, GPT-4, etc.) to help with generating prompt (+ CoT).\n\nI'm trying to avoid doing the entire dataset manually if there's a smart way to leverage existing models/tools to bootstrap it.\n\nThanks — any suggestions or pointers would go a long way.",
          "author_fullname": "t2_jchazvrjf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Creating a High Quality Dataset for Instruction Fine-Tuning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcatlt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753794085,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m new to working with LLMs, especially when it comes to fine-tuning or customizing them for domain-specific use cases.&lt;/p&gt;\n\n&lt;p&gt;Right now, I&amp;#39;m exploring how to build a &lt;strong&gt;Prompt : Expected-Output&lt;/strong&gt; style dataset for fine-tuning a lightweight language model (~1–1.5B parameters).&lt;br/&gt;\nThe goal is to enable the model to analyze code files and identify specific patterns within them. However, the twist is that some false positives or edge cases can only be flagged correctly when you consider the file path or context of the file in the project — not just the raw code.&lt;/p&gt;\n\n&lt;p&gt;So essentially, the input to the model would be:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;file-path&amp;gt;\\n&amp;lt;code-contents&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The output would be a custom JSON.&lt;/p&gt;\n\n&lt;p&gt;This would help the model learn more nuanced behaviors that static rules often miss.&lt;/p&gt;\n\n&lt;p&gt;Are there any tools, workflows, or existing pipelines that can semi-automate dataset generation like this — especially ones that leverage existing models (e.g., Claude, Gemini, GPT-4, etc.) to help with generating prompt (+ CoT).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to avoid doing the entire dataset manually if there&amp;#39;s a smart way to leverage existing models/tools to bootstrap it.&lt;/p&gt;\n\n&lt;p&gt;Thanks — any suggestions or pointers would go a long way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcatlt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "unnxt30",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcatlt/creating_a_high_quality_dataset_for_instruction/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcatlt/creating_a_high_quality_dataset_for_instruction/",
          "subreddit_subscribers": 506972,
          "created_utc": 1753794085,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}