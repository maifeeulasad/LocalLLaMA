{
  "kind": "Listing",
  "data": {
    "after": "t3_1m6ht1r",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;&gt;&gt; Qwen3-Coder is here! ✅\n\nWe’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀\n\nAlongside the model, we're also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! ",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is here!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qdet",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 1404,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 1404,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lAVBKeQpbXFJZ84JgZVPph8kD3MjUQeFX9TO1gsVqgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Qwen3-Coder is here! ✅&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;We’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀&lt;/p&gt;\n\n&lt;p&gt;Alongside the model, we&amp;#39;re also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?auto=webp&amp;s=e161efd029b20a9bcbbc26db043c320a38b26d7f",
                  "width": 2048,
                  "height": 1175
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0e7ce793e40e6f9057df0ac4084bef74851aa3c",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97f4add6c188177de3f1a921fce3e9fdcd751975",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d3ed6788720878c02cad2db45833f254f311864",
                    "width": 320,
                    "height": 183
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=470c1e7a0a6df4a35a09ad70120a5fef4e93a97b",
                    "width": 640,
                    "height": 367
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eca8024659f203e8b748284b077d47d512488b3",
                    "width": 960,
                    "height": 550
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=348beac8c62000f3a203a6467f098a1c8a696369",
                    "width": 1080,
                    "height": 619
                  }
                ],
                "variants": {},
                "id": "kx6kRcRUBkO_mMM0khkM5jTQgMXazrrYG6wlH3UPCCs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qdet",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 190,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/",
          "stickied": false,
          "url": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "subreddit_subscribers": 503258,
          "created_utc": 1753218847,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_a05srvks",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recent Qwen Benchmark Scores are Questionable",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 86,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6wb5o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 279,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 279,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4lABFcqtTCklC4dui7K-UMEXRbWONTLpUEMMKBfKSoM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753234294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8gjn0yhf1jef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8gjn0yhf1jef1.png?auto=webp&amp;s=1c15b72a1a3ebfe0f19f0d765beb22b39ec10dcd",
                  "width": 1194,
                  "height": 734
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6a6ec17b5a0a0b95756eb50adde48b41ee2f601",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e055247e334ebf18193ce1f1d33b6c9da1725406",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=224c05a95e89dc2c0fa3b5b4b9a7782ddb3f25a0",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5550a5410e5e1c751c0140c16c192e6bd86fddd",
                    "width": 640,
                    "height": 393
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=29dcc9276be7fbebc077a8bce537c9338061da86",
                    "width": 960,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd26c16ae9488c7ee59a13a97af993a1d0b068ba",
                    "width": 1080,
                    "height": 663
                  }
                ],
                "variants": {},
                "id": "USv5UurWPKmkzrWAAgHt6q_pqJzLWJPl6eKLZh9JvqU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6wb5o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Electronic_Ad8889",
          "discussion_type": null,
          "num_comments": 48,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/",
          "stickied": false,
          "url": "https://i.redd.it/8gjn0yhf1jef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753234294,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen3 235B 2507 scores 60 on the Artificial Analysis Intelligence Index, surpassing Claude 4 Opus and Kimi K2 (both 58), and DeepSeek V3 0324 and GPT-4.1 (both 53). This marks a 13-point leap over the May 2025 non-reasoning release and brings it within two points of the May 2025 reasoning variant.",
          "author_fullname": "t2_1n5r32wumb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Alibaba’s upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "oyaa6be25kef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab849378391008c8477795135825d52268bdd089"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53e7c01d46996d28bab6c0eb21ebf73f19d344d0"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=101dd3f12db35511b6a0b3746e6b6a540438f16a"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a517570e0bde0d35e032e730f623cbf398e56d34"
                },
                {
                  "y": 479,
                  "x": 960,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bd46b3fee52bf1de3e3566854ce997d0dd1225a"
                },
                {
                  "y": 538,
                  "x": 1080,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89c6654712fe811c8362225e6681fc8b54707958"
                }
              ],
              "s": {
                "y": 1022,
                "x": 2048,
                "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=e4811114c0b584dfcb6ee44f7f9bd5de02b8b550"
              },
              "id": "oyaa6be25kef1"
            },
            "yycfab625kef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ddf3c6b84f0f72a7669eb6187f719c7aca48cf1"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=020ffd2f38bf6dc2d4d813526ab1f68d37467a35"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d01829fee9b1b624a03e3c46434cc0464197eb6"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=590c21d82b1b1b130de0259f2b921cf5f6a9a736"
                },
                {
                  "y": 479,
                  "x": 960,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f1617800278fa9228386271be95a16d7067a04c"
                },
                {
                  "y": 538,
                  "x": 1080,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79f249d73cc6e2ef9eba49c4faaff2315c0c5daf"
                }
              ],
              "s": {
                "y": 1022,
                "x": 2048,
                "u": "https://preview.redd.it/yycfab625kef1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=b0b54666a21f5112942012d6921a4898cd5bb66d"
              },
              "id": "yycfab625kef1"
            }
          },
          "name": "t3_1m70n7q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 135,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "yycfab625kef1",
                "id": 712061604
              },
              {
                "caption": "",
                "media_id": "oyaa6be25kef1",
                "id": 712061605
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 135,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/44x5FARtQun2iK2pU9UkqoLiKnQmMoq90mJNUYMTKbw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753247536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 235B 2507 scores 60 on the Artificial Analysis Intelligence Index, surpassing Claude 4 Opus and Kimi K2 (both 58), and DeepSeek V3 0324 and GPT-4.1 (both 53). This marks a 13-point leap over the May 2025 non-reasoning release and brings it within two points of the May 2025 reasoning variant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m70n7q",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m70n7q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fantastic-Emu-3819",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m70n7q",
          "subreddit_subscribers": 503258,
          "created_utc": 1753247536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we're making **1M context length variants**!\n\nYou can achieve &gt;6 tokens/s on **182GB unified memory or 158GB RAM + 24GB VRAM** via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via \n\n    -ot \".ffn_.*_exps.=CPU\"\n\nUnfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we're investigating why this happens.\n\nYou can also run the **un-quantized 8bit / 16bit** versions also using llama,cpp offloading! Use Q8\\_K\\_XL which will be completed in an hour or so.\n\nTo increase performance and context length, use KV cache quantization, especially the \\_1 variants (higher accuracy than \\_0 variants). More details [here](https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m).\n\n`--cache-type-k q4_1`\n\nEnable flash attention as well and also try llama.cpp's NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are [here](https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed).\n\nQwen3-Coder-480B-A35B GGUFs (still ongoing) are at [https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF)\n\n1 million context length variants will be up at [https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF)\n\nDocs on how to run it are here: [https://docs.unsloth.ai/basics/qwen3-coder](https://docs.unsloth.ai/basics/qwen3-coder)",
          "author_fullname": "t2_5wukhd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Unsloth dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6wgs7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 187,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 187,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/BrdyPpD7KtsDEQRMB38b-Z6TRovCBfRYhvapsX-O7BM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753234725,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we&amp;#39;re making &lt;strong&gt;1M context length variants&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;You can achieve &amp;gt;6 tokens/s on &lt;strong&gt;182GB unified memory or 158GB RAM + 24GB VRAM&lt;/strong&gt; via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Unfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we&amp;#39;re investigating why this happens.&lt;/p&gt;\n\n&lt;p&gt;You can also run the &lt;strong&gt;un-quantized 8bit / 16bit&lt;/strong&gt; versions also using llama,cpp offloading! Use Q8_K_XL which will be completed in an hour or so.&lt;/p&gt;\n\n&lt;p&gt;To increase performance and context length, use KV cache quantization, especially the _1 variants (higher accuracy than _0 variants). More details &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--cache-type-k q4_1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Enable flash attention as well and also try llama.cpp&amp;#39;s NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Qwen3-Coder-480B-A35B GGUFs (still ongoing) are at &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;1 million context length variants will be up at &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Docs on how to run it are here: &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder\"&gt;https://docs.unsloth.ai/basics/qwen3-coder&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/s9cwrvwg1jef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/s9cwrvwg1jef1.png?auto=webp&amp;s=c00619420d39151822d49bcce97ea3e78e847971",
                  "width": 2560,
                  "height": 2740
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6a9ffee96aa9dcd39f3c41d338a8e758af4d75",
                    "width": 108,
                    "height": 115
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c70c0936d1d66995daaa0390deaa956eaec815e1",
                    "width": 216,
                    "height": 231
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89539969c58182206b11d57c691f1c822b4002b3",
                    "width": 320,
                    "height": 342
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=75c9ba63f5cc1768819789d0934d7d2a1e5a5926",
                    "width": 640,
                    "height": 685
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=812205928bb5b587c3e1e23bd16cae5c1d48ae0e",
                    "width": 960,
                    "height": 1027
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32245973741ece306547037bf3f6efa37649cb42",
                    "width": 1080,
                    "height": 1155
                  }
                ],
                "variants": {},
                "id": "ERO7uofUircHQEW6vRJ41kGEOL83JHug3Yrw0FJ_7oM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6wgs7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danielhanchen",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/",
          "stickied": false,
          "url": "https://i.redd.it/s9cwrvwg1jef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753234725,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a semi complex web project that I use with Claude Code. a few days ago I used Kimi K2 (via Groq Q4) with Claude Code (CCR) to add a permissions system / ACL into my web project to lock down certain people from doing certain things.\n\n  \nI use SuperClaude and a 1200 line context/architecture document, which basically starts a conversation off at about 30k input tokens (though, well worth it).\n\n  \nKimi K2 failed horribly, tool use errors, random garbage and basically didn't work properly. It was a Q4 version so maybe that had something to do with it, but I wasn't impressed.\n\n  \nToday I used Qwen 3 Coder via Openrouter (using only Alibaba cloud servers) for about 60 tps. Gave it the same task, and after about 10 minutes it finished. One shotted it (though one shotting is common for me with such a high amount of pre-context and auto fixing).\n\n  \nIt all worked great, I am actually really impressed and for me personally, it marks the first time an open source coding model actually has real world potential to rival paid LLMs like sonnet, opus and gemini. I would compare this model directly as good as Sonnet 4, which is a very capable model when using the right tools and prompts.\n\n  \nbig W for the open source community.\n\n  \nthe downside? THE PRICE. this one feature I added cost me $5 USD in credits via OpenRouter. That might not seem like much, but with Claude Pro for example you get an entire month of Sonnet 4 for 4x the price of that task. I don't know how well its using caching but at this point id rather stick with subscription based usage because that could get out of hand fast.",
          "author_fullname": "t2_i5ycefja",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Coder is actually pretty decent in my testing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m73yrb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753260180,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a semi complex web project that I use with Claude Code. a few days ago I used Kimi K2 (via Groq Q4) with Claude Code (CCR) to add a permissions system / ACL into my web project to lock down certain people from doing certain things.&lt;/p&gt;\n\n&lt;p&gt;I use SuperClaude and a 1200 line context/architecture document, which basically starts a conversation off at about 30k input tokens (though, well worth it).&lt;/p&gt;\n\n&lt;p&gt;Kimi K2 failed horribly, tool use errors, random garbage and basically didn&amp;#39;t work properly. It was a Q4 version so maybe that had something to do with it, but I wasn&amp;#39;t impressed.&lt;/p&gt;\n\n&lt;p&gt;Today I used Qwen 3 Coder via Openrouter (using only Alibaba cloud servers) for about 60 tps. Gave it the same task, and after about 10 minutes it finished. One shotted it (though one shotting is common for me with such a high amount of pre-context and auto fixing).&lt;/p&gt;\n\n&lt;p&gt;It all worked great, I am actually really impressed and for me personally, it marks the first time an open source coding model actually has real world potential to rival paid LLMs like sonnet, opus and gemini. I would compare this model directly as good as Sonnet 4, which is a very capable model when using the right tools and prompts.&lt;/p&gt;\n\n&lt;p&gt;big W for the open source community.&lt;/p&gt;\n\n&lt;p&gt;the downside? THE PRICE. this one feature I added cost me $5 USD in credits via OpenRouter. That might not seem like much, but with Claude Pro for example you get an entire month of Sonnet 4 for 4x the price of that task. I don&amp;#39;t know how well its using caching but at this point id rather stick with subscription based usage because that could get out of hand fast.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m73yrb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hodler-mane",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753260180,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen out here releasing models like it’s a Costco sample table",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qixu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 382,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 382,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/S7tH6DgPEGKSzcu1dZlyahFSv5skqhzjfdd4AVfWVi4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219204,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5eb8n31sshef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5eb8n31sshef1.png?auto=webp&amp;s=c1694040f87c60dc765d805ee64b6518e3bd108b",
                  "width": 722,
                  "height": 1032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47116ec0e7ef90202d820540f88598c3cfd0a160",
                    "width": 108,
                    "height": 154
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3d8c4cb9e760fdd971704cc87722923b6445146",
                    "width": 216,
                    "height": 308
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a14963b84a65b1261a6b4b6b451fce1c2285102",
                    "width": 320,
                    "height": 457
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f24e0235850da677693988507655dde73bf8e60",
                    "width": 640,
                    "height": 914
                  }
                ],
                "variants": {},
                "id": "yWOKVm4sVIve_VHC2bO92aBIp15Yh_tuiWnp6wkEtnE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m6qixu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/",
          "stickied": false,
          "url": "https://i.redd.it/5eb8n31sshef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753219204,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Available in https://chat.qwen.ai",
          "author_fullname": "t2_e9mfhlg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3- Coder 👀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mew9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 606,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 606,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/e6gFp_J-Dv7QIFguXfhuN4U3lDC6MMgny7SMuBnt9pI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209850,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Available in &lt;a href=\"https://chat.qwen.ai\"&gt;https://chat.qwen.ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?auto=webp&amp;s=a57681c6848dc38714b9bea86a26c30bed7d4d42",
                  "width": 1036,
                  "height": 695
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4a02434a648980c01b1a76032aa8e02027937c6",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be152167170e73dd02f7850c4f9bb67cf143ec4a",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c86d3d06fd820523c1470692b2726d59dbaf6d3",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=92b455544fdc9f84aebcf9cf995f7e3e643179a1",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d63431a252098393997b8c247ff6a0a80b67f78",
                    "width": 960,
                    "height": 644
                  }
                ],
                "variants": {},
                "id": "52S4zww-hEGiuCbEDUlQZAn66M2iCNb-181uTVxpyGY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6mew9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xhehab_",
          "discussion_type": null,
          "num_comments": 171,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/",
          "stickied": false,
          "url": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "subreddit_subscribers": 503258,
          "created_utc": 1753209850,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So like the title suggests, I have ADHD; which on a good day is a double edged sword in terms of learning new things. I gave a demo to the director of engineering at my job with a Unity chat application that was largely powered by a Unity Asset which is a C# wrapper of llama.cpp. \n\nMy director was like \"I'm going on vacation for a month, but I want you to focus on learning as much as you can to see where we can apply it in our work\" (the really basic summary of what was an hour and a half meeting). \n\nThis triggered The Dopamine hit, which then triggered the obsessive need to keep triggering it and has since led me down an absolute BANGER of a rabbit hole into the world of Local LLMs. I had previously dipped my toes in the water, and now I'm just nose diving into the wonderful world of local inference. \n\nSleep? None. Focus? Out the window. Learning new things? Damn near constantly. I fear for my brain, but the dopamine commands that I keep diving. Pray for me brothers and sisters. ",
          "author_fullname": "t2_3vm0jq9j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ADHD &amp; LLM development is a risky cocktail",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m760rq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 27,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 27,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753267871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So like the title suggests, I have ADHD; which on a good day is a double edged sword in terms of learning new things. I gave a demo to the director of engineering at my job with a Unity chat application that was largely powered by a Unity Asset which is a C# wrapper of llama.cpp. &lt;/p&gt;\n\n&lt;p&gt;My director was like &amp;quot;I&amp;#39;m going on vacation for a month, but I want you to focus on learning as much as you can to see where we can apply it in our work&amp;quot; (the really basic summary of what was an hour and a half meeting). &lt;/p&gt;\n\n&lt;p&gt;This triggered The Dopamine hit, which then triggered the obsessive need to keep triggering it and has since led me down an absolute BANGER of a rabbit hole into the world of Local LLMs. I had previously dipped my toes in the water, and now I&amp;#39;m just nose diving into the wonderful world of local inference. &lt;/p&gt;\n\n&lt;p&gt;Sleep? None. Focus? Out the window. Learning new things? Damn near constantly. I fear for my brain, but the dopamine commands that I keep diving. Pray for me brothers and sisters. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m760rq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zichaelpathic",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m760rq/adhd_llm_development_is_a_risky_cocktail/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m760rq/adhd_llm_development_is_a_risky_cocktail/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753267871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\n\nToday, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.\n\n",
          "author_fullname": "t2_e7q9h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 coder will be in multiple sizes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qnpq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 318,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 318,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Today, we&amp;#39;re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we&amp;#39;re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qnpq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dinesh2609",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 503258,
          "created_utc": 1753219525,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been testing Qwen3-Coder-480B (on Hyperbolics) and  Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear.  \n\nQwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like it’s just filling in templates rather than thinking through the task. It’s not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks.  \n\nSome of this might be because Hyperbolics hasn’t fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.\n\nKimi K2 works much better. Even though it’s not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, it’s consistently the better option.",
          "author_fullname": "t2_a29pmyxj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Qwen3 Coder 480B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zz1v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753245282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been testing Qwen3-Coder-480B (on Hyperbolics) and  Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear.  &lt;/p&gt;\n\n&lt;p&gt;Qwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like it’s just filling in templates rather than thinking through the task. It’s not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks.  &lt;/p&gt;\n\n&lt;p&gt;Some of this might be because Hyperbolics hasn’t fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.&lt;/p&gt;\n\n&lt;p&gt;Kimi K2 works much better. Even though it’s not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, it’s consistently the better option.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6zz1v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Pattern9779",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753245282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.\n\nVery strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.\n\nCreds [The Feature Crew](https://www.youtube.com/@TheFeatureCrew) for the original idea.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Web Development",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ny2q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 302,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755865784%2CNWE0NTgyYjVmNzkzNDIwN2EyMGZlNWNjMDZmYmNmMzVjNzQ2NGU4NzQzZGE2MTcxMzA3MjE0OTlhM2UxYmM5OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755865784%2COTE5ZWViMGZiZGIzMTdjYzZjNGU3ZDczZjJmOGU2YjNiOTFhZGUwMzM1YWRiNjExOTNlNTBmY2ZiNGJjNTRmNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 302,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=127481f43d7a622f7d4c23a977a165102347dc33",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213272,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.&lt;/p&gt;\n\n&lt;p&gt;Very strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.&lt;/p&gt;\n\n&lt;p&gt;Creds &lt;a href=\"https://www.youtube.com/@TheFeatureCrew\"&gt;The Feature Crew&lt;/a&gt; for the original idea.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ob9yhvcjahef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?format=pjpg&amp;auto=webp&amp;s=9da74680d1673a7d5086bef35987945fda2390f7",
                  "width": 3024,
                  "height": 1964
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=69ec82c87ea25ca0cf09c32e6e2e65fd1ebe0353",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a7e4214f7585ef0bf76db563e79ceb7b7b73df5",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=da1a4545269003e5a3164b1074444b181b803a22",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ee471871b0892b000bd102b783d4c1fea31bbdf2",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b638262bf1c7530fbee91a29e1a5465444ef5500",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a910147ce667ac1bb6fed7eb4d08a092f3974fe",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ny2q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/",
          "stickied": false,
          "url": "https://v.redd.it/ob9yhvcjahef1",
          "subreddit_subscribers": 503258,
          "created_utc": 1753213272,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755865784%2CNWE0NTgyYjVmNzkzNDIwN2EyMGZlNWNjMDZmYmNmMzVjNzQ2NGU4NzQzZGE2MTcxMzA3MjE0OTlhM2UxYmM5OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755865784%2COTE5ZWViMGZiZGIzMTdjYzZjNGU3ZDczZjJmOGU2YjNiOTFhZGUwMzM1YWRiNjExOTNlNTBmY2ZiNGJjNTRmNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Could this be Deepseek?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 33,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6lf9s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 348,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 348,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WAXw-XuvIZ9mRKeenbrWXREbY65LvO1BDwwwlpUBowY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753207666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qzkjkgegugef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qzkjkgegugef1.png?auto=webp&amp;s=982da5cfa0575f138ae47f73b6eddafc3a141895",
                  "width": 822,
                  "height": 197
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=917acda1e7d58dd2b0c466213686f858f3d1d90f",
                    "width": 108,
                    "height": 25
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac14c427611c1eca254c8bb52ac34a30bf33d9f1",
                    "width": 216,
                    "height": 51
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8524670d8b150c1ad50fa23613a644190b14608f",
                    "width": 320,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e224ff9a214f929b3917304102fe92d67371e639",
                    "width": 640,
                    "height": 153
                  }
                ],
                "variants": {},
                "id": "LX0EdZ_oilMQBPfRRihja4cnZPDhv01xC24KCslp3qQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6lf9s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 59,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/",
          "stickied": false,
          "url": "https://i.redd.it/qzkjkgegugef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753207666,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone brace up for qwen !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 121,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nxh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 233,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 233,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vP7s1FThQpvmySmVJXMTU3-8PcS1dzgy5zKouaE_2IM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mn8auem2bhef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mn8auem2bhef1.png?auto=webp&amp;s=f8d9250eb919b06b9873df5541dfb4181c23ecb3",
                  "width": 1080,
                  "height": 938
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f18ccc22bd1429048af2d71903a4986f10f4370",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e9b0375cba8b59a1f2ff6a059540b35b2e80af5",
                    "width": 216,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd33b6c0d9557d99a79e31264f8c7962a467e6de",
                    "width": 320,
                    "height": 277
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=855c907a55cf3f70afe582932d52350878ef5e68",
                    "width": 640,
                    "height": 555
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=727a28b13a2a4b6feeeb2646b4c5ef5d4feba605",
                    "width": 960,
                    "height": 833
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cdd53e8429b6be0b40bf16e28d255d818f7b04a",
                    "width": 1080,
                    "height": 938
                  }
                ],
                "variants": {},
                "id": "rMAWLMOw9tEiFwd35Iv66C0AmNRfGhg4PeoHVtVBYI4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6nxh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/",
          "stickied": false,
          "url": "https://i.redd.it/mn8auem2bhef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753213236,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "You probably already know about my [benchmark](https://www.designarena.ai/), but here's [context](https://www.reddit.com/r/LocalLLaMA/comments/1lxth6s/comment/n2qoqtk/?context=3) if you missed it. The tldr is that it's a crowdsource benchmark that takes human preferences on frontend and image generations from different models to produce a leaderboard ranking for which models are currently the best at UI and design generation. \n\nI'm going to try to keep these update posts to once-a-week or every other week to not come off as spam (sorry for that earlier, though I'm just seeing interesting results). Also, we realize there are flaws to the leaderboard (as all leaderboards and benchmarks have) that we're progressively trying to improve, but think it has been a good barometer for evaluating the models in particular tiers when it comes to coding. \n\nAnyways, since my last update on the 11th, we've added a few models, and in the last 24 hours, specifically Qwen3-235B-A22B-Instruct-2507 and Qwen3-Coder (less than an hour ago). Though the sample size is still very small, Qwen3-235B-A22B-Instruct-2507 appears to be killing it. I was reading through remarks on Twitter and Reddit that the Instruct model was on par with Opus which I thought was hyperbole at the time, but maybe that claim will hold true in the long run. \n\nWhat has been your experience with these Qwen models and what do you think? Open source is killing it right now. ",
          "author_fullname": "t2_98ouo03z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UI/UX benchmark update 7/22: Newest Qwen models added, Qwen3 takes the lead in terms of win rate (though still early)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ztb2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/yOnjsudSDwwMasrhJ1H10swcbMrmX3jIP8XzRlgDA6k.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753244753,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You probably already know about my &lt;a href=\"https://www.designarena.ai/\"&gt;benchmark&lt;/a&gt;, but here&amp;#39;s &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lxth6s/comment/n2qoqtk/?context=3\"&gt;context&lt;/a&gt; if you missed it. The tldr is that it&amp;#39;s a crowdsource benchmark that takes human preferences on frontend and image generations from different models to produce a leaderboard ranking for which models are currently the best at UI and design generation. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to try to keep these update posts to once-a-week or every other week to not come off as spam (sorry for that earlier, though I&amp;#39;m just seeing interesting results). Also, we realize there are flaws to the leaderboard (as all leaderboards and benchmarks have) that we&amp;#39;re progressively trying to improve, but think it has been a good barometer for evaluating the models in particular tiers when it comes to coding. &lt;/p&gt;\n\n&lt;p&gt;Anyways, since my last update on the 11th, we&amp;#39;ve added a few models, and in the last 24 hours, specifically Qwen3-235B-A22B-Instruct-2507 and Qwen3-Coder (less than an hour ago). Though the sample size is still very small, Qwen3-235B-A22B-Instruct-2507 appears to be killing it. I was reading through remarks on Twitter and Reddit that the Instruct model was on par with Opus which I thought was hyperbole at the time, but maybe that claim will hold true in the long run. &lt;/p&gt;\n\n&lt;p&gt;What has been your experience with these Qwen models and what do you think? Open source is killing it right now. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lcjgeavzvjef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lcjgeavzvjef1.png?auto=webp&amp;s=31df1bef3a627d2ba33e030e86d5d66a2b9b0ee0",
                  "width": 1333,
                  "height": 881
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d4b029e6012124dc9f449076bfcd1f4cfdf2ac1",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1205127af73fd4083008a096571d899bc2b6aadd",
                    "width": 216,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf64c9f604b8aef2fea7c80e52aa0d92de5fb99a",
                    "width": 320,
                    "height": 211
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8550da9c204aaebc89b401002be06079a6beec29",
                    "width": 640,
                    "height": 422
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c69386465fdc08532cb1ba6e8d60c731e32bc4b",
                    "width": 960,
                    "height": 634
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c87d7c433bad8b28ef732c515d7baa7baf7a784c",
                    "width": 1080,
                    "height": 713
                  }
                ],
                "variants": {},
                "id": "nLkR4n4kuyoAJ5FYt5FCPobA_n2hnoAkT68Lm-yyAho"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6ztb2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished-Copy332",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/",
          "stickied": false,
          "url": "https://i.redd.it/lcjgeavzvjef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753244753,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct](https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct)\n\n hyperolic already has it\n\n",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mlbk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 235,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 235,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753210248,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct\"&gt;https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;hyperolic already has it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mlbk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753210248,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_lsixf36sr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m71f20",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 27,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 27,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8d404162721f954167cc891f633466a429b34c96",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753250317,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?auto=webp&amp;s=b0e606fe60c3b427cf1340db7a0ca6006dff3e57",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ad6d1b6c4559472693b7af1de31e24e4a8023a3",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1145966d2cde6471e76bb43f495683a63b013b72",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7fff728a74e01125301fc6c9d2699680540ef0a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60e9638973c964d4a82b7f30f192158867f7fc48",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=350210cdc15e1c044856de883fd8d259a90dd1f0",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9863761bbb4db313f92685d42bb3689971cd9fe8",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m71f20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Wolf-2007",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m71f20/unslothqwen3coder480ba35binstructgguf_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF",
          "subreddit_subscribers": 503258,
          "created_utc": 1753250317,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qc8c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#ab96c2",
          "ups": 127,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 127,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218772,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qc8c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 503258,
          "created_utc": 1753218772,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "It's here guys and qwen nailed it !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4aoalqp6thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebd63c731638a1db5036229c80b9ef7c6e9824fd"
                },
                {
                  "y": 123,
                  "x": 216,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4ac724a3670cd9c86d3d9eed68ced783891cd55"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27543fd2099856fe749e4022dd8e44bdf2a203ec"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c67265c054dccbb5299415ace1ae413e53f4ba40"
                },
                {
                  "y": 550,
                  "x": 960,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9060138fd041f795e2cf4d5c898b3e767230dbfb"
                },
                {
                  "y": 619,
                  "x": 1080,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0aea2f77c1907ccf952d971c3b5b2331c1f6aba9"
                }
              ],
              "s": {
                "y": 1837,
                "x": 3202,
                "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=3202&amp;format=pjpg&amp;auto=webp&amp;s=8e55f7d4fbd65d488aa6606016ce609320a82186"
              },
              "id": "4aoalqp6thef1"
            },
            "mloztw07thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1da39fa93c261ee74d697606209312d155d2610"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08ecfdf3e13689eb562525b1962e0901e009181c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=873f78f3d4bc2c3fc9641a129d06524b3f3f4951"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=468a855136721f0ffae9fb7beacce6df6030447b"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b695450503944ff02456a685ccc317281f8bf8f7"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0ed9b87aa6525e3c700e7835ab55f9fe859d7aa"
                }
              ],
              "s": {
                "y": 1715,
                "x": 3306,
                "u": "https://preview.redd.it/mloztw07thef1.jpg?width=3306&amp;format=pjpg&amp;auto=webp&amp;s=a4886e1284e4cebe0e8558f5cf664cfc4c36b481"
              },
              "id": "mloztw07thef1"
            }
          },
          "name": "t3_1m6qkse",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 82,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "4aoalqp6thef1",
                "id": 711818629
              },
              {
                "caption": "",
                "media_id": "mloztw07thef1",
                "id": 711818630
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 82,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tng4SvC83rVHk9iUXovrs4GeXZmRkFJ59wlPU2wB1GM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m6qkse",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qkse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m6qkse",
          "subreddit_subscribers": 503258,
          "created_utc": 1753219329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6rsym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 64,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6acd0a4d30c117c56e597d84c1ebb5cedb6e4e00",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753222281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/QwenLM/qwen-code",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?auto=webp&amp;s=ea430b9854a08b70e3dd0972ad9e4758c7fc266d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb193a50d7978c33be16ebec135a318dc6943ea1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6992a1a70171bd4f98508b22498e5ac88cdc45df",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5465c1e679bafd45447bd81f6753867f296ffb49",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=21b1ec40f95d195f9c34bb5728616a2b4c3162fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a9eb8de1aed9882d7841738230f2ecef892f334f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a21f51e6f25035a3147bda1057127718b3b29129",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6rsym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/",
          "stickied": false,
          "url": "https://github.com/QwenLM/qwen-code",
          "subreddit_subscribers": 503258,
          "created_utc": 1753222281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "See https://x.com/makingAGI/status/1947286324735856747",
          "author_fullname": "t2_1nt1n3y6xj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone here who has been able to reproduce their results yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6orbr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 91,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 91,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XcNs-lUbqrAcvyj8WfRxfyYgGorJ8nCrbsxZweyByLc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753215098,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;See &lt;a href=\"https://x.com/makingAGI/status/1947286324735856747\"&gt;https://x.com/makingAGI/status/1947286324735856747&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/cfffg12fghef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/cfffg12fghef1.jpeg?auto=webp&amp;s=25f023da9eda3ae6d327e173ef9c7cba8f89880c",
                  "width": 948,
                  "height": 876
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad8d41aa9654515fde6f4b396a86ebf1ad4b0687",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d65ac6156085250ea9aa09ddb48e1e0ca0d499b3",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69d24ed7a665a31761d01a39290d42a299442408",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f02acda8fde9368279ce55c247aa3eb87536a6a5",
                    "width": 640,
                    "height": 591
                  }
                ],
                "variants": {},
                "id": "eXDcYXmDs3JXlXLxg7VAJanyEvhS_GKAlZPpe8O1v6Y"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6orbr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Original_Log_9899",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/",
          "stickied": false,
          "url": "https://i.redd.it/cfffg12fghef1.jpeg",
          "subreddit_subscribers": 503258,
          "created_utc": 1753215098,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_6vcmk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is imminent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6medy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mmNhnm_QiKDvZ8nOArY9M-gXEHPij6ccQfZ3Z4a4vrs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209818,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mruaiodv0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mruaiodv0hef1.png?auto=webp&amp;s=be215118da4d1c5ae5fc739c077ba4bbf8354f1a",
                  "width": 501,
                  "height": 251
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49a20e04a28093446580d2909236b45d1e2f568e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=772268126ecad9399aa5fb8ad3dc61fa7a8e5af0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=daa5e07dcd586edd4e8488215b2df66df2d2c809",
                    "width": 320,
                    "height": 160
                  }
                ],
                "variants": {},
                "id": "gxF1-bhuks7kobb2JTcsN29raeY4IvwO_eL--8kAZ38"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6medy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dudensen",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/",
          "stickied": false,
          "url": "https://i.redd.it/mruaiodv0hef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753209818,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nI just published research on \"thought anchors\" - a method to analyze which specific reasoning steps matter most for task success in locally-runnable models. Thought this community would find the results interesting since it directly compares two popular local models.\n\n**TL;DR: Qwen3-0.6B and DeepSeek-R1-Distill-1.5B have fundamentally different reasoning architectures, not just different performance levels.**\n\n# What are Thought Anchors?\n\nBuilding on work by Bogdan et al., thought anchors identify critical sentences in a model's chain-of-thought reasoning that significantly impact whether it gets the right answer. Instead of looking at individual tokens, we analyze complete reasoning steps.\n\n# Key Findings on GSM8K Math Problems:\n\n**DeepSeek-R1-Distill (1.5B):**\n\n* Concentrated reasoning: fewer steps, higher impact per step (0.408 avg)\n* 82.7% positive reasoning steps - very consistent\n* Single primary failure mode (logical errors)\n* Optimized for reliability over exploration\n\n**Qwen3 (0.6B):**\n\n* Distributed reasoning: more steps, spread impact (0.278 avg)\n* 71.6% positive steps but higher variance\n* Multiple failure modes (logical, computational, missing steps)\n* More experimental approach with higher risk/reward\n\n# Practical Implications for Local Users:\n\nIf you're choosing between these models:\n\n* **Need consistent, reliable outputs?** → DeepSeek-R1's concentrated approach\n* **Want more creative/exploratory reasoning?** → Qwen3's distributed approach\n* **Resource constraints?** → Qwen3 at 0.6B vs DeepSeek at 1.5B\n\nThis isn't about one being \"better\" - they're optimized for different reasoning strategies.\n\n# Open Source Everything:\n\n* **PTS Library**: [https://github.com/codelion/pts](https://github.com/codelion/pts) (tool for generating thought anchors)\n* **Datasets**: Available on HuggingFace for both models\n* **Analysis Code**: Full reproducibility\n* **Article**: [https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors](https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors)\n\nThe PTS library works with any local model that supports structured output, so you can analyze your own models' reasoning patterns.\n\n# Questions for the Community:\n\n1. Has anyone noticed similar reasoning pattern differences in their local setups?\n2. Which reasoning approach works better for your specific use cases?\n3. Any interest in extending this analysis to other popular local models (Llama, Mistral, etc.)?\n\nWould love to hear your experiences and thoughts on model reasoning approaches!\n\n**Edit**: Original thought anchors concept credit goes to Paul Bogdan's team - this research extends their methodology to compare local model architectures.",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Research] Thought Anchors: Understanding How Qwen3-0.6B vs DeepSeek-R1-Distill-1.5B Actually Reason - Different Cognitive Architectures Revealed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zce0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753243256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I just published research on &amp;quot;thought anchors&amp;quot; - a method to analyze which specific reasoning steps matter most for task success in locally-runnable models. Thought this community would find the results interesting since it directly compares two popular local models.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: Qwen3-0.6B and DeepSeek-R1-Distill-1.5B have fundamentally different reasoning architectures, not just different performance levels.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;What are Thought Anchors?&lt;/h1&gt;\n\n&lt;p&gt;Building on work by Bogdan et al., thought anchors identify critical sentences in a model&amp;#39;s chain-of-thought reasoning that significantly impact whether it gets the right answer. Instead of looking at individual tokens, we analyze complete reasoning steps.&lt;/p&gt;\n\n&lt;h1&gt;Key Findings on GSM8K Math Problems:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;DeepSeek-R1-Distill (1.5B):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Concentrated reasoning: fewer steps, higher impact per step (0.408 avg)&lt;/li&gt;\n&lt;li&gt;82.7% positive reasoning steps - very consistent&lt;/li&gt;\n&lt;li&gt;Single primary failure mode (logical errors)&lt;/li&gt;\n&lt;li&gt;Optimized for reliability over exploration&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 (0.6B):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Distributed reasoning: more steps, spread impact (0.278 avg)&lt;/li&gt;\n&lt;li&gt;71.6% positive steps but higher variance&lt;/li&gt;\n&lt;li&gt;Multiple failure modes (logical, computational, missing steps)&lt;/li&gt;\n&lt;li&gt;More experimental approach with higher risk/reward&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Practical Implications for Local Users:&lt;/h1&gt;\n\n&lt;p&gt;If you&amp;#39;re choosing between these models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Need consistent, reliable outputs?&lt;/strong&gt; → DeepSeek-R1&amp;#39;s concentrated approach&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Want more creative/exploratory reasoning?&lt;/strong&gt; → Qwen3&amp;#39;s distributed approach&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Resource constraints?&lt;/strong&gt; → Qwen3 at 0.6B vs DeepSeek at 1.5B&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This isn&amp;#39;t about one being &amp;quot;better&amp;quot; - they&amp;#39;re optimized for different reasoning strategies.&lt;/p&gt;\n\n&lt;h1&gt;Open Source Everything:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;PTS Library&lt;/strong&gt;: &lt;a href=\"https://github.com/codelion/pts\"&gt;https://github.com/codelion/pts&lt;/a&gt; (tool for generating thought anchors)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Datasets&lt;/strong&gt;: Available on HuggingFace for both models&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Analysis Code&lt;/strong&gt;: Full reproducibility&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Article&lt;/strong&gt;: &lt;a href=\"https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors\"&gt;https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The PTS library works with any local model that supports structured output, so you can analyze your own models&amp;#39; reasoning patterns.&lt;/p&gt;\n\n&lt;h1&gt;Questions for the Community:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Has anyone noticed similar reasoning pattern differences in their local setups?&lt;/li&gt;\n&lt;li&gt;Which reasoning approach works better for your specific use cases?&lt;/li&gt;\n&lt;li&gt;Any interest in extending this analysis to other popular local models (Llama, Mistral, etc.)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would love to hear your experiences and thoughts on model reasoning approaches!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Original thought anchors concept credit goes to Paul Bogdan&amp;#39;s team - this research extends their methodology to compare local model architectures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?auto=webp&amp;s=187504b6a6cdaab3b5025c91a3798e0b46bcb9f0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=76a98416b90c3288a04cac47b99811464ff316e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9389c4b2ca46a4b05928b5941369ea699ccec4e6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5964cc2bf092a1202a804fdcec163a7e14497c35",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a192ca72cfe990c9ccd3456b264cd2914962c19",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67133c9db0890c820ce5cdc0549dcaeb9f9e95de",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=978ef4cf2867cf2f582d6936f382955179b16eba",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6zce0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6zce0/research_thought_anchors_understanding_how/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zce0/research_thought_anchors_understanding_how/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753243256,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1M token context length\n\nNo model weights yet, but Qwen3-Coder is already available for testing on [Qwen Chat](https://chat.qwen.ai)",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Available on chat.qwen.ai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mfic",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 87,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/bs1O5LLiPqNQ-8leTmrD3PNczGaZCiiN00b8eacOHzE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;1M token context length&lt;/p&gt;\n\n&lt;p&gt;No model weights yet, but Qwen3-Coder is already available for testing on &lt;a href=\"https://chat.qwen.ai\"&gt;Qwen Chat&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8xj4raow0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8xj4raow0hef1.png?auto=webp&amp;s=6c9d8670b9960f64e78150ca2039fc5471464158",
                  "width": 450,
                  "height": 150
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=67fbf003dc0b0cdf945b3ac5069eddaeeaf26ed5",
                    "width": 108,
                    "height": 36
                  },
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ab53d55f78ada2bc730c345d2b92f3cbd18dad6",
                    "width": 216,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf0cbd6e19276ab7bbf6b36687af35cdf6c00d83",
                    "width": 320,
                    "height": 106
                  }
                ],
                "variants": {},
                "id": "jj-Sn3KQKbzD6M9iYIRnKA0q_gXhkpIWDEoj36IQyis"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mfic",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/",
          "stickied": false,
          "url": "https://i.redd.it/8xj4raow0hef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753209889,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\n\nThis model showed up on my LinkedIn feed today. After listening to a few examples on their [website](https://www.boson.ai/technologies/voice), I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. \n\nListen to this [demo video](https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d), it will just enable so many use cases.\n\nI tried a few examples in their HF [playground](https://huggingface.co/spaces/smola/higgs_audio_v2), it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. \n\n",
          "author_fullname": "t2_6nwb1mbe6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 45,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rmmpgv36tief1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 35,
                  "x": 108,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b8a7927fef36a8dc9cafddd20ca7395324bb30"
                },
                {
                  "y": 70,
                  "x": 216,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa3bc0976c73f20478fe4c41ae0d81d56d9b5efa"
                },
                {
                  "y": 103,
                  "x": 320,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c779645fab14bc4ce1ecf7b4cca7ce06002977dd"
                },
                {
                  "y": 207,
                  "x": 640,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d2890cda589e72ebb380d268d25b2f0c730e4153"
                },
                {
                  "y": 311,
                  "x": 960,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b671678ce82ca5a2abdd86ced5de4262f068a656"
                },
                {
                  "y": 350,
                  "x": 1080,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea15a827df9bb20ae059bea42e6b92ad1d59800b"
                }
              ],
              "s": {
                "y": 872,
                "x": 2686,
                "u": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4"
              },
              "id": "rmmpgv36tief1"
            }
          },
          "name": "t3_1m6vbds",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lDeKkUsKVKJvujnGWUqXtUhpkbsWufoj2laEkKgzAUI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753231503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\"&gt;https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This model showed up on my LinkedIn feed today. After listening to a few examples on their &lt;a href=\"https://www.boson.ai/technologies/voice\"&gt;website&lt;/a&gt;, I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. &lt;/p&gt;\n\n&lt;p&gt;Listen to this &lt;a href=\"https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d\"&gt;demo video&lt;/a&gt;, it will just enable so many use cases.&lt;/p&gt;\n\n&lt;p&gt;I tried a few examples in their HF &lt;a href=\"https://huggingface.co/spaces/smola/higgs_audio_v2\"&gt;playground&lt;/a&gt;, it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6vbds",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Tap3484",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753231503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unsloth quants already starting to roll out for Qwen3-Coder",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6u0gt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=df2ca14ad32406cbfd2154f6392b11b3062c0b80",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753227930,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?auto=webp&amp;s=c6a55f1fe010145ae8782e1593f28ec04aee30a9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ca2157367c76507911bd02cc27f2bd77fdeb58f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a1260c12ba170ca3258b3d164bd71b26d3fd637",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=82c9ef33075d79d05f812d774d3d9963a2ca93c2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=651f424884542b7c34073b3bc62c0fc1b199eaae",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=287c77e1abcef6dc2ad9cac5e8a8d70a85c3f900",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6671827d2f6adf5ce554df23310df3e1e4805228",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6u0gt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/",
          "stickied": false,
          "url": "https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d",
          "subreddit_subscribers": 503258,
          "created_utc": 1753227930,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is available on OpenRouter",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6u3kd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9352d9aaa19f84f05307725c60c6280cb5ce4153",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753228157,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/qwen/qwen3-coder",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?auto=webp&amp;s=8a80c032c084b7af008f30d36302aa3e2b303841",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=81bbe5b26b024567de7a02963aa1047661c30d21",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31982c88f4e8f0ba3e19de3cb4fe1aecc0737271",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4de872e3b3e085cf7e3edcad2410dce6e017ff0c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f4da7fa00b2fee69899af4df9a137f3645df9e7",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b233723391e8a2e702cc58632dd60a7a281a8fbd",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d4e480d1b5455587e7a48ddb01c62b2f0bdbd5a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6u3kd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/",
          "stickied": false,
          "url": "https://openrouter.ai/qwen/qwen3-coder",
          "subreddit_subscribers": 503258,
          "created_utc": 1753228157,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been reading papers where the main contribution is creating a synthetic dataset for a specific task, followed by fine-tuning an LLM on it. One thing I keep noticing: most of them don't seem to perform hyperparameter tuning (e.g., learning rate, epochs, weight decay) using a validation set. Instead, they just reuse common/default values.\n\nI'm wondering—why is this so common?\n\n* Is it because hyperparameter tuning is considered less important, so they did search but skipped reporting it?\n* Or is it because the main contribution is in data creation, so they just don't care much about the fine-tuning details?",
          "author_fullname": "t2_5z9ud297u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why do many papers skip hyperparameter search?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7503r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753264251,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been reading papers where the main contribution is creating a synthetic dataset for a specific task, followed by fine-tuning an LLM on it. One thing I keep noticing: most of them don&amp;#39;t seem to perform hyperparameter tuning (e.g., learning rate, epochs, weight decay) using a validation set. Instead, they just reuse common/default values.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering—why is this so common?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is it because hyperparameter tuning is considered less important, so they did search but skipped reporting it?&lt;/li&gt;\n&lt;li&gt;Or is it because the main contribution is in data creation, so they just don&amp;#39;t care much about the fine-tuning details?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7503r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hwanchang",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753264251,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I saw this claude code router repo on github, but was broken for me, so I rewrote the thing in Go. Is called [Claude Code Open](https://github.com/Davincible/claude-code-open)\n\nNow you can simply `CCO_API_KEY=\"&lt;open router key&gt;\" cco code` and then select `openrouter,qwen/qwen3-coder` as model and voila. Also blocks any Anthropic monitoring requests as a bonus\n\nComplex config available as well and very extensible\n\nHope it helps someone like it did me\n\n[https://github.com/Davincible/claude-code-open](https://github.com/Davincible/claude-code-open)",
          "author_fullname": "t2_166q2z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Github Repo] - Use Qwen3 coder or any other LLM provider with Claude Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zkmm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753243976,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw this claude code router repo on github, but was broken for me, so I rewrote the thing in Go. Is called &lt;a href=\"https://github.com/Davincible/claude-code-open\"&gt;Claude Code Open&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Now you can simply &lt;code&gt;CCO_API_KEY=&amp;quot;&amp;lt;open router key&amp;gt;&amp;quot; cco code&lt;/code&gt; and then select &lt;code&gt;openrouter,qwen/qwen3-coder&lt;/code&gt; as model and voila. Also blocks any Anthropic monitoring requests as a bonus&lt;/p&gt;\n\n&lt;p&gt;Complex config available as well and very extensible&lt;/p&gt;\n\n&lt;p&gt;Hope it helps someone like it did me&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Davincible/claude-code-open\"&gt;https://github.com/Davincible/claude-code-open&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?auto=webp&amp;s=69762d8c4094e5a3ed92bf377de14e0031d69157",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=68a0ec129f7959f5fcb0d85891b6490389d0313a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=98f4159ada43609234c675240fe20cb5e02bfb61",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a539701736c67ff899c97a885e2a1fb429b32749",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=33db0f916eff806bc0ba84a1c10b6d2cd3115f2f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=425a695f92096a4942fa8387ff901601ecfcee49",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=985e0c18d4c3aeb2f4a59c448bfa5f7a3fd196d6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "uAwD8gw9JjbYrLYu8QEcbxdyqjGulmC1fIpWWjJiViY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6zkmm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "davincible",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6zkmm/github_repo_use_qwen3_coder_or_any_other_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zkmm/github_repo_use_qwen3_coder_or_any_other_llm/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753243976,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When I installed this model on a Samsung phone more than a month ago, I didn't find much. When I tested other gemma models today, I found that the output of 3n is very different from other gemma models, and it is also very different from gemini 2.5 flash models. The most similar one is gemini 2.5pro.\n\nhttps://preview.redd.it/pbvwe0821lef1.jpg?width=1848&amp;format=pjpg&amp;auto=webp&amp;s=5459bd24f6ee93191069e104f74ad274ae928006\n\n  \n//The testing method I use is different from most benchmarks. And I don’t use English (which is what many models are optimized for)This avoids falling into the circle of most model optimizations.\n\n[gemini2.5 pro](https://preview.redd.it/pzskn2681lef1.jpg?width=3813&amp;format=pjpg&amp;auto=webp&amp;s=6eeb30dbf49dca2d32e53b6f6cd96fa20a0b7e6c)\n\n[gemini 25. flash](https://preview.redd.it/uj6mt2681lef1.jpg?width=3551&amp;format=pjpg&amp;auto=webp&amp;s=ecfd4e8f097faa8c0ca41c182f150e2116d2a23f)\n\n[gemma 3 27B](https://preview.redd.it/u1zrkifh1lef1.png?width=3523&amp;format=png&amp;auto=webp&amp;s=6ac99fa0e552198813ec146b7ea83911a0675ec9)\n\n  \n\n\n//Judging from the output content, **the knowledge bases of 3N and gemini2.5 pro are highly overlapping**.\n\n//gemma 3 27B's answer actually contains many errors.\n\nhttps://preview.redd.it/6xz6k6sk1lef1.jpg?width=2560&amp;format=pjpg&amp;auto=webp&amp;s=e62a0c13d13b5e12d7511071d1284d5708afe6a7\n\n//There is a very difficult point here. The photo I posted was taken by myself, and it is located in Tibet. Because this is an edge direction that many models will not deliberately strengthen during training, I often use it to test the model's knowledge base. In addition, many models do not recognize this photo as Lhasa, but as Nepal, etc. This error will be very obvious on models with small parameters. 3N does not have this problem at all. You can notice that even the gemini2.5flash model did not correctly identify the specific city and temple.\n\n//In fact, some people also mentioned geographic information matching, or image matching on the Internet. You should know that 3N is an offline model. Even with a geographic information matching module, this image is an extremely difficult problem. Because this image is more than ten years old, there is no obvious landmark in Lhasa in the distance to match.  \n//By the way, I have tried for more than a week to convert  medgemma into an Android APP version, but I have not been successful.\n\n\n\n\n\n  \n\n\n",
          "author_fullname": "t2_chilocifj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone noticed that the gemma3n model doesn't look like a gemma, but more like a gemini mini?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "pbvwe0821lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 172,
                  "x": 108,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c306d897c1118e1c1311453b98d50c51f4d47275"
                },
                {
                  "y": 345,
                  "x": 216,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13408ebd2e5cefe51e1b438d2ad82f0a11ce4baa"
                },
                {
                  "y": 512,
                  "x": 320,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=081e65c703af3111b9fda2713a3d62aec064e23e"
                },
                {
                  "y": 1025,
                  "x": 640,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba7d25947a96df24e125543b9ba373e4882fd70a"
                },
                {
                  "y": 1537,
                  "x": 960,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49fcd723039eef70d914184f8f2850eb25186909"
                },
                {
                  "y": 1729,
                  "x": 1080,
                  "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a0e8c255d0c3a3bcb9ddc8d5ef4841e2753f1b6"
                }
              ],
              "s": {
                "y": 2960,
                "x": 1848,
                "u": "https://preview.redd.it/pbvwe0821lef1.jpg?width=1848&amp;format=pjpg&amp;auto=webp&amp;s=5459bd24f6ee93191069e104f74ad274ae928006"
              },
              "id": "pbvwe0821lef1"
            },
            "uj6mt2681lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 58,
                  "x": 108,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aca53d35bf10e8bb0294a8937e81439c8f7cad36"
                },
                {
                  "y": 117,
                  "x": 216,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b59d02aac658ca78f6d571274c40a94ef2d0d80b"
                },
                {
                  "y": 174,
                  "x": 320,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13ed3f686e7839077347ffa846500fbc3b6a565b"
                },
                {
                  "y": 348,
                  "x": 640,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=faa7da4e124fcc1ebe44e688871e7e4ce726b5cd"
                },
                {
                  "y": 522,
                  "x": 960,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd69a179eef834753c933b43ae3006dbab5ba577"
                },
                {
                  "y": 587,
                  "x": 1080,
                  "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bfa551bfdb50f03fc463b819210526cd1512964"
                }
              ],
              "s": {
                "y": 1933,
                "x": 3551,
                "u": "https://preview.redd.it/uj6mt2681lef1.jpg?width=3551&amp;format=pjpg&amp;auto=webp&amp;s=ecfd4e8f097faa8c0ca41c182f150e2116d2a23f"
              },
              "id": "uj6mt2681lef1"
            },
            "u1zrkifh1lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 52,
                  "x": 108,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=83dd625944274fe8b57e70fd4ec30db7f8bc4f58"
                },
                {
                  "y": 105,
                  "x": 216,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a2837bf3e99dce9e463b520761b03190c1d0ce6"
                },
                {
                  "y": 155,
                  "x": 320,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2cfa596c0e26212f8e0e8a068447b932e380726"
                },
                {
                  "y": 311,
                  "x": 640,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d36dbd406424ed2d7a2aa738bcd5b3177057d2c9"
                },
                {
                  "y": 467,
                  "x": 960,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6dbd8ef6e920c8534ae0d0915428719b3d197ab0"
                },
                {
                  "y": 526,
                  "x": 1080,
                  "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23a508f94056f900e29543cca166dc4cb1d54d68"
                }
              ],
              "s": {
                "y": 1717,
                "x": 3523,
                "u": "https://preview.redd.it/u1zrkifh1lef1.png?width=3523&amp;format=png&amp;auto=webp&amp;s=6ac99fa0e552198813ec146b7ea83911a0675ec9"
              },
              "id": "u1zrkifh1lef1"
            },
            "6xz6k6sk1lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0347afc5a48973355a6ccd141c87bd43beb7306"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eae2cc8ce89b24d0cbeb7fbd51c38c208308be8e"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cc2d05745d3982582bde110dcf5e9587d0931de"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=232d1b94ba3aa4555837abb257be597ba67dc3f1"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d6deed7ff68aff4b756f62e4f7e6c4b5d7dcf7f"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d3da6858fc9d21478afe51eb83d1569d9114fd7"
                }
              ],
              "s": {
                "y": 1440,
                "x": 2560,
                "u": "https://preview.redd.it/6xz6k6sk1lef1.jpg?width=2560&amp;format=pjpg&amp;auto=webp&amp;s=e62a0c13d13b5e12d7511071d1284d5708afe6a7"
              },
              "id": "6xz6k6sk1lef1"
            },
            "pzskn2681lef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 58,
                  "x": 108,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=34948da6ac1630f6d0ec343feebee7e8decb5b18"
                },
                {
                  "y": 117,
                  "x": 216,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6d54a0ce4eeadbe62a0f628500c6a079ca8c809"
                },
                {
                  "y": 173,
                  "x": 320,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdc846f07511a088f327cc59c3e3b677054e3ab5"
                },
                {
                  "y": 347,
                  "x": 640,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17ab4e68ce36485e424735df36efe341f64194a5"
                },
                {
                  "y": 520,
                  "x": 960,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=46900fb962688998795aeb62ed990b7444920df5"
                },
                {
                  "y": 586,
                  "x": 1080,
                  "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1251a93f7117c64dca70d64525cede4025acd7f1"
                }
              ],
              "s": {
                "y": 2069,
                "x": 3813,
                "u": "https://preview.redd.it/pzskn2681lef1.jpg?width=3813&amp;format=pjpg&amp;auto=webp&amp;s=6eeb30dbf49dca2d32e53b6f6cd96fa20a0b7e6c"
              },
              "id": "pzskn2681lef1"
            }
          },
          "name": "t3_1m73ohk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ip6dBmHZN-29Fal7BH5Naow0oNhtVP0fjBeYTW1RLcg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753258999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I installed this model on a Samsung phone more than a month ago, I didn&amp;#39;t find much. When I tested other gemma models today, I found that the output of 3n is very different from other gemma models, and it is also very different from gemini 2.5 flash models. The most similar one is gemini 2.5pro.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pbvwe0821lef1.jpg?width=1848&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5459bd24f6ee93191069e104f74ad274ae928006\"&gt;https://preview.redd.it/pbvwe0821lef1.jpg?width=1848&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5459bd24f6ee93191069e104f74ad274ae928006&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;//The testing method I use is different from most benchmarks. And I don’t use English (which is what many models are optimized for)This avoids falling into the circle of most model optimizations.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pzskn2681lef1.jpg?width=3813&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6eeb30dbf49dca2d32e53b6f6cd96fa20a0b7e6c\"&gt;gemini2.5 pro&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uj6mt2681lef1.jpg?width=3551&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ecfd4e8f097faa8c0ca41c182f150e2116d2a23f\"&gt;gemini 25. flash&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u1zrkifh1lef1.png?width=3523&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ac99fa0e552198813ec146b7ea83911a0675ec9\"&gt;gemma 3 27B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;//Judging from the output content, &lt;strong&gt;the knowledge bases of 3N and gemini2.5 pro are highly overlapping&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;//gemma 3 27B&amp;#39;s answer actually contains many errors.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6xz6k6sk1lef1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e62a0c13d13b5e12d7511071d1284d5708afe6a7\"&gt;https://preview.redd.it/6xz6k6sk1lef1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e62a0c13d13b5e12d7511071d1284d5708afe6a7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;//There is a very difficult point here. The photo I posted was taken by myself, and it is located in Tibet. Because this is an edge direction that many models will not deliberately strengthen during training, I often use it to test the model&amp;#39;s knowledge base. In addition, many models do not recognize this photo as Lhasa, but as Nepal, etc. This error will be very obvious on models with small parameters. 3N does not have this problem at all. You can notice that even the gemini2.5flash model did not correctly identify the specific city and temple.&lt;/p&gt;\n\n&lt;p&gt;//In fact, some people also mentioned geographic information matching, or image matching on the Internet. You should know that 3N is an offline model. Even with a geographic information matching module, this image is an extremely difficult problem. Because this image is more than ten years old, there is no obvious landmark in Lhasa in the distance to match.&lt;br/&gt;\n//By the way, I have tried for more than a week to convert  medgemma into an Android APP version, but I have not been successful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m73ohk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mountain_TANG",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73ohk/has_anyone_noticed_that_the_gemma3n_model_doesnt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73ohk/has_anyone_noticed_that_the_gemma3n_model_doesnt/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753258999,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/ikawrakow/ik\\_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp)\n\nFriendly reminder to back up all the things!",
          "author_fullname": "t2_8u7n5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The ik_llama.cpp repository is back! \\o/",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6cfzi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 198,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 198,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186412,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?auto=webp&amp;s=7c74a86a8d22a1d2e90ce704f456a5a36cf050e7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9057a5a31598407ca7946c278de43e70cf0c9ed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=533c07c0d65f89514a6ba54ce5f1c6649e969c77",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9699bd0efa2b26a1c034cdb0fe8abc1317589b6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=355e07ff2e46e3a253b40e25c06644c7282af5b2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c7a47a3cb456bb05dfd53a716bae5ef6addff5e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fbd5e6235d1a60da5c17ef35a7bd40a655c4d80",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6cfzi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thireus",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753186412,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,  \nI have 2x 7900 XTX and not getting any model run with them in a docker.\n\n    docker pull rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702\n    \n    docker run -it \\\n      --dns=1.1.1.1 \\\n      --dns=8.8.8.8 \\\n      --network=host \\\n      --group-add=video \\\n      --ipc=host \\\n      --cap-add=SYS_PTRACE \\\n      --security-opt seccomp=unconfined \\\n      --privileged \\\n      --device /dev/kfd \\\n      --device /dev/dri \\\n      -e ROCM_VISIBLE_DEVICES=0,1,2,3 \\\n      -e HIP_VISIBLE_DEVICES=0,1,2,3 \\\n      -e CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n      -e VLLM_USE_TRITON_FLASH_ATTN=0 \\\n      -e PYTORCH_TUNABLEOP_ENABLED=1 \\\n      -e HSA_OVERRIDE_GFX_VERSION=11.0.0 \\\n      -e PYTORCH_ROCM_ARCH=\"gfx1100\" \\\n      -e GPU_MAX_HW_QUEUES=1 \\\n      -v /home/ubuntu/vllm_models:/workspace/models \\\n      rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702 bash\n\napt update &amp;&amp; apt install -y git build-essential\n\npip install ninja\n\npip3 install -U xformers --index-url [https://download.pytorch.org/whl/rocm6.3](https://download.pytorch.org/whl/rocm6.3)\n\n    vllm serve /workspace/models/DeepSeek-R1-Distill-Qwen-14B/ \\\n      --dtype float16 \\\n      --kv-cache-dtype auto \\\n      --tensor-parallel-size 2 \\\n      --trust-remote-code \\\n      --tokenizer_mode auto \\\n      --port 8000 \\\n      --host 0.0.0.0 \n\nHave tried different models, some may go to a point where vllm says \"started and waiting\" but then when trying to chat with it all crashes.\n\nHow this is so hard? What is AMD doing for this? Or are we dummer meant to fall back to Ollama? AMD makes me very sad.\n\nEDIT: I trusted you AMD, I really did....",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting a model run with vLLM and 7900 XTX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m75i0b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753272524,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753266053,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI have 2x 7900 XTX and not getting any model run with them in a docker.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker pull rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702\n\ndocker run -it \\\n  --dns=1.1.1.1 \\\n  --dns=8.8.8.8 \\\n  --network=host \\\n  --group-add=video \\\n  --ipc=host \\\n  --cap-add=SYS_PTRACE \\\n  --security-opt seccomp=unconfined \\\n  --privileged \\\n  --device /dev/kfd \\\n  --device /dev/dri \\\n  -e ROCM_VISIBLE_DEVICES=0,1,2,3 \\\n  -e HIP_VISIBLE_DEVICES=0,1,2,3 \\\n  -e CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n  -e VLLM_USE_TRITON_FLASH_ATTN=0 \\\n  -e PYTORCH_TUNABLEOP_ENABLED=1 \\\n  -e HSA_OVERRIDE_GFX_VERSION=11.0.0 \\\n  -e PYTORCH_ROCM_ARCH=&amp;quot;gfx1100&amp;quot; \\\n  -e GPU_MAX_HW_QUEUES=1 \\\n  -v /home/ubuntu/vllm_models:/workspace/models \\\n  rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702 bash\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;apt update &amp;amp;&amp;amp; apt install -y git build-essential&lt;/p&gt;\n\n&lt;p&gt;pip install ninja&lt;/p&gt;\n\n&lt;p&gt;pip3 install -U xformers --index-url &lt;a href=\"https://download.pytorch.org/whl/rocm6.3\"&gt;https://download.pytorch.org/whl/rocm6.3&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;vllm serve /workspace/models/DeepSeek-R1-Distill-Qwen-14B/ \\\n  --dtype float16 \\\n  --kv-cache-dtype auto \\\n  --tensor-parallel-size 2 \\\n  --trust-remote-code \\\n  --tokenizer_mode auto \\\n  --port 8000 \\\n  --host 0.0.0.0 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Have tried different models, some may go to a point where vllm says &amp;quot;started and waiting&amp;quot; but then when trying to chat with it all crashes.&lt;/p&gt;\n\n&lt;p&gt;How this is so hard? What is AMD doing for this? Or are we dummer meant to fall back to Ollama? AMD makes me very sad.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I trusted you AMD, I really did....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m75i0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753266053,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday's release! I'll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3\\_K\\_L is runnable (though slow) for anybody with similar PC specs and produces something usable!\n\nI used LM Studio for loading the model:\n\n* Context: 4096 (default)\n* GPU Offload: 18 / 94\n* CPU Thread Pool: 16\n* ... all else default besides ...\n* Flash Attention: On\n\nWhen loaded, it used up 23.3GB of VRAM and \\~80GB of RAM.\n\nBasic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token",
          "author_fullname": "t2_8l0jj9jq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ct7u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 166,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 698,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1x5u9hrp5fef1/DASHPlaylist.mpd?a=1755865784%2CMzdlZTViY2VkMjAwYjM2Mzc5NDk4MDE0ZWJhMmQyMjBlMDc4MGNhOTE3NWYxZGVkN2E3NGU0ZTE0OTVjMGM3Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/1x5u9hrp5fef1/HLSPlaylist.m3u8?a=1755865784%2CNTgyNjk1YWE2YTliNDM2OGExZGU0NmE1NDRjMjQ4MzA5YmRmY2UyZTYwNGRmZDRiM2Y3NWFjZDI1MmMxNGRlZg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 166,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=448f64b1e900a0ecbdc8a71bf39468b788eff73b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753187462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday&amp;#39;s release! I&amp;#39;ll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3_K_L is runnable (though slow) for anybody with similar PC specs and produces something usable!&lt;/p&gt;\n\n&lt;p&gt;I used LM Studio for loading the model:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Context: 4096 (default)&lt;/li&gt;\n&lt;li&gt;GPU Offload: 18 / 94&lt;/li&gt;\n&lt;li&gt;CPU Thread Pool: 16&lt;/li&gt;\n&lt;li&gt;... all else default besides ...&lt;/li&gt;\n&lt;li&gt;Flash Attention: On&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When loaded, it used up 23.3GB of VRAM and ~80GB of RAM.&lt;/p&gt;\n\n&lt;p&gt;Basic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/1x5u9hrp5fef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?format=pjpg&amp;auto=webp&amp;s=58a925e2785f62712af69dad90636ab48df32160",
                  "width": 480,
                  "height": 698
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=74909995fdb7a4a31d72b707fc5a6406503d7c48",
                    "width": 108,
                    "height": 157
                  },
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a8bc0a3ee9ac60bd5820c383ea906cb68b5b9d1d",
                    "width": 216,
                    "height": 314
                  },
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4b70a04846a7db43f33c721283fb6e592dd8d570",
                    "width": 320,
                    "height": 465
                  }
                ],
                "variants": {},
                "id": "MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ct7u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aidanjustsayin",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/",
          "stickied": false,
          "url": "https://v.redd.it/1x5u9hrp5fef1",
          "subreddit_subscribers": 503258,
          "created_utc": 1753187462,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 698,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1x5u9hrp5fef1/DASHPlaylist.mpd?a=1755865784%2CMzdlZTViY2VkMjAwYjM2Mzc5NDk4MDE0ZWJhMmQyMjBlMDc4MGNhOTE3NWYxZGVkN2E3NGU0ZTE0OTVjMGM3Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/1x5u9hrp5fef1/HLSPlaylist.m3u8?a=1755865784%2CNTgyNjk1YWE2YTliNDM2OGExZGU0NmE1NDRjMjQ4MzA5YmRmY2UyZTYwNGRmZDRiM2Y3NWFjZDI1MmMxNGRlZg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys I was recently looking at this paper, which mentions that finetuning models on even benign datasets (both full FT and LoRA) can cause safety regressions : [https://arxiv.org/abs/2310.03693](https://arxiv.org/abs/2310.03693) \n\nHave you ever observed a model getting less safe / more likely to respond to off-limits prompts after fine-tuning it, even though you fine-tuned it on clean, benign data? I'm interested if this happens in real world use cases or if it's just a research artifact.",
          "author_fullname": "t2_3r8rnyl2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone seen safety regressions after fine-tuning LLaMA or Mistral on clean data?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m73n0t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753258836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys I was recently looking at this paper, which mentions that finetuning models on even benign datasets (both full FT and LoRA) can cause safety regressions : &lt;a href=\"https://arxiv.org/abs/2310.03693\"&gt;https://arxiv.org/abs/2310.03693&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Have you ever observed a model getting less safe / more likely to respond to off-limits prompts after fine-tuning it, even though you fine-tuned it on clean, benign data? I&amp;#39;m interested if this happens in real world use cases or if it&amp;#39;s just a research artifact.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m73n0t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "whalefal",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73n0t/anyone_seen_safety_regressions_after_finetuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73n0t/anyone_seen_safety_regressions_after_finetuning/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753258836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title - the goal is to be able to run 70b models for free using p2p sharding like BitTorrent. Have a lil node network!\n\nAnyone building in rust/wasm?? I’m a python / ts dev at heart so it’s going to be a steep learning curve! ",
          "author_fullname": "t2_d7sagw09",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a p2p inference engine in rust and hugging face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m775h2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753271484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title - the goal is to be able to run 70b models for free using p2p sharding like BitTorrent. Have a lil node network!&lt;/p&gt;\n\n&lt;p&gt;Anyone building in rust/wasm?? I’m a python / ts dev at heart so it’s going to be a steep learning curve! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m775h2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "earningtheewage",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m775h2/building_a_p2p_inference_engine_in_rust_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m775h2/building_a_p2p_inference_engine_in_rust_and/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753271484,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, i was scrolling on LM studio and always saw model like \"model_name_q4_k_m.gguf\" everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some \"dynamic quantization\" but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?\n\nthank by advance ",
          "author_fullname": "t2_jgegifux8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does the _K _S _M _L mean behind the quantization of a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6tbhm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753226118,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, i was scrolling on LM studio and always saw model like &amp;quot;model_name_q4_k_m.gguf&amp;quot; everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some &amp;quot;dynamic quantization&amp;quot; but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?&lt;/p&gt;\n\n&lt;p&gt;thank by advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6tbhm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hurtcraft01",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753226118,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I’ve been diving into alternative architectures to transformers recently, and I came across a few interesting ones. liquid foundation models (lfm), Mamba (ssm based) and RWKV. I’m curious about what these new architectures offer and what their limitations are. From what I understand, they all seem to be better at handling long sequences, SSMs and LFMs are more resource efficient and LFMs seem to struggle with wide area applications (?)\nI’m still trying to fully grasp how these models compare to transformers, so I’d love to hear more about the strengths and weaknesses of these newer architectures. Any insights would be appreciated!",
          "author_fullname": "t2_y1vyie97k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do new architectures offer and what are their limits?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m76df6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753269030,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I’ve been diving into alternative architectures to transformers recently, and I came across a few interesting ones. liquid foundation models (lfm), Mamba (ssm based) and RWKV. I’m curious about what these new architectures offer and what their limitations are. From what I understand, they all seem to be better at handling long sequences, SSMs and LFMs are more resource efficient and LFMs seem to struggle with wide area applications (?)\nI’m still trying to fully grasp how these models compare to transformers, so I’d love to hear more about the strengths and weaknesses of these newer architectures. Any insights would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m76df6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ba2sYd",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753269030,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?",
          "author_fullname": "t2_10rx6s0f1q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone tried Hierarchical Reasoning Models yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ufm4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753229040,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6ufm4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackboulder33",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753229040,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Strix Halo \"Ryzen AI MAX\" APUs Come To DIY PC Builders With New MoDT \"Mini-ITX\" Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6bddm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 120,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 120,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=140&amp;height=81&amp;crop=140:81,smart&amp;auto=webp&amp;s=00290105ce815b049672399f0e7e28e9d1afcbc9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753183102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?auto=webp&amp;s=10c3b72c0b82b9a62677b9306104bb21064031ab",
                  "width": 2471,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=097bbe6e55a92f58db20c497b5cd55b71c248bb0",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=20e37d29ee5af324410ac9397017c9ae2f497b28",
                    "width": 216,
                    "height": 125
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9a76caea9352dd47f6185e29c67bab1c2374a02",
                    "width": 320,
                    "height": 186
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c63f2527e38ed9f9fb783cd700b8e831108fe01",
                    "width": 640,
                    "height": 372
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92e328c881739b5b57de5c1f24a60a877d78657",
                    "width": 960,
                    "height": 559
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61c8a758226b77b202d02e702bc26f1ac195c306",
                    "width": 1080,
                    "height": 629
                  }
                ],
                "variants": {},
                "id": "wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6bddm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 71,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/",
          "stickied": false,
          "url": "https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753183102,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/lechmazur/bazaar](https://github.com/lechmazur/bazaar)\n\nEach LLM is a buyer or seller with a secret price limit. In 30 rounds, they submit sealed bids/asks. They only see the results of past rounds. 8 agents per game: 4 buyers and 4 sellers, each with a private value drawn from one of the distributions.\n\nFour market conditions (distributions) to measure their adaptability: uniform, correlated, bimodal, heavy-tailed.\n\nKey Metric: Conditional Surplus Alpha (CSα) – normalizes profit against a \"truthful\" baseline (bid your exact value).\n\nAll agents simultaneously submit bids (buyers) or asks (sellers). The engine matches the highest bids with the lowest asks. Trades clear at the midpoint between matched quotes. After each round, all quotes and trades become public history.\n\nBAZAAR compares LLMs to 30+ algorithmic baselines: classic ZIP, Gjerstad-Dickhaut, Q-learning, Momentum, Adaptive Aggressive, Mean Reversion, Roth-Erev, Risk-Aware, Enhanced Bayesian, Contrarian, Sniper, Adversarial Exploiter, even a genetic optimizer.\n\nWith chat enabled, LLMs form illegal cartels.\n\n",
          "author_fullname": "t2_p2tr0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "A new LLM benchmark for markets, supply chains, and trading: BAZAAR. Agents must understand supply, demand, and risk, and learn to bid strategically.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 130,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "krskqj5kxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52842a9e07595d9c9c3601a2dea21160adf1de8a"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51debf7127ddb9eb252f98403727de4ea71e3b26"
                },
                {
                  "y": 220,
                  "x": 320,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc37da5f71f16ac60f4021f12058eb6f7d1d1ffa"
                },
                {
                  "y": 440,
                  "x": 640,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd7e2d668330dc48f70489a82adc44bb6ac8b783"
                },
                {
                  "y": 660,
                  "x": 960,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=20373c287598ec9c9ec1b3675c4e6bfc60416e91"
                },
                {
                  "y": 742,
                  "x": 1080,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b83f6051676e0cda84cceb93c1ed1814a2480891"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1600,
                "u": "https://preview.redd.it/krskqj5kxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=4ff785353e78862c3e5aa1051c1ed608279d4ce2"
              },
              "id": "krskqj5kxgef1"
            },
            "sm28w3jkxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a106a7c31b839e001fb11d95974dd872e057d3ea"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=abdee405d15714aab50ee9bac28da807c9c5dce5"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ee13743536f18f83ee647bad91676e134d8b73"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=68347db5d6b681079896f10816cf011e6ea016f7"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba2a511f53896c7fcf10c27c5dd3474ac86cebaa"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b99c23960f58a2a10376f2b66638477cbb0759e"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1760,
                "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=1760&amp;format=png&amp;auto=webp&amp;s=0dd57e157ff15bab79010c7c0f70eb09a62fd728"
              },
              "id": "sm28w3jkxgef1"
            },
            "1lxnjuhjxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 100,
                  "x": 108,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=72e24d5b1d0e1dd0daa8f02a9b2f774f306babb2"
                },
                {
                  "y": 201,
                  "x": 216,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1def4cd9fb7732ec050a5cd207c21d461eec1206"
                },
                {
                  "y": 298,
                  "x": 320,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c280e397d6012029315b0e383ea58668cf027866"
                },
                {
                  "y": 597,
                  "x": 640,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=760be0af867d8774678164fab47b6cc0171286f3"
                },
                {
                  "y": 896,
                  "x": 960,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=17061c04c488808ca4b62703ccc6ec12c844d024"
                },
                {
                  "y": 1008,
                  "x": 1080,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15167538cbae4dd9f2ff0fa9f7ac8aac5a4a2975"
                }
              ],
              "s": {
                "y": 1400,
                "x": 1500,
                "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=249b3b40abc90da4b77f7d32efeb03508565afde"
              },
              "id": "1lxnjuhjxgef1"
            },
            "hpjd1p6ixgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 100,
                  "x": 108,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6293b2a19de7d9ba0bbaab4f4dcb0149ebed7f9e"
                },
                {
                  "y": 201,
                  "x": 216,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa5d9cccc426c744502b2fcb6d56ec099fafef8a"
                },
                {
                  "y": 298,
                  "x": 320,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fba153c47c966a618a5ab2e86458569dc3546eb"
                },
                {
                  "y": 597,
                  "x": 640,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d92f53373a034c556e7508e70078158b5109615"
                },
                {
                  "y": 896,
                  "x": 960,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee327f3041a80dee0a0cc5ba5125fb752a8e87df"
                },
                {
                  "y": 1008,
                  "x": 1080,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a74df25ea42c2f72fd616e87306c704f3e0ed510"
                }
              ],
              "s": {
                "y": 1400,
                "x": 1500,
                "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=8e0c2e04bb81b4cafd539e93483917338b2a7cbf"
              },
              "id": "hpjd1p6ixgef1"
            },
            "halpj9alxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=71af75f43565b17c222ee28e6536557bf0c3b66b"
                },
                {
                  "y": 316,
                  "x": 216,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=baf6edfa44cc6dd4c0857a1d46e9e8430115b516"
                },
                {
                  "y": 469,
                  "x": 320,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b38103d8019a9abc83c8d4e322eac7e3f93e149"
                },
                {
                  "y": 938,
                  "x": 640,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=803ba0808b04d93186cedc3fd4412004029ce523"
                },
                {
                  "y": 1408,
                  "x": 960,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4403712c39c9d5c921f773419ac84c9c967acdab"
                },
                {
                  "y": 1584,
                  "x": 1080,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b02503a0234e0e29e9cc7cbda79516bc27d5ce8d"
                }
              ],
              "s": {
                "y": 2200,
                "x": 1500,
                "u": "https://preview.redd.it/halpj9alxgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=36b00f321d44dbba4e8b0c7041f074a668f9f5cc"
              },
              "id": "halpj9alxgef1"
            },
            "kbimogvjxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=21566b4eaa8e0018afdbcde3998ab0c87bec3864"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2e518755ea98beae9b0afc7a74e41272fec936a"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a101a0df12dcd241bd826dbfacefcce95f5a07d"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=158915b11d94f85eb7c4d52f3d5a5666038e173d"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c11c0e086b3dff1a9c401a1b5d827e8decdba3d2"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01e4040beb9b0c46060f90a7ef583b60fd18db8e"
                }
              ],
              "s": {
                "y": 1000,
                "x": 1600,
                "u": "https://preview.redd.it/kbimogvjxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=f440c7f305ed6913c05879610375ba51cf28b48a"
              },
              "id": "kbimogvjxgef1"
            },
            "n6pbexvixgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c138d9032ae311f137755885cc0fcc0f6238e39"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b33900c58d0c49879f33d0b02cf46e500ecc981a"
                },
                {
                  "y": 220,
                  "x": 320,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5abe3365b02062b47ea098312f8e91aaa9b1c020"
                },
                {
                  "y": 440,
                  "x": 640,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d65f8cad0c9583d4dba040e27503305982260bd"
                },
                {
                  "y": 660,
                  "x": 960,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2870cf430de5b5aee66e3c2a277bac99eb35522"
                },
                {
                  "y": 742,
                  "x": 1080,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2bb320350260da19bfcd63aad9e05c223ff7c949"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1600,
                "u": "https://preview.redd.it/n6pbexvixgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=ab9cb5daeba91d7a5dcc8702e8682e511b2c39c4"
              },
              "id": "n6pbexvixgef1"
            },
            "ynn314skxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 101,
                  "x": 108,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1999d49a70e9400e739c2a68cfaa8117e059ceb7"
                },
                {
                  "y": 202,
                  "x": 216,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c0408ab7d16d8a0ffa87e03021c2e84a7cd0f47"
                },
                {
                  "y": 300,
                  "x": 320,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d09aa8b184cc4a2b2acb4c7d41278907884d79d8"
                },
                {
                  "y": 600,
                  "x": 640,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2bd12e78edee4049b2ea5f39bd728943a1f72cf"
                },
                {
                  "y": 900,
                  "x": 960,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7d2d61c313d3bcf2e9dbab634c3444733306afb"
                },
                {
                  "y": 1012,
                  "x": 1080,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8384cbde378a8494e262b0397b8c7e0f21c606f5"
                }
              ],
              "s": {
                "y": 1500,
                "x": 1600,
                "u": "https://preview.redd.it/ynn314skxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=70915d68ffb1eb141632310b69caba6aa08dc162"
              },
              "id": "ynn314skxgef1"
            }
          },
          "name": "t3_1m6m0f7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 30,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "hpjd1p6ixgef1",
                "id": 711713756
              },
              {
                "media_id": "n6pbexvixgef1",
                "id": 711713757
              },
              {
                "media_id": "1lxnjuhjxgef1",
                "id": 711713758
              },
              {
                "media_id": "kbimogvjxgef1",
                "id": 711713759
              },
              {
                "media_id": "krskqj5kxgef1",
                "id": 711713760
              },
              {
                "media_id": "sm28w3jkxgef1",
                "id": 711713761
              },
              {
                "media_id": "ynn314skxgef1",
                "id": 711713762
              },
              {
                "media_id": "halpj9alxgef1",
                "id": 711713763
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/EFJzlTp5mgKfQzJuDpD-TjIuXrwPWnGGidBsBvFKiPg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753208973,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/lechmazur/bazaar\"&gt;https://github.com/lechmazur/bazaar&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Each LLM is a buyer or seller with a secret price limit. In 30 rounds, they submit sealed bids/asks. They only see the results of past rounds. 8 agents per game: 4 buyers and 4 sellers, each with a private value drawn from one of the distributions.&lt;/p&gt;\n\n&lt;p&gt;Four market conditions (distributions) to measure their adaptability: uniform, correlated, bimodal, heavy-tailed.&lt;/p&gt;\n\n&lt;p&gt;Key Metric: Conditional Surplus Alpha (CSα) – normalizes profit against a &amp;quot;truthful&amp;quot; baseline (bid your exact value).&lt;/p&gt;\n\n&lt;p&gt;All agents simultaneously submit bids (buyers) or asks (sellers). The engine matches the highest bids with the lowest asks. Trades clear at the midpoint between matched quotes. After each round, all quotes and trades become public history.&lt;/p&gt;\n\n&lt;p&gt;BAZAAR compares LLMs to 30+ algorithmic baselines: classic ZIP, Gjerstad-Dickhaut, Q-learning, Momentum, Adaptive Aggressive, Mean Reversion, Roth-Erev, Risk-Aware, Enhanced Bayesian, Contrarian, Sniper, Adversarial Exploiter, even a genetic optimizer.&lt;/p&gt;\n\n&lt;p&gt;With chat enabled, LLMs form illegal cartels.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m6m0f7",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6m0f7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zero0_one1",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6m0f7/a_new_llm_benchmark_for_markets_supply_chains_and/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m6m0f7",
          "subreddit_subscribers": 503258,
          "created_utc": 1753208973,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m trying to understand how models like **Gemini 2.5 Pro** achieve *native* 1 million token context windows.\n\nFrom what I’ve seen in models like **Qwen3** or **LLaMA**, they use techniques like **RoPE scaling** (e.g., YaRN, NTK-aware RoPE, Position Interpolation) to extrapolate context beyond what was trained. These methods usually need fine-tuning, and even then, there's often a soft limit beyond which attention weakens significantly.\n\nBut Gemini claims *native* 1M context, and benchmarks (like Needle-in-a-Haystack, RULER) suggest it actually performs well across that full range. So my questions are:\n\n* Does Gemini use **YaRN** or **RoPE scaling** internally?\n* Is it trained from scratch with 1M tokens per sequence (i.e., truly native)?\n* Or is it just doing **clever chunking** or sparse attention under the hood (e.g., blockwise, ring attention)?\n* Does it use **ALiBi** or some modified positional encoding to stabilize long contexts?\n\nIf anyone has insight from papers, leaks, logs, or architecture details, I'd love to learn more.  \nEven speculation grounded in similar architectures is welcome.",
          "author_fullname": "t2_1qyykcj4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does Gemini 2.5 Pro natively support 1M tokens of context? Is it using YaRN, or some kind of disguised chunking?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6xbru",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753237168,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m trying to understand how models like &lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt; achieve &lt;em&gt;native&lt;/em&gt; 1 million token context windows.&lt;/p&gt;\n\n&lt;p&gt;From what I’ve seen in models like &lt;strong&gt;Qwen3&lt;/strong&gt; or &lt;strong&gt;LLaMA&lt;/strong&gt;, they use techniques like &lt;strong&gt;RoPE scaling&lt;/strong&gt; (e.g., YaRN, NTK-aware RoPE, Position Interpolation) to extrapolate context beyond what was trained. These methods usually need fine-tuning, and even then, there&amp;#39;s often a soft limit beyond which attention weakens significantly.&lt;/p&gt;\n\n&lt;p&gt;But Gemini claims &lt;em&gt;native&lt;/em&gt; 1M context, and benchmarks (like Needle-in-a-Haystack, RULER) suggest it actually performs well across that full range. So my questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Does Gemini use &lt;strong&gt;YaRN&lt;/strong&gt; or &lt;strong&gt;RoPE scaling&lt;/strong&gt; internally?&lt;/li&gt;\n&lt;li&gt;Is it trained from scratch with 1M tokens per sequence (i.e., truly native)?&lt;/li&gt;\n&lt;li&gt;Or is it just doing &lt;strong&gt;clever chunking&lt;/strong&gt; or sparse attention under the hood (e.g., blockwise, ring attention)?&lt;/li&gt;\n&lt;li&gt;Does it use &lt;strong&gt;ALiBi&lt;/strong&gt; or some modified positional encoding to stabilize long contexts?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If anyone has insight from papers, leaks, logs, or architecture details, I&amp;#39;d love to learn more.&lt;br/&gt;\nEven speculation grounded in similar architectures is welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6xbru",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ranteck",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753237168,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!\\_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We’re running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.\n\nRight now, we’re testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:\n\n* Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?\n* Anyone running into issues with NUMA alignment or memory pressure in 96–128GB DDR5 systems when training on large batches?\n* How do you ensure smooth RAM → VRAM feeding at 5090 throughput without overloading I/O threads?\n\nWould love to compare notes — especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We’re deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM → GPU VRAM would be killer.",
          "author_fullname": "t2_1tltnwoxsz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people staging AI training datasets from NVMe → DDR5 → GPU VRAM for fine-tuning on RTX 5090s?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6vj8o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755865784%2CNmM1YmM5ZTJjMTliY2I2ODg0OGYzOTliOGYzYjgyOWYyYjkxY2U0NDY0MDcxZjNiNDcwYWI2MzliOTAxMTc1Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755865784%2CNjRlYzQ5NDY2YTg5MjE3MTYyN2I0MzgyODBkYWRhMGNlZjk2MWM0OWQ5NGQzZDhmOWU2MGQ2MDRlM2Q3M2EwZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=7fa7bed1c348998994fed16cd386547e5aac176b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753232111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We’re running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.&lt;/p&gt;\n\n&lt;p&gt;Right now, we’re testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?&lt;/li&gt;\n&lt;li&gt;Anyone running into issues with NUMA alignment or memory pressure in 96–128GB DDR5 systems when training on large batches?&lt;/li&gt;\n&lt;li&gt;How do you ensure smooth RAM → VRAM feeding at 5090 throughput without overloading I/O threads?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to compare notes — especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We’re deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM → GPU VRAM would be killer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/m3v13th5vief1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?format=pjpg&amp;auto=webp&amp;s=8a06bae144080a3451d0fb255b750bcab9e21c69",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=930f76891585f27565f3d929f2d1d4df9fbbe6f7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c2cc4160f0d866354b83bad0ce200177193907cd",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=337db9c93858d2e6c9db6e22822d525e7600240d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1915548352adb0259f40c35397f4626912fc93d4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e2a49235d59469a5f29b50b3c42efc9cc7f4d39",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81350e82df349faf24b9ef86ecafb7b97303ebd3",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6vj8o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DJAI9LAB",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vj8o/how_are_people_staging_ai_training_datasets_from/",
          "stickied": false,
          "url": "https://v.redd.it/m3v13th5vief1",
          "subreddit_subscribers": 503258,
          "created_utc": 1753232111,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755865784%2CNmM1YmM5ZTJjMTliY2I2ODg0OGYzOTliOGYzYjgyOWYyYjkxY2U0NDY0MDcxZjNiNDcwYWI2MzliOTAxMTc1Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755865784%2CNjRlYzQ5NDY2YTg5MjE3MTYyN2I0MzgyODBkYWRhMGNlZjk2MWM0OWQ5NGQzZDhmOWU2MGQ2MDRlM2Q3M2EwZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\n\nI've built an app called **Protexo**, which uses **Google's Gemma 3** LLM **entirely on-device** to detect scam messages across SMS, WhatsApp, and other messaging apps. The goal is to stop social engineering scams *before* they escalate — especially those that start with a friendly human-sounding message.\n\n# 🧠 Model Details:\n\n* Main detection runs through Google Gemma 3, quantized and compiled to .task\n* Running via GeckoEmbeddingModel + LocalAgents RAG API\n* Prompt tuning and RAG context crafted specifically for scam classification \n\n# 🌐 Privacy Breakdown:\n\n* Message analysis: Done locally on-device via LLM\n* Links (URLs): Checked via a encrypted cloud API\n* No messages, contacts, or chat history leave the device\n\n# 🔗 Download:\n\n👉 [https://play.google.com/store/apps/details?id=ai.protexo]()\n\n**More info:**  \n🌐 [https://protexo.ai](https://protexo.ai/)\n\n# 🙏 Would love feedback from this community:\n\n* How’s performance on your phone? (Latency, CPU/memory usage, battery)\n* Prompt design improvements or other tricks for making Gemma 3 more scam-aware\n* Ideas for swapping in smaller models \n* Anything you think could improve UX or transparency\n\nIf you're curious or want to test it out, I'm happy to send **promo codes** — just DM me.\n\nThanks all — excited to hear what you all folks think!",
          "author_fullname": "t2_1inf92eupz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Consumer usecase for on-device AI - an Android app to detect scams",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zdx4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753243380,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built an app called &lt;strong&gt;Protexo&lt;/strong&gt;, which uses &lt;strong&gt;Google&amp;#39;s Gemma 3&lt;/strong&gt; LLM &lt;strong&gt;entirely on-device&lt;/strong&gt; to detect scam messages across SMS, WhatsApp, and other messaging apps. The goal is to stop social engineering scams &lt;em&gt;before&lt;/em&gt; they escalate — especially those that start with a friendly human-sounding message.&lt;/p&gt;\n\n&lt;h1&gt;🧠 Model Details:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Main detection runs through Google Gemma 3, quantized and compiled to .task&lt;/li&gt;\n&lt;li&gt;Running via GeckoEmbeddingModel + LocalAgents RAG API&lt;/li&gt;\n&lt;li&gt;Prompt tuning and RAG context crafted specifically for scam classification &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;🌐 Privacy Breakdown:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Message analysis: Done locally on-device via LLM&lt;/li&gt;\n&lt;li&gt;Links (URLs): Checked via a encrypted cloud API&lt;/li&gt;\n&lt;li&gt;No messages, contacts, or chat history leave the device&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;🔗 Download:&lt;/h1&gt;\n\n&lt;p&gt;👉 [&lt;a href=\"https://play.google.com/store/apps/details?id=ai.protexo%5D()\"&gt;https://play.google.com/store/apps/details?id=ai.protexo]()&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;More info:&lt;/strong&gt;&lt;br/&gt;\n🌐 &lt;a href=\"https://protexo.ai/\"&gt;https://protexo.ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;🙏 Would love feedback from this community:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How’s performance on your phone? (Latency, CPU/memory usage, battery)&lt;/li&gt;\n&lt;li&gt;Prompt design improvements or other tricks for making Gemma 3 more scam-aware&lt;/li&gt;\n&lt;li&gt;Ideas for swapping in smaller models &lt;/li&gt;\n&lt;li&gt;Anything you think could improve UX or transparency&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you&amp;#39;re curious or want to test it out, I&amp;#39;m happy to send &lt;strong&gt;promo codes&lt;/strong&gt; — just DM me.&lt;/p&gt;\n\n&lt;p&gt;Thanks all — excited to hear what you all folks think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Q95cr2UVYw13fU7wf6vwGXNosGBaerSRSek7ztxKJ1Q.png?auto=webp&amp;s=6e4b57177393bb939385f8debdd6bdac3307a7bf",
                  "width": 512,
                  "height": 512
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Q95cr2UVYw13fU7wf6vwGXNosGBaerSRSek7ztxKJ1Q.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e7bfd1e7903549de4741aa59fe06cc42dcbff2e3",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Q95cr2UVYw13fU7wf6vwGXNosGBaerSRSek7ztxKJ1Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f6d7690b7c6442872666e30b4863154ba82494e",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/Q95cr2UVYw13fU7wf6vwGXNosGBaerSRSek7ztxKJ1Q.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6efaacb466d6f859f9dc06d362bbe1f71096f90d",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "Q95cr2UVYw13fU7wf6vwGXNosGBaerSRSek7ztxKJ1Q"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6zdx4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Basic-Donut1740",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6zdx4/consumer_usecase_for_ondevice_ai_an_android_app/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zdx4/consumer_usecase_for_ondevice_ai_an_android_app/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753243380,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We had a lot of posts about the updated [235b model](https://x.com/Alibaba_Qwen/status/1947344511988076547) and the [Unsloth quants](https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF). I tested it with my Mac Studio and decided to merge the Q3 K XL ggufs and upload them to Ollama in case someone es might find this useful.\n\nRuns great with up to 18 tokens per second and consuming 108 to 117 GB VRAM.\n\n[More details on the Ollama library page](https://ollama.com/awaescher/qwen3-235b-2507-unsloth-q3-k-xl), performance benchmarks included.",
          "author_fullname": "t2_1gpif4cz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The LLM for M4 Max 128GB: Unsloth Qwen3-235B-A22B-Instruct-2507 Q3 K XL for Ollama",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ocfd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/FGAC4_FgxMG-7e9w8V8PkDHHC0Spkue03KT-5Vo9mU4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753214166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We had a lot of posts about the updated &lt;a href=\"https://x.com/Alibaba_Qwen/status/1947344511988076547\"&gt;235b model&lt;/a&gt; and the &lt;a href=\"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF\"&gt;Unsloth quants&lt;/a&gt;. I tested it with my Mac Studio and decided to merge the Q3 K XL ggufs and upload them to Ollama in case someone es might find this useful.&lt;/p&gt;\n\n&lt;p&gt;Runs great with up to 18 tokens per second and consuming 108 to 117 GB VRAM.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ollama.com/awaescher/qwen3-235b-2507-unsloth-q3-k-xl\"&gt;More details on the Ollama library page&lt;/a&gt;, performance benchmarks included.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/y3x24rxqchef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/y3x24rxqchef1.png?auto=webp&amp;s=8871c292b16bce4a1a3ebad50bdc70a4755edfb1",
                  "width": 1119,
                  "height": 699
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e41bb7b82dd23ca399246b0ad273bfca55313312",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8196b3b26f6cebea121b85e8d13e70ccede750b",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=71b905987f840aecad2d4d33ebe4b67e00e55446",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c4dd5da6091ae77e58d63dfd95935c34e266d7e",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d2b6677488033099a15dec1df1c7088540544b7",
                    "width": 960,
                    "height": 599
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96a500d949b1758bb6f522dd4c3912c8e4c57f64",
                    "width": 1080,
                    "height": 674
                  }
                ],
                "variants": {},
                "id": "65hJ7hQzTtv2DTpl4kNAtfCBSaTl0aIGG0bqSxzPvcM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6ocfd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "waescher",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ocfd/the_llm_for_m4_max_128gb_unsloth/",
          "stickied": false,
          "url": "https://i.redd.it/y3x24rxqchef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753214166,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.\n\n# The problem: too many options\n\nI've been building [Kiln AI](https://github.com/kiln-ai/kiln): an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.\n\nHere's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (`&lt;think&gt;...&lt;/think&gt;`), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top\\_p, etc), and much more.\n\n# How a focus on usability turned into over 2000 test cases\n\nI wanted things to \"just work\" as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.\n\nTo make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.\n\nThe solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.\n\n# Wait, doesn't that cost a lot of money and take forever?\n\n**Yes it does!** Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.\n\nOur blog has some details on the [Python pytest setup we used to make this manageable](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time).\n\n# The Result\n\nThe end result is that it's much easier to rapidly evaluate AI models and methods. It includes\n\n* The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.\n* Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).\n\nHowever, you're in control. You can always override any suggestion.\n\n# Next Step: A Giant Ollama Server\n\nI can run a decent sampling of our Ollama tests locally, but I lack the \\~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!\n\n# How to Find the Best Model for Your Task with Kiln\n\nAll of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.\n\nKiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.\n\nTo get started, check out the tool or our guides:\n\n* [Kiln AI on Github - over 3900 stars](https://getkiln.ai/)\n* [Quickstart Guide](https://docs.getkiln.ai/docs/quickstart)\n* [Kiln Discord](https://getkiln.ai/discord)\n* [Blog post with more details on our LLM testing (more detailed version of above)](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time)\n\nI'm happy to answer questions if anyone wants to dive deeper on specific aspects!",
          "author_fullname": "t2_slbscky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I wrote 2000 LLM test cases so you don't have to",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6gq8e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753199300,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753197164,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.&lt;/p&gt;\n\n&lt;h1&gt;The problem: too many options&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;ve been building &lt;a href=\"https://github.com/kiln-ai/kiln\"&gt;Kiln AI&lt;/a&gt;: an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn&amp;#39;t easy. If evaluating an additional model is painful, you&amp;#39;re less likely to do it, which makes you less likely to find the best way to run your AI workload.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (&lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt;), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top_p, etc), and much more.&lt;/p&gt;\n\n&lt;h1&gt;How a focus on usability turned into over 2000 test cases&lt;/h1&gt;\n\n&lt;p&gt;I wanted things to &amp;quot;just work&amp;quot; as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.&lt;/p&gt;\n\n&lt;p&gt;To make it easy to use, we needed reasonable defaults for every major model. That&amp;#39;s no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.&lt;/p&gt;\n\n&lt;p&gt;The solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.&lt;/p&gt;\n\n&lt;h1&gt;Wait, doesn&amp;#39;t that cost a lot of money and take forever?&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Yes it does!&lt;/strong&gt; Each time we run these tests, we&amp;#39;re making thousands of LLM calls against a wide variety of providers. There&amp;#39;s no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn&amp;#39;t an option.&lt;/p&gt;\n\n&lt;p&gt;Our blog has some details on the &lt;a href=\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time\"&gt;Python pytest setup we used to make this manageable&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h1&gt;The Result&lt;/h1&gt;\n\n&lt;p&gt;The end result is that it&amp;#39;s much easier to rapidly evaluate AI models and methods. It includes&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.&lt;/li&gt;\n&lt;li&gt;Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, you&amp;#39;re in control. You can always override any suggestion.&lt;/p&gt;\n\n&lt;h1&gt;Next Step: A Giant Ollama Server&lt;/h1&gt;\n\n&lt;p&gt;I can run a decent sampling of our Ollama tests locally, but I lack the ~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I&amp;#39;d love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!&lt;/p&gt;\n\n&lt;h1&gt;How to Find the Best Model for Your Task with Kiln&lt;/h1&gt;\n\n&lt;p&gt;All of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.&lt;/p&gt;\n\n&lt;p&gt;Kiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.&lt;/p&gt;\n\n&lt;p&gt;To get started, check out the tool or our guides:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/\"&gt;Kiln AI on Github - over 3900 stars&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://docs.getkiln.ai/docs/quickstart\"&gt;Quickstart Guide&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/discord\"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time\"&gt;Blog post with more details on our LLM testing (more detailed version of above)&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?auto=webp&amp;s=23e4ff0dbe2d03ff352aea774053e4e9cdb80d20",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd9815f077288b33817e75895d23e661f1193778",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7df51b519d6d99631039f2563f587d4f7fb7f337",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=584735f7b916c00d422195a7ea012563d4e134db",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ceb01849b330103f92aaf6b1331cd97e415c722",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0594f7e041119a136f22914764b2a128e73d5ff",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=415b728bd16022b553cb45cb75a1a8fee65a2e5b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6gq8e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "davernow",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753197164,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI wanted to check if I'm missing anything relevant in performance or quality with my quant strategy.\n\nMy setup is an EPYC Rome (no avx512 instruction set) with 512 GB RAM and a bunch of 3060 / 3090s. The inference engine is llama.cpp and I run almost everything large (r1, q3 235, q3 480) in UD-Q4\\_K\\_XL, while Kimi K2 uses UD-Q3\\_K\\_XL - CPU offload ofc. Smaller 30b/32b (Devstral, Magistral, Gemma-3, etc.) I run in UD-Q6\\_K\\_XL on the GPUs only.\n\nI settled on these quants after seeing tests on unrelated models some time ago that suggested diminishing returns after Q4\\_K\\_M. Another source I can't remember claimed Q8\\_0 for KV cache doesn't hurt quality and that even Q4\\_0 for the v cache is acceptable.\n\nAre my generalized assumptions still correct or where they ever correct?\n\n* larger models are more insensitive to quant\n* diminishing returns after \\~4.5bpw\n* Q8\\_0 KV is the way to go\n\nWould the ik\\_llama fork (with their special quants) provide a significant increase of quality/speed in my CPU-poor setup?",
          "author_fullname": "t2_1tnm5zafaw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which quantization approach is the way to go? (llama.cpp)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m77az5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753271962,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I wanted to check if I&amp;#39;m missing anything relevant in performance or quality with my quant strategy.&lt;/p&gt;\n\n&lt;p&gt;My setup is an EPYC Rome (no avx512 instruction set) with 512 GB RAM and a bunch of 3060 / 3090s. The inference engine is llama.cpp and I run almost everything large (r1, q3 235, q3 480) in UD-Q4_K_XL, while Kimi K2 uses UD-Q3_K_XL - CPU offload ofc. Smaller 30b/32b (Devstral, Magistral, Gemma-3, etc.) I run in UD-Q6_K_XL on the GPUs only.&lt;/p&gt;\n\n&lt;p&gt;I settled on these quants after seeing tests on unrelated models some time ago that suggested diminishing returns after Q4_K_M. Another source I can&amp;#39;t remember claimed Q8_0 for KV cache doesn&amp;#39;t hurt quality and that even Q4_0 for the v cache is acceptable.&lt;/p&gt;\n\n&lt;p&gt;Are my generalized assumptions still correct or where they ever correct?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;larger models are more insensitive to quant&lt;/li&gt;\n&lt;li&gt;diminishing returns after ~4.5bpw&lt;/li&gt;\n&lt;li&gt;Q8_0 KV is the way to go&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would the ik_llama fork (with their special quants) provide a significant increase of quality/speed in my CPU-poor setup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m77az5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pixelterpy",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753271962,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MegaTTS 3 voice cloning is here!\n\nFor context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.\n\nRecently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: [https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT](https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT) with quite promising results.\n\nI reuploaded the weights to Hugging Face: [https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning](https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning)\n\nAnd put up a quick Gradio demo to try it out: [https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning](https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning)\n\nOverall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!\n\nh/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MegaTTS 3 Voice Cloning is Here",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m641zg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 370,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 370,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca44b1060cf304798e39247090bed7e9f195130b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753156417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MegaTTS 3 voice cloning is here!&lt;/p&gt;\n\n&lt;p&gt;For context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.&lt;/p&gt;\n\n&lt;p&gt;Recently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: &lt;a href=\"https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT\"&gt;https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT&lt;/a&gt; with quite promising results.&lt;/p&gt;\n\n&lt;p&gt;I reuploaded the weights to Hugging Face: &lt;a href=\"https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning\"&gt;https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And put up a quick Gradio demo to try it out: &lt;a href=\"https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning\"&gt;https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Overall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!&lt;/p&gt;\n\n&lt;p&gt;h/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?auto=webp&amp;s=0e8f184606f9f3e558a6971b8dfbfc9a3f0d1af8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8ad97c6cb72e96abaf27c1cc2565dda7970c68",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=85f1ec201fac1de1a714a3b74b2040ea838d357f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=623e3d06175018d3caaaf85d7742c402b0f0a84d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=13bd3c86a79666218395f17439b714df6a5fc52c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08124b488d263f78d5cebc4fffc2a8bd5fa5f05b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3c4e8f40ef1fb69f68df6601b917c02f65c89ad",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m641zg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning",
          "subreddit_subscribers": 503258,
          "created_utc": 1753156417,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A while back I posted some [Strix Halo LLM performance testing](https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/) benchmarks. I'm back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).\n\nThe biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.\n\nThis is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp \n\nAll the full data and latest info is available in the Github repo: [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench) but here are the topline stats below:\n\n# Strix Halo LLM Benchmark Results\n\nAll testing was done on pre-production [Framework Desktop](https://frame.work/desktop) systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)\n\nExact testing/system details are in the results folders, but roughly these are running:\n\n* Close to production BIOS/EC\n* Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1\n* Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels\n* Recent llama.cpp builds (eg b5863 from 2005-07-10)\n\nJust to get a ballpark on the hardware:\n\n* \\~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)\n* theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is *much* lower\n\n# Results\n\n# Prompt Processing (pp) Performance\n\nhttps://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan||998.0|46.5|4237|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|HIP|hipBLASLt|906.1|40.8|4720|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|HIP|hipBLASLt|878.2|37.2|5308|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|fa=1|604.8|66.3|17527|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|HIP|hipBLASLt|316.9|13.6|14638|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1|270.5|17.1|68785|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|HIP|hipBLASLt|264.1|17.2|59720|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|HIP rocWMMA||94.7|4.5|41522|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n\n# Text Generation (tg) Performance\n\nhttps://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|b=256|591.1|72.0|17377|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|Vulkan|fa=1|620.9|47.9|4463|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan|fa=1|1014.1|45.8|4219|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|Vulkan|fa=1|614.2|42.0|5333|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|Vulkan|fa=1 b=256|146.1|19.3|59917|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1 b=256|223.9|17.1|68608|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|Vulkan|fa=1|119.6|14.3|14540|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|Vulkan|fa=1|26.4|5.0|41456|\n\n# Testing Notes\n\nThe best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.\n\nThere's a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#'s might be a bit much).\n\nOne thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it's now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.\n\nUnlike last time, I won't be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).\n\nFor testing, the HIP backend, I highly recommend trying `ROCBLAS_USE_HIPBLASLT=1` as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) `HSA_OVERRIDE_GFX_VERSION=11.0.0` \\- in prior testing I've found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... 🤔",
          "author_fullname": "t2_eztox",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "mjr2d31ujeef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80836e3346fcd0e6847fcb3f1d33c5f2ac3c12e3"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cd34de136a42ad65ac9facd37455912c1a13410"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=675c17530bfdbbd8ae2830c3b694e07b5163a1b0"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bdc9a2d7260d9fd3acbab23e12917b54e651493"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e051180cc70357e18ffe4725601fb9811ba1193"
                },
                {
                  "y": 715,
                  "x": 1080,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75e938dac2457cc4191363a5cd8cbcaf777049ed"
                }
              ],
              "s": {
                "y": 1181,
                "x": 1782,
                "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f"
              },
              "id": "mjr2d31ujeef1"
            },
            "7y0pdbqujeef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=493fe8a11d9ee1d7a485e1b7233ca7e945637599"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c43022989f8b24ef64216d0b17642e3f80013763"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7f683ec0155e0b8f196a71bccaf428b44550399"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb65c24153d597ae592f5012aabff2a638b88357"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f57d68c01ba672fba3a5062a2ec12ac45a621fac"
                },
                {
                  "y": 715,
                  "x": 1080,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59b563c8992d2568941d7c73829b8f7ebc4f5585"
                }
              ],
              "s": {
                "y": 1181,
                "x": 1782,
                "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7"
              },
              "id": "7y0pdbqujeef1"
            }
          },
          "name": "t3_1m6b151",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 84,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 84,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/iZq9ApFg7F044Ny8obqZ27FfndXjE_7xNkH5oORO2gc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753182004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while back I posted some &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/\"&gt;Strix Halo LLM performance testing&lt;/a&gt; benchmarks. I&amp;#39;m back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).&lt;/p&gt;\n\n&lt;p&gt;The biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.&lt;/p&gt;\n\n&lt;p&gt;This is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp &lt;/p&gt;\n\n&lt;p&gt;All the full data and latest info is available in the Github repo: &lt;a href=\"https://github.com/lhl/strix-halo-testing/tree/main/llm-bench\"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt; but here are the topline stats below:&lt;/p&gt;\n\n&lt;h1&gt;Strix Halo LLM Benchmark Results&lt;/h1&gt;\n\n&lt;p&gt;All testing was done on pre-production &lt;a href=\"https://frame.work/desktop\"&gt;Framework Desktop&lt;/a&gt; systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)&lt;/p&gt;\n\n&lt;p&gt;Exact testing/system details are in the results folders, but roughly these are running:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Close to production BIOS/EC&lt;/li&gt;\n&lt;li&gt;Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1&lt;/li&gt;\n&lt;li&gt;Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels&lt;/li&gt;\n&lt;li&gt;Recent llama.cpp builds (eg b5863 from 2005-07-10)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Just to get a ballpark on the hardware:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)&lt;/li&gt;\n&lt;li&gt;theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is &lt;em&gt;much&lt;/em&gt; lower&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Results&lt;/h1&gt;\n\n&lt;h1&gt;Prompt Processing (pp) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\"&gt;https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;998.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;46.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;4237&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;906.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;40.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;878.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;5308&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;66.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;17527&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;316.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14638&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;270.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68785&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;264.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;59720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP rocWMMA&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;94.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;41522&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Text Generation (tg) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\"&gt;https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;591.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;72.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17377&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;620.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;47.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4463&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1014.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;45.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4219&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;614.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;5333&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;146.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;59917&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;223.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68608&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;119.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;14540&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;26.4&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;41456&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Testing Notes&lt;/h1&gt;\n\n&lt;p&gt;The best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#&amp;#39;s might be a bit much).&lt;/p&gt;\n\n&lt;p&gt;One thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it&amp;#39;s now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.&lt;/p&gt;\n\n&lt;p&gt;Unlike last time, I won&amp;#39;t be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).&lt;/p&gt;\n\n&lt;p&gt;For testing, the HIP backend, I highly recommend trying &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt; - in prior testing I&amp;#39;ve found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... 🤔&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6b151",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "randomfoo2",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753182004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with “digital twins” who are standing in for an absent employee.\n\nTo find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example. \n\nBut there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn't simply plan better meetings instead.\n\nWhat do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?",
          "author_fullname": "t2_4tnm5az4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Digital twins that attend meetings for you. Dystopia or soon reality?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6pw0o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wzygbrp0nhef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/wzygbrp0nhef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wzygbrp0nhef1/DASHPlaylist.mpd?a=1755865784%2CNjE2NmE0OTU0NWU3NTJlNWU2MDg3NWI2M2Y5ZDIwYjI2NzU2N2ZkMjZhMDMzNzE3MmNiYjk3NzFlODE1NTFhMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 132,
              "hls_url": "https://v.redd.it/wzygbrp0nhef1/HLSPlaylist.m3u8?a=1755865784%2CZGNmNmFiYmE5MGJlOTcxN2EyMGYyMmZhZmZhMzNiMWM1ZTM3OTUyNzllZjZlOWEzNTQyYWFhMjJmYTMzMTY1Yg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=a69584394b29451108583c0345b3b4af510dbdc1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753217710,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with “digital twins” who are standing in for an absent employee.&lt;/p&gt;\n\n&lt;p&gt;To find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example. &lt;/p&gt;\n\n&lt;p&gt;But there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn&amp;#39;t simply plan better meetings instead.&lt;/p&gt;\n\n&lt;p&gt;What do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/wzygbrp0nhef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?format=pjpg&amp;auto=webp&amp;s=364cf4d9aed4af7c6e67f0ad4201a809f25338cf",
                  "width": 1662,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c3f0710b64e022e54027bd6589659498e18682ba",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=98174bca9eb25e7cc20a591cc15f5ea33c8b5a11",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ed0becb52791f8efb623b51e5eda057c3e3bde52",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a7708460ab1cd8fdd958ab10e8b4301f392326d",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=91df4abe288ad6402853ff17541bfa618d570410",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7c1aaaa8918b2d9857a12c93d828793e424f997c",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6pw0o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DerErzfeind61",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6pw0o/digital_twins_that_attend_meetings_for_you/",
          "stickied": false,
          "url": "https://v.redd.it/wzygbrp0nhef1",
          "subreddit_subscribers": 503258,
          "created_utc": 1753217710,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wzygbrp0nhef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/wzygbrp0nhef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wzygbrp0nhef1/DASHPlaylist.mpd?a=1755865784%2CNjE2NmE0OTU0NWU3NTJlNWU2MDg3NWI2M2Y5ZDIwYjI2NzU2N2ZkMjZhMDMzNzE3MmNiYjk3NzFlODE1NTFhMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 132,
              "hls_url": "https://v.redd.it/wzygbrp0nhef1/HLSPlaylist.m3u8?a=1755865784%2CZGNmNmFiYmE5MGJlOTcxN2EyMGYyMmZhZmZhMzNiMWM1ZTM3OTUyNzllZjZlOWEzNTQyYWFhMjJmYTMzMTY1Yg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using locallama, newsletters and much more for quite some time now, but i think both can be somewhat saturate at times and i still often feel like i miss out on stuff. Therefore, I've been looking for a more consolidated way to read and learn about new research, releases and more. I was thinking X, but never really used it, so if you use X, who are you following? Alternatively, are there any good newsletters or similarly that you prefer following i would love to hear about them. And more generally, if you have a method that you think works well for you i would be interested to hear about it.",
          "author_fullname": "t2_5t7ezz02",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do you do to keep up to date on new research, trends and more?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m741so",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753260529,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using locallama, newsletters and much more for quite some time now, but i think both can be somewhat saturate at times and i still often feel like i miss out on stuff. Therefore, I&amp;#39;ve been looking for a more consolidated way to read and learn about new research, releases and more. I was thinking X, but never really used it, so if you use X, who are you following? Alternatively, are there any good newsletters or similarly that you prefer following i would love to hear about them. And more generally, if you have a method that you think works well for you i would be interested to hear about it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m741so",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Professional_Pop_240",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m741so/what_do_you_do_to_keep_up_to_date_on_new_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m741so/what_do_you_do_to_keep_up_to_date_on_new_research/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753260529,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For once, I’m not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post.\n\nThat said, just sharing something that’s been on mind. I’ve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)\n\nAI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.\n\nLike how the Internet is “open” for anyone to use and build on top of it, AI should be the same way.\n\nIt’s fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.\n\nWhat are your thoughts on this?",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI should just be open-source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m67zde",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753198579,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753170472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For once, I’m not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post.&lt;/p&gt;\n\n&lt;p&gt;That said, just sharing something that’s been on mind. I’ve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)&lt;/p&gt;\n\n&lt;p&gt;AI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.&lt;/p&gt;\n\n&lt;p&gt;Like how the Internet is “open” for anyone to use and build on top of it, AI should be the same way.&lt;/p&gt;\n\n&lt;p&gt;It’s fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m67zde",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 89,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753170472,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm trying to get an LLM to analyze a bunch of documents (around 30 PDFs or TXT files), but I’m running into some issues. These are pretty sensitive communications, so keeping everything local is a must – no sending them off to online services!\n\nI've been playing around with LM Studio, but it seems like it can only handle a few files at a time. It processes 2 or 3 PDFs, grabs some info from them, and then just stops. I really want the LLM to look at all my documents every time I ask it something, re-checking everything as needed.  I'm not worried about how long it takes to respond – I just need it to be thorough.\n\nDoes anyone have any suggestions for other local LLM tools that can handle a larger document set? Something that doesn’t get overwhelmed by 30 files. Or, are there any online LLM services out there that actually guarantee data privacy and security?  I'm looking for something more than just the usual \"we protect your data\" – I need real assurances.\n\nAny advice would be appreciated!   \nThanks",
          "author_fullname": "t2_1bvl9ir2ne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need Help - Local LLM &amp; Lots of Files! (Privacy Concerns)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m73q8n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753259199,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to get an LLM to analyze a bunch of documents (around 30 PDFs or TXT files), but I’m running into some issues. These are pretty sensitive communications, so keeping everything local is a must – no sending them off to online services!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing around with LM Studio, but it seems like it can only handle a few files at a time. It processes 2 or 3 PDFs, grabs some info from them, and then just stops. I really want the LLM to look at all my documents every time I ask it something, re-checking everything as needed.  I&amp;#39;m not worried about how long it takes to respond – I just need it to be thorough.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any suggestions for other local LLM tools that can handle a larger document set? Something that doesn’t get overwhelmed by 30 files. Or, are there any online LLM services out there that actually guarantee data privacy and security?  I&amp;#39;m looking for something more than just the usual &amp;quot;we protect your data&amp;quot; – I need real assurances.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated!&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m73q8n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AreBee73",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753259199,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Among the open source models that can be deployed by rtx 4090, which one is better in terms of comprehensive performance?",
          "author_fullname": "t2_1tgjtlkhzd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m75bwe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753265439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Among the open source models that can be deployed by rtx 4090, which one is better in terms of comprehensive performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m75bwe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jeremysse",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m75bwe/llama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m75bwe/llama/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753265439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What options do we have for Qwen3 Coder, either local or cloud services?",
          "author_fullname": "t2_mxdkomgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the cheapest option for hosting llama cpp with Qwen Coder at Q8?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nvhs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753213772,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753213111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What options do we have for Qwen3 Coder, either local or cloud services?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6nvhs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Available_Driver6406",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753213111,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a ***Private*** eval that has been updated for over a year by Zhihu user \"toyama nao\".  So qwen cannot be benchmaxxing on it because it is ***Private*** and the questions are being updated constantly.\n\nThe score of this 2507 update is amazing, especially since it's a non-reasoning model that ranks among other reasoning ones.\n\n[logic](https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;format=png&amp;auto=webp&amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9)\n\n[coding](https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;format=png&amp;auto=webp&amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc)\n\n\\*These 2 tables are OCR and translated by gemini, so it may contain small errors\n\nDo note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese\n\nSource:\n\n[Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873](Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873)",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Private Eval result of Qwen3-235B-A22B-Instruct-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 30,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "q1ld1vkvcdef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 17,
                  "x": 108,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6db0188556e8b5a6ad0b4e78e643d750f9c8eb3b"
                },
                {
                  "y": 34,
                  "x": 216,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=def0b755adbe43c0ef6c669ce96229bdf8642e8d"
                },
                {
                  "y": 51,
                  "x": 320,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3eabe87b380ad52a47b2c07b92750b4da2580788"
                },
                {
                  "y": 102,
                  "x": 640,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6df88708d1d58d4dfb207f2e3d4de7ff7ba44f34"
                },
                {
                  "y": 153,
                  "x": 960,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=52be3772b829b14a916bd0dba033163893df170e"
                },
                {
                  "y": 172,
                  "x": 1080,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83243c7b94f1c3bbb4c4965fc1926369f860f03a"
                }
              ],
              "s": {
                "y": 211,
                "x": 1319,
                "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;format=png&amp;auto=webp&amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc"
              },
              "id": "q1ld1vkvcdef1"
            },
            "s5t1rm4dcdef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 23,
                  "x": 108,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c77a3824658d5cc719a3b06a8dad382fb80e75b0"
                },
                {
                  "y": 46,
                  "x": 216,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cec128245e15150bcd1e79751e2e6d55996d697"
                },
                {
                  "y": 69,
                  "x": 320,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89eced78277fb9616c09bbe0f9fd5df078cbd54e"
                },
                {
                  "y": 139,
                  "x": 640,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=043d1bb82c725708bb35a58acd4d7c9504c7de3a"
                },
                {
                  "y": 208,
                  "x": 960,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=28844535a11fae5fff659cb0b949c1d842e4569e"
                }
              ],
              "s": {
                "y": 229,
                "x": 1054,
                "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;format=png&amp;auto=webp&amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9"
              },
              "id": "s5t1rm4dcdef1"
            }
          },
          "name": "t3_1m66qks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 82,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 82,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/z5Hij9VVgEz-n0a_TqfphvLzGcPNiRWrZnXSAwXHg_Q.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753165706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; eval that has been updated for over a year by Zhihu user &amp;quot;toyama nao&amp;quot;.  So qwen cannot be benchmaxxing on it because it is &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; and the questions are being updated constantly.&lt;/p&gt;\n\n&lt;p&gt;The score of this 2507 update is amazing, especially since it&amp;#39;s a non-reasoning model that ranks among other reasoning ones.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9\"&gt;logic&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc\"&gt;coding&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;*These 2 tables are OCR and translated by gemini, so it may contain small errors&lt;/p&gt;\n\n&lt;p&gt;Do note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese&lt;/p&gt;\n\n&lt;p&gt;Source:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873\"&gt;Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m66qks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753165706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3l9wjlq0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507 Released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5owi8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 825,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 825,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sKHygFxyatHEivMjwhoU0rKccpX3n5vMlMuGtN0ebyc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118247,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/Alibaba_Qwen/status/1947344511988076547",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?auto=webp&amp;s=2ca86b1c53db0d11a0c488d1d12c8c9cb55eaf20",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c0c862769016dce18130a1fb791dbf78757f922",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f8596c55842b58dd3bf4190c4c47e309432ad77",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d51316624bbec96c8a0b28b2e3756e68ffadf98",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9739fe5698145f958eb2e1c66da1875fc6d34a00",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b66905b8a3f2f560c571babd372861e032c5ca94",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72cbdd4b47610ef9ddfdf989b7900703487934d6",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DZ_J_yAfR8TLjLmR0s6ZMb4IqBdDowTQUhHZ335Z0r8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5owi8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pseudoreddituser",
          "discussion_type": null,
          "num_comments": 246,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/",
          "stickied": false,
          "url": "https://x.com/Alibaba_Qwen/status/1947344511988076547",
          "subreddit_subscribers": 503258,
          "created_utc": 1753118247,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Paper: [https://arxiv.org/pdf/2507.12806](https://arxiv.org/pdf/2507.12806)\n\nCode: [https://github.com/SalesforceAIResearch/MCPEval](https://github.com/SalesforceAIResearch/MCPEval)",
          "author_fullname": "t2_9e4oies",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m70ra1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753247923,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/pdf/2507.12806\"&gt;https://arxiv.org/pdf/2507.12806&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/SalesforceAIResearch/MCPEval\"&gt;https://github.com/SalesforceAIResearch/MCPEval&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m70ra1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "palindsay",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m70ra1/mcpeval_automatic_mcpbased_deep_evaluation_for_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m70ra1/mcpeval_automatic_mcpbased_deep_evaluation_for_ai/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753247923,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation?  Ideally with a 3090 or better for prompt processing.\n\nI've been looking at Kimi, but I've been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.\n\nWas wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.",
          "author_fullname": "t2_ijzb7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Epyc Qwen3 235B Q8 speed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6h67y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753198166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation?  Ideally with a 3090 or better for prompt processing.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at Kimi, but I&amp;#39;ve been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.&lt;/p&gt;\n\n&lt;p&gt;Was wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6h67y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MidnightProgrammer",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753198166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI wanted to ask, in theory what setup would be able to run such models at superspeed? Is such setup possible with 30k? Or would you need way more, 100-500k? \n\n\\[Deepseek, Qwen etc...\\]\n\nI'm not familiar with setups or common knowledge within this realm.\n\nThank you.",
          "author_fullname": "t2_kct7ji0y2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Noob: In theory what set up would you need to run the best LLMs locally at the same speed as the public LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m704yl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753246028,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753245833,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I wanted to ask, in theory what setup would be able to run such models at superspeed? Is such setup possible with 30k? Or would you need way more, 100-500k? &lt;/p&gt;\n\n&lt;p&gt;[Deepseek, Qwen etc...]&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not familiar with setups or common knowledge within this realm.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m704yl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prudent_Garden9033",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m704yl/noob_in_theory_what_set_up_would_you_need_to_run/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m704yl/noob_in_theory_what_set_up_would_you_need_to_run/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753245833,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\*\\*TL;DR\\*\\* Thinking about building an LLM rig with 5 used AMD MI50 32GB GPUs to run Qwen 3 32b and 235b. Estimated token speeds look promising for the price (\\~$1125 total). Biggest hurdles are PCIe lane bandwidth &amp; power, which I'm attempting to solve with bifurcation cards and a new PSU.  Looking for feedback!\n\nHi everyone,\n\nLately I've been thinking about treating myself to a 3090 and a ram upgrade to run Qwen 3 32b and 235b, but the MI50 posts got me napkin mathing that rabbit hole. The numbers I'm seeing are 19 tok/s in 235b(I get 3 tok/s running q2), and 60 tok/s with 4x tensor parallel with 32b(I usually get 10-15 tok/s), which seems great for the price. To me that would be worth it to convert my desktop into a dedicated server. Other than slower prompt processing, is there a catch?\n\n  \nIf its as good as some posts claim, then I'd be limited by cost and my existing hardware. The biggest problem is PCIe lanes, or lack thereof as low bandwidth will tank performance when running models in tensor parallel. To make the problem less bad, I'm going to try and keep everything PCIe gen 4. My motherboard supports bifurcation of the gen4 16x slot, which can be broken out by PCIe 4.0 bifurcation cards. The only gen 4 card I could find splits lanes, so that's why theres 3 of them. Another problem would be power, as the cards will need to be power limited slightly even with a 1600w PSU.\n\nCurrent system:  \n\\*   \\*\\*CPU:\\*\\* Ryzen 5 7600  \n\\*   \\*\\*RAM:\\*\\* 48GB DDR5 5200MHz  \n\\*   \\*\\*Motherboard:\\*\\* MSI Mortar AM5  \n\\*   \\*\\*SSD (Primary):\\*\\* 1TB SSD  \n\\*   \\*\\*SSD (Secondary):\\*\\* 2TB SSD  \n\\*   \\*\\*PSU:\\*\\* 850W  \n\\*   \\*\\*GPU(s):\\*\\* 2x AMD RX6800 \n\n  \nProspective system:  \n\\*   \\*\\*CPU:\\*\\* Ryzen 5 7600  \n\\*   \\*\\*RAM:\\*\\* 48GB DDR5 5200MHz  \n\\*   \\*\\*Motherboard:\\*\\* MSI Mortar AM5(with bifurcation enabled)  \n\\*   \\*\\*SSD (Primary):\\*\\* 1TB SSD  \n\\*   \\*\\*SSD (Secondary):\\*\\* 2TB SSD  \n\\*   \\*\\*GPUs (New):\\*\\* 5 x MI50 32GB ($130 each + $100 shipping = $750 total)  \n\\*   \\*\\*PSU (New):\\*\\* 1600W PSU - $200  \n\\*   \\*\\*Bifurcation Cards:\\*\\* Three PCIe 4.0 Bifurcation Cards - $75 ($25 each)  \n\\*   \\*\\*Riser Cables:\\*\\* Four PCIe 4.0 8x Cables - $100 ($25 each)  \n\\*   \\*\\*Cooling Shrouds:\\*\\* DIY MI50 GPU Cooling Shrouds (DIY)\n\n\\*   \\*\\*Total Cost of New Hardware:\\*\\* $1,125\n\nWhich doesn't seem too bad. The rx6800 gpus could be sold off too. Honestly the biggest loss would be not having a desktop, but I've been wanting a LLM focused homelab for a while now anyway. Maybe I could game on a VM in the server and stream it? Would love some feedback before I make an expensive mistake!",
          "author_fullname": "t2_3f9vjjno",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Considering 5xMI50 for Qwen 3 235b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6eggp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7dba5c08-72f1-11ee-9b6f-ca195bc297d4",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 70B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753191801,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;**TL;DR** Thinking about building an LLM rig with 5 used AMD MI50 32GB GPUs to run Qwen 3 32b and 235b. Estimated token speeds look promising for the price (~$1125 total). Biggest hurdles are PCIe lane bandwidth &amp;amp; power, which I&amp;#39;m attempting to solve with bifurcation cards and a new PSU.  Looking for feedback!&lt;/p&gt;\n\n&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Lately I&amp;#39;ve been thinking about treating myself to a 3090 and a ram upgrade to run Qwen 3 32b and 235b, but the MI50 posts got me napkin mathing that rabbit hole. The numbers I&amp;#39;m seeing are 19 tok/s in 235b(I get 3 tok/s running q2), and 60 tok/s with 4x tensor parallel with 32b(I usually get 10-15 tok/s), which seems great for the price. To me that would be worth it to convert my desktop into a dedicated server. Other than slower prompt processing, is there a catch?&lt;/p&gt;\n\n&lt;p&gt;If its as good as some posts claim, then I&amp;#39;d be limited by cost and my existing hardware. The biggest problem is PCIe lanes, or lack thereof as low bandwidth will tank performance when running models in tensor parallel. To make the problem less bad, I&amp;#39;m going to try and keep everything PCIe gen 4. My motherboard supports bifurcation of the gen4 16x slot, which can be broken out by PCIe 4.0 bifurcation cards. The only gen 4 card I could find splits lanes, so that&amp;#39;s why theres 3 of them. Another problem would be power, as the cards will need to be power limited slightly even with a 1600w PSU.&lt;/p&gt;\n\n&lt;p&gt;Current system:&lt;br/&gt;\n*   **CPU:** Ryzen 5 7600&lt;br/&gt;\n*   **RAM:** 48GB DDR5 5200MHz&lt;br/&gt;\n*   **Motherboard:** MSI Mortar AM5&lt;br/&gt;\n*   **SSD (Primary):** 1TB SSD&lt;br/&gt;\n*   **SSD (Secondary):** 2TB SSD&lt;br/&gt;\n*   **PSU:** 850W&lt;br/&gt;\n*   **GPU(s):** 2x AMD RX6800 &lt;/p&gt;\n\n&lt;p&gt;Prospective system:&lt;br/&gt;\n*   **CPU:** Ryzen 5 7600&lt;br/&gt;\n*   **RAM:** 48GB DDR5 5200MHz&lt;br/&gt;\n*   **Motherboard:** MSI Mortar AM5(with bifurcation enabled)&lt;br/&gt;\n*   **SSD (Primary):** 1TB SSD&lt;br/&gt;\n*   **SSD (Secondary):** 2TB SSD&lt;br/&gt;\n*   **GPUs (New):** 5 x MI50 32GB ($130 each + $100 shipping = $750 total)&lt;br/&gt;\n*   **PSU (New):** 1600W PSU - $200&lt;br/&gt;\n*   **Bifurcation Cards:** Three PCIe 4.0 Bifurcation Cards - $75 ($25 each)&lt;br/&gt;\n*   **Riser Cables:** Four PCIe 4.0 8x Cables - $100 ($25 each)&lt;br/&gt;\n*   **Cooling Shrouds:** DIY MI50 GPU Cooling Shrouds (DIY)&lt;/p&gt;\n\n&lt;p&gt;*   **Total Cost of New Hardware:** $1,125&lt;/p&gt;\n\n&lt;p&gt;Which doesn&amp;#39;t seem too bad. The rx6800 gpus could be sold off too. Honestly the biggest loss would be not having a desktop, but I&amp;#39;ve been wanting a LLM focused homelab for a while now anyway. Maybe I could game on a VM in the server and stream it? Would love some feedback before I make an expensive mistake!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 70B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6eggp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PraxisOG",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753191801,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1947344511988076547\n\nNew Qwen3-235B-A22B with thinking mode only –– no more hybrid reasoning.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 122,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ox8z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 520,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 520,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BzOaH3m_YlAhztCv_hRYQh_Ms3ouqOY06ZKKT3zNke8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1947344511988076547\"&gt;https://x.com/Alibaba_Qwen/status/1947344511988076547&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;New Qwen3-235B-A22B with thinking mode only –– no more hybrid reasoning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w2uh7h5lg9ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?auto=webp&amp;s=d32732e5748b82ca37787c55ccd57f5d5f705318",
                  "width": 1186,
                  "height": 1038
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=32d1f0ad8ac85f1518bb4e197d86320d03376d96",
                    "width": 108,
                    "height": 94
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf9ca6a58cad66052d5ca05375d82b3c7bf8f1cb",
                    "width": 216,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c38bec3415e13d4801fadbc3fe0e9ec1df461dbf",
                    "width": 320,
                    "height": 280
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5242a889814e823acb6da0b1179758e2947ea2a7",
                    "width": 640,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7330c9d38901abf96ac6373f03b5933dd9e99710",
                    "width": 960,
                    "height": 840
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15a616880d08cfc896e48a40a70a2aa2315dc9e4",
                    "width": 1080,
                    "height": 945
                  }
                ],
                "variants": {},
                "id": "15L72ZL9LJC_7vzVz7gsW_m-zmkTuQofd74ZqjjUwaA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ox8z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 91,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/w2uh7h5lg9ef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753118294,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey!\n\nI’m looking for a study buddy (or a small group) to go through [Maxime Labonne’s “LLM From Scratch” course](https://github.com/mlabonne/llm-course) together. It’s an amazing resource for building a large language model from scratch, and I think it’d be way more fun to learn together\n\n# My plan:\n\n* **Set weekly goals** based on the course structure\n* **Meet once a week** (probably one evening over the weekend) for a **voice call** to review what we’ve learned, share insights, and help each other with anything confusing\n* Stay accountable and motivated through shared progress\n\nDrop a comment or DM me if you’re interested! Thank you",
          "author_fullname": "t2_bywk86ik",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for LLMs Study Buddy",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6fvd5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753195208,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;I’m looking for a study buddy (or a small group) to go through &lt;a href=\"https://github.com/mlabonne/llm-course\"&gt;Maxime Labonne’s “LLM From Scratch” course&lt;/a&gt; together. It’s an amazing resource for building a large language model from scratch, and I think it’d be way more fun to learn together&lt;/p&gt;\n\n&lt;h1&gt;My plan:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Set weekly goals&lt;/strong&gt; based on the course structure&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Meet once a week&lt;/strong&gt; (probably one evening over the weekend) for a &lt;strong&gt;voice call&lt;/strong&gt; to review what we’ve learned, share insights, and help each other with anything confusing&lt;/li&gt;\n&lt;li&gt;Stay accountable and motivated through shared progress&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Drop a comment or DM me if you’re interested! Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?auto=webp&amp;s=dbc698010f56afa71dd99dc709b0ca685c29aee4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3f3260a76cb9648a81e4ffd047ff8a749b3bc74",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=21e3cfc0b6e73548c3f100a3b24c8e03c4cf7290",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=361d87092f3bd81a159645012629ae0fe171dc2e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c4bc7a322635ecbf6feaad42bf125031a8dec84",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=697fabf421f2c39c88cbed190d56d8d3653dd0e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2fea0b39998df7813abea97a6cbf5ead21679476",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6fvd5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaiKawaii0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753195208,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been using RAG with open ai over a product description document which is rather technical. I chunked up sections of my document and then do hybrid search with weaviate. It does good but sometimes certain queries require retrieval from more than 1 sections and then it's 50/50. Will fine-tuning solve this? What model should I look into?",
          "author_fullname": "t2_vdwm0f4m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rag vs fine-tuning.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qb6p",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753218703,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using RAG with open ai over a product description document which is rather technical. I chunked up sections of my document and then do hybrid search with weaviate. It does good but sometimes certain queries require retrieval from more than 1 sections and then it&amp;#39;s 50/50. Will fine-tuning solve this? What model should I look into?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6qb6p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Parking_Bluebird826",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qb6p/rag_vs_finetuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6qb6p/rag_vs_finetuning/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753218703,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "been messing around with local multi-agent setups and it’s honestly kind of a mess. juggling agent comms, memory, task routing, fallback logic, all of it just feels duct-taped together.\n\ni’ve tried using queues, redis, even writing my own little message handlers, but nothing really scales cleanly. langchain is fine if you’re doing basic stuff, but as soon as you want more control or complexity, it falls apart. crewai/autogen feel either too rigid or too tied to cloud stuff.\n\nanyone here have a local setup they actually *like*? or are we all just kinda suffering through the chaos and calling it a pipeline?\n\ncurious how you’re handling agent-to-agent stuff + memory sharing without everything turning into spaghetti.",
          "author_fullname": "t2_cmo0i3e2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it just me or does building local multi-agent LLM systems kind of suck right now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6y6c7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753239679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;been messing around with local multi-agent setups and it’s honestly kind of a mess. juggling agent comms, memory, task routing, fallback logic, all of it just feels duct-taped together.&lt;/p&gt;\n\n&lt;p&gt;i’ve tried using queues, redis, even writing my own little message handlers, but nothing really scales cleanly. langchain is fine if you’re doing basic stuff, but as soon as you want more control or complexity, it falls apart. crewai/autogen feel either too rigid or too tied to cloud stuff.&lt;/p&gt;\n\n&lt;p&gt;anyone here have a local setup they actually &lt;em&gt;like&lt;/em&gt;? or are we all just kinda suffering through the chaos and calling it a pipeline?&lt;/p&gt;\n\n&lt;p&gt;curious how you’re handling agent-to-agent stuff + memory sharing without everything turning into spaghetti.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6y6c7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Soggy-Guava-1218",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6y6c7/is_it_just_me_or_does_building_local_multiagent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6y6c7/is_it_just_me_or_does_building_local_multiagent/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753239679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI21 has just made Jamba 1.7 available on Kaggle:\n\n[https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7](https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7) \n\n* You can run and test the model without needing to install it locally\n* No need to harness setup, hardware and engineering knowledge via Hugging Face anymore\n* Now you can run sample tasks, benchmark against other models and share public notebooks with results\n\nPretty significant as the model is now available for non technical users. Here is what we know about 1.7 and Jamba in general:\n\n* Combination of Transformer architecture and Mamba, making it more efficient at handling long sequences\n* 256k context window - well-suited for long document summarization and memory-heavy chat agents\n* Improved capabilities in understanding and following user instructions, and generating more factual, relevant outputs\n\nWho is going to try it out? What use cases do you have in mind?",
          "author_fullname": "t2_1kwk178bd9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jamba 1.7 is now available on Kaggle",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6dco7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753202307,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753188954,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI21 has just made Jamba 1.7 available on Kaggle:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7\"&gt;https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7&lt;/a&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You can run and test the model without needing to install it locally&lt;/li&gt;\n&lt;li&gt;No need to harness setup, hardware and engineering knowledge via Hugging Face anymore&lt;/li&gt;\n&lt;li&gt;Now you can run sample tasks, benchmark against other models and share public notebooks with results&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Pretty significant as the model is now available for non technical users. Here is what we know about 1.7 and Jamba in general:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Combination of Transformer architecture and Mamba, making it more efficient at handling long sequences&lt;/li&gt;\n&lt;li&gt;256k context window - well-suited for long document summarization and memory-heavy chat agents&lt;/li&gt;\n&lt;li&gt;Improved capabilities in understanding and following user instructions, and generating more factual, relevant outputs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Who is going to try it out? What use cases do you have in mind?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6dco7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NullPointerJack",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753188954,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone. I hope you're doing well. I'm sorry if this post is unrelated to the topic of large language models, but I haven't found any other community that focuses on open source AI in general. My question is, are there any open source models for Arabic audio enhancement? Basically, the use case is making good quality data for training Arabic text-to-speech models, since the current ones are either afflicted with bad licenses or they are not up to the task. Thanks for your answers.",
          "author_fullname": "t2_9xer9y5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Models for Arabic tts and audio enhancement?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nbb7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753211838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I hope you&amp;#39;re doing well. I&amp;#39;m sorry if this post is unrelated to the topic of large language models, but I haven&amp;#39;t found any other community that focuses on open source AI in general. My question is, are there any open source models for Arabic audio enhancement? Basically, the use case is making good quality data for training Arabic text-to-speech models, since the current ones are either afflicted with bad licenses or they are not up to the task. Thanks for your answers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6nbb7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Silver-Champion-4846",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nbb7/best_models_for_arabic_tts_and_audio_enhancement/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6nbb7/best_models_for_arabic_tts_and_audio_enhancement/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753211838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm working on a low-level experimental setup where, instead of just using embeddings generated by the model, I inject custom embeddings directly into a LLaMA model (specifically a GGUF version using llama.cpp).\n\nThese embeddings come from another domain (e.g. images), but I project them into the same space as LLaMA’s token embeddings using a learned encoder.\n\nNo fine-tuning, no LoRA, no weight modification.\n\nMy idea is:\n\n* Compute cosine similarity between each custom embedding and the model's token embeddings.\n* Find the nearest token ID.\n* Replace that token in the prompt.\n* Let LLaMA generate from there.\n\nSo far, I haven’t seen anyone try this with llama.cpp and GGUF.\n\nAnyone doing something similar? Or know how to cleanly access tok\\_embeddings.weight in GGUF?",
          "author_fullname": "t2_dwnsr8pxq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Injecting custom embeddings into LLaMA 3.2 GGUF model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6xrfj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753238458,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a low-level experimental setup where, instead of just using embeddings generated by the model, I inject custom embeddings directly into a LLaMA model (specifically a GGUF version using llama.cpp).&lt;/p&gt;\n\n&lt;p&gt;These embeddings come from another domain (e.g. images), but I project them into the same space as LLaMA’s token embeddings using a learned encoder.&lt;/p&gt;\n\n&lt;p&gt;No fine-tuning, no LoRA, no weight modification.&lt;/p&gt;\n\n&lt;p&gt;My idea is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compute cosine similarity between each custom embedding and the model&amp;#39;s token embeddings.&lt;/li&gt;\n&lt;li&gt;Find the nearest token ID.&lt;/li&gt;\n&lt;li&gt;Replace that token in the prompt.&lt;/li&gt;\n&lt;li&gt;Let LLaMA generate from there.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So far, I haven’t seen anyone try this with llama.cpp and GGUF.&lt;/p&gt;\n\n&lt;p&gt;Anyone doing something similar? Or know how to cleanly access tok_embeddings.weight in GGUF?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6xrfj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Old-Toe6442",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6xrfj/injecting_custom_embeddings_into_llama_32_gguf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6xrfj/injecting_custom_embeddings_into_llama_32_gguf/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753238458,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I'm pretty new to all this and running a modest laptop with 8gb ram. I created a character (magicuser) in tavernai and wanted to play as that character. chatgpt told me that I could do that with prompts and proceded to give me bad advice for many hours...'this is how you do it' me: didn't work, 'well this will definately work' ...nope.  So I'm wondering, can I play as this character through prompting? Should I get sillytavern instead? I've tried on wizardllama, and llama, with kobold and tavernai. I keep getting the AI responding as me. then when it finally did kind of work, it would end with ...what do you do next? (ruining the immersion). Then I'd instruct it: only narrate 3rd person and play npc's, I'm playing as (magic user).  Can't get it to work. Can any advise on whether I should just put my character into memory scroll in tavernai? or give up? Only attempt with ...eg 13b or higher?  Thanks for any help.",
          "author_fullname": "t2_kueigpm3u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to play a character as user with Tavern, Kobold, llama 3.2b?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6xhgg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753237628,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m pretty new to all this and running a modest laptop with 8gb ram. I created a character (magicuser) in tavernai and wanted to play as that character. chatgpt told me that I could do that with prompts and proceded to give me bad advice for many hours...&amp;#39;this is how you do it&amp;#39; me: didn&amp;#39;t work, &amp;#39;well this will definately work&amp;#39; ...nope.  So I&amp;#39;m wondering, can I play as this character through prompting? Should I get sillytavern instead? I&amp;#39;ve tried on wizardllama, and llama, with kobold and tavernai. I keep getting the AI responding as me. then when it finally did kind of work, it would end with ...what do you do next? (ruining the immersion). Then I&amp;#39;d instruct it: only narrate 3rd person and play npc&amp;#39;s, I&amp;#39;m playing as (magic user).  Can&amp;#39;t get it to work. Can any advise on whether I should just put my character into memory scroll in tavernai? or give up? Only attempt with ...eg 13b or higher?  Thanks for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6xhgg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lephuey",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6xhgg/how_to_play_a_character_as_user_with_tavern/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6xhgg/how_to_play_a_character_as_user_with_tavern/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753237628,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New qwen tested on Fiction.liveBench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6172l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 99,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 99,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TLf5BqdXyD8b18S_CjlBuka8R6DaWW-Nnyc_DD4KFcw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753148000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/9rynne03xbef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/9rynne03xbef1.png?auto=webp&amp;s=4f7e2275d4e835b0f01387fc4e2f5de4682c92f8",
                  "width": 1520,
                  "height": 2266
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c72f32848579ff8381bbc07e00d52af73ccb790",
                    "width": 108,
                    "height": 161
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c05a3489ce34f1c0d3c48da6ac4fb493a3af2239",
                    "width": 216,
                    "height": 322
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7758dc0729434a0929fb46f9640d8ed72e9ba4f",
                    "width": 320,
                    "height": 477
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8cc832729da290425257b97f9e8171f9cd64ec1e",
                    "width": 640,
                    "height": 954
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1dd97abb7b7a97ba53a5a60797a4df72dbe1e9e",
                    "width": 960,
                    "height": 1431
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51749430f3b47afde5a4eee467854e441af14310",
                    "width": 1080,
                    "height": 1610
                  }
                ],
                "variants": {},
                "id": "OE4XOhVwW7bVZ94xo4IF074gf7GQOtJgsoNbI-IyttA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6172l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/9rynne03xbef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753148000,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Throwback to 3 months ago: [https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg\\_a\\_unified\\_scalable\\_vector\\_graphics/](https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/)\n\nWeights: [https://huggingface.co/OmniSVG/OmniSVG](https://huggingface.co/OmniSVG/OmniSVG)\n\nHuggingFace demo: [https://huggingface.co/spaces/OmniSVG/OmniSVG-3B](https://huggingface.co/spaces/OmniSVG/OmniSVG-3B)\n\nGitHub: [https://github.com/OmniSVG/OmniSVG/](https://github.com/OmniSVG/OmniSVG/)",
          "author_fullname": "t2_w4j8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OmniSVG weights released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m61u94",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 87,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753149834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throwback to 3 months ago: &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Weights: &lt;a href=\"https://huggingface.co/OmniSVG/OmniSVG\"&gt;https://huggingface.co/OmniSVG/OmniSVG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;HuggingFace demo: &lt;a href=\"https://huggingface.co/spaces/OmniSVG/OmniSVG-3B\"&gt;https://huggingface.co/spaces/OmniSVG/OmniSVG-3B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/OmniSVG/OmniSVG/\"&gt;https://github.com/OmniSVG/OmniSVG/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?auto=webp&amp;s=879a29e047e8b5a9e7c3cc213f6732c60bc2a1a7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a72266ba63c0bc5f87d6bf4f1a9d21ca8a03fb2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8554240e67e2f71d1e81cbf7f1b701e59cb5fefd",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e1d031a2135bd08b701037085db4506b941ab6d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b70a2f5fdb810f142ac53ef2d47901cc0c789f95",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a856840edcf86155de1faff51118fe59e453e241",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=479fbae9e67984006acc087b84b30e16cca24f24",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m61u94",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeProgrammer99",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753149834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Unfortunately it's on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It'll be an interesting piece of localllama HW.",
          "author_fullname": "t2_bjeo1gwy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Used A100 40GB just dropped below $2000, for those who care with caveat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m60ahf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 103,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 103,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753145404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately it&amp;#39;s on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It&amp;#39;ll be an interesting piece of localllama HW.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m60ahf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "--dany--",
          "discussion_type": null,
          "num_comments": 64,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753145404,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!",
          "author_fullname": "t2_mxles3cs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking about updating Llama 3.3-70B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ahsu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753180144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ahsu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Only_Emergencies",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753180144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone looking to test Qwen3-Coder i just added it to my extension so i can play with it.  You need to sign up at [qwen.ai](http://qwen.ai) for api access, and you should even get free credits to try it out.  Let me know if you have any issues, I mostly created the extension for my own use, but it works awesome, and its by far the best experience ive ever had for Claude Code, and love sitting in the pool using it on my phone :p\n\nYou can also just search vscode marketplace for coders in flow, its live now.\n\n  \nI know this is a Local AI group, ollama and lmstudio of course work too, but i really wanted to test out qwen3-coder so i added it in.. ",
          "author_fullname": "t2_4dx55sw2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Added Qwen3-Coder to my VsCode extension",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6wq77",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.42,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753235465,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone looking to test Qwen3-Coder i just added it to my extension so i can play with it.  You need to sign up at &lt;a href=\"http://qwen.ai\"&gt;qwen.ai&lt;/a&gt; for api access, and you should even get free credits to try it out.  Let me know if you have any issues, I mostly created the extension for my own use, but it works awesome, and its by far the best experience ive ever had for Claude Code, and love sitting in the pool using it on my phone :p&lt;/p&gt;\n\n&lt;p&gt;You can also just search vscode marketplace for coders in flow, its live now.&lt;/p&gt;\n\n&lt;p&gt;I know this is a Local AI group, ollama and lmstudio of course work too, but i really wanted to test out qwen3-coder so i added it in.. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6wq77",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PositiveEnergyMatter",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6wq77/added_qwen3coder_to_my_vscode_extension/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6wq77/added_qwen3coder_to_my_vscode_extension/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753235465,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/hx255agsalef1.png?width=932&amp;format=png&amp;auto=webp&amp;s=9e314e0904871f71278151b307047e3b464b3abe\n\n",
          "author_fullname": "t2_4dx55sw2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is VERY expensive maybe one day You can run it locally.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "hx255agsalef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 85,
                  "x": 108,
                  "u": "https://preview.redd.it/hx255agsalef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=063f39b86117f95e8228b4552fd603c5bbd7e1bd"
                },
                {
                  "y": 171,
                  "x": 216,
                  "u": "https://preview.redd.it/hx255agsalef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5889077fccc99ad0896d2b18ddd7ae26302b033f"
                },
                {
                  "y": 253,
                  "x": 320,
                  "u": "https://preview.redd.it/hx255agsalef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab42e8d5e41cae2912da8cdc8580ad23107c4ee1"
                },
                {
                  "y": 507,
                  "x": 640,
                  "u": "https://preview.redd.it/hx255agsalef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a59c57fd28596a519375805b27c122c7fb354ec"
                }
              ],
              "s": {
                "y": 739,
                "x": 932,
                "u": "https://preview.redd.it/hx255agsalef1.png?width=932&amp;format=png&amp;auto=webp&amp;s=9e314e0904871f71278151b307047e3b464b3abe"
              },
              "id": "hx255agsalef1"
            }
          },
          "name": "t3_1m74b87",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.35,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ofD_PGhYFdSfHB4tfxLg9C_lgOPt3cFUilmSEBETViA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753261576,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/hx255agsalef1.png?width=932&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e314e0904871f71278151b307047e3b464b3abe\"&gt;https://preview.redd.it/hx255agsalef1.png?width=932&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e314e0904871f71278151b307047e3b464b3abe&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m74b87",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PositiveEnergyMatter",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m74b87/qwen3coder_is_very_expensive_maybe_one_day_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m74b87/qwen3coder_is_very_expensive_maybe_one_day_you/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753261576,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/JustinLin610/status/1947281769134170147\n\nMaybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng [here](https://x.com/ChujieZheng/status/1947307034980089905).",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Imminent release from Qwen tonight",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5n148",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 436,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 436,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DBvwe9bi2sUadVKTh4wZB-h_0n3lxygls9SlP-B36wg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753114102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/JustinLin610/status/1947281769134170147\"&gt;https://x.com/JustinLin610/status/1947281769134170147&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Maybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng &lt;a href=\"https://x.com/ChujieZheng/status/1947307034980089905\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/um0pwye549ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/um0pwye549ef1.png?auto=webp&amp;s=034e1360dd4d1a71075a1978e81cc176280c0940",
                  "width": 570,
                  "height": 284
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac602ae1dcb08fc594a97f2c504da7e053543395",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=34209fd3279089bca920b8e313de5e6ea1d3d074",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3401860a4fccf34b2ae631236b2b714dafe0ec28",
                    "width": 320,
                    "height": 159
                  }
                ],
                "variants": {},
                "id": "XO8ytuncZItjhr3hlokHSgG-P5B6fKV-FMv68ZqKLXQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5n148",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 86,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/",
          "stickied": false,
          "url": "https://i.redd.it/um0pwye549ef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753114102,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1tpuoj72sa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Frankenserver for sale at a steep discount. 2x96GB GH200 converted from liquid- to air-cooled.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m65iga",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/ObGRZZdEJPhNPxSGc-Hl-FLX_u-ODU9Q-A84Zj7Q5Z4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753161224,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ifz3sua70def1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ifz3sua70def1.jpeg?auto=webp&amp;s=3c8da0138281644b0b232fb926afb72c94068d2d",
                  "width": 4037,
                  "height": 3077
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cb9bd9d6aa78574351fa9778ec9d0b129263457",
                    "width": 108,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7c4b9ff9064655d107fe8aab0d33aa934050ef",
                    "width": 216,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34ad0918b9b41221e6a4d64ec2a5686a6466206b",
                    "width": 320,
                    "height": 243
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784878f38d8fa43b398531435fad5ad46f80423f",
                    "width": 640,
                    "height": 487
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fba24ff9f318cff389422ef0a5e7d51301d79f6c",
                    "width": 960,
                    "height": 731
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=674e7d2f5ae18055ec6bf581e5ec73734512ba00",
                    "width": 1080,
                    "height": 823
                  }
                ],
                "variants": {},
                "id": "xdYmB0YCSsaGa7OmRBAUHmZvhAr6D2eweUUdkl-syTg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m65iga",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTrack_ai",
          "discussion_type": null,
          "num_comments": 72,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m65iga/frankenserver_for_sale_at_a_steep_discount_2x96gb/",
          "stickied": false,
          "url": "https://i.redd.it/ifz3sua70def1.jpeg",
          "subreddit_subscribers": 503258,
          "created_utc": 1753161224,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Seeking recommendations for Android LLM apps with GPU acceleration and customisation like promts.",
          "author_fullname": "t2_8pq43jfs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best android local llm apk with gpu acceleration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6q0oh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753218021,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeking recommendations for Android LLM apps with GPU acceleration and customisation like promts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6q0oh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Moose-228",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6q0oh/best_android_local_llm_apk_with_gpu_acceleration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6q0oh/best_android_local_llm_apk_with_gpu_acceleration/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753218021,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to set up a homelab. I’ve got 2 NVIDIA Quadro RTX 6000’s laying around that I was given a few years back. I don’t have any server equipment yet, but I’m gonna buy a rack, PSU, server motherboard, Processor, RAM, and storage enclaves to set up my first homelab. \n\nI want to build an AI to help me with my job in Cybersecurity, I’d like to train it on big data sets like Stack Overflow and CVE.\n\nMy question is, are my GPU’s good enough for this task? What kind of CPU/S do I need to keep up? Ram capacity/speed recommendations?",
          "author_fullname": "t2_1cam2liip6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I own a few Quadro’s, can I build an AI with these?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6v9yq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753231391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to set up a homelab. I’ve got 2 NVIDIA Quadro RTX 6000’s laying around that I was given a few years back. I don’t have any server equipment yet, but I’m gonna buy a rack, PSU, server motherboard, Processor, RAM, and storage enclaves to set up my first homelab. &lt;/p&gt;\n\n&lt;p&gt;I want to build an AI to help me with my job in Cybersecurity, I’d like to train it on big data sets like Stack Overflow and CVE.&lt;/p&gt;\n\n&lt;p&gt;My question is, are my GPU’s good enough for this task? What kind of CPU/S do I need to keep up? Ram capacity/speed recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6v9yq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NetTechMan",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6v9yq/i_own_a_few_quadros_can_i_build_an_ai_with_these/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6v9yq/i_own_a_few_quadros_can_i_build_an_ai_with_these/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753231391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "have a project that uses the `deepseek-r1` model from `https://api.llama-api.com`. However, it seems Llama API has launched a new console. My email is not recognized in the new beta console, although I have an account and have added credit to it. \n\nThe old console links no longer work. Additionally, the DeepSeek models are not listed on the documentation page anymore (`https://llama.developer.meta.com/docs/models`).",
          "author_fullname": "t2_tqeqwc14",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek not available at LLama API?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6pjpx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753216907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;have a project that uses the &lt;code&gt;deepseek-r1&lt;/code&gt; model from &lt;code&gt;https://api.llama-api.com&lt;/code&gt;. However, it seems Llama API has launched a new console. My email is not recognized in the new beta console, although I have an account and have added credit to it. &lt;/p&gt;\n\n&lt;p&gt;The old console links no longer work. Additionally, the DeepSeek models are not listed on the documentation page anymore (&lt;code&gt;https://llama.developer.meta.com/docs/models&lt;/code&gt;).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6pjpx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AncientMayar",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6pjpx/deepseek_not_available_at_llama_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6pjpx/deepseek_not_available_at_llama_api/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753216907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi folks!\n\nThanks to this community, I pulled the trigger about a month ago to get a machine with a 3090. It's been a crazy month for me, and I've been coding local AI tools non-stop.\n\nI'm excited to share my favorite creation so far: **[agent-cli](https://github.com/basnijholt/agent-cli)**, a suite of tools that lets me interact with local models using system-wide hotkeys on my Mac.\n\n**What does it do?**\n\n*   **Hotkey-Powered Workflow:** I can transcribe audio, correct grammar, or have a voice-based conversation with my clipboard content without ever leaving my current application.\n*   **Transcription (`Cmd+Shift+R`):** Instantly transcribe my voice into the clipboard using a local Whisper model.\n*   **Autocorrect (`Cmd+Shift+A`):** Fix spelling and grammar on any copied text.\n*   **Voice Edit (`Cmd+Shift+V`):** I can copy some text, then use my voice to command an LLM to edit it, summarize it, or even answer a question based on it.\n\nThen it also has an interactive voice chat and one that is activated by a wake word.\n\n**It's 100% Local &amp; Private**\n\nThe whole stack is designed to run completely offline on your own machine:\n*   **LLM:** Works with any model via Ollama.\n*   **STT (Speech-to-Text):** Uses `wyoming-faster-whisper`.\n*   **TTS (Text-to-Speech):** Supports `wyoming-piper` and `Kokoro-FastAPI`.\n*   **Wake Word:** Integrates with `wyoming-openwakeword` for a hands-free assistant.\n\nI'd never recorded a video before, but I put together a short demo to make it easier to see how it all works in practice.\n\n- https://www.youtube.com/watch?v=7sBTCgttH48\n- https://github.com/basnijholt/agent-cli\n\nI'd love to get your feedback. Let me know what you think!\n",
          "author_fullname": "t2_yquzb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I stopped typing. Now I just use a hotkey. I built Agent-CLI to make it possible.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6uq8q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "Agent-CLI: Local AI Voice &amp; Text Tools on Your Desktop (macOS Demo)",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "johnbaltis",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7sBTCgttH48/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@BasNij"
            },
            "type": "youtube.com"
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m6uq8q",
            "height": 200
          },
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=8be70c16dc7a0e0e375b94ff98e0971b5cfbac91",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753229837,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks!&lt;/p&gt;\n\n&lt;p&gt;Thanks to this community, I pulled the trigger about a month ago to get a machine with a 3090. It&amp;#39;s been a crazy month for me, and I&amp;#39;ve been coding local AI tools non-stop.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m excited to share my favorite creation so far: &lt;strong&gt;&lt;a href=\"https://github.com/basnijholt/agent-cli\"&gt;agent-cli&lt;/a&gt;&lt;/strong&gt;, a suite of tools that lets me interact with local models using system-wide hotkeys on my Mac.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;  &lt;strong&gt;Hotkey-Powered Workflow:&lt;/strong&gt; I can transcribe audio, correct grammar, or have a voice-based conversation with my clipboard content without ever leaving my current application.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Transcription (&lt;code&gt;Cmd+Shift+R&lt;/code&gt;):&lt;/strong&gt; Instantly transcribe my voice into the clipboard using a local Whisper model.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Autocorrect (&lt;code&gt;Cmd+Shift+A&lt;/code&gt;):&lt;/strong&gt; Fix spelling and grammar on any copied text.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Voice Edit (&lt;code&gt;Cmd+Shift+V&lt;/code&gt;):&lt;/strong&gt; I can copy some text, then use my voice to command an LLM to edit it, summarize it, or even answer a question based on it.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then it also has an interactive voice chat and one that is activated by a wake word.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;It&amp;#39;s 100% Local &amp;amp; Private&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The whole stack is designed to run completely offline on your own machine:\n*   &lt;strong&gt;LLM:&lt;/strong&gt; Works with any model via Ollama.\n*   &lt;strong&gt;STT (Speech-to-Text):&lt;/strong&gt; Uses &lt;code&gt;wyoming-faster-whisper&lt;/code&gt;.\n*   &lt;strong&gt;TTS (Text-to-Speech):&lt;/strong&gt; Supports &lt;code&gt;wyoming-piper&lt;/code&gt; and &lt;code&gt;Kokoro-FastAPI&lt;/code&gt;.\n*   &lt;strong&gt;Wake Word:&lt;/strong&gt; Integrates with &lt;code&gt;wyoming-openwakeword&lt;/code&gt; for a hands-free assistant.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d never recorded a video before, but I put together a short demo to make it easier to see how it all works in practice.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=7sBTCgttH48\"&gt;https://www.youtube.com/watch?v=7sBTCgttH48&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/basnijholt/agent-cli\"&gt;https://github.com/basnijholt/agent-cli&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love to get your feedback. Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=7sBTCgttH48",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?auto=webp&amp;s=b46176561bea253bdaeac9ab75bd82fbf837bb7f",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b60a9338c5518f795a38fedcae4b0e1233d18742",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec3a10104da04aff2c9950c5c90919a9bb1f40d4",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d14c7c0424b23b6f7f7f0c9a66d57e7440d29402",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m6uq8q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "basnijholt",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6uq8q/i_stopped_typing_now_i_just_use_a_hotkey_i_built/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=7sBTCgttH48",
          "subreddit_subscribers": 503258,
          "created_utc": 1753229837,
          "num_crossposts": 0,
          "media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "Agent-CLI: Local AI Voice &amp; Text Tools on Your Desktop (macOS Demo)",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "johnbaltis",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7sBTCgttH48/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@BasNij"
            },
            "type": "youtube.com"
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Even if we use a smart model to fully automate the process, the quality will be poor and the cost will be high. It seems very difficult to completely eliminate manual work.",
          "author_fullname": "t2_6sew99etq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you solve this dilemma?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m71oqv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/UW9Ojp-N_2xGv3aCrhTqb4B6LWcw6WDtYcKRjDCsvn0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753251304,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Even if we use a smart model to fully automate the process, the quality will be poor and the cost will be high. It seems very difficult to completely eliminate manual work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/e5bcw2jjfkef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/e5bcw2jjfkef1.jpeg?auto=webp&amp;s=7e0c3217a7ed3eb643132913859398447e2412f7",
                  "width": 704,
                  "height": 354
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/e5bcw2jjfkef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2ba9fd3be1478ce1ce7883547f7ea4556dda1a2",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/e5bcw2jjfkef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0696c8d1994491cb253c342b6f29db3d1abd5473",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/e5bcw2jjfkef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a69eee3e2b1f60411801843c905b7203ef0526e5",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://preview.redd.it/e5bcw2jjfkef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c1782d5647f56994e15a0eee9236bae38286cf40",
                    "width": 640,
                    "height": 321
                  }
                ],
                "variants": {},
                "id": "BsOCKQAXqozR1rMOhf64IUBDfs8nLJvx2Qw2Ag2TKTc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m71oqv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dahara111",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m71oqv/how_do_you_solve_this_dilemma/",
          "stickied": false,
          "url": "https://i.redd.it/e5bcw2jjfkef1.jpeg",
          "subreddit_subscribers": 503258,
          "created_utc": 1753251304,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m currently exploring multimodal LLMs — specifically models that can handle image input (like OCR, screenshot analysis, or general image understanding). I’m curious if anyone here has successfully deployed one of these models on a VPS.",
          "author_fullname": "t2_i9xt9vl2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone here worked with LLMs that can read images? Were you able to deploy it on a VPS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ucc0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753228806,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m currently exploring multimodal LLMs — specifically models that can handle image input (like OCR, screenshot analysis, or general image understanding). I’m curious if anyone here has successfully deployed one of these models on a VPS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ucc0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Turbulent-Cow4848",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ucc0/has_anyone_here_worked_with_llms_that_can_read/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ucc0/has_anyone_here_worked_with_llms_that_can_read/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753228806,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vgnr5u5gg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If Qwen3-235B-A22B-2507 can't think, why does it think when the thinking button is on?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m650ow",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/eOMW_Oov8IQoYGj2MLhSVhdZR_G_D49PS5lS7HSSsnk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753159541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lxwf5fgevcef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?auto=webp&amp;s=bcac025445c3e19050b77650ec0313d164d2cb63",
                  "width": 696,
                  "height": 181
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=93400e194109a5036a8e420d94408434e9409fa7",
                    "width": 108,
                    "height": 28
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=904c5d55682c555ac50948455d22e99da1ab864b",
                    "width": 216,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae695aac40a41943b105be24e34d521e45b9b0b3",
                    "width": 320,
                    "height": 83
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf2250dfec91bd4da5ce280844d385d4702942d1",
                    "width": 640,
                    "height": 166
                  }
                ],
                "variants": {},
                "id": "9dzCnXa9HN8TH6BqvCudoVpKpcesYzMwj4FmuXf8IdA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m650ow",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JeffreySons_90",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m650ow/if_qwen3235ba22b2507_cant_think_why_does_it_think/",
          "stickied": false,
          "url": "https://i.redd.it/lxwf5fgevcef1.jpeg",
          "subreddit_subscribers": 503258,
          "created_utc": 1753159541,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I'm curious to see if others find the same. PRs accepted and encouraged!",
          "author_fullname": "t2_5n8i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running LLMs against a sandbox airport to see if they can make the correct decisions in real time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m62vbw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=44392f6267e504eff05965daa4cd423000d27a80",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753152819,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I&amp;#39;m curious to see if others find the same. PRs accepted and encouraged!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/jjasghar/ai-airport-simulation",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?auto=webp&amp;s=27c081b855fd81984a0fbd5a3cd8d041afd40a2a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e5cedfeb2acc17ed96c354aea24f51d83b107d8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f04243269cf381d5d666e8ad9cc1f63960e31ef9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4b0fca68877cd9584992c2a2b35a39c83a82f6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c17ae1c6715cde69de4cb21dd94c66e0f2a16d0b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04a1b374daef1cc2d36172a124f4af1e43e7a5c7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df058f8c50256c5ab0fda05e61968a5c791096db",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m62vbw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jjasghar",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/",
          "stickied": false,
          "url": "https://github.com/jjasghar/ai-airport-simulation",
          "subreddit_subscribers": 503258,
          "created_utc": 1753152819,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently want to get into playing with LLMs and am starting my first PC build (only have owned laptops before on integrated graphics). Based in USA. Is the 5060 8GB at $280 enough to mess with local AI stuff and potentially move on when I've hit the limits, or am I going to be hitting limits so early on that I should just get a faster/more VRAM/better memory bus/etc card from the start? Right now the options in that price range seem like $280 5060 8GB or maybe used ~$320ish 3080 10GB. The big swing move for me right now would be something like a 5070 ti 16GB at $800 (already stretching budget a lot), but it seems like if I can get away with around $300 and then upgrade later it would be better overall. If I'm playing down in 8GB territory anyways, should I just find whatever cheap $100ish card on ebay I can to mess for now?\n\nAre there big differences in the technologies incorporated in the 10xx, 20xx, 30xx, 40xx, 50xx cards that are relevant to AI loads? Or can I just roughly use the (mostly fps-based/gaming) benchmarks as a guide for relative performance? Other things I should worry about in the build other than GPU? Currently thinking CPU as AMD 9600x with 32GB DDR5-6000.\n\nLong-term goal is to play around enough with LLMs to be able to understand what is happening in the research papers i.e. play around with building smaller LLMs/change around architectures/measure performance; download models to play around with inference; and maybe doing useful fine-tuning of (smaller) models. Basically dipping my toes in right now. I have a long-term goal, but let's be honest, you don't decide to buy a Strad because you want to learn violin, and I'm not looking to drop $$$$ on a GPU if it's avoidable.\n\nUpgrade paths will depend on progress on playing around with small model building, fine-tuning existing small footprint models and useful inference from downloaded models. They would include better GPU or just buying time from a cloud provider.",
          "author_fullname": "t2_fs6q6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Entry GPU options - 5060 8GB enough to play with?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6knhw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753205961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently want to get into playing with LLMs and am starting my first PC build (only have owned laptops before on integrated graphics). Based in USA. Is the 5060 8GB at $280 enough to mess with local AI stuff and potentially move on when I&amp;#39;ve hit the limits, or am I going to be hitting limits so early on that I should just get a faster/more VRAM/better memory bus/etc card from the start? Right now the options in that price range seem like $280 5060 8GB or maybe used ~$320ish 3080 10GB. The big swing move for me right now would be something like a 5070 ti 16GB at $800 (already stretching budget a lot), but it seems like if I can get away with around $300 and then upgrade later it would be better overall. If I&amp;#39;m playing down in 8GB territory anyways, should I just find whatever cheap $100ish card on ebay I can to mess for now?&lt;/p&gt;\n\n&lt;p&gt;Are there big differences in the technologies incorporated in the 10xx, 20xx, 30xx, 40xx, 50xx cards that are relevant to AI loads? Or can I just roughly use the (mostly fps-based/gaming) benchmarks as a guide for relative performance? Other things I should worry about in the build other than GPU? Currently thinking CPU as AMD 9600x with 32GB DDR5-6000.&lt;/p&gt;\n\n&lt;p&gt;Long-term goal is to play around enough with LLMs to be able to understand what is happening in the research papers i.e. play around with building smaller LLMs/change around architectures/measure performance; download models to play around with inference; and maybe doing useful fine-tuning of (smaller) models. Basically dipping my toes in right now. I have a long-term goal, but let&amp;#39;s be honest, you don&amp;#39;t decide to buy a Strad because you want to learn violin, and I&amp;#39;m not looking to drop $$$$ on a GPU if it&amp;#39;s avoidable.&lt;/p&gt;\n\n&lt;p&gt;Upgrade paths will depend on progress on playing around with small model building, fine-tuning existing small footprint models and useful inference from downloaded models. They would include better GPU or just buying time from a cloud provider.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6knhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "drabbiticus",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753205961,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Cards like 3090, 4090, 5090 has very high electric consumption. Isn't it possible to make 24,32gb cards with like 5060 level electric consumption?",
          "author_fullname": "t2_d9gk5hdlt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "+24GB VRAM with low electric consumption",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hzf0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753200005,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cards like 3090, 4090, 5090 has very high electric consumption. Isn&amp;#39;t it possible to make 24,32gb cards with like 5060 level electric consumption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6hzf0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "narca_hakan",
          "discussion_type": null,
          "num_comments": 55,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753200005,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! 👋\n\nI recently put together a desktop AI chat interface called **Hearth-UI**, made for anyone using [Ollama](https://ollama.com/) for local LLMs like LLaMA3, Mistral, Gemma, etc.\n\nIt includes everything I wish existed in a typical Ollama UI — and it’s fully offline, customizable, and open-source.\n\n🧠 Features:\n\n✅ **Multi-session chat history** (rename, delete, auto-save)  \n✅ **Markdown + syntax highlighting** (like ChatGPT)  \n✅ **Streaming responses** \\+ prompt **queueing while streaming**  \n✅ **File uploads** &amp; **drag-and-drop attachments**  \n✅ Beautiful **theme picker** (Dark/Light/Blue/Green/etc)  \n✅ **Cancel response mid-generation** (Stop button)  \n✅ Export chat to `.txt`, `.json`, `.md`  \n✅ **Electron-powered desktop app for Windows** (macOS/Linux coming)  \n✅ Works with your existing `ollama serve` — no cloud, no signup\n\n# 🔧 Tech stack:\n\n* Ollama (as LLM backend)\n* HTML/CSS/JS (Vanilla frontend)\n* Electron for standalone app\n* Node.js backend (for model list &amp; /chat proxy)\n\n\n\n# GitHub link:\n\nhttps://preview.redd.it/4m9yutminkef1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=26080fb1cc9c008d8d8c4c1964165d23d6f7134c\n\n👉 [https://github.com/Saurabh682/Hearth-UI](https://github.com/Saurabh682/Hearth-UI)\n\n# 🙏 I'd love your feedback on:\n\n* Other must-have features?\n* Would a Windows/exe help?\n* Any bugs or improvement ideas?\n\nThanks for checking it out. Hope it helps the self-hosted LLM community!  \n❤️\n\n# 🏷️ Tags:\n\n`[Electron] [Ollama] [Local LLM] [Desktop AI UI] [Markdown] [Self Hosted]`",
          "author_fullname": "t2_8747zjh8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🔓 I built Hearth-UI — A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4m9yutminkef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 52,
                  "x": 108,
                  "u": "https://preview.redd.it/4m9yutminkef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d832446b4a0daef35a1ac98a796b5e6380b7b2a7"
                },
                {
                  "y": 105,
                  "x": 216,
                  "u": "https://preview.redd.it/4m9yutminkef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c9d85a01ca9dff84d5c4174c852a2183ca4e963"
                },
                {
                  "y": 156,
                  "x": 320,
                  "u": "https://preview.redd.it/4m9yutminkef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9eab1c6229fcb09b22c974b8e7a1fc0f8d07b521"
                },
                {
                  "y": 312,
                  "x": 640,
                  "u": "https://preview.redd.it/4m9yutminkef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=11d38a746b38e86edf0d91c15cd88db6680541de"
                },
                {
                  "y": 469,
                  "x": 960,
                  "u": "https://preview.redd.it/4m9yutminkef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4748a33d5853ab554ea425e46f3c9905287fbd8b"
                },
                {
                  "y": 528,
                  "x": 1080,
                  "u": "https://preview.redd.it/4m9yutminkef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=976ac7210176cc9414bff2885b9b3df7842e5d47"
                }
              ],
              "s": {
                "y": 528,
                "x": 1080,
                "u": "https://preview.redd.it/4m9yutminkef1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=26080fb1cc9c008d8d8c4c1964165d23d6f7134c"
              },
              "id": "4m9yutminkef1"
            }
          },
          "name": "t3_1m72d5y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=37c71bd6eb9aa2635eac207ee75bc00f189acaf8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753253880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! 👋&lt;/p&gt;\n\n&lt;p&gt;I recently put together a desktop AI chat interface called &lt;strong&gt;Hearth-UI&lt;/strong&gt;, made for anyone using &lt;a href=\"https://ollama.com/\"&gt;Ollama&lt;/a&gt; for local LLMs like LLaMA3, Mistral, Gemma, etc.&lt;/p&gt;\n\n&lt;p&gt;It includes everything I wish existed in a typical Ollama UI — and it’s fully offline, customizable, and open-source.&lt;/p&gt;\n\n&lt;p&gt;🧠 Features:&lt;/p&gt;\n\n&lt;p&gt;✅ &lt;strong&gt;Multi-session chat history&lt;/strong&gt; (rename, delete, auto-save)&lt;br/&gt;\n✅ &lt;strong&gt;Markdown + syntax highlighting&lt;/strong&gt; (like ChatGPT)&lt;br/&gt;\n✅ &lt;strong&gt;Streaming responses&lt;/strong&gt; + prompt &lt;strong&gt;queueing while streaming&lt;/strong&gt;&lt;br/&gt;\n✅ &lt;strong&gt;File uploads&lt;/strong&gt; &amp;amp; &lt;strong&gt;drag-and-drop attachments&lt;/strong&gt;&lt;br/&gt;\n✅ Beautiful &lt;strong&gt;theme picker&lt;/strong&gt; (Dark/Light/Blue/Green/etc)&lt;br/&gt;\n✅ &lt;strong&gt;Cancel response mid-generation&lt;/strong&gt; (Stop button)&lt;br/&gt;\n✅ Export chat to &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.json&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;&lt;br/&gt;\n✅ &lt;strong&gt;Electron-powered desktop app for Windows&lt;/strong&gt; (macOS/Linux coming)&lt;br/&gt;\n✅ Works with your existing &lt;code&gt;ollama serve&lt;/code&gt; — no cloud, no signup&lt;/p&gt;\n\n&lt;h1&gt;🔧 Tech stack:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ollama (as LLM backend)&lt;/li&gt;\n&lt;li&gt;HTML/CSS/JS (Vanilla frontend)&lt;/li&gt;\n&lt;li&gt;Electron for standalone app&lt;/li&gt;\n&lt;li&gt;Node.js backend (for model list &amp;amp; /chat proxy)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;GitHub link:&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4m9yutminkef1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26080fb1cc9c008d8d8c4c1964165d23d6f7134c\"&gt;https://preview.redd.it/4m9yutminkef1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26080fb1cc9c008d8d8c4c1964165d23d6f7134c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👉 &lt;a href=\"https://github.com/Saurabh682/Hearth-UI\"&gt;https://github.com/Saurabh682/Hearth-UI&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;🙏 I&amp;#39;d love your feedback on:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Other must-have features?&lt;/li&gt;\n&lt;li&gt;Would a Windows/exe help?&lt;/li&gt;\n&lt;li&gt;Any bugs or improvement ideas?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for checking it out. Hope it helps the self-hosted LLM community!&lt;br/&gt;\n❤️&lt;/p&gt;\n\n&lt;h1&gt;🏷️ Tags:&lt;/h1&gt;\n\n&lt;p&gt;&lt;code&gt;[Electron] [Ollama] [Local LLM] [Desktop AI UI] [Markdown] [Self Hosted]&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m72d5y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Vast-Helicopter-3719",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m72d5y/i_built_hearthui_a_fullyfeatured_desktop_app_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m72d5y/i_built_hearthui_a_fullyfeatured_desktop_app_for/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753253880,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m running a 16-inch MacBook Pro with the new M4 Pro chip (48 GB unified RAM, 512 GB SSD). I’ve narrowed my local LLM experiments down to two heavy hitters:\n\nDeepSeek-Coder V3-Lite 33B for coding powerhouse \n\nQwen3-32B-Instruct-MoE for coding and reasoning all purpose \n\ni want your opinion how these two how these two feels in real world, for a person like me, i need it for writing python script , do some research, in VS we can use api in cline for execution and auto completion of the code without limit\n\nmy current setup\n\nmacOS 15.2 (Sonoma++)\nLM Studio 0.4.3 – MLX engine\nQwen3 GGUF Q4_K_M  — 18 GB\nDeepSeek-Coder Q4_K_M — 27 GB\nSwap disabled, running on mains (140 W)\n\nyour thoughts what are the other model we can try and test with limited hardware. thank you\n",
          "author_fullname": "t2_3nc1bpb5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "M4 Pro Owners: I Want Your Biased Hot-Takes – DeepSeek-Coder V3-Lite 33B vs Qwen3-32B-Instruct-MoE on a 48 GB MacBook Pro",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6tf9v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753226393,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m running a 16-inch MacBook Pro with the new M4 Pro chip (48 GB unified RAM, 512 GB SSD). I’ve narrowed my local LLM experiments down to two heavy hitters:&lt;/p&gt;\n\n&lt;p&gt;DeepSeek-Coder V3-Lite 33B for coding powerhouse &lt;/p&gt;\n\n&lt;p&gt;Qwen3-32B-Instruct-MoE for coding and reasoning all purpose &lt;/p&gt;\n\n&lt;p&gt;i want your opinion how these two how these two feels in real world, for a person like me, i need it for writing python script , do some research, in VS we can use api in cline for execution and auto completion of the code without limit&lt;/p&gt;\n\n&lt;p&gt;my current setup&lt;/p&gt;\n\n&lt;p&gt;macOS 15.2 (Sonoma++)\nLM Studio 0.4.3 – MLX engine\nQwen3 GGUF Q4_K_M  — 18 GB\nDeepSeek-Coder Q4_K_M — 27 GB\nSwap disabled, running on mains (140 W)&lt;/p&gt;\n\n&lt;p&gt;your thoughts what are the other model we can try and test with limited hardware. thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6tf9v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WestPush7",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6tf9v/m4_pro_owners_i_want_your_biased_hottakes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6tf9v/m4_pro_owners_i_want_your_biased_hottakes/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753226393,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for an uncensored LLM I can run on LM Studio that specializes in producing highly spicy prompts. Sometimes I just don't know what I want, or end up producing too many similar images and would rather be surprised. Asking an image generation model for creativity is not going to work - it wants highly specific and descriptive prompts. But an LLM fine tuned for spicy prompts could make them for me. I just tried with Qwen 30B A3B and it spit out censorship :/\n\nAny recommendations? (4090)",
          "author_fullname": "t2_38cpv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'm looking for an Uncensored LLM to produce extremely spicy prompts - What would you recommend?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6xbs7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753237169,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for an uncensored LLM I can run on LM Studio that specializes in producing highly spicy prompts. Sometimes I just don&amp;#39;t know what I want, or end up producing too many similar images and would rather be surprised. Asking an image generation model for creativity is not going to work - it wants highly specific and descriptive prompts. But an LLM fine tuned for spicy prompts could make them for me. I just tried with Qwen 30B A3B and it spit out censorship :/&lt;/p&gt;\n\n&lt;p&gt;Any recommendations? (4090)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6xbs7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Whipit",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6xbs7/im_looking_for_an_uncensored_llm_to_produce/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6xbs7/im_looking_for_an_uncensored_llm_to_produce/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753237169,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi I'm a college student from India.\n\nSo i'm looking for a language model for code generation to run locally. I only have 16 GB of ram and iris xe gpu, so looking for some good opensource SLMs which can be decent enough. I could use something like llama.cpp given performance and latency would be decent(currently using a gguf version of mistral 7B-instruct and it's working fine) . Can also consider using raspberry pi if it'll be of any use",
          "author_fullname": "t2_7q8gvaa19",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best opensource SLM/ lightweight llm for code generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6dvhi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753190318,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m a college student from India.&lt;/p&gt;\n\n&lt;p&gt;So i&amp;#39;m looking for a language model for code generation to run locally. I only have 16 GB of ram and iris xe gpu, so looking for some good opensource SLMs which can be decent enough. I could use something like llama.cpp given performance and latency would be decent(currently using a gguf version of mistral 7B-instruct and it&amp;#39;s working fine) . Can also consider using raspberry pi if it&amp;#39;ll be of any use&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6dvhi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RustinChole11",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6dvhi/best_opensource_slm_lightweight_llm_for_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6dvhi/best_opensource_slm_lightweight_llm_for_code/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753190318,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "_A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him \"completely exhausted.\"_\n\nhttps://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/",
          "author_fullname": "t2_1gnii9bkc9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Exhausted man defeats AI model in world coding championship",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5r9ss",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 146,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 146,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753123500,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him &amp;quot;completely exhausted.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/\"&gt;https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?auto=webp&amp;s=01c6c5989382448ceacfb16d4716e1d43882c07d",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c56d051b4b4e63e5c6627f8639b6bc541ebe7a70",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06c2478ecab47c428699645361b3f004e394ce98",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dcfa8df2a1e6cafbca016c61a6781fc1bd66b6e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=089271aa272fc9321b33a83d2db06a95c38b6ce9",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73adafa18daac7bc6b0c9c52f9fe6f72db681c41",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3d93ef0ea7ac95a421f4d7a2e7e1d2f2350aac9",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5r9ss",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational_Sun_8813",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753123500,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "​Disclaimer: I made [hyprnote](https://hyprnote.com) \\- went trending in [here](https://www.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/) 3 months ago.\n\n**context:**\n\na lot of our users are using ollama at the moment and I thought why not make something for STT just like ollama. we are also getting more and more requests on the parakeet model so really looking into this right now.\n\n**research:**\n\nI haven't come across anything related to this. I found some projects using whisperX but haven't actually found one where you can just use different models like ollama.\n\n**owhisper:**\n\nI'm building an open-source alternative for granola ai. I want to make hyprnote self-hostable so people can play around with various stt and llms. thinking about making a unified proxy server that can be deployed and manages owhisper and custom llm endpoints - including ollama.\n\nCurious - if this existed, would you try it out? And what features would you want built in?",
          "author_fullname": "t2_j1t6g97wv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking about \"owhisper\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hck1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753198571,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;​Disclaimer: I made &lt;a href=\"https://hyprnote.com\"&gt;hyprnote&lt;/a&gt; - went trending in &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/\"&gt;here&lt;/a&gt; 3 months ago.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;a lot of our users are using ollama at the moment and I thought why not make something for STT just like ollama. we are also getting more and more requests on the parakeet model so really looking into this right now.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;research:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t come across anything related to this. I found some projects using whisperX but haven&amp;#39;t actually found one where you can just use different models like ollama.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;owhisper:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building an open-source alternative for granola ai. I want to make hyprnote self-hostable so people can play around with various stt and llms. thinking about making a unified proxy server that can be deployed and manages owhisper and custom llm endpoints - including ollama.&lt;/p&gt;\n\n&lt;p&gt;Curious - if this existed, would you try it out? And what features would you want built in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?auto=webp&amp;s=070858a1cfeec763c131615ac513538e6a19426b",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bee6065abfb7b73c45f622b4c1cc472253ace4e",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=82ae4d019c19940aa95321c7be83b0edd546f2a0",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9757ca6f1422e8e8039213794d3c5ab9ab68d765",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=925c91e5f6ddcbcaa0e6039c50650c2027ac8c2b",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d7081fb329bed3002c26fa7002ce376573bb805",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=084bb2b0fc5cc7efd82866fa4b3992449215e74c",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6hck1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "beerbellyman4vr",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hck1/thinking_about_owhisper/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hck1/thinking_about_owhisper/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753198571,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)\n\nDidn't see that with the \"old\" 235b with /no\\_think \n\nIs that expected?",
          "author_fullname": "t2_joxwuyje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In Qwen3-235B-A22B-Instruct-2507-UD-Q4 (unsloth) I'm seeing some \"but wait\" and related ones (like kinda questioning and answering itself), were the model seems to \"think\" (even when is a non-thinking model and I haven't setup any system prompt), have you seen something similar?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69sb6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177544,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)&lt;/p&gt;\n\n&lt;p&gt;Didn&amp;#39;t see that with the &amp;quot;old&amp;quot; 235b with /no_think &lt;/p&gt;\n\n&lt;p&gt;Is that expected?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m69sb6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "relmny",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753177544,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Mind-Blowing](https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128)",
          "author_fullname": "t2_1tbloqabyg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "7by2astxg9ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c498a4606859ea7dc3344cea600a6287666c52bb"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a0f911df63c617cf62eddf44f25fd9d1526f4f9"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=18f18ad0a953d5f6cce28f16501265fc5f1b65fa"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a2c30752b5e9e36ee257e083921c23b7bd51214"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce6ecb7eb20a5950434de688e9e3e318d8dcf552"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b198e562025147aead0f6a4065ef96012102dc8e"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128"
              },
              "id": "7by2astxg9ef1"
            }
          },
          "name": "t3_1m5oz0h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 162,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 162,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sK-ChiNoLnwj2ggKTtNTJYTvWhnsGqdGF-BtGXWSrIM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753118398,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128\"&gt;Mind-Blowing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oz0h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ken-senseii",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753118398,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Nvidia flagship GB200 NVL72 is available 08/04 - 08/05 (bare metal root access!). Anyone interested just ask.",
          "author_fullname": "t2_1tpuoj72sa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Get your hands on Nvidia GB200 NVL72 for free!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m75afx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.24,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aJJpnd8cdxgNqlaclOZbeCF6pE8DjiUX5iO-0kDLnpI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753265295,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Nvidia flagship GB200 NVL72 is available 08/04 - 08/05 (bare metal root access!). Anyone interested just ask.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hvtv2vs4llef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hvtv2vs4llef1.png?auto=webp&amp;s=df437114b6bae05a61b7c46466a1b038813b020d",
                  "width": 1290,
                  "height": 969
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hvtv2vs4llef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e33c92bbe8127e45dad1e99e8801b6f23ad3da1",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/hvtv2vs4llef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fca0957b032e667fd336a1d253b292b7bffb51c7",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/hvtv2vs4llef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4637f7714f69cff355b63d91a84e492ae5e81374",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/hvtv2vs4llef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b76e4af69f2a9ed1a8d01804666534275ef17dcd",
                    "width": 640,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/hvtv2vs4llef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a091da295c74bb558d2682fb11f8180604b1413b",
                    "width": 960,
                    "height": 721
                  },
                  {
                    "url": "https://preview.redd.it/hvtv2vs4llef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c3a5a1f4d587e97c6f1844507c10c6310c13c50e",
                    "width": 1080,
                    "height": 811
                  }
                ],
                "variants": {},
                "id": "YKsv5jCQsA4-vVUb9YIRIcKe6Nf1VAhO1qQau1f-2PA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m75afx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTrack_ai",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m75afx/get_your_hands_on_nvidia_gb200_nvl72_for_free/",
          "stickied": false,
          "url": "https://i.redd.it/hvtv2vs4llef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753265295,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Doesn't it sound cool?\nSounds movie like",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Project: Print AI Replies on a Ticket Printer",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6izt7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753202280,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doesn&amp;#39;t it sound cool?\nSounds movie like&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6izt7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6izt7/project_print_ai_replies_on_a_ticket_printer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6izt7/project_print_ai_replies_on_a_ticket_printer/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753202280,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello there,\n\nMy project to extract and collect the \"secret\" system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it's incredibly useful.\n\n**The idea is to see the advanced \"prompt architecture\" that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.**\n\nInstead of trying to reinvent the wheel, you can see exactly how they force models to \"think step-by-step\" in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It's a goldmine of ideas for crafting better system prompts.\n\nFor example, here's a small snippet from the Cursor prompt that shows how they establish the AI's role and capabilities right away:\n\n    Knowledge cutoff: 2024-06\n    \n    You are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. \n    \n    You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide.\n    \n    You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user.\n    \n    Your main goal is to follow the USER's instructions at each message, denoted by the &lt;user_query&gt; tag.\n    \n    &lt;communication&gt;\n    When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \\( and \\) for inline math, \\[ and \\] for block math.\n    &lt;/communication&gt;\n\nI wrote a full article that does a deep dive into these patterns and also discusses the \"dual-use\" aspect of making these normally-hidden prompts public.\n\nI'm super curious: **How are you all structuring system prompts for your favorite models?**\n\n**Links:**\n\n* **The full article with more analysis:** [The Open Source Project That Became an Essential Library for Modern AI Engineering](https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------)[](https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------)\n\n* **The GitHub Repo (to grab the prompts):** [https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools)\n\nHope you find it useful!",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I extracted the system prompts from closed-source tools like Cursor &amp; v0. The repo just hit 70k stars.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gwzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 397,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 397,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753099199,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753099002,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;My project to extract and collect the &amp;quot;secret&amp;quot; system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it&amp;#39;s incredibly useful.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The idea is to see the advanced &amp;quot;prompt architecture&amp;quot; that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Instead of trying to reinvent the wheel, you can see exactly how they force models to &amp;quot;think step-by-step&amp;quot; in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It&amp;#39;s a goldmine of ideas for crafting better system prompts.&lt;/p&gt;\n\n&lt;p&gt;For example, here&amp;#39;s a small snippet from the Cursor prompt that shows how they establish the AI&amp;#39;s role and capabilities right away:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Knowledge cutoff: 2024-06\n\nYou are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. \n\nYou are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide.\n\nYou are an agent - please keep going until the user&amp;#39;s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user.\n\nYour main goal is to follow the USER&amp;#39;s instructions at each message, denoted by the &amp;lt;user_query&amp;gt; tag.\n\n&amp;lt;communication&amp;gt;\nWhen using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \\( and \\) for inline math, \\[ and \\] for block math.\n&amp;lt;/communication&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I wrote a full article that does a deep dive into these patterns and also discusses the &amp;quot;dual-use&amp;quot; aspect of making these normally-hidden prompts public.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super curious: &lt;strong&gt;How are you all structuring system prompts for your favorite models?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;The full article with more analysis:&lt;/strong&gt; &lt;a href=\"https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------\"&gt;The Open Source Project That Became an Essential Library for Modern AI Engineering&lt;/a&gt;&lt;a href=\"https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------\"&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;The GitHub Repo (to grab the prompts):&lt;/strong&gt; &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hope you find it useful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5gwzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753099002,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, I am running granite-embedding-125m-english on a Docker container with LocalAI and it works great on my laptop, but when I move the project to github, and pull it onto my external server, the API always responds with the same embeddings. \n\nI've pulled the project back to make sure there are no differences between what's on the server and what's on my laptop, and my laptop works as expected. \n\nThe server doesn't have access to the outside world, but once everything is up and running, it shouldn't need it, right? \n\nAnyone have any ideas? I've never seen a model behave like this.",
          "author_fullname": "t2_243il8gu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "embedding model giving same embeddings regardless of input text?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6oqxw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753215074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am running granite-embedding-125m-english on a Docker container with LocalAI and it works great on my laptop, but when I move the project to github, and pull it onto my external server, the API always responds with the same embeddings. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve pulled the project back to make sure there are no differences between what&amp;#39;s on the server and what&amp;#39;s on my laptop, and my laptop works as expected. &lt;/p&gt;\n\n&lt;p&gt;The server doesn&amp;#39;t have access to the outside world, but once everything is up and running, it shouldn&amp;#39;t need it, right? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any ideas? I&amp;#39;ve never seen a model behave like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6oqxw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "User1539",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753215074,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_rkmud0isr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The reason why local models are better/necessary.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5iymb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 283,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 283,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_0QOwkUP9B0YXB-SAHakr_6UlxBhOQpPKlYLA0LCuiQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753104611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vdngpglhb8ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vdngpglhb8ef1.png?auto=webp&amp;s=62e9d97048e91daee3582390075fb00d1887e202",
                  "width": 854,
                  "height": 504
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f4c8c8ea760457e111d37a839bbe4882b86b520",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=adc1595b273f56bd1537f9efa03ae0704717b281",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fca7b7544874c7613248f95246d1810ffb9ffb7b",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae855f8543a7b34526b58ea6c68423bf02a9e2ac",
                    "width": 640,
                    "height": 377
                  }
                ],
                "variants": {},
                "id": "f9sIdJ3aHKBFJd8WvU4XqZxgrL8gg-EBXrNrZSagJuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m5iymb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTshop_ai",
          "discussion_type": null,
          "num_comments": 151,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/",
          "stickied": false,
          "url": "https://i.redd.it/vdngpglhb8ef1.png",
          "subreddit_subscribers": 503258,
          "created_utc": 1753104611,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!\n\nAfter talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible. Today, we’re releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.\n\nThis model performs better than our last release, and we hope you’ll like it thanks to its strong overall abilities.\n\nQwen Chat: chat.qwen.ai — just start chatting with the default model, and feel free to use the search button!",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen released Qwen3-235B-A22B-2507!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5oxyp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 138,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 138,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3Sv34Z6mz9FmhoyS3JnMLxTuxuV0E_Efi7EvfiWKcOs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!&lt;/p&gt;\n\n&lt;p&gt;After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible. Today, we’re releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.&lt;/p&gt;\n\n&lt;p&gt;This model performs better than our last release, and we hope you’ll like it thanks to its strong overall abilities.&lt;/p&gt;\n\n&lt;p&gt;Qwen Chat: chat.qwen.ai — just start chatting with the default model, and feel free to use the search button!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6csu4o4wg9ef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?auto=webp&amp;s=10c54f96c9b0f8a2ead569e3e5e97915476224de",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6baea679e548efcaaac74cffb282ff70f159dd23",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df4fdf4fdf2d8f18aa169c3917aff6a59354480a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd168c86688eec1b15ed22e24f23dd96a0b2be9d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=13fd449c88365fae792fbacc8076a6e633ad74e2",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e563d574e4328f00b0b38e4cd232ecc1c21ff8cb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dc97776dcddebc962894dcf5ba459481257122c",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "fCmGXIZe3dEk6g_UwQiEUl1wK88M5Bv1vrt320AGj8o"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oxyp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/6csu4o4wg9ef1.jpeg",
          "subreddit_subscribers": 503258,
          "created_utc": 1753118337,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there an active leaderboard for local models that ranks them by function calling capability?",
          "author_fullname": "t2_cxq4h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Leaderboard for function calling models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ht1r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753199602,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an active leaderboard for local models that ranks them by function calling capability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ht1r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tvmaly",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ht1r/leaderboard_for_function_calling_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ht1r/leaderboard_for_function_calling_models/",
          "subreddit_subscribers": 503258,
          "created_utc": 1753199602,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}