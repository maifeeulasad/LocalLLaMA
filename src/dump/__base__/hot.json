{
  "kind": "Listing",
  "data": {
    "after": "t3_1ma5yw4",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aa96f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suprise suprise!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 128,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1majemr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 793,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 793,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/9iCds3N2k2-ZYt14S0ZFMUsTtjzG3w6e835k2IuCnE8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753613719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/k64e9lwtdeff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/k64e9lwtdeff1.png?auto=webp&amp;s=6d114b029fdaaee896bc4e5d5a7d43d206e39297",
                  "width": 845,
                  "height": 774
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=654734e23bb5447e379cf550989c3fbafc64f227",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=524ffaaa581fe97e1e7a9cc6c305b3015e336295",
                    "width": 216,
                    "height": 197
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3db32c475829f2bbd9b7113e823dc70bba23038",
                    "width": 320,
                    "height": 293
                  },
                  {
                    "url": "https://preview.redd.it/k64e9lwtdeff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d09af7edf96adcd3793cd8970c2cab58d53352b",
                    "width": 640,
                    "height": 586
                  }
                ],
                "variants": {},
                "id": "7LqJSDe2PuCHTRar6CZQ7nrOdJ1amozrq-VVdgoKEEo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1majemr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GoodGuyLafarge",
          "discussion_type": null,
          "num_comments": 118,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1majemr/suprise_suprise/",
          "stickied": false,
          "url": "https://i.redd.it/k64e9lwtdeff1.png",
          "subreddit_subscribers": 505617,
          "created_utc": 1753613719,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Tesslate/UIGEN-X-32B-0727](https://huggingface.co/Tesslate/UIGEN-X-32B-0727) Releasing 4B in 24 hours and 32B now. \n\nSpecifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.",
          "author_fullname": "t2_15kd4d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "UIGEN-X-0727 Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 115,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "6lu0usna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 75,
                  "x": 108,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=deab3dfb266b42572eb504c47ac7186103a150d7"
                },
                {
                  "y": 151,
                  "x": 216,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4158125ce8d6d928a49741d11fab0a88b58bb93a"
                },
                {
                  "y": 224,
                  "x": 320,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5644ed64b3fcc163a0cca59e5509830bc98de89f"
                },
                {
                  "y": 448,
                  "x": 640,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3fb74188e524cfea7901d2d20f45efd6328de8e"
                },
                {
                  "y": 672,
                  "x": 960,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37b1cb70e05158b0e0614140e38cac8a16a70e2d"
                },
                {
                  "y": 756,
                  "x": 1080,
                  "u": "https://preview.redd.it/6lu0usna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3f8a3812d6aa0ced075d6d909a4bf20150790ca"
                }
              ],
              "s": {
                "y": 1177,
                "x": 1680,
                "u": "https://preview.redd.it/6lu0usna3iff1.png?width=1680&amp;format=png&amp;auto=webp&amp;s=2c78ab518a90ae2fb4242e463b9c9e28de21fbc3"
              },
              "id": "6lu0usna3iff1"
            },
            "fqga84tl3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 91,
                  "x": 108,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b03984152a9c0fc475fcb302bc51781aeb16a11"
                },
                {
                  "y": 183,
                  "x": 216,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25bacc68723ca59a1419e509d34a5a6eacf0ceaa"
                },
                {
                  "y": 271,
                  "x": 320,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80dbd3e0ce2c8cc6cd1eaa6b6cb8edeee8c9d80b"
                },
                {
                  "y": 543,
                  "x": 640,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fae4453f63fff6ed13c42e3ed07360bc6cad55bf"
                },
                {
                  "y": 815,
                  "x": 960,
                  "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8961827432c583d795ce780d68a4abfc83dbea4a"
                }
              ],
              "s": {
                "y": 905,
                "x": 1066,
                "u": "https://preview.redd.it/fqga84tl3iff1.jpg?width=1066&amp;format=pjpg&amp;auto=webp&amp;s=217ff81b55adc46dd9760e2394a5c07b7737f272"
              },
              "id": "fqga84tl3iff1"
            },
            "lj87vona3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce180cd51da47f7ac8d134e955c504e2f9006ad1"
                },
                {
                  "y": 177,
                  "x": 216,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a30d3d7ca0f74fad170e8deb659404df0658034"
                },
                {
                  "y": 262,
                  "x": 320,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ae493f3bec6dd57ae2f103becb380afa41f3b23"
                },
                {
                  "y": 525,
                  "x": 640,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ef5dbcbeeab9377fbbcd05b2d5a653c37556f7e8"
                },
                {
                  "y": 788,
                  "x": 960,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb11c15584b4d0ef3d44dd0b4ed377196c06c20a"
                },
                {
                  "y": 887,
                  "x": 1080,
                  "u": "https://preview.redd.it/lj87vona3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=948f73efd51ccdc0c2d7d7df4b9c0f37d9ca6f84"
                }
              ],
              "s": {
                "y": 1321,
                "x": 1608,
                "u": "https://preview.redd.it/lj87vona3iff1.png?width=1608&amp;format=png&amp;auto=webp&amp;s=2a44bbc2ded69a2a72427325a8fc09a0864b53af"
              },
              "id": "lj87vona3iff1"
            },
            "568yctna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d533701e05b87680057d8e9af82ef32b0e43c5b"
                },
                {
                  "y": 173,
                  "x": 216,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=003f5e1446b33fb35a6eca5ce2b0977a9ee1b114"
                },
                {
                  "y": 256,
                  "x": 320,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7bc6cb2af1b823c07fccc700298c9c107a2ee330"
                },
                {
                  "y": 512,
                  "x": 640,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5d493618596d709bf52a572d752e0582c3dbdb6"
                },
                {
                  "y": 769,
                  "x": 960,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9398c6217463a4419fb684920821492291d3eb9e"
                },
                {
                  "y": 865,
                  "x": 1080,
                  "u": "https://preview.redd.it/568yctna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7e4e79187521212d8de6fdb790cd8903937cca6"
                }
              ],
              "s": {
                "y": 1360,
                "x": 1697,
                "u": "https://preview.redd.it/568yctna3iff1.png?width=1697&amp;format=png&amp;auto=webp&amp;s=34efb167cc6a38dcf0ab822d9d23fbdb70c6da68"
              },
              "id": "568yctna3iff1"
            },
            "xa0quqna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5a791ea33f50c76dcc8db04017fde383cdaab6f"
                },
                {
                  "y": 172,
                  "x": 216,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f5a382f619f237613a9b6ac3a2abc3692cc632c"
                },
                {
                  "y": 255,
                  "x": 320,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5d6e9a383cf8d88d7061d571d4d20d7b4e566ac"
                },
                {
                  "y": 511,
                  "x": 640,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e9aae610033dbf1fcf9d7dcd5e4438c654c2487"
                },
                {
                  "y": 766,
                  "x": 960,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f4835be4bb7926d3f111febddc0798b2aa19b020"
                },
                {
                  "y": 862,
                  "x": 1080,
                  "u": "https://preview.redd.it/xa0quqna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95c70a767605d2891b11f524360721203374186b"
                }
              ],
              "s": {
                "y": 1335,
                "x": 1672,
                "u": "https://preview.redd.it/xa0quqna3iff1.png?width=1672&amp;format=png&amp;auto=webp&amp;s=7bbe3839b2fe115711a5a3465640b5ea132f5c71"
              },
              "id": "xa0quqna3iff1"
            },
            "8evmvona3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=57fa5209d5320474166ee777f02f2e6b1007ac5b"
                },
                {
                  "y": 177,
                  "x": 216,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=271d43c3076b1ade0ce3ffd1b6dc0d2325d091e5"
                },
                {
                  "y": 263,
                  "x": 320,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdfc13c56d2c028e41080472453482c567b254aa"
                },
                {
                  "y": 526,
                  "x": 640,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=18fba910a738dbe5e10ace06e3008e8025cbf76f"
                },
                {
                  "y": 790,
                  "x": 960,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6e92b2d5cbf15bdc4da2c7838d79356e2858a6d"
                },
                {
                  "y": 889,
                  "x": 1080,
                  "u": "https://preview.redd.it/8evmvona3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34cb0460a9041db07a5ce09ec4cf1e3b33842570"
                }
              ],
              "s": {
                "y": 1328,
                "x": 1613,
                "u": "https://preview.redd.it/8evmvona3iff1.png?width=1613&amp;format=png&amp;auto=webp&amp;s=0bbd0a939b8991f508983cff083db1e2582ef153"
              },
              "id": "8evmvona3iff1"
            },
            "wjplmqna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=281109cc87c3b719633ce5d1b46481ba8fceb4d3"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=614035b76303782422353ab82d7eadd5bb721c48"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=821ce28ccace38f75a542b5245a6705c1ab584e6"
                },
                {
                  "y": 559,
                  "x": 640,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8df66ed82a82ea153ee2e29ba4774534aa9233c8"
                },
                {
                  "y": 839,
                  "x": 960,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c312e0066f456e854853283c78a156c1e1679d33"
                },
                {
                  "y": 944,
                  "x": 1080,
                  "u": "https://preview.redd.it/wjplmqna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3296c1b95f8db9ddbc7631c2b7ed24aec81c0d07"
                }
              ],
              "s": {
                "y": 1355,
                "x": 1550,
                "u": "https://preview.redd.it/wjplmqna3iff1.png?width=1550&amp;format=png&amp;auto=webp&amp;s=4b744a4a742405b12b191d25bb44e61039dcc533"
              },
              "id": "wjplmqna3iff1"
            },
            "a6hhr2tl3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6957c55a1655a2a1355e3f72bf10dc282c8d304"
                },
                {
                  "y": 149,
                  "x": 216,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e5b78605b02ad5336600c1f6467c2db85568a37"
                },
                {
                  "y": 221,
                  "x": 320,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=939c74785ba2a3d33b019def84369724604b478b"
                },
                {
                  "y": 443,
                  "x": 640,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e6e7c3b4e94a1ee9c1cfff60a556de43da48bf5"
                },
                {
                  "y": 665,
                  "x": 960,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e27c55559a9e8127edd2254762a7bf08a06ef7e8"
                },
                {
                  "y": 748,
                  "x": 1080,
                  "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eba184e7d637f4ad93c8ed5208d24b663c114d9f"
                }
              ],
              "s": {
                "y": 904,
                "x": 1304,
                "u": "https://preview.redd.it/a6hhr2tl3iff1.jpg?width=1304&amp;format=pjpg&amp;auto=webp&amp;s=92ea6c27e7a11609adf03772392ef8e8f2ecc904"
              },
              "id": "a6hhr2tl3iff1"
            },
            "4xs78qna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 95,
                  "x": 108,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00aaaa33c45dad58596aa77d3835f0426da50d63"
                },
                {
                  "y": 191,
                  "x": 216,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea1c3b5db6452e4829800360a72f5c4e768f0649"
                },
                {
                  "y": 283,
                  "x": 320,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eea8293fe63b4956ffc7b3731feacab45dcce4a8"
                },
                {
                  "y": 567,
                  "x": 640,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4ef5b34f6d23429061d9f389e38ab5b13324b22"
                },
                {
                  "y": 851,
                  "x": 960,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e25eb90bdecd5e2a696dee4fb242c494da89a69"
                },
                {
                  "y": 957,
                  "x": 1080,
                  "u": "https://preview.redd.it/4xs78qna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fb16abffed778d771d05660adde3699fd08928a"
                }
              ],
              "s": {
                "y": 1360,
                "x": 1534,
                "u": "https://preview.redd.it/4xs78qna3iff1.png?width=1534&amp;format=png&amp;auto=webp&amp;s=749c7670b8bea200176ece83dfafedb6e0627ca4"
              },
              "id": "4xs78qna3iff1"
            },
            "jqadfmna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b4f073142ff601e87c39bfba9e4d0c3e01c5b5e"
                },
                {
                  "y": 177,
                  "x": 216,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b85a8b60f1225909ba2d35761d7d2c96bdd8f737"
                },
                {
                  "y": 263,
                  "x": 320,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c6e4859e65dd72f299541534d390f6cfc48c9e3"
                },
                {
                  "y": 526,
                  "x": 640,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=820f264df452942133d9a32f9f1bb83f86a1d455"
                },
                {
                  "y": 789,
                  "x": 960,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0112ac692193ff27fe61a0535379f1528a5a70bb"
                },
                {
                  "y": 887,
                  "x": 1080,
                  "u": "https://preview.redd.it/jqadfmna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b44aff715ee04feeae8932feb274407cc1266f40"
                }
              ],
              "s": {
                "y": 1326,
                "x": 1613,
                "u": "https://preview.redd.it/jqadfmna3iff1.png?width=1613&amp;format=png&amp;auto=webp&amp;s=edf2f4ec7ef6e24228aa17e81f109980269ab432"
              },
              "id": "jqadfmna3iff1"
            },
            "f3kjutna3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 85,
                  "x": 108,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=77356c733dd0f15bca3b729e24175cbfe27f1bae"
                },
                {
                  "y": 171,
                  "x": 216,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ecec12042662a899a0ab23ac395c6329abe9ecc"
                },
                {
                  "y": 254,
                  "x": 320,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88b0dd0f0097658b636b68f1d303bf551fe5ecd0"
                },
                {
                  "y": 509,
                  "x": 640,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76269b9806552e60df90c6a094a23e7bb9d52d30"
                },
                {
                  "y": 763,
                  "x": 960,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=174014b4ee7f19541f39958c088e82393299bdb0"
                },
                {
                  "y": 859,
                  "x": 1080,
                  "u": "https://preview.redd.it/f3kjutna3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=276b3e075b4384bafcba71b6f552c376d6a6cf98"
                }
              ],
              "s": {
                "y": 1366,
                "x": 1717,
                "u": "https://preview.redd.it/f3kjutna3iff1.png?width=1717&amp;format=png&amp;auto=webp&amp;s=5c9115a015bf720a4c406122b30afcffb36c24d5"
              },
              "id": "f3kjutna3iff1"
            },
            "gztklona3iff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 88,
                  "x": 108,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6685a61a1a0a168f790cd78f3619e956861e7752"
                },
                {
                  "y": 176,
                  "x": 216,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb874a6e332ad993dd03bf5a5956e0f9019ecfa9"
                },
                {
                  "y": 261,
                  "x": 320,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7bda91d5aa059bdead60355b491728d5e27a70fb"
                },
                {
                  "y": 523,
                  "x": 640,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9fa777610705d0ab137be44c9efe3b1d55b8e502"
                },
                {
                  "y": 785,
                  "x": 960,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d7c04232fd64d7d11092643b188bd318bef0d7b"
                },
                {
                  "y": 883,
                  "x": 1080,
                  "u": "https://preview.redd.it/gztklona3iff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b6d68753f0873121cd9ba2f08b95a005a9bff5e"
                }
              ],
              "s": {
                "y": 1325,
                "x": 1620,
                "u": "https://preview.redd.it/gztklona3iff1.png?width=1620&amp;format=png&amp;auto=webp&amp;s=a65b536cfb199897dffabfd741981cada855eb41"
              },
              "id": "gztklona3iff1"
            }
          },
          "name": "t3_1mb15g2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 45,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "jqadfmna3iff1",
                "id": 715490939
              },
              {
                "media_id": "fqga84tl3iff1",
                "id": 715490940
              },
              {
                "media_id": "a6hhr2tl3iff1",
                "id": 715490941
              },
              {
                "media_id": "568yctna3iff1",
                "id": 715490942
              },
              {
                "media_id": "gztklona3iff1",
                "id": 715490943
              },
              {
                "media_id": "8evmvona3iff1",
                "id": 715490944
              },
              {
                "media_id": "4xs78qna3iff1",
                "id": 715490945
              },
              {
                "media_id": "lj87vona3iff1",
                "id": 715490946
              },
              {
                "media_id": "wjplmqna3iff1",
                "id": 715490947
              },
              {
                "media_id": "xa0quqna3iff1",
                "id": 715490948
              },
              {
                "media_id": "6lu0usna3iff1",
                "id": 715490949
              },
              {
                "media_id": "f3kjutna3iff1",
                "id": 715490950
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/AX8rU0Ar21fTE1Um6Zy39yTbpXN7nfUgowthOtgI49Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753659757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-32B-0727\"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt; Releasing 4B in 24 hours and 32B now. &lt;/p&gt;\n\n&lt;p&gt;Specifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mb15g2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mb15g2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "smirkishere",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb15g2/uigenx0727_runs_locally_and_crushes_it_reasoning/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mb15g2",
          "subreddit_subscribers": 505617,
          "created_utc": 1753659757,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The non-reasoning model is about as good as 2.5 flash with 4k reasoning tokens. The latency of no reasoning vs reasoning makes it so much better than 2.5 flash. I also prefer the shorter outputs than the verbose asf gemini. \n\nThe markdown formatting is so much better and the outputs are just so much nicer to read than flash. Knowledge wise, it's a bit worse than 2.5 flash but that's probably because it's smaller model. better at coding than flash too.  \n  \nrunning unsloth Q8. I haven't tried the thinking one yet. what do you guys think?",
          "author_fullname": "t2_askwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B 2507 is so good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mammv5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 230,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 230,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753623817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The non-reasoning model is about as good as 2.5 flash with 4k reasoning tokens. The latency of no reasoning vs reasoning makes it so much better than 2.5 flash. I also prefer the shorter outputs than the verbose asf gemini. &lt;/p&gt;\n\n&lt;p&gt;The markdown formatting is so much better and the outputs are just so much nicer to read than flash. Knowledge wise, it&amp;#39;s a bit worse than 2.5 flash but that&amp;#39;s probably because it&amp;#39;s smaller model. better at coding than flash too.  &lt;/p&gt;\n\n&lt;p&gt;running unsloth Q8. I haven&amp;#39;t tried the thinking one yet. what do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mammv5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z_3454_pfk",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753623817,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We’re a small team building **FastFlowLM** — a fast, runtime for running **LLaMA, Qwen, DeepSeek**, and other models **entirely on the AMD Ryzen AI NPU**. No CPU or iGPU fallback — just lean, efficient, **NPU-native inference**. Think **Ollama**, but purpose-built and deeply optimized for AMD NPUs — with both **CLI** and **server mode (REST API)**.\n\n# Key Features\n\n* Supports **LLaMA, Qwen, DeepSeek**, and more\n* **Deeply hardware-optimized**, NPU-only inference\n* **Full context** support (e.g., 128K for LLaMA)\n* Over **11× power efficiency** compared to iGPU/CPU\n\nWe’re iterating quickly and would **love your feedback, critiques, and ideas**.\n\n# Try It Out\n\n* **GitHub:** [github.com/FastFlowLM/FastFlowLM](https://github.com/FastFlowLM/FastFlowLM)\n* **Live Demo (on remote machine):** Don’t have a Ryzen AI PC? Instantly try FastFlowLM on a **remote AMD Ryzen AI 5 340 NPU system with 32 GB RAM** — no installation needed. [Launch Demo](https://open-webui.testdrive-fastflowlm.com/) **Login:** `guest@flm.npu` **Password:** `0000`\n* **YouTube Demos:** [youtube.com/@FastFlowLM-YT](https://www.youtube.com/@FastFlowLM-YT) *→ Quick start guide, performance benchmarks, and comparisons vs Ollama / LM Studio / Lemonade*\n* **Discord Community:** [discord.gg/Sze3Qsv5](https://discord.gg/Sze3Qsv5) *→ Join us to ask questions, report issues, or contribute ideas*\n\nLet us know what works, what breaks, and what you’d love to see next!",
          "author_fullname": "t2_jrsbr6os",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running LLMs exclusively on AMD Ryzen AI NPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mao95d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 117,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 117,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753630907,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753627953,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM&lt;/strong&gt; — a fast, runtime for running &lt;strong&gt;LLaMA, Qwen, DeepSeek&lt;/strong&gt;, and other models &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;. No CPU or iGPU fallback — just lean, efficient, &lt;strong&gt;NPU-native inference&lt;/strong&gt;. Think &lt;strong&gt;Ollama&lt;/strong&gt;, but purpose-built and deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;server mode (REST API)&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;h1&gt;Key Features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Supports &lt;strong&gt;LLaMA, Qwen, DeepSeek&lt;/strong&gt;, and more&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Deeply hardware-optimized&lt;/strong&gt;, NPU-only inference&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Full context&lt;/strong&gt; support (e.g., 128K for LLaMA)&lt;/li&gt;\n&lt;li&gt;Over &lt;strong&gt;11× power efficiency&lt;/strong&gt; compared to iGPU/CPU&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We’re iterating quickly and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;h1&gt;Try It Out&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/FastFlowLM/FastFlowLM\"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Live Demo (on remote machine):&lt;/strong&gt; Don’t have a Ryzen AI PC? Instantly try FastFlowLM on a &lt;strong&gt;remote AMD Ryzen AI 5 340 NPU system with 32 GB RAM&lt;/strong&gt; — no installation needed. &lt;a href=\"https://open-webui.testdrive-fastflowlm.com/\"&gt;Launch Demo&lt;/a&gt; &lt;strong&gt;Login:&lt;/strong&gt; &lt;code&gt;guest@flm.npu&lt;/code&gt; &lt;strong&gt;Password:&lt;/strong&gt; &lt;code&gt;0000&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href=\"https://www.youtube.com/@FastFlowLM-YT\"&gt;youtube.com/@FastFlowLM-YT&lt;/a&gt; &lt;em&gt;→ Quick start guide, performance benchmarks, and comparisons vs Ollama / LM Studio / Lemonade&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Discord Community:&lt;/strong&gt; &lt;a href=\"https://discord.gg/Sze3Qsv5\"&gt;discord.gg/Sze3Qsv5&lt;/a&gt; &lt;em&gt;→ Join us to ask questions, report issues, or contribute ideas&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Let us know what works, what breaks, and what you’d love to see next!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?auto=webp&amp;s=4a11abd914fb5ab749f3f093b10ce2b529fb8c8e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97afc3fc381198ec693e0055e6c72c2c0c3cad84",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=223eb6c47aa4922185402abdd994f0d4167b8587",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c855da9485d7673105d65ccede1e3da883ab9dcb",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5391c68c3aa09eb5da1c87bd1883d8712981e33",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=76ac02010d805630644eabca91f54e1df087cc14",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=68dfc8aed1bea82afaeda0f80b40e2d9c22407bb",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "vJGRc2UlTJrSFHnGlJYDN0YsOLC8w4mlAwQVmF6tcgo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mao95d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BandEnvironmental834",
          "discussion_type": null,
          "num_comments": 69,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753627953,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f\n\nhttps://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;format=png&amp;auto=webp&amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3\n\n[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct)\n\n[https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker](https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker)",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A new 21B-A3B model that can run 30 token/s on i9 CPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5t6qakxf6eff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 42,
                  "x": 108,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b7aee7f5ee4d636cc350980d4a2d402a51990f3"
                },
                {
                  "y": 84,
                  "x": 216,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81d1bfcb2fbab04e033467fedce9b775306d0816"
                },
                {
                  "y": 125,
                  "x": 320,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38b4e75cce0962953603e8bda734369a2c829748"
                },
                {
                  "y": 251,
                  "x": 640,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de21ca82e374cabea65b7bd4f9ca7695eb3ac76c"
                },
                {
                  "y": 376,
                  "x": 960,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f335ee62d4874ca26b9e6e0098a8851c4538afa"
                },
                {
                  "y": 423,
                  "x": 1080,
                  "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95699afb27e3aa99fcc75ef6aa805969b9e66cf1"
                }
              ],
              "s": {
                "y": 786,
                "x": 2004,
                "u": "https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;format=png&amp;auto=webp&amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3"
              },
              "id": "5t6qakxf6eff1"
            },
            "e54liysd6eff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7f5f991e9ce2c009935e1248f2782a5bdd1a201"
                },
                {
                  "y": 120,
                  "x": 216,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b615b9ee2e041c4be64619a07d886e58e3e07f63"
                },
                {
                  "y": 178,
                  "x": 320,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=805558e5004842b4caccc53ce524f721763d8ea2"
                },
                {
                  "y": 356,
                  "x": 640,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b5259fd259cf94ad371181ed4eb21af07c6bc94"
                },
                {
                  "y": 534,
                  "x": 960,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=24b113fb84316d200bddffca613f26e4f2877925"
                },
                {
                  "y": 601,
                  "x": 1080,
                  "u": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b39d6ef3edba22d3d5331aaf9d2220075d801bcc"
                }
              ],
              "s": {
                "y": 601,
                "x": 1080,
                "u": "https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f"
              },
              "id": "e54liysd6eff1"
            }
          },
          "name": "t3_1maipzo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 202,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 202,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=4ac39f1493be4418ea9b0513e0ab785db9d728b9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753611112,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f\"&gt;https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3\"&gt;https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker\"&gt;https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?auto=webp&amp;s=de62685b703b746715dcc2b0df2aefcbbc3e3737",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33918a5d809431c198816a64f4512804c3bb5409",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4045a881ccf50c282b74da08c7a22d9d97f0821b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39771fc11b97b39eae914bdee8e861864005e9bf",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb9a721495b0f0942a0fc51a0050cda33d2ef637",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a9c25310dbb6785760eb6704b72c9666cd180f6",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d2ffd21d7412bbd7a2765d0696394d9fd40dc27",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1maipzo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753611112,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I remember some of them were really solid, but it's been over a year since we've seen a new release.   \nIs the team still active, or has the project quietly died?",
          "author_fullname": "t2_pv1nb9469",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What happened to the Yi models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maxfeb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753649989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember some of them were really solid, but it&amp;#39;s been over a year since we&amp;#39;ve seen a new release.&lt;br/&gt;\nIs the team still active, or has the project quietly died?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maxfeb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GabryIta",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753649989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In my impression, the focus is mostly on MCP, A2A, and RAG. While these are great for their respective use cases, you still have to send prompts to LLMs with 70 to 500 billion parameters, which is quite resource-intensive and expensive. The alternative is to settle for one of the smaller LLMs with around 8 billion parameters, but then the experience can feel too inconsistent. In search of a solution, I recently stumbled upon LoRA, which to my understanding, allows you to use a smaller LLM as a base and fine-tune it to become an expert in very specific topics. This results in a model that’s lighter and faster to run, with output that’s comparable (in a specific domain) to that of a 500-billion-parameter model. If that’s the case, why hasn’t there been more noticeable interest in fine-tuning with LoRA? I can imagine this could save a lot of money for businesses planning to build systems that rely on LLMs for constant inference.",
          "author_fullname": "t2_12y48q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why hasn't LoRA gained more popularity?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maq0hg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 56,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 56,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753632201,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my impression, the focus is mostly on MCP, A2A, and RAG. While these are great for their respective use cases, you still have to send prompts to LLMs with 70 to 500 billion parameters, which is quite resource-intensive and expensive. The alternative is to settle for one of the smaller LLMs with around 8 billion parameters, but then the experience can feel too inconsistent. In search of a solution, I recently stumbled upon LoRA, which to my understanding, allows you to use a smaller LLM as a base and fine-tune it to become an expert in very specific topics. This results in a model that’s lighter and faster to run, with output that’s comparable (in a specific domain) to that of a 500-billion-parameter model. If that’s the case, why hasn’t there been more noticeable interest in fine-tuning with LoRA? I can imagine this could save a lot of money for businesses planning to build systems that rely on LLMs for constant inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maq0hg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dabomb007",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753632201,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3l9wjlq0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tencent releases Hunyuan3D World Model 1.0 - first open-source 3D world generation model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mab2i2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 540,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 540,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753583285,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/TencentHunyuan/status/1949288986192834718",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mab2i2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pseudoreddituser",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mab2i2/tencent_releases_hunyuan3d_world_model_10_first/",
          "stickied": false,
          "url": "https://x.com/TencentHunyuan/status/1949288986192834718",
          "subreddit_subscribers": 505617,
          "created_utc": 1753583285,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.\n\nBack then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.\n\nThis is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.\n\nThere is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.\n\nDeepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it's fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.\n\nWith the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn't been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that's a long while ago...\n\nI also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.\n\nThis does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the *really* large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.\n\nI suppose I'm partially reminiscing, and partially trying to start a dialogue on where the \"sweet spot\" for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.\n\nAre \\~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?\n\n**EDIT:** If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).",
          "author_fullname": "t2_cyw8u51dt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are ~70B Models Going Out of Fashion?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1majfwi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 110,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 110,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753617926,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753613850,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.&lt;/p&gt;\n\n&lt;p&gt;Back then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.&lt;/p&gt;\n\n&lt;p&gt;This is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.&lt;/p&gt;\n\n&lt;p&gt;There is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.&lt;/p&gt;\n\n&lt;p&gt;Deepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it&amp;#39;s fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.&lt;/p&gt;\n\n&lt;p&gt;With the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn&amp;#39;t been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that&amp;#39;s a long while ago...&lt;/p&gt;\n\n&lt;p&gt;I also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.&lt;/p&gt;\n\n&lt;p&gt;This does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the &lt;em&gt;really&lt;/em&gt; large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.&lt;/p&gt;\n\n&lt;p&gt;I suppose I&amp;#39;m partially reminiscing, and partially trying to start a dialogue on where the &amp;quot;sweet spot&amp;quot; for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.&lt;/p&gt;\n\n&lt;p&gt;Are ~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1majfwi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HvskyAI",
          "discussion_type": null,
          "num_comments": 72,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753613850,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen has introduced a new technique called **GSPO** (Group Sequence Policy Optimization)\n\nPut simply:\n\n* It's a new method for training large language models\n* Instead of focusing on individual words like older methods, it optimizes entire sentences or passages as a whole — which is more logical and leads to better performance\n* This approach makes training more **stable** and less prone to crashes or errors, especially when used with large, modular models like **MoE (Mixture of Experts)**\n* The training process is **simpler** and doesn’t rely on complex tricks used in the past, making it cleaner and easier to manage\n* The more compute you throw at it, the better the model becomes — it **scales efficiently**.\n* The latest **Qwen3 models** (like those that can code or follow instructions) were trained using this method\n* Compared to the older **GRPO** method, GSPO leads to **faster convergence** (the model learns faster) and uses **fewer resources**\n\nPaper: [https://huggingface.co/papers/2507.18071](https://huggingface.co/papers/2507.18071)",
          "author_fullname": "t2_1heeqeidfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen GSPO (Group Sequence Policy Optimization)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1man0hu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753624825,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen has introduced a new technique called &lt;strong&gt;GSPO&lt;/strong&gt; (Group Sequence Policy Optimization)&lt;/p&gt;\n\n&lt;p&gt;Put simply:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s a new method for training large language models&lt;/li&gt;\n&lt;li&gt;Instead of focusing on individual words like older methods, it optimizes entire sentences or passages as a whole — which is more logical and leads to better performance&lt;/li&gt;\n&lt;li&gt;This approach makes training more &lt;strong&gt;stable&lt;/strong&gt; and less prone to crashes or errors, especially when used with large, modular models like &lt;strong&gt;MoE (Mixture of Experts)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;The training process is &lt;strong&gt;simpler&lt;/strong&gt; and doesn’t rely on complex tricks used in the past, making it cleaner and easier to manage&lt;/li&gt;\n&lt;li&gt;The more compute you throw at it, the better the model becomes — it &lt;strong&gt;scales efficiently&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;The latest &lt;strong&gt;Qwen3 models&lt;/strong&gt; (like those that can code or follow instructions) were trained using this method&lt;/li&gt;\n&lt;li&gt;Compared to the older &lt;strong&gt;GRPO&lt;/strong&gt; method, GSPO leads to &lt;strong&gt;faster convergence&lt;/strong&gt; (the model learns faster) and uses &lt;strong&gt;fewer resources&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://huggingface.co/papers/2507.18071\"&gt;https://huggingface.co/papers/2507.18071&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?auto=webp&amp;s=b7678a8af1d1d28f96c34fbaeb2656718573d56c",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=204816acf3c4a486bb403207785321d33214adc7",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81cb7197b22ed427b6c24ae43d3f69dc4cb2730d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=329be156f229f90b7be0f62070e92a848fedc1f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=82297d64dd06513853c691039970c2747e099d87",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=03df11e5919855e527dbf686542a55fb52fd228c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d08b01c22320c544d58ffd0a85b1d12a04e7402",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1man0hu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "koc_Z3",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753624825,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drummer's Mixtral 4x3B v1 - A finetuned clown MoE experiment with Voxtral 3B!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maptvc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=6f4d41157daea37fd6fce793fd1d455f3a33889f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753631776,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?auto=webp&amp;s=5160ca0d10d89e064f39433938efce8c841fe074",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbb0ffe720c33190a7c35c23d0bfa7c0465f73ba",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8977f01203b9d02ddfd018af3eefecb7b4e22ab1",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=98bd6abbe5d09b66b80ef08bb40973db912270c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb469872360a699d06db5617b0f24cf8eea8d5f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=94906681697735d019628c51708b7ecace28c3e4",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=10f09b575aa5fd8b5a3ee87b4e22904545ea9ca9",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1maptvc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maptvc/drummers_mixtral_4x3b_v1_a_finetuned_clown_moe/",
          "stickied": false,
          "url": "https://huggingface.co/TheDrummer/Mixtral-4x3B-v1",
          "subreddit_subscribers": 505617,
          "created_utc": 1753631776,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[The initials of Devstral, Mistral, and Magistral as connected puzzle pieces](https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;format=png&amp;auto=webp&amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7)\n\n\n\ntl;dr: title. Here are the weights: [Devstral-Small-2507-Rebased-Vision](https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision) &amp; [Magistral-Small-2507-Rebased-Vision](https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision) &amp; [Devstral-Small-2507-Rebased-Vision-LoRA](https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision-LoRA)\n\nI've been using Mistral-Small-3.2 for the past few weeks. It's pretty solid, and the combination of vision and speed make it a really good pick for me, but...\n\nI'm using sglang and it's really memory hungry which means it's hard to fit another model side-by-side without much extra VRAM or low quantization (GPTQ/AWQ). Instead, I've tuned the various parameters until I brought the VRAM usage low enough that I can also run Devstral with exllamav3 (Q6), but once in a while sglang throws an OOM when there are multiple queries with images, and I need to load the two servers in a specific order for it to work. It kinda sucks. Running exllama is much slower for any individual model, but would probably work fine for all the at \\~Q6-Q8, but meh.\n\nThen I got an idea: how about I treat retrofit Devstral/Magistral as LoRAs? 3 models for \\~1.1x the VRAM? Yes, please! I tried [mergekit](https://github.com/arcee-ai/mergekit#lora-extraction) but it requires the same architecture, so I'd either have to drop vision (which I also tried, and it seemed to work, but I don't like it!) or try to add vision to Devstral and Magistral. Since these two are trained on the same architecture, it's actually pretty easy, you just have to copy the `model` weights over the `language_model` weights. I did this for both models, and spent a few hours running some benchmarks (in each repo README) to see if there was any significant issue, and it seems to be fine with most being well within the standard error range. I tested a few images and it seemed to work too. There is a significant difference between models, so I probably did that correct too. However, make sure to test on your own and tell me if you notice any issues! &gt;!Yes, I know 2+ other attempts were made (*one by unsloth, from whom I stole the weights, lol*) for the *exact* same thing, and could've saved me a whole day of pain, but I only remembered about it \\~5 mins ago, but this wasn't the core of what I wanted to do anyway so we'll conveniently call it a draw D:!&lt;\n\nWith the \"new\" models in place, the next step was to try creating LoRAs again. Well, mergekit didn't work. I almost quit, but decided to search the web for another method and I ended up finding [LoRD](https://github.com/thomasgauthier/LoRD), the original version of the mergekit code (and it has an Apache license!). It required quite a bit of tweaking to get it working for the Mistral model (and not OOM constantly), but after a few hours I think it succeeded in creating the adapter. I briefly tested with transformers in the same notebook, but sadly it cannot be loaded by sglang. It doesn't even tell me why, I just get a generic error, but it's probably the vision parts, or 1+ of the modules (linear\\_1 / linear\\_2 / merging\\_layer / lm\\_head). Or LoRA might not be support at all for Mistral 3.1 (e.g. like in [vLLM](https://github.com/vllm-project/vllm/issues/18574)). In either case, it meant I couldn't run benchmarks to evaluate quality degration, so I uploaded that to huggingface as well if anyone wants to try.\n\nIf I'm not too lazy (which I'll likely be), I'll give this another go sometime, but now I'll just start my 761435 Karl Franz campaign.",
          "author_fullname": "t2_k6u7rfxb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Devstral &amp; Magistral as adapters of Mistral",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tshdyj57ghff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9cc805736aa2013d7ca4bd816bdb649a9cd2d871"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41ed921c4e6a128b0fbd80d0921c46b2d6755243"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0ad4cd99e1863be63b3a4c08034865431c69624"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=897ba3761d52e61715c3eb1d34ba8e3708e3ee6f"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b2689357836116b0804c2c34e284749c615b663"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=932f2fe782da56be5e6bef05578c4e9edc202cda"
                }
              ],
              "s": {
                "y": 2048,
                "x": 2048,
                "u": "https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;format=png&amp;auto=webp&amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7"
              },
              "id": "tshdyj57ghff1"
            }
          },
          "name": "t3_1maywaw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=32c7761ccccf9d9cd3d58f04849c4249d01be54a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753653691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7\"&gt;The initials of Devstral, Mistral, and Magistral as connected puzzle pieces&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;tl;dr: title. Here are the weights: &lt;a href=\"https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision\"&gt;Devstral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href=\"https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision\"&gt;Magistral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href=\"https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision-LoRA\"&gt;Devstral-Small-2507-Rebased-Vision-LoRA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using Mistral-Small-3.2 for the past few weeks. It&amp;#39;s pretty solid, and the combination of vision and speed make it a really good pick for me, but...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using sglang and it&amp;#39;s really memory hungry which means it&amp;#39;s hard to fit another model side-by-side without much extra VRAM or low quantization (GPTQ/AWQ). Instead, I&amp;#39;ve tuned the various parameters until I brought the VRAM usage low enough that I can also run Devstral with exllamav3 (Q6), but once in a while sglang throws an OOM when there are multiple queries with images, and I need to load the two servers in a specific order for it to work. It kinda sucks. Running exllama is much slower for any individual model, but would probably work fine for all the at ~Q6-Q8, but meh.&lt;/p&gt;\n\n&lt;p&gt;Then I got an idea: how about I treat retrofit Devstral/Magistral as LoRAs? 3 models for ~1.1x the VRAM? Yes, please! I tried &lt;a href=\"https://github.com/arcee-ai/mergekit#lora-extraction\"&gt;mergekit&lt;/a&gt; but it requires the same architecture, so I&amp;#39;d either have to drop vision (which I also tried, and it seemed to work, but I don&amp;#39;t like it!) or try to add vision to Devstral and Magistral. Since these two are trained on the same architecture, it&amp;#39;s actually pretty easy, you just have to copy the &lt;code&gt;model&lt;/code&gt; weights over the &lt;code&gt;language_model&lt;/code&gt; weights. I did this for both models, and spent a few hours running some benchmarks (in each repo README) to see if there was any significant issue, and it seems to be fine with most being well within the standard error range. I tested a few images and it seemed to work too. There is a significant difference between models, so I probably did that correct too. However, make sure to test on your own and tell me if you notice any issues! &lt;span class=\"md-spoiler-text\"&gt;Yes, I know 2+ other attempts were made (&lt;em&gt;one by unsloth, from whom I stole the weights, lol&lt;/em&gt;) for the &lt;em&gt;exact&lt;/em&gt; same thing, and could&amp;#39;ve saved me a whole day of pain, but I only remembered about it ~5 mins ago, but this wasn&amp;#39;t the core of what I wanted to do anyway so we&amp;#39;ll conveniently call it a draw D:&lt;/span&gt;&lt;/p&gt;\n\n&lt;p&gt;With the &amp;quot;new&amp;quot; models in place, the next step was to try creating LoRAs again. Well, mergekit didn&amp;#39;t work. I almost quit, but decided to search the web for another method and I ended up finding &lt;a href=\"https://github.com/thomasgauthier/LoRD\"&gt;LoRD&lt;/a&gt;, the original version of the mergekit code (and it has an Apache license!). It required quite a bit of tweaking to get it working for the Mistral model (and not OOM constantly), but after a few hours I think it succeeded in creating the adapter. I briefly tested with transformers in the same notebook, but sadly it cannot be loaded by sglang. It doesn&amp;#39;t even tell me why, I just get a generic error, but it&amp;#39;s probably the vision parts, or 1+ of the modules (linear_1 / linear_2 / merging_layer / lm_head). Or LoRA might not be support at all for Mistral 3.1 (e.g. like in &lt;a href=\"https://github.com/vllm-project/vllm/issues/18574\"&gt;vLLM&lt;/a&gt;). In either case, it meant I couldn&amp;#39;t run benchmarks to evaluate quality degration, so I uploaded that to huggingface as well if anyone wants to try.&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m not too lazy (which I&amp;#39;ll likely be), I&amp;#39;ll give this another go sometime, but now I&amp;#39;ll just start my 761435 Karl Franz campaign.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?auto=webp&amp;s=f33db570c3a8f16c2ac464fb9062565d9b50b904",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=336ca45300d9ad8f941487b0ce465efa53dd0e02",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fa2934bddfe176555bff114786099245f85abcf",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8cd13e62d30f7a7e58e32919b6d165c24a125225",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=22ed0b082947f94ee079c2a6004328efe6c66fc9",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fcd80bad37121082c8b613252204288829d1c8be",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=11b16dfa1d4b69bdf4dc3a4f5e8a530bc6d72c2d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1maywaw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kmouratidis",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753653691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14lqxvy1qk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Byte-Vision is a privacy-first (Llama.cpp) document intelligence platform that transforms static documents into an interactive, searchable knowledge base. Built on Elasticsearch with RAG (Retrieval-Augmented Generation) capabilities, it offers document parsing, OCR processing, and modern UI.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mb2dcp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4fd244957a130db419b6074f34a711a8f7259e0a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753663260,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/kbrisso/byte-vision",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?auto=webp&amp;s=68ee57c49a8451c63c200df64fb463ac5b026c9d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca560c73715d7330212b1645381ce757ae0517c8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40f9eb891b537e50f5bd63d16a3678d31b33ac60",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=797d486098e995a54706fe4f140d3601cf369b67",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d7c803f441c5cf105e320d67c7290e56955a330",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=46ec34699b2c72770fe0cd6e134d5402ad10365c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f1aed7829e91d539e27a1de8fe237d509505121",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mb2dcp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Important_Half_8277",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb2dcp/bytevision_is_a_privacyfirst_llamacpp_document/",
          "stickied": false,
          "url": "https://github.com/kbrisso/byte-vision",
          "subreddit_subscribers": 505617,
          "created_utc": 1753663260,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Abstract\n\n&gt;To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maw5dy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753646844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.16784",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maw5dy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maw5dy/beyond_context_limits_subconscious_threads_for/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.16784",
          "subreddit_subscribers": 505617,
          "created_utc": 1753646844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4ou3rslj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan 2.2 coming out Monday July 28th",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mae4yz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 124,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 124,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wiS40lScO4Tz3DVaIJvgYMaDlS_RixRf5GYhbQQ4Q8Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753593470,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6fhk0wjppcff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?auto=webp&amp;s=fec6294a55a3e104e1bb18786c446c76d3380ada",
                  "width": 1320,
                  "height": 738
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b03db1c6122c5627833ddc9e2fdbcb6d6f7ed744",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1ac1b73bcaf847b2fa4c394ce28ce658e1f328a",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95e50168620f70ce8176a420355e73393cd86fb4",
                    "width": 320,
                    "height": 178
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5dc0d6b8afe675c915a40e90b3006a47c5764036",
                    "width": 640,
                    "height": 357
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=512e28d76ac154808029aa9902dd873b5432ddfa",
                    "width": 960,
                    "height": 536
                  },
                  {
                    "url": "https://preview.redd.it/6fhk0wjppcff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67b6d4b9b8c656f0cbf08017fc3c7809884c53aa",
                    "width": 1080,
                    "height": 603
                  }
                ],
                "variants": {},
                "id": "3150d24SFUF3S7LHu63_t91aK5WawBeVinxIS7aSy80"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mae4yz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Comed_Ai_n",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mae4yz/wan_22_coming_out_monday_july_28th/",
          "stickied": false,
          "url": "https://i.redd.it/6fhk0wjppcff1.jpeg",
          "subreddit_subscribers": 505617,
          "created_utc": 1753593470,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;format=png&amp;auto=webp&amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc\n\nSam Altman admitting that ChatGPT will never protect your privacy ",
          "author_fullname": "t2_dmaijfods",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM is more important than ever",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8l7johy2cbff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 141,
                  "x": 108,
                  "u": "https://preview.redd.it/8l7johy2cbff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=872620916c84058b05b19836e3dbac1eec6b6209"
                },
                {
                  "y": 283,
                  "x": 216,
                  "u": "https://preview.redd.it/8l7johy2cbff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d2d77bed5f2cfc487be789d312ecff0ac96f0b0"
                },
                {
                  "y": 419,
                  "x": 320,
                  "u": "https://preview.redd.it/8l7johy2cbff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=72cbdb4e09ae54ebec08d464c14350211c34aee5"
                }
              ],
              "s": {
                "y": 776,
                "x": 592,
                "u": "https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;format=png&amp;auto=webp&amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc"
              },
              "id": "8l7johy2cbff1"
            }
          },
          "name": "t3_1ma8yua",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 289,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 289,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/uE7wV-zcvDiT-3iVUwWcdDhkrRrgCwBg-OrGsKIspS0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753576857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc\"&gt;https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sam Altman admitting that ChatGPT will never protect your privacy &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ma8yua",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeedleworkerDull7886",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753576857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What are people's thoughts on Sapient Intelligence's recent paper? Apparently, they developed a new architecture called Hierarchical Reasoning Model (HRM) that performs as well as LLMs on complex reasoning tasks with significantly less training samples and examples. ",
          "author_fullname": "t2_98ouo03z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma6b57",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 421,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 421,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=c61601b7d043fcb4533a5e8eaf63e29936b16a11",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753569167,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "venturebeat.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are people&amp;#39;s thoughts on Sapient Intelligence&amp;#39;s recent paper? Apparently, they developed a new architecture called Hierarchical Reasoning Model (HRM) that performs as well as LLMs on complex reasoning tasks with significantly less training samples and examples. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://venturebeat.com/ai/new-ai-architecture-delivers-100x-faster-reasoning-than-llms-with-just-1000-training-examples/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?auto=webp&amp;s=d3dfd5864403531c1acea95e81b49f3b5b256f90",
                  "width": 986,
                  "height": 553
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a6925cbb0e94e0ba7147f6bddbacbfbacabb3ba",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb0df9d130eced3444802c4c7335e42bcd1de23d",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d14873f56f0a1c5c45686ea214b463e961c58fa5",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=79243a5ca169d7972acf9a3bdc240df386129d25",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e514262e4886ce2271151b95e18d9cf4cb3e627a",
                    "width": 960,
                    "height": 538
                  }
                ],
                "variants": {},
                "id": "eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1ma6b57",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished-Copy332",
          "discussion_type": null,
          "num_comments": 95,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma6b57/new_ai_architecture_delivers_100x_faster/",
          "stickied": false,
          "url": "https://venturebeat.com/ai/new-ai-architecture-delivers-100x-faster-reasoning-than-llms-with-just-1000-training-examples/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753569167,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PowerInfer/SmallThinker-21BA3B-Instruct · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maipjy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=4ac39f1493be4418ea9b0513e0ab785db9d728b9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753611065,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?auto=webp&amp;s=de62685b703b746715dcc2b0df2aefcbbc3e3737",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33918a5d809431c198816a64f4512804c3bb5409",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4045a881ccf50c282b74da08c7a22d9d97f0821b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39771fc11b97b39eae914bdee8e861864005e9bf",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb9a721495b0f0942a0fc51a0050cda33d2ef637",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a9c25310dbb6785760eb6704b72c9666cd180f6",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d2ffd21d7412bbd7a2765d0696394d9fd40dc27",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1maipjy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maipjy/powerinfersmallthinker21ba3binstruct_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct",
          "subreddit_subscribers": 505617,
          "created_utc": 1753611065,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "tl;dr: faster grammar check and minor code edits without a draft model: a C# proof-of-concept.\n\n[https://github.com/dpmm99/ModelFreeSpeculation](https://github.com/dpmm99/ModelFreeSpeculation)\n\nThis is a toy project built on LLamaSharp. It's a toy because it assumes the output will be nearly identical to the input--no particularly large added sequences and such. A better difference-tracking algorithm would make it more usable, and I think it could also be better if it fell back to a real draft model smartly when there are big differences. I'd been thinking about this since I saw a statement that **a draft \"model\" isn't limited to LLMs**, and I remember it every time I accidentally click \"Apply\" in GitHub Copilot and watch it scan through a few hundred lines of code just to add one function, haha.\n\n\n\nI tested it on two prompts using Phi-4-14B-Q4\\_K\\_M with **8 draft tokens** per inference loop iteration on my RTX 4060 Ti using CUDA and [this pre-release of LLamaSharp](https://github.com/SciSharp/LLamaSharp/pull/1225).\n\nFor the spell-check prompt:\n\nDuration: 7.39s, Tokens: 135, Tokens/sec: 18.28\n\nDuration: 4.89s, Tokens: 135, Tokens/sec: 27.60 (88 accepted, 283 rejected) **(+51%)**\n\n\n\nFor the code editing prompt:\n\nDuration: 17.84s, Tokens: 328, Tokens/sec: 18.39\n\nDuration: 10.40s, Tokens: 328, Tokens/sec: 31.55 (237 accepted, 473 rejected) **(+71%)**\n\nDuration: 9.50s, Tokens: 328, Tokens/sec: 34.52 (250 draft tokens accepted; **draft length 20**) **(+88%)**\n\n\n\nI was also thinking this approach could go nicely with a model fine-tuned for *applying* code edits like [https://huggingface.co/models?other=base\\_model:quantized:microsoft/NextCoder-32B](https://huggingface.co/models?other=base_model:quantized:microsoft/NextCoder-32B).",
          "author_fullname": "t2_w4j8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Speculative decoding without a draft model (C#)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1max9qz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753649582,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tl;dr: faster grammar check and minor code edits without a draft model: a C# proof-of-concept.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dpmm99/ModelFreeSpeculation\"&gt;https://github.com/dpmm99/ModelFreeSpeculation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a toy project built on LLamaSharp. It&amp;#39;s a toy because it assumes the output will be nearly identical to the input--no particularly large added sequences and such. A better difference-tracking algorithm would make it more usable, and I think it could also be better if it fell back to a real draft model smartly when there are big differences. I&amp;#39;d been thinking about this since I saw a statement that &lt;strong&gt;a draft &amp;quot;model&amp;quot; isn&amp;#39;t limited to LLMs&lt;/strong&gt;, and I remember it every time I accidentally click &amp;quot;Apply&amp;quot; in GitHub Copilot and watch it scan through a few hundred lines of code just to add one function, haha.&lt;/p&gt;\n\n&lt;p&gt;I tested it on two prompts using Phi-4-14B-Q4_K_M with &lt;strong&gt;8 draft tokens&lt;/strong&gt; per inference loop iteration on my RTX 4060 Ti using CUDA and &lt;a href=\"https://github.com/SciSharp/LLamaSharp/pull/1225\"&gt;this pre-release of LLamaSharp&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;For the spell-check prompt:&lt;/p&gt;\n\n&lt;p&gt;Duration: 7.39s, Tokens: 135, Tokens/sec: 18.28&lt;/p&gt;\n\n&lt;p&gt;Duration: 4.89s, Tokens: 135, Tokens/sec: 27.60 (88 accepted, 283 rejected) &lt;strong&gt;(+51%)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For the code editing prompt:&lt;/p&gt;\n\n&lt;p&gt;Duration: 17.84s, Tokens: 328, Tokens/sec: 18.39&lt;/p&gt;\n\n&lt;p&gt;Duration: 10.40s, Tokens: 328, Tokens/sec: 31.55 (237 accepted, 473 rejected) &lt;strong&gt;(+71%)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Duration: 9.50s, Tokens: 328, Tokens/sec: 34.52 (250 draft tokens accepted; &lt;strong&gt;draft length 20&lt;/strong&gt;) &lt;strong&gt;(+88%)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I was also thinking this approach could go nicely with a model fine-tuned for &lt;em&gt;applying&lt;/em&gt; code edits like &lt;a href=\"https://huggingface.co/models?other=base_model:quantized:microsoft/NextCoder-32B\"&gt;https://huggingface.co/models?other=base_model:quantized:microsoft/NextCoder-32B&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?auto=webp&amp;s=21f9ad6a2c9be091302197df5996afbfcacd7658",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4443b14344a6b363cc7c1a64c3d726e9bc418ef",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dff0fadc0682d76cacb86e7d2c019d5bcf8e855",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=92ffde2012c4f9c734960a7c583b8534d115770e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=750e6acb13f5fcb85d48abe49c6abc28652f4cc4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2443879b742afc48ec4f2b63ea5c5b854084c209",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b3d4c67cd12eddf041d96d0598ed7950a45fc6f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6SBbFAaNO6y4KvdR37bmJv25qEqiWaX9KiswAYV7NXY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1max9qz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeProgrammer99",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1max9qz/speculative_decoding_without_a_draft_model_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1max9qz/speculative_decoding_without_a_draft_model_c/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753649582,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thank you so much getting ggufs baked and delivered. It must have been busy last few days. How is it looking behind the scenes?\n\nEdit yeah and llama.cpp team",
          "author_fullname": "t2_1vmv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Appreciation Post - Thank you unsloth team, and thank you bartowski",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma08e0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 628,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 628,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": 1753554026,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753553653,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thank you so much getting ggufs baked and delivered. It must have been busy last few days. How is it looking behind the scenes?&lt;/p&gt;\n\n&lt;p&gt;Edit yeah and llama.cpp team&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ma08e0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fuutott",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753553653,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spent the weekend crafting this open-source PyTorch implementation of Google's [CRISP paper (arXiv:2505.11471)](https://arxiv.org/pdf/2505.11471). The repository provides a direct, hands-on comparison between CRISP's in-training clustering and the more traditional post-hoc approach.\n\nFor context, the core problem with multi-vector models (e.g., ColBERT) is their massive index size. The common solution is to cluster embeddings *after* training (post-hoc), but this is an imperfect patch. CRISP argues for integrating clustering *during* training to force the model to learn inherently \"clusterable\" representations.\n\nThe repository sets up a clean head-to-head experiment to test that claim. Here's a breakdown of the results from its built-in pipeline.\n\n[https://github.com/sigridjineth/crisp-py](https://github.com/sigridjineth/crisp-py)\n\nI tried few experiments with minilm-l6-v2 in Macbook Pro and found that CRISP-tuned model assigns a significantly higher similarity score to the correct document.",
          "author_fullname": "t2_iu3wj4b6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I tried implementing the CRISP paper from Google Deepmind in Python",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maixye",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753621643,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753611989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spent the weekend crafting this open-source PyTorch implementation of Google&amp;#39;s &lt;a href=\"https://arxiv.org/pdf/2505.11471\"&gt;CRISP paper (arXiv:2505.11471)&lt;/a&gt;. The repository provides a direct, hands-on comparison between CRISP&amp;#39;s in-training clustering and the more traditional post-hoc approach.&lt;/p&gt;\n\n&lt;p&gt;For context, the core problem with multi-vector models (e.g., ColBERT) is their massive index size. The common solution is to cluster embeddings &lt;em&gt;after&lt;/em&gt; training (post-hoc), but this is an imperfect patch. CRISP argues for integrating clustering &lt;em&gt;during&lt;/em&gt; training to force the model to learn inherently &amp;quot;clusterable&amp;quot; representations.&lt;/p&gt;\n\n&lt;p&gt;The repository sets up a clean head-to-head experiment to test that claim. Here&amp;#39;s a breakdown of the results from its built-in pipeline.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/sigridjineth/crisp-py\"&gt;https://github.com/sigridjineth/crisp-py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I tried few experiments with minilm-l6-v2 in Macbook Pro and found that CRISP-tuned model assigns a significantly higher similarity score to the correct document.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1maixye",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Rub1689",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maixye/i_tried_implementing_the_crisp_paper_from_google/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maixye/i_tried_implementing_the_crisp_paper_from_google/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753611989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_e7yuu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Untold Revolution in iOS 26: WebGPU Is Coming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 117,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mb2y1z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=140&amp;height=117&amp;crop=140:117,smart&amp;auto=webp&amp;s=085275ed0519ddf3774e318dc6ad4a43267fd48e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753664937,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "brandlens.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://brandlens.io/blog/the-untold-revolution-beneath-ios-26-webgpu-is-coming-everywhere-and-it-changes-everything/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?auto=webp&amp;s=278be6a9556ebe6bb914a28f00b475770d406fee",
                  "width": 1280,
                  "height": 1074
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd59353d15b225ac7141154eca19d5658accf506",
                    "width": 108,
                    "height": 90
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=af59da7213fa67151937d694f2e0c3404a6cf906",
                    "width": 216,
                    "height": 181
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1963e5fd12baf1afdd037d10188c9a3a0f7023e7",
                    "width": 320,
                    "height": 268
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bac2a180967dbb6dd0c4544eaf16660950fa7c43",
                    "width": 640,
                    "height": 537
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f4742b3334e7a0766de5f772d8dd7461bdcc3516",
                    "width": 960,
                    "height": 805
                  },
                  {
                    "url": "https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34cac364f141d7cfb76ee841fc71d40e7f141430",
                    "width": 1080,
                    "height": 906
                  }
                ],
                "variants": {},
                "id": "LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mb2y1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WooFL",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb2y1z/the_untold_revolution_in_ios_26_webgpu_is_coming/",
          "stickied": false,
          "url": "https://brandlens.io/blog/the-untold-revolution-beneath-ios-26-webgpu-is-coming-everywhere-and-it-changes-everything/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753664937,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Found this paper recently and thought the idea was worth sharing.\n\nIt is a language model trained specifically for debugging rather than general-purpose code generation. It’s built to understand large codebases over time, using something called Adaptive Graph-Guided Retrieval to pull in relevant files, logs, and commit history when tracing bugs.\n\nThe model is trained on millions of real debugging examples like stack traces, test failures, and CI logs. Instead of just predicting code, it runs through a full debugging loop: retrieve context, propose fix, test, refine, and update memory.\n\nA few standout points:\n\n* Claims 65% success on real-world debugging tasks, compared to \\~10% for GPT-4 or Claude\n* Retrieval seems to prioritize structural relationships between code, not just token similarity\n* Focus is on producing fixes, tests, and docs, not just autocomplete\n\nHonestly surprised we haven’t seen more models focus purely on debugging like this. Most tools still treat it like another code generation task. Would be interested to hear thoughts on how this compares to retrieval-augmented agents or if anyone’s explored similar approaches.\n\nPaper: [https://arxiv.org/abs/2507.12482](https://arxiv.org/abs/2507.12482)",
          "author_fullname": "t2_dcd7znsr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "An LLM Focused Just on Debugging",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mazi8m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753655276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found this paper recently and thought the idea was worth sharing.&lt;/p&gt;\n\n&lt;p&gt;It is a language model trained specifically for debugging rather than general-purpose code generation. It’s built to understand large codebases over time, using something called Adaptive Graph-Guided Retrieval to pull in relevant files, logs, and commit history when tracing bugs.&lt;/p&gt;\n\n&lt;p&gt;The model is trained on millions of real debugging examples like stack traces, test failures, and CI logs. Instead of just predicting code, it runs through a full debugging loop: retrieve context, propose fix, test, refine, and update memory.&lt;/p&gt;\n\n&lt;p&gt;A few standout points:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Claims 65% success on real-world debugging tasks, compared to ~10% for GPT-4 or Claude&lt;/li&gt;\n&lt;li&gt;Retrieval seems to prioritize structural relationships between code, not just token similarity&lt;/li&gt;\n&lt;li&gt;Focus is on producing fixes, tests, and docs, not just autocomplete&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Honestly surprised we haven’t seen more models focus purely on debugging like this. Most tools still treat it like another code generation task. Would be interested to hear thoughts on how this compares to retrieval-augmented agents or if anyone’s explored similar approaches.&lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2507.12482\"&gt;https://arxiv.org/abs/2507.12482&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mazi8m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sharp-Arachnid-8760",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mazi8m/an_llm_focused_just_on_debugging/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mazi8m/an_llm_focused_just_on_debugging/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753655276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There's good hyper around gemini deep think. Can we simulate it using the DeepSeek models or Qwen?\n\nIs that simply gemini 2.5 pro with a much higher thinking budget or it's using some branch of thoughts or Graph of thoughts behind the scenes using multiple parallel instances???? \n\nHas anyone tested something like this? ",
          "author_fullname": "t2_yfi9sqrzf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can we simulate gemini deepthink with models like deepseek/qwen or other open models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1marx3v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753636686,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s good hyper around gemini deep think. Can we simulate it using the DeepSeek models or Qwen?&lt;/p&gt;\n\n&lt;p&gt;Is that simply gemini 2.5 pro with a much higher thinking budget or it&amp;#39;s using some branch of thoughts or Graph of thoughts behind the scenes using multiple parallel instances???? &lt;/p&gt;\n\n&lt;p&gt;Has anyone tested something like this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1marx3v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "True_Requirement_891",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1marx3v/how_can_we_simulate_gemini_deepthink_with_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1marx3v/how_can_we_simulate_gemini_deepthink_with_models/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753636686,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m hacking together the Hierarchical Reasoning Model (temporal slots) with Axiom’s object‑centric slots.\n\nHere’s my brain dump:\n\nLoaded HRM: “past, present, future loops”\n\nIdentified sample‑efficiency as core driver\n\nSpotted Axiom: “spatial slots, as in, object centroids expanding on the fly”\n\nNoticed both ditch big offline pretraining\n\nMapped overlap: inductive bias → fewer samples\n\nDecided: unify time‑based and space‑based slotting into one architecture\n\nNext step: define joint slot tensor with [time × object] axes and online clustering\n\nThoughts?\n\n\n\nWhy bother?\n\nBuilding it because HRM handles time, Axiom handles space. One gives memory, one gives structure. Separately, they’re decent. Together, they cover each other’s blind spots. No pretraining, learns on the fly, handles changing stuff better. Thinking of pointing it at computers next, to see if it can watch, adapt, click.\n\nLinks:\nHierarchical Reasoning Model (HRM) repo:\nhttps://github.com/sapientinc/HRM\n\nAXIOM repo:\nhttps://github.com/VersesTech/axiom\n\nHierarchical Reasoning Model (HRM):\nhttps://arxiv.org/abs/2506.21734 arXiv\n\nAXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models:\nhttps://arxiv.org/abs/2505.24784 arXiv\n\nDropping the implementation in the next few days.",
          "author_fullname": "t2_190nkoim2p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying a temporal + spatial slot fusion model (HRM × Axiom)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maxquu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753650786,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m hacking together the Hierarchical Reasoning Model (temporal slots) with Axiom’s object‑centric slots.&lt;/p&gt;\n\n&lt;p&gt;Here’s my brain dump:&lt;/p&gt;\n\n&lt;p&gt;Loaded HRM: “past, present, future loops”&lt;/p&gt;\n\n&lt;p&gt;Identified sample‑efficiency as core driver&lt;/p&gt;\n\n&lt;p&gt;Spotted Axiom: “spatial slots, as in, object centroids expanding on the fly”&lt;/p&gt;\n\n&lt;p&gt;Noticed both ditch big offline pretraining&lt;/p&gt;\n\n&lt;p&gt;Mapped overlap: inductive bias → fewer samples&lt;/p&gt;\n\n&lt;p&gt;Decided: unify time‑based and space‑based slotting into one architecture&lt;/p&gt;\n\n&lt;p&gt;Next step: define joint slot tensor with [time × object] axes and online clustering&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n\n&lt;p&gt;Why bother?&lt;/p&gt;\n\n&lt;p&gt;Building it because HRM handles time, Axiom handles space. One gives memory, one gives structure. Separately, they’re decent. Together, they cover each other’s blind spots. No pretraining, learns on the fly, handles changing stuff better. Thinking of pointing it at computers next, to see if it can watch, adapt, click.&lt;/p&gt;\n\n&lt;p&gt;Links:\nHierarchical Reasoning Model (HRM) repo:\n&lt;a href=\"https://github.com/sapientinc/HRM\"&gt;https://github.com/sapientinc/HRM&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;AXIOM repo:\n&lt;a href=\"https://github.com/VersesTech/axiom\"&gt;https://github.com/VersesTech/axiom&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hierarchical Reasoning Model (HRM):\n&lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt; arXiv&lt;/p&gt;\n\n&lt;p&gt;AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models:\n&lt;a href=\"https://arxiv.org/abs/2505.24784\"&gt;https://arxiv.org/abs/2505.24784&lt;/a&gt; arXiv&lt;/p&gt;\n\n&lt;p&gt;Dropping the implementation in the next few days.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YsHmtOqqO5eQRfbMvLMBSMGxXsuMLox0sk8UIYyDRho.png?auto=webp&amp;s=0c756234007ac93ba4b3fb26d826891ef2190964",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YsHmtOqqO5eQRfbMvLMBSMGxXsuMLox0sk8UIYyDRho.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e8b6ad2ebbebead71fafd89ce76556f4fa005de",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YsHmtOqqO5eQRfbMvLMBSMGxXsuMLox0sk8UIYyDRho.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb8c25651832a4ee7bed0dca58ccc11f09877786",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YsHmtOqqO5eQRfbMvLMBSMGxXsuMLox0sk8UIYyDRho.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0b1bc81f967a3cbd58b277469cecf503258bf6f",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YsHmtOqqO5eQRfbMvLMBSMGxXsuMLox0sk8UIYyDRho.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=85810dc35da3e3fe7343be507b4e735c0bb6dd8b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YsHmtOqqO5eQRfbMvLMBSMGxXsuMLox0sk8UIYyDRho.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b6bd21936012af59c2401344e25efb09d61f2b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YsHmtOqqO5eQRfbMvLMBSMGxXsuMLox0sk8UIYyDRho.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91646f1228c774aed85023d243775855a30ef8b7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YsHmtOqqO5eQRfbMvLMBSMGxXsuMLox0sk8UIYyDRho"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maxquu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Key_Clerk_1431",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maxquu/trying_a_temporal_spatial_slot_fusion_model_hrm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maxquu/trying_a_temporal_spatial_slot_fusion_model_hrm/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753650786,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\n\nI’m putting together a PC mainly for running large language models like Qwen, LLaMA3, DeepSeek, etc. It’ll mostly be used for **code generation tasks**, and I want it to run **24/7**, quietly, in my home office.\n\nHere’s what I’ve picked so far:\n\n* **Case**: Lian Li O11D EVO XL\n* **CPU**: AMD Ryzen 9 7950X3D\n* **GPU**: MSI RTX 4090 Suprim Liquid X\n* **Motherboard**: ASUS ProArt X670E-Creator\n* **RAM**: 64GB DDR5 G.Skill Trident Z5\n* **AIO Coolers**: 360mm for CPU, 240mm for GPU (built-in)\n* **SSD**: Samsung 990 Pro 2TB\n* **PSU**: Corsair AX1600i Titanium (probably overkill, but wanted room to grow)\n\n**What I’m wondering:**\n\n1. Anyone running something similar — how **quiet** is it under load? Any tips to make it even quieter?\n2. Can this handle models like **Qwen2.5-32B** comfortably in 4-bit? Or am I dreaming?\n3. I’m also thinking of renting the GPU out on [**Vast.ai**](http://Vast.ai) **/ RunPod** when I’m not using it. Anyone making decent side income doing that?\n4. Any parts you’d swap out or downscale without losing performance?\n\nGoal is to have something powerful but also quiet enough to keep on 24/7 — and if it can earn a bit while idle, even better.\n\nAppreciate any thoughts!",
          "author_fullname": "t2_px0vov1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a quiet LLM machine for 24/7 use, is this setup overkill or smart?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1malflg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753620460,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I’m putting together a PC mainly for running large language models like Qwen, LLaMA3, DeepSeek, etc. It’ll mostly be used for &lt;strong&gt;code generation tasks&lt;/strong&gt;, and I want it to run &lt;strong&gt;24/7&lt;/strong&gt;, quietly, in my home office.&lt;/p&gt;\n\n&lt;p&gt;Here’s what I’ve picked so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Case&lt;/strong&gt;: Lian Li O11D EVO XL&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 9 7950X3D&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: MSI RTX 4090 Suprim Liquid X&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: ASUS ProArt X670E-Creator&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 64GB DDR5 G.Skill Trident Z5&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AIO Coolers&lt;/strong&gt;: 360mm for CPU, 240mm for GPU (built-in)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt;: Samsung 990 Pro 2TB&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PSU&lt;/strong&gt;: Corsair AX1600i Titanium (probably overkill, but wanted room to grow)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I’m wondering:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Anyone running something similar — how &lt;strong&gt;quiet&lt;/strong&gt; is it under load? Any tips to make it even quieter?&lt;/li&gt;\n&lt;li&gt;Can this handle models like &lt;strong&gt;Qwen2.5-32B&lt;/strong&gt; comfortably in 4-bit? Or am I dreaming?&lt;/li&gt;\n&lt;li&gt;I’m also thinking of renting the GPU out on &lt;a href=\"http://Vast.ai\"&gt;&lt;strong&gt;Vast.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/ RunPod&lt;/strong&gt; when I’m not using it. Anyone making decent side income doing that?&lt;/li&gt;\n&lt;li&gt;Any parts you’d swap out or downscale without losing performance?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Goal is to have something powerful but also quiet enough to keep on 24/7 — and if it can earn a bit while idle, even better.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?auto=webp&amp;s=c5b1db2b11bd21a955cbe1e863cde94ef57607f4",
                  "width": 4000,
                  "height": 2250
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a08158a2ec290c8157b492f314bfb148408be1fc",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d4693d9fc011431e9348152136fa7a13c95504b",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=93ef867725a538dad3a6209e5062d3d1de60aeaa",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc186b216811c20876ecdaf0e913cc0b59498d7a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67812638cc7d2b930cd8bebf733409c3b2d92397",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc092f31a95e3a3df682dc8f7222b0fb1363a5df",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1malflg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bardanaadam",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1malflg/building_a_quiet_llm_machine_for_247_use_is_this/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1malflg/building_a_quiet_llm_machine_for_247_use_is_this/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753620460,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Do you live in the UK and have you bought a 4090 48GB?\n\nWhere exactly did you get it from? eBay? Which vendor?\n",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "4090 48GB for UK - Where?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1makqv4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753618325,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you live in the UK and have you bought a 4090 48GB?&lt;/p&gt;\n\n&lt;p&gt;Where exactly did you get it from? eBay? Which vendor?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1makqv4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1makqv4/4090_48gb_for_uk_where/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1makqv4/4090_48gb_for_uk_where/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753618325,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone else starting to feel this way when a new model 'breaks the charts' but need like 15k thinking tokens to do it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma2j62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 236,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 236,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=a8e13c2b8d5f080234acafb6a37205454684d6ef",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753559381,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "c.tenor.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://c.tenor.com/65jRkhUA2MIAAAAd/tenor.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?format=png8&amp;s=5b30e772f97a59958cc821e357627e74142630ea",
                  "width": 640,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=c2a369b7162c594e361e845f2fc32276e74a79c4",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=d55014d3cfe062800092325bed67d7a4a27d4a64",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=94715de3b1ed3900039c057a42a57f2376a29fb6",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=309a72cf26ac99e73a5c3e6d981a001d0433be9f",
                    "width": 640,
                    "height": 640
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?s=9904c2f61fbc48bb11103a49b05a915bba887ea4",
                      "width": 640,
                      "height": 640
                    },
                    "resolutions": [
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=108&amp;crop=smart&amp;s=0fe62f124941f6a41a792f4434c13e7e6e5789b1",
                        "width": 108,
                        "height": 108
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=216&amp;crop=smart&amp;s=892fad93a240f8be12ccad1ca387d43e89abdac5",
                        "width": 216,
                        "height": 216
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=320&amp;crop=smart&amp;s=d75cbabb532d691ef6fef62cbcdbb65bd4525258",
                        "width": 320,
                        "height": 320
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=640&amp;crop=smart&amp;s=a4df17ec2e79efaeb8495d39a06eefe4fc80e5a6",
                        "width": 640,
                        "height": 640
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?format=mp4&amp;s=113b9fc7a8ecd3c8ee77d2687be6a164e2b56f6a",
                      "width": 640,
                      "height": 640
                    },
                    "resolutions": [
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=108&amp;format=mp4&amp;s=7675756c21aa354214a6202383805f091c91bd0e",
                        "width": 108,
                        "height": 108
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=216&amp;format=mp4&amp;s=131afb78edd87185c61d77d827ff3016ebd7e694",
                        "width": 216,
                        "height": 216
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=320&amp;format=mp4&amp;s=8527775b264b0f47ea74c327d982397d2258d99d",
                        "width": 320,
                        "height": 320
                      },
                      {
                        "url": "https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=640&amp;format=mp4&amp;s=005a8d43fdcb337f9c8667ed0d27ed4f65933682",
                        "width": 640,
                        "height": 640
                      }
                    ]
                  }
                },
                "id": "FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1ma2j62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1ma2j62/anyone_else_starting_to_feel_this_way_when_a_new/",
          "stickied": false,
          "url": "https://c.tenor.com/65jRkhUA2MIAAAAd/tenor.gif",
          "subreddit_subscribers": 505617,
          "created_utc": 1753559381,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Basically benchmark of benchmarks. AI companies generally just show the benchmarks which suits accordingly to them, and hiding others. \nIs there a place where I can all of the benchmarks, so that I can take an informed decision before using any LLM API or downloading any new models?",
          "author_fullname": "t2_w1mitrw3x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a website which has a collection of all benchmarks perfomed for LLM models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mav8p7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753644603,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically benchmark of benchmarks. AI companies generally just show the benchmarks which suits accordingly to them, and hiding others. \nIs there a place where I can all of the benchmarks, so that I can take an informed decision before using any LLM API or downloading any new models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mav8p7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Special_System_6627",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mav8p7/is_there_a_website_which_has_a_collection_of_all/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mav8p7/is_there_a_website_which_has_a_collection_of_all/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753644603,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a few PCs at home with different GPUs sitting around. I was thinking it would be great if these idle GPUs can all work together to process AI prompts sent from one machine. Is there an out of the box solution that allows me to leverage the multiple computers in my house to do ai work load? note pulling the gpus into a single machine is not an option for me. ",
          "author_fullname": "t2_15o3gy1oht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local Distributed GPU Use",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1may4ut",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753651757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a few PCs at home with different GPUs sitting around. I was thinking it would be great if these idle GPUs can all work together to process AI prompts sent from one machine. Is there an out of the box solution that allows me to leverage the multiple computers in my house to do ai work load? note pulling the gpus into a single machine is not an option for me. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1may4ut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "deathcom65",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1may4ut/local_distributed_gpu_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1may4ut/local_distributed_gpu_use/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753651757,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I know you are probably tired of seeing these posts, but I'd really appreciate the input\n\nCurrent GPU set up:  \n\\* gtx 1080ti (11Gb)  \n\\* gtx 1050ti (4Gb)  \n\\* pcie gen 3.0  \n\\* 16Gb DDR3 RAM  \n\\* Very old i5-4460 with 4 cores at 3.2GHz\n\nSo CPU inference is out of the question\n\nI want to upgrade it because the 1050ti isn't doing much work with only 4gb, and when it is, it's 2x slower, so most of the time its only the 1080ti.\n\nI don't have much money, so I was thinking of either:\n\n|Sell|Replace with|Total Cost|\n|:-|:-|:-|\n|1050ti|1080ti|$100|\n|1050ti|3060 (12Gb)|$150|\n|1050ti &amp; 1080ti|2x 3060 (12Gb)|$200|\n|1050ti|5060ti (16Gb)|$380|\n|1050ti &amp; 1080ti|2x 5060ti (16Gb)|$660|\n\nlmk if the table is confusing.\n\n  \nRight now I am leaning towards 2x 3060's, but idk if it will have less total compute than 2x 1080's, or if they will be nearly identical and if I am just wasting money there. I am also unsure about the advantages of newer hardware with the 50 series, and if its worth the $660 (wich is at the very outer edge of what I want to spend, so a $750-900 3090 is out of the question). Or maybe at the stage in life I am in, maybe it's just better for me to save the money, and upgrade a few years down the line.\n\nAlso I know from experience having two different GPU's doesn't work very well.\n\nI'd love to hear your thoughts!!!",
          "author_fullname": "t2_idqkwio0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU Help (1080ti vs 3060 vs 5060ti)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1map5pe",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753630171,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I know you are probably tired of seeing these posts, but I&amp;#39;d really appreciate the input&lt;/p&gt;\n\n&lt;p&gt;Current GPU set up:&lt;br/&gt;\n* gtx 1080ti (11Gb)&lt;br/&gt;\n* gtx 1050ti (4Gb)&lt;br/&gt;\n* pcie gen 3.0&lt;br/&gt;\n* 16Gb DDR3 RAM&lt;br/&gt;\n* Very old i5-4460 with 4 cores at 3.2GHz&lt;/p&gt;\n\n&lt;p&gt;So CPU inference is out of the question&lt;/p&gt;\n\n&lt;p&gt;I want to upgrade it because the 1050ti isn&amp;#39;t doing much work with only 4gb, and when it is, it&amp;#39;s 2x slower, so most of the time its only the 1080ti.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have much money, so I was thinking of either:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Sell&lt;/th&gt;\n&lt;th align=\"left\"&gt;Replace with&lt;/th&gt;\n&lt;th align=\"left\"&gt;Total Cost&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1050ti&lt;/td&gt;\n&lt;td align=\"left\"&gt;1080ti&lt;/td&gt;\n&lt;td align=\"left\"&gt;$100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1050ti&lt;/td&gt;\n&lt;td align=\"left\"&gt;3060 (12Gb)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$150&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1050ti &amp;amp; 1080ti&lt;/td&gt;\n&lt;td align=\"left\"&gt;2x 3060 (12Gb)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$200&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1050ti&lt;/td&gt;\n&lt;td align=\"left\"&gt;5060ti (16Gb)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$380&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1050ti &amp;amp; 1080ti&lt;/td&gt;\n&lt;td align=\"left\"&gt;2x 5060ti (16Gb)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$660&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;lmk if the table is confusing.&lt;/p&gt;\n\n&lt;p&gt;Right now I am leaning towards 2x 3060&amp;#39;s, but idk if it will have less total compute than 2x 1080&amp;#39;s, or if they will be nearly identical and if I am just wasting money there. I am also unsure about the advantages of newer hardware with the 50 series, and if its worth the $660 (wich is at the very outer edge of what I want to spend, so a $750-900 3090 is out of the question). Or maybe at the stage in life I am in, maybe it&amp;#39;s just better for me to save the money, and upgrade a few years down the line.&lt;/p&gt;\n\n&lt;p&gt;Also I know from experience having two different GPU&amp;#39;s doesn&amp;#39;t work very well.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear your thoughts!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1map5pe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Expensive-Apricot-25",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1map5pe/gpu_help_1080ti_vs_3060_vs_5060ti/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1map5pe/gpu_help_1080ti_vs_3060_vs_5060ti/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753630171,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Its great! It's a clear step above Qwen3 32b imo. Id recommend trying it out \n\nMy experience with it:\n- it generates far less \"slop\" than Qwen models\n- it handles long context really well \n- it easily handles trick questions like \"What should be the punishment for looking at your opponent's board in chess?\"\n- handled all my coding questions really well\n- has a weird ass architecture where some layers dont have attention tensors which messed up llama.cpp tensor split allocation, but was pretty easy to overcome \n\nMy driver for a long time was Qwen3 32b FP16 but this model at Q8 has been a massive step up for me and ill be using it going forward. \n\nAnyone else tried this bad boy out?",
          "author_fullname": "t2_o015g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone else been using the new nvidia/Llama-3_3-Nemotron-Super-49B-v1_5 model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1madjq6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753591386,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its great! It&amp;#39;s a clear step above Qwen3 32b imo. Id recommend trying it out &lt;/p&gt;\n\n&lt;p&gt;My experience with it:\n- it generates far less &amp;quot;slop&amp;quot; than Qwen models\n- it handles long context really well \n- it easily handles trick questions like &amp;quot;What should be the punishment for looking at your opponent&amp;#39;s board in chess?&amp;quot;\n- handled all my coding questions really well\n- has a weird ass architecture where some layers dont have attention tensors which messed up llama.cpp tensor split allocation, but was pretty easy to overcome &lt;/p&gt;\n\n&lt;p&gt;My driver for a long time was Qwen3 32b FP16 but this model at Q8 has been a massive step up for me and ill be using it going forward. &lt;/p&gt;\n\n&lt;p&gt;Anyone else tried this bad boy out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1madjq6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kevin_1994",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1madjq6/anyone_else_been_using_the_new_nvidiallama3/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1madjq6/anyone_else_been_using_the_new_nvidiallama3/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753591386,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My own personal desktop workstation.\n\nSpecs:\n\n1. GPUs -- Quad 4090 48GB (Roughly 3200 USD each, 450 watts max energy use)\n2. CPUs -- Intel 6530 32 Cores Emerald Rapids (1350 USD)\n3. Motherboard -- Tyan S5652-2T (836 USD)\n4. RAM -- eight sticks of M321RYGA0PB0-CWMKH 96GB (768GB total, 470 USD per stick)\n5. Case -- Jonsbo N5 (160 USD)\n6. PSU -- Great Wall fully modular 2600 watt with quad 12VHPWR plugs (326 USD)\n7. CPU cooler -- coolserver M98 (40 USD)\n8. SSD -- Western Digital 4TB SN850X (290 USD)\n9. Case fans -- Three fans, Liquid Crystal Polymer Huntbow ProArtist H14PE (21 USD per fan)\n10. HDD -- Eight 20 TB Seagate (pending delivery)",
          "author_fullname": "t2_nm52x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "f9ga1xe6a8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=92260477ef81ca48784f83804fbab76670633c70"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=44e07c0f9267c487ab805b7c8f23012423884c9e"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6cec7d5641f919437841cee8246fa1632c796885"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ea04e45eb48af451532a1df41e586b696ae9deb"
                }
              ],
              "s": {
                "y": 1200,
                "x": 900,
                "u": "https://preview.redd.it/f9ga1xe6a8ff1.png?width=900&amp;format=png&amp;auto=webp&amp;s=dbaeb78766767a0adc9fb9eb3a3dc33bda7e5b89"
              },
              "id": "f9ga1xe6a8ff1"
            },
            "icwwq04aa8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff2342618253487d74ef4c4d5ec067862e9b6ac6"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad674b7840f7dbb23233f41a9ea87a8e9973c787"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7816e33734ea243d88d2ca4a7d35908d7e4c3739"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a06465ac67e7a630374ba51d596e35375f4959b"
                }
              ],
              "s": {
                "y": 1200,
                "x": 900,
                "u": "https://preview.redd.it/icwwq04aa8ff1.png?width=900&amp;format=png&amp;auto=webp&amp;s=bb9d49aa42cfcfa194fd014a00afb8636ea3a677"
              },
              "id": "icwwq04aa8ff1"
            },
            "wi7xh5jda8ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=107d0bf59a8e453641317502b3ff29ce0b66104c"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a60a88c1e4a7a6cbdd28fadcf775f91aaf10279"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dae632cdd76f18d1391c00d9ad3516df29dbe6eb"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e1c75a5eefce2d54bc0d2ba5d559e597f076b20"
                }
              ],
              "s": {
                "y": 1200,
                "x": 900,
                "u": "https://preview.redd.it/wi7xh5jda8ff1.png?width=900&amp;format=png&amp;auto=webp&amp;s=d57d9c0f8ae81486eaa612ba84bbf7a94cba5985"
              },
              "id": "wi7xh5jda8ff1"
            }
          },
          "name": "t3_1m9uwxg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 525,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "f9ga1xe6a8ff1",
                "id": 714450146
              },
              {
                "media_id": "icwwq04aa8ff1",
                "id": 714450147
              },
              {
                "media_id": "wi7xh5jda8ff1",
                "id": 714450148
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 525,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ValHYB67eTbAZL-7K0hFnR6LkMb2R6iJ0iuJtur8Ksg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753540660,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My own personal desktop workstation.&lt;/p&gt;\n\n&lt;p&gt;Specs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;GPUs -- Quad 4090 48GB (Roughly 3200 USD each, 450 watts max energy use)&lt;/li&gt;\n&lt;li&gt;CPUs -- Intel 6530 32 Cores Emerald Rapids (1350 USD)&lt;/li&gt;\n&lt;li&gt;Motherboard -- Tyan S5652-2T (836 USD)&lt;/li&gt;\n&lt;li&gt;RAM -- eight sticks of M321RYGA0PB0-CWMKH 96GB (768GB total, 470 USD per stick)&lt;/li&gt;\n&lt;li&gt;Case -- Jonsbo N5 (160 USD)&lt;/li&gt;\n&lt;li&gt;PSU -- Great Wall fully modular 2600 watt with quad 12VHPWR plugs (326 USD)&lt;/li&gt;\n&lt;li&gt;CPU cooler -- coolserver M98 (40 USD)&lt;/li&gt;\n&lt;li&gt;SSD -- Western Digital 4TB SN850X (290 USD)&lt;/li&gt;\n&lt;li&gt;Case fans -- Three fans, Liquid Crystal Polymer Huntbow ProArtist H14PE (21 USD per fan)&lt;/li&gt;\n&lt;li&gt;HDD -- Eight 20 TB Seagate (pending delivery)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m9uwxg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9uwxg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "44seconds",
          "discussion_type": null,
          "num_comments": 158,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m9uwxg",
          "subreddit_subscribers": 505617,
          "created_utc": 1753540660,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I saw a post saying Qwen 2.5 Bakemono was the best but that was 4 months ago and was wondering if something better is currently available.",
          "author_fullname": "t2_1fzwz14ce2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local LLM for Japanese to English translation and explanation for 24gb VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mb286h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753662838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw a post saying Qwen 2.5 Bakemono was the best but that was 4 months ago and was wondering if something better is currently available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb286h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Abject-Obligation406",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb286h/best_local_llm_for_japanese_to_english/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb286h/best_local_llm_for_japanese_to_english/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753662838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_8bwjj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Me after getting excited by a new model release and checking on Hugging Face if I can run it locally.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 128,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9rqxa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 811,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 811,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3maTLMR0SbyBvb7hQx7_PpxeyyK-KvUWrbbwbMU4Q3I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753531781,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0tnbd1i9m7ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?auto=webp&amp;s=947ab42af41c4c1311628c38d1cd8412b2b60729",
                  "width": 738,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6793b3d09acfa0d71fd64ec0893a75e4685dc3e5",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159c33df3d3950e75292f85013407fff7b389f04",
                    "width": 216,
                    "height": 197
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=023fa90959cadf44a37e2c2af52b2f17e81a2692",
                    "width": 320,
                    "height": 292
                  },
                  {
                    "url": "https://preview.redd.it/0tnbd1i9m7ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c3c6aab9c8b44a1cec98dbeca3972f5d0885fd8",
                    "width": 640,
                    "height": 585
                  }
                ],
                "variants": {},
                "id": "6H_reI7bQd-xJWBvmKQHOgTWZ6lYDuz0OFnnivjFsfQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9rqxa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "alew3",
          "discussion_type": null,
          "num_comments": 147,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/",
          "stickied": false,
          "url": "https://i.redd.it/0tnbd1i9m7ff1.png",
          "subreddit_subscribers": 505617,
          "created_utc": 1753531781,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**The title says most of it, but to be exact, I'm using an HP EliteBook 840 G3.**  \nI'm trying to generate some gory artwork for a book I'm writing, but I'm running into a problem, most of the good (and free 😅) AI tools have heavy censorship. The ones that don’t either seem sketchy or just aren’t very good.  \nAny help would be really appreciated!",
          "author_fullname": "t2_r0zyqols",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the best (free) LLM for a potato laptop, I still want to be able to generate images.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mb2486",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753662519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;The title says most of it, but to be exact, I&amp;#39;m using an HP EliteBook 840 G3.&lt;/strong&gt;&lt;br/&gt;\nI&amp;#39;m trying to generate some gory artwork for a book I&amp;#39;m writing, but I&amp;#39;m running into a problem, most of the good (and free 😅) AI tools have heavy censorship. The ones that don’t either seem sketchy or just aren’t very good.&lt;br/&gt;\nAny help would be really appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb2486",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roxlife1",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb2486/whats_the_best_free_llm_for_a_potato_laptop_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb2486/whats_the_best_free_llm_for_a_potato_laptop_i/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753662519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I often see products put out by makers in China posted here as \"China does X\", either with or sometimes even without the maker being mentioned. Some examples:\n\n* [Is China the only hope for factual models?](https://www.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/)\n* [China launches its first 6nm GPUs for gaming and AI](https://www.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/)\n* [Looks like China is the one playing 5D chess](https://www.reddit.com/r/LocalLLaMA/comments/1ka3hlm/looks_like_china_is_the_one_playing_5d_chess/)\n* [China has delivered yet again](https://www.reddit.com/r/LocalLLaMA/comments/1kbneq2/china_has_delivered_yet_again/)\n* [China is leading open-source](https://www.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/)\n* [China's Huawei develops new AI chip](https://www.reddit.com/r/LocalLLaMA/comments/1kb59p2/chinas_huawei_develops_new_ai_chip_seeking_to/)\n* [Chinese researchers find multimodal LLMs develop ...](https://www.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/)\n\nWhereas U.S. makers are always named: Anthropic, OpenAI, Meta, etc.. U.S. researchers are also always named, but research papers from a lab in China is posted as \"Chinese researchers ...\".\n\nHow do Chinese makers and researchers feel about this? As a researcher myself, I would *hate* if my work was lumped into the output of an entire country of billions and not attributed to *me* specifically.\n\nSame if someone referred to my company as \"American Company\".\n\nI think we, as a community, could do a better job naming names and giving credit to the makers. We know Sam Altman, Ilya Sutskever, Jensen Huang, etc. but I rarely see Liang Wenfeng mentioned here.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Crediting Chinese makers by name",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9xw4c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 344,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 344,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753548204,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753547946,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I often see products put out by makers in China posted here as &amp;quot;China does X&amp;quot;, either with or sometimes even without the maker being mentioned. Some examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/\"&gt;Is China the only hope for factual models?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/\"&gt;China launches its first 6nm GPUs for gaming and AI&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1ka3hlm/looks_like_china_is_the_one_playing_5d_chess/\"&gt;Looks like China is the one playing 5D chess&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kbneq2/china_has_delivered_yet_again/\"&gt;China has delivered yet again&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/\"&gt;China is leading open-source&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kb59p2/chinas_huawei_develops_new_ai_chip_seeking_to/\"&gt;China&amp;#39;s Huawei develops new AI chip&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/\"&gt;Chinese researchers find multimodal LLMs develop ...&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Whereas U.S. makers are always named: Anthropic, OpenAI, Meta, etc.. U.S. researchers are also always named, but research papers from a lab in China is posted as &amp;quot;Chinese researchers ...&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;How do Chinese makers and researchers feel about this? As a researcher myself, I would &lt;em&gt;hate&lt;/em&gt; if my work was lumped into the output of an entire country of billions and not attributed to &lt;em&gt;me&lt;/em&gt; specifically.&lt;/p&gt;\n\n&lt;p&gt;Same if someone referred to my company as &amp;quot;American Company&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I think we, as a community, could do a better job naming names and giving credit to the makers. We know Sam Altman, Ilya Sutskever, Jensen Huang, etc. but I rarely see Liang Wenfeng mentioned here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9xw4c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 85,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1m9xw4c/crediting_chinese_makers_by_name/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9xw4c/crediting_chinese_makers_by_name/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753547946,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just bought a computer with a 3090, and I was wondering if I could get advice on the best models for my gpu. Specifically, I am looking for:\n• Best model for vision+tool use\n• Best uncensored\n• Best for coding\n• Best for context length\n• And maybe best for just vision or just tool use",
          "author_fullname": "t2_1efi4dcf6i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best models for 3090?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mb1of0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753661236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought a computer with a 3090, and I was wondering if I could get advice on the best models for my gpu. Specifically, I am looking for:\n• Best model for vision+tool use\n• Best uncensored\n• Best for coding\n• Best for context length\n• And maybe best for just vision or just tool use&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mb1of0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Yak4416",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mb1of0/best_models_for_3090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb1of0/best_models_for_3090/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753661236,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Could get NVIDIA RTX PRO 4000 Blackwell - 24GB GDDR7  1 275,50 euros without VAT.   \nBut its only 140W and 8960 CUDA  cores. Takes only 1 slot. Is it worth? Some Epyc board could fit 6 of these...with pci-e 5.0",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA RTX PRO 4000 Blackwell - 24GB GDDR7",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1majha1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753613986,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could get NVIDIA RTX PRO 4000 Blackwell - 24GB GDDR7  1 275,50 euros without VAT.&lt;br/&gt;\nBut its only 140W and 8960 CUDA  cores. Takes only 1 slot. Is it worth? Some Epyc board could fit 6 of these...with pci-e 5.0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1majha1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1majha1/nvidia_rtx_pro_4000_blackwell_24gb_gddr7/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1majha1/nvidia_rtx_pro_4000_blackwell_24gb_gddr7/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753613986,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a chance to travel to China the end of this year. I'm thinking about buying the 48 GB dual B60 GPU, if I could find one (not really the goal of my travel there). Can you guys give me some insights on the Intel's previous GPUs compatibility with Nvidia kit? I've read that AMD's Rocm is a bit of a pain. That's why I'm interested with intel Arc. I'm currently using 3060 TI (8gb), just to mess around with comfyui on Windows 10. But I want to upgrade. I don't mind the speed, more interested in capability (training, generation, etc). Thanks.",
          "author_fullname": "t2_7uclp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "General Intel Arc compatibility with Nvidia",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1martn1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753636465,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a chance to travel to China the end of this year. I&amp;#39;m thinking about buying the 48 GB dual B60 GPU, if I could find one (not really the goal of my travel there). Can you guys give me some insights on the Intel&amp;#39;s previous GPUs compatibility with Nvidia kit? I&amp;#39;ve read that AMD&amp;#39;s Rocm is a bit of a pain. That&amp;#39;s why I&amp;#39;m interested with intel Arc. I&amp;#39;m currently using 3060 TI (8gb), just to mess around with comfyui on Windows 10. But I want to upgrade. I don&amp;#39;t mind the speed, more interested in capability (training, generation, etc). Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1martn1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SwingNinja",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1martn1/general_intel_arc_compatibility_with_nvidia/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1martn1/general_intel_arc_compatibility_with_nvidia/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753636465,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have gemma 3 12b. Been playing around with it and love it. I am interested in a (easily) jailbreakable model or a model without as much restrictions. Thanks in advance. ",
          "author_fullname": "t2_4f29l03e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best models to run on m4 pro 24gb",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1marks7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753635900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have gemma 3 12b. Been playing around with it and love it. I am interested in a (easily) jailbreakable model or a model without as much restrictions. Thanks in advance. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1marks7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "brayo1st",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1marks7/best_models_to_run_on_m4_pro_24gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1marks7/best_models_to_run_on_m4_pro_24gb/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753635900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I built an Overlay AI.\n\nsource code: [https://github.com/kamlendras/aerogel](https://github.com/kamlendras/aerogel)",
          "author_fullname": "t2_nfeia46jv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built an Overlay AI.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maflh5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/0l6ttkdl5dff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/0l6ttkdl5dff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/0l6ttkdl5dff1/DASHPlaylist.mpd?a=1756257402%2CMzlkMmNiMTNhMTE4NjkwNWY0NTFlMDk5MGIyNDgyMjg2MGVjZGQ2Nzc0OThkMTExM2MwNzQyZDFiOTk5YmU1OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 53,
              "hls_url": "https://v.redd.it/0l6ttkdl5dff1/HLSPlaylist.m3u8?a=1756257402%2CNDM3ODFiZGRjOWM3NzUwNWY1OWY5ZDBjNDg0ZDM1NGE2ZGE1MTUyM2FkNzM0YmFmMTE5ZTUwMThjNjRkNTNiZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=b32100e815917a17fdd3175ea1978387d2a3e72a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753598881,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built an Overlay AI.&lt;/p&gt;\n\n&lt;p&gt;source code: &lt;a href=\"https://github.com/kamlendras/aerogel\"&gt;https://github.com/kamlendras/aerogel&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/0l6ttkdl5dff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?format=pjpg&amp;auto=webp&amp;s=fad99bc3e71313ea5ab3b0a0ad86e73e8dfa593e",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1b15ef08582c8b7b063e9121ffe57496bc4afd6d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ff820a8bde80c3854a377f8f125403d0f21fae4f",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b82eba37261c2dc0f42856bd050fa2cc6b472212",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9f84ba6b0cfbf844b869eef2b2ed5285eeea3e82",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a97c9ba0d7ce670888592edcab4dd707bb5ba54e",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=328d682040f2bcbef5bd18f5ab7976caaa1bb9ff",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "emswOXhrZGw1ZGZmMSoKrI-6nwPl5Obl65Jwi_LBPrT12vkaeVKztVPL6I1W"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1maflh5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kamlendras",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maflh5/i_built_an_overlay_ai/",
          "stickied": false,
          "url": "https://v.redd.it/0l6ttkdl5dff1",
          "subreddit_subscribers": 505617,
          "created_utc": 1753598881,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/0l6ttkdl5dff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/0l6ttkdl5dff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/0l6ttkdl5dff1/DASHPlaylist.mpd?a=1756257402%2CMzlkMmNiMTNhMTE4NjkwNWY0NTFlMDk5MGIyNDgyMjg2MGVjZGQ2Nzc0OThkMTExM2MwNzQyZDFiOTk5YmU1OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 53,
              "hls_url": "https://v.redd.it/0l6ttkdl5dff1/HLSPlaylist.m3u8?a=1756257402%2CNDM3ODFiZGRjOWM3NzUwNWY1OWY5ZDBjNDg0ZDM1NGE2ZGE1MTUyM2FkNzM0YmFmMTE5ZTUwMThjNjRkNTNiZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am running an ollama server as a container in unraid, but I am running up against some problems where models are failing for some use cases. I have several different clients connecting to the server. But I don't know the best way to monitor ollama, for example even just for token usage. But really I want to have some way to monitor what ollama is doing, how models are performing, and to help diagnose problems. But I am having trouble finding a good way to do it. How are you monitoring your ollama server and checking model performance? ",
          "author_fullname": "t2_4ibsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you monitor your Ollama instance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mazvnk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753656282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running an ollama server as a container in unraid, but I am running up against some problems where models are failing for some use cases. I have several different clients connecting to the server. But I don&amp;#39;t know the best way to monitor ollama, for example even just for token usage. But really I want to have some way to monitor what ollama is doing, how models are performing, and to help diagnose problems. But I am having trouble finding a good way to do it. How are you monitoring your ollama server and checking model performance? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mazvnk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ishbuggy",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753656282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys, I am just new here.\n\nI installed ollama and runing model qwen3:8b  \nWhen I run iot through terminal, I get full utilisation of the GPU (3060 Mobile 60W).  \nbut slow response and bad utilisation when run in VS Code.  \nprovided some of my debug log-\n\nubuntu terminal:\n\n    $ ollama ps\n    NAME        ID              SIZE      PROCESSOR          UNTIL              \n    qwen3:8b    500a1f067a9f    6.5 GB    10%/90% CPU/GPU    4 minutes from now \n    \n    udo journalctl -u ollama -f\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:      CUDA0 KV buffer size =   560.00 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context:      CUDA0 compute buffer size =   791.61 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context:  CUDA_Host compute buffer size =    16.01 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context: graph nodes  = 1374\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context: graph splits = 17 (with bs=512), 5 (with bs=1)\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:49:14.189+02:00 level=INFO source=server.go:637 msg=\"llama runner started in 1.51 seconds\"\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:49:14 | 200 |  2.029277689s |       127.0.0.1 | POST     \"/api/generate\"\n    Jul 27 11:50:00 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:50:00 | 200 |  4.942696751s |       127.0.0.1 | POST     \"/api/chat\"\n    Jul 27 11:51:40 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:51:40 | 200 | 19.605748657s |       127.0.0.1 | POST     \"/api/chat\"\n\nwhen I run through the continue chat in VS Code\n\n    ollama ps\n    NAME        ID              SIZE     PROCESSOR          UNTIL               \n    qwen3:8b    500a1f067a9f    13 GB    58%/42% CPU/GPU    29 minutes from now \n    \n    sudo journalctl -u ollama -f\n    [sudo] password for abdelrahman: \n    Jul 27 11:50:00 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:50:00 | 200 |  4.942696751s |       127.0.0.1 | POST     \"/api/chat\"\n    Jul 27 11:51:40 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:51:40 | 200 | 19.605748657s |       127.0.0.1 | POST     \"/api/chat\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |     321.358µs |       127.0.0.1 | GET      \"/api/tags\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |     249.342µs |       127.0.0.1 | GET      \"/api/tags\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   49.584345ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   54.905231ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   57.173959ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   48.834545ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:06 | 200 |   59.986822ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:06 | 200 |   63.046354ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:54:01 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:01 | 200 |      18.856µs |       127.0.0.1 | HEAD     \"/\"\n    Jul 27 11:54:01 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:01 | 200 |      73.667µs |       127.0.0.1 | GET      \"/api/ps\"\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:05.945+02:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"15.3 GiB\" free=\"10.4 GiB\" free_swap=\"2.3 GiB\"\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:05.946+02:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=7 layers.split=\"\" memory.available=\"[5.5 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"12.7 GiB\" memory.required.partial=\"5.4 GiB\" memory.required.kv=\"4.5 GiB\" memory.required.allocations=\"[5.4 GiB]\" memory.weights.total=\"4.5 GiB\" memory.weights.repeating=\"4.1 GiB\" memory.weights.nonrepeating=\"486.9 MiB\" memory.graph.full=\"3.0 GiB\" memory.graph.partial=\"3.0 GiB\"\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   1:                               general.type str              = model\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   4:                         general.size_label str              = 8B\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '&lt;|im_start|&gt;...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f32:  145 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f16:   36 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q4_K:  199 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q6_K:   19 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file format = GGUF V3 (latest)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file type   = Q4_K - Medium\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file size   = 4.86 GiB (5.10 BPW)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: special tokens cache size = 26\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: token to piece cache size = 0.9311 MB\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: arch             = qwen3\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab_only       = 1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model type       = ?B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model params     = 8.19 B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: general.name     = Qwen3 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab type       = BPE\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_vocab          = 151936\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_merges         = 151387\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: BOS token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOS token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOT token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: PAD token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: LF token         = 198 'Ċ'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PRE token    = 151659 '&lt;|fim_prefix|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SUF token    = 151661 '&lt;|fim_suffix|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM MID token    = 151660 '&lt;|fim_middle|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PAD token    = 151662 '&lt;|fim_pad|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM REP token    = 151663 '&lt;|repo_name|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SEP token    = 151664 '&lt;|file_sep|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151662 '&lt;|fim_pad|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151663 '&lt;|repo_name|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151664 '&lt;|file_sep|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: max token length = 256\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_load: vocab only - skipping tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.156+02:00 level=INFO source=server.go:438 msg=\"starting llama server\" cmd=\"/home/abdelrahman/install_directory/ollama/bin/ollama runner --model /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32768 --batch-size 512 --n-gpu-layers 7 --threads 8 --no-mmap --parallel 1 --port 35311\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=server.go:598 msg=\"waiting for llama runner to start responding\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.165+02:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: found 1 CUDA devices:\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]:   Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_backend: loaded CUDA backend from /home/abdelrahman/install_directory/ollama/lib/ollama/libggml-cuda.so\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_backend: loaded CPU backend from /home/abdelrahman/install_directory/ollama/lib/ollama/libggml-cpu-icelake.so\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.225+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.225+02:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:35311\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060 Laptop GPU) - 5617 MiB free\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   1:                               general.type str              = model\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   4:                         general.size_label str              = 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '&lt;|im_start|&gt;...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f32:  145 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f16:   36 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q4_K:  199 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q6_K:   19 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file format = GGUF V3 (latest)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file type   = Q4_K - Medium\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file size   = 4.86 GiB (5.10 BPW)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.408+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: special tokens cache size = 26\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: token to piece cache size = 0.9311 MB\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: arch             = qwen3\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab_only       = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ctx_train      = 40960\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd           = 4096\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_layer          = 36\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_head           = 32\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_head_kv        = 8\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_rot            = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_swa            = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_swa_pattern    = 1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_head_k    = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_head_v    = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_gqa            = 4\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_k_gqa     = 1024\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_v_gqa     = 1024\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_norm_eps       = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_norm_rms_eps   = 1.0e-06\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_clamp_kqv      = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_max_alibi_bias = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_logit_scale    = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_attn_scale     = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ff             = 12288\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_expert         = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_expert_used    = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: causal attn      = 1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: pooling type     = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope type        = 2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope scaling     = linear\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: freq_base_train  = 1000000.0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: freq_scale_train = 1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ctx_orig_yarn  = 40960\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope_finetuned   = unknown\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_conv       = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_inner      = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_state      = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_dt_rank      = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_dt_b_c_rms   = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model type       = 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model params     = 8.19 B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: general.name     = Qwen3 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab type       = BPE\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_vocab          = 151936\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_merges         = 151387\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: BOS token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOS token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOT token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: PAD token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: LF token         = 198 'Ċ'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PRE token    = 151659 '&lt;|fim_prefix|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SUF token    = 151661 '&lt;|fim_suffix|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM MID token    = 151660 '&lt;|fim_middle|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PAD token    = 151662 '&lt;|fim_pad|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM REP token    = 151663 '&lt;|repo_name|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SEP token    = 151664 '&lt;|file_sep|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151662 '&lt;|fim_pad|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151663 '&lt;|repo_name|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151664 '&lt;|file_sep|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: max token length = 256\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_tensors: loading model tensors, this can take a while... (mmap = false)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:06 | 200 |      21.813µs |       127.0.0.1 | HEAD     \"/\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:06 | 200 |      55.253µs |       127.0.0.1 | GET      \"/api/ps\"\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors: offloading 7 repeating layers to GPU\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors: offloaded 7/37 layers to GPU\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:    CUDA_Host model buffer size =  3804.56 MiB\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:        CUDA0 model buffer size =   839.23 MiB\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:          CPU model buffer size =   333.84 MiB\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: constructing llama_context\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_seq_max     = 1\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx         = 32768\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx_per_seq = 32768\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_batch       = 512\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ubatch      = 512\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: causal_attn   = 1\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: flash_attn    = 0\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: freq_base     = 1000000.0\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: freq_scale    = 1\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx_per_seq (32768) &lt; n_ctx_train (40960) -- the full capacity of the model will not be utilized\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context:        CPU  output buffer size =     0.60 MiB\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: kv_size = 32768, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:        CPU KV buffer size =  3712.00 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: KV self size  = 4608.00 MiB, K (f16): 2304.00 MiB, V (f16): 2304.00 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context:      CUDA0 compute buffer size =  2328.00 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context:  CUDA_Host compute buffer size =    72.01 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context: graph nodes  = 1374\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context: graph splits = 381 (with bs=512), 61 (with bs=1)\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:11.175+02:00 level=INFO source=server.go:637 msg=\"llama runner started in 5.02 seconds\n\nthanks in advance.",
          "author_fullname": "t2_11kzex88x8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "low perfomance on Contionue extension Vs code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mau9os",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753642277,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I am just new here.&lt;/p&gt;\n\n&lt;p&gt;I installed ollama and runing model qwen3:8b&lt;br/&gt;\nWhen I run iot through terminal, I get full utilisation of the GPU (3060 Mobile 60W).&lt;br/&gt;\nbut slow response and bad utilisation when run in VS Code.&lt;br/&gt;\nprovided some of my debug log-&lt;/p&gt;\n\n&lt;p&gt;ubuntu terminal:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ ollama ps\nNAME        ID              SIZE      PROCESSOR          UNTIL              \nqwen3:8b    500a1f067a9f    6.5 GB    10%/90% CPU/GPU    4 minutes from now \n\nudo journalctl -u ollama -f\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:      CUDA0 KV buffer size =   560.00 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context:      CUDA0 compute buffer size =   791.61 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context:  CUDA_Host compute buffer size =    16.01 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context: graph nodes  = 1374\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context: graph splits = 17 (with bs=512), 5 (with bs=1)\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:49:14.189+02:00 level=INFO source=server.go:637 msg=&amp;quot;llama runner started in 1.51 seconds&amp;quot;\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:49:14 | 200 |  2.029277689s |       127.0.0.1 | POST     &amp;quot;/api/generate&amp;quot;\nJul 27 11:50:00 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:50:00 | 200 |  4.942696751s |       127.0.0.1 | POST     &amp;quot;/api/chat&amp;quot;\nJul 27 11:51:40 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:51:40 | 200 | 19.605748657s |       127.0.0.1 | POST     &amp;quot;/api/chat&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;when I run through the continue chat in VS Code&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ollama ps\nNAME        ID              SIZE     PROCESSOR          UNTIL               \nqwen3:8b    500a1f067a9f    13 GB    58%/42% CPU/GPU    29 minutes from now \n\nsudo journalctl -u ollama -f\n[sudo] password for abdelrahman: \nJul 27 11:50:00 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:50:00 | 200 |  4.942696751s |       127.0.0.1 | POST     &amp;quot;/api/chat&amp;quot;\nJul 27 11:51:40 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:51:40 | 200 | 19.605748657s |       127.0.0.1 | POST     &amp;quot;/api/chat&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |     321.358µs |       127.0.0.1 | GET      &amp;quot;/api/tags&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |     249.342µs |       127.0.0.1 | GET      &amp;quot;/api/tags&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   49.584345ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   54.905231ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   57.173959ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   48.834545ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:06 | 200 |   59.986822ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:06 | 200 |   63.046354ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:54:01 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:01 | 200 |      18.856µs |       127.0.0.1 | HEAD     &amp;quot;/&amp;quot;\nJul 27 11:54:01 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:01 | 200 |      73.667µs |       127.0.0.1 | GET      &amp;quot;/api/ps&amp;quot;\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:05.945+02:00 level=INFO source=server.go:135 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;15.3 GiB&amp;quot; free=&amp;quot;10.4 GiB&amp;quot; free_swap=&amp;quot;2.3 GiB&amp;quot;\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:05.946+02:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=7 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[5.5 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;12.7 GiB&amp;quot; memory.required.partial=&amp;quot;5.4 GiB&amp;quot; memory.required.kv=&amp;quot;4.5 GiB&amp;quot; memory.required.allocations=&amp;quot;[5.4 GiB]&amp;quot; memory.weights.total=&amp;quot;4.5 GiB&amp;quot; memory.weights.repeating=&amp;quot;4.1 GiB&amp;quot; memory.weights.nonrepeating=&amp;quot;486.9 MiB&amp;quot; memory.graph.full=&amp;quot;3.0 GiB&amp;quot; memory.graph.partial=&amp;quot;3.0 GiB&amp;quot;\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   1:                               general.type str              = model\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   4:                         general.size_label str              = 8B\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [&amp;quot;!&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;&amp;#39;&amp;quot;, ...\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [&amp;quot;Ġ Ġ&amp;quot;, &amp;quot;ĠĠ ĠĠ&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;Ġ t&amp;quot;,...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- &amp;#39;&amp;lt;|im_start|&amp;gt;...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f32:  145 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f16:   36 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q4_K:  199 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q6_K:   19 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file format = GGUF V3 (latest)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file type   = Q4_K - Medium\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file size   = 4.86 GiB (5.10 BPW)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: special tokens cache size = 26\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: token to piece cache size = 0.9311 MB\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: arch             = qwen3\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab_only       = 1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model type       = ?B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model params     = 8.19 B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: general.name     = Qwen3 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab type       = BPE\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_vocab          = 151936\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_merges         = 151387\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: BOS token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOS token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOT token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: PAD token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: LF token         = 198 &amp;#39;Ċ&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PRE token    = 151659 &amp;#39;&amp;lt;|fim_prefix|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SUF token    = 151661 &amp;#39;&amp;lt;|fim_suffix|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM MID token    = 151660 &amp;#39;&amp;lt;|fim_middle|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PAD token    = 151662 &amp;#39;&amp;lt;|fim_pad|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM REP token    = 151663 &amp;#39;&amp;lt;|repo_name|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SEP token    = 151664 &amp;#39;&amp;lt;|file_sep|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151662 &amp;#39;&amp;lt;|fim_pad|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151663 &amp;#39;&amp;lt;|repo_name|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151664 &amp;#39;&amp;lt;|file_sep|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: max token length = 256\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_load: vocab only - skipping tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.156+02:00 level=INFO source=server.go:438 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/home/abdelrahman/install_directory/ollama/bin/ollama runner --model /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32768 --batch-size 512 --n-gpu-layers 7 --threads 8 --no-mmap --parallel 1 --port 35311&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=sched.go:483 msg=&amp;quot;loaded runners&amp;quot; count=1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=server.go:598 msg=&amp;quot;waiting for llama runner to start responding&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.165+02:00 level=INFO source=runner.go:815 msg=&amp;quot;starting go runner&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: found 1 CUDA devices:\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]:   Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_backend: loaded CUDA backend from /home/abdelrahman/install_directory/ollama/lib/ollama/libggml-cuda.so\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_backend: loaded CPU backend from /home/abdelrahman/install_directory/ollama/lib/ollama/libggml-cpu-icelake.so\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.225+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.225+02:00 level=INFO source=runner.go:874 msg=&amp;quot;Server listening on 127.0.0.1:35311&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060 Laptop GPU) - 5617 MiB free\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   1:                               general.type str              = model\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   4:                         general.size_label str              = 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [&amp;quot;!&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;&amp;#39;&amp;quot;, ...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [&amp;quot;Ġ Ġ&amp;quot;, &amp;quot;ĠĠ ĠĠ&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;Ġ t&amp;quot;,...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- &amp;#39;&amp;lt;|im_start|&amp;gt;...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f32:  145 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f16:   36 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q4_K:  199 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q6_K:   19 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file format = GGUF V3 (latest)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file type   = Q4_K - Medium\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file size   = 4.86 GiB (5.10 BPW)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.408+02:00 level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: special tokens cache size = 26\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: token to piece cache size = 0.9311 MB\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: arch             = qwen3\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab_only       = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ctx_train      = 40960\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd           = 4096\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_layer          = 36\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_head           = 32\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_head_kv        = 8\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_rot            = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_swa            = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_swa_pattern    = 1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_head_k    = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_head_v    = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_gqa            = 4\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_k_gqa     = 1024\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_v_gqa     = 1024\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_norm_eps       = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_norm_rms_eps   = 1.0e-06\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_clamp_kqv      = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_max_alibi_bias = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_logit_scale    = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_attn_scale     = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ff             = 12288\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_expert         = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_expert_used    = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: causal attn      = 1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: pooling type     = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope type        = 2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope scaling     = linear\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: freq_base_train  = 1000000.0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: freq_scale_train = 1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ctx_orig_yarn  = 40960\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope_finetuned   = unknown\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_conv       = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_inner      = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_state      = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_dt_rank      = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_dt_b_c_rms   = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model type       = 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model params     = 8.19 B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: general.name     = Qwen3 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab type       = BPE\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_vocab          = 151936\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_merges         = 151387\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: BOS token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOS token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOT token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: PAD token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: LF token         = 198 &amp;#39;Ċ&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PRE token    = 151659 &amp;#39;&amp;lt;|fim_prefix|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SUF token    = 151661 &amp;#39;&amp;lt;|fim_suffix|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM MID token    = 151660 &amp;#39;&amp;lt;|fim_middle|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PAD token    = 151662 &amp;#39;&amp;lt;|fim_pad|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM REP token    = 151663 &amp;#39;&amp;lt;|repo_name|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SEP token    = 151664 &amp;#39;&amp;lt;|file_sep|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151662 &amp;#39;&amp;lt;|fim_pad|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151663 &amp;#39;&amp;lt;|repo_name|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151664 &amp;#39;&amp;lt;|file_sep|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: max token length = 256\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_tensors: loading model tensors, this can take a while... (mmap = false)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:06 | 200 |      21.813µs |       127.0.0.1 | HEAD     &amp;quot;/&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:06 | 200 |      55.253µs |       127.0.0.1 | GET      &amp;quot;/api/ps&amp;quot;\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors: offloading 7 repeating layers to GPU\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors: offloaded 7/37 layers to GPU\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:    CUDA_Host model buffer size =  3804.56 MiB\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:        CUDA0 model buffer size =   839.23 MiB\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:          CPU model buffer size =   333.84 MiB\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: constructing llama_context\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_seq_max     = 1\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx         = 32768\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx_per_seq = 32768\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_batch       = 512\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ubatch      = 512\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: causal_attn   = 1\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: flash_attn    = 0\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: freq_base     = 1000000.0\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: freq_scale    = 1\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx_per_seq (32768) &amp;lt; n_ctx_train (40960) -- the full capacity of the model will not be utilized\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context:        CPU  output buffer size =     0.60 MiB\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: kv_size = 32768, type_k = &amp;#39;f16&amp;#39;, type_v = &amp;#39;f16&amp;#39;, n_layer = 36, can_shift = 1, padding = 32\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:        CPU KV buffer size =  3712.00 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: KV self size  = 4608.00 MiB, K (f16): 2304.00 MiB, V (f16): 2304.00 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context:      CUDA0 compute buffer size =  2328.00 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context:  CUDA_Host compute buffer size =    72.01 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context: graph nodes  = 1374\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context: graph splits = 381 (with bs=512), 61 (with bs=1)\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:11.175+02:00 level=INFO source=server.go:637 msg=&amp;quot;llama runner started in 5.02 seconds\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mau9os",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "0-sigma-0",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mau9os/low_perfomance_on_contionue_extension_vs_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mau9os/low_perfomance_on_contionue_extension_vs_code/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753642277,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve spent a good amount of time enjoying narrative driven games and open world style games alike. I wonder how much nondeterminism through “AI” can enhance the experience. I’ve had claude 3.5 (or 3.7 can’t really remember) write stories for me from a seed concept, and they did alright. But I definitely needed to “anchor” the llm to make the story progress in an appealing manner.\n\n\nI asked the gpt about this topic and some interesting papers came up. Anyone have any interesting papers, blog posts, or just thoughts on this subject?\n",
          "author_fullname": "t2_mcazknoi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Non-deterministic Dialogue in games, how much would LLMs really help here?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1malsbp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753621482,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve spent a good amount of time enjoying narrative driven games and open world style games alike. I wonder how much nondeterminism through “AI” can enhance the experience. I’ve had claude 3.5 (or 3.7 can’t really remember) write stories for me from a seed concept, and they did alright. But I definitely needed to “anchor” the llm to make the story progress in an appealing manner.&lt;/p&gt;\n\n&lt;p&gt;I asked the gpt about this topic and some interesting papers came up. Anyone have any interesting papers, blog posts, or just thoughts on this subject?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1malsbp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "m1tm0",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1malsbp/nondeterministic_dialogue_in_games_how_much_would/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1malsbp/nondeterministic_dialogue_in_games_how_much_would/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753621482,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Someone hacked our Portkey, and Okay, this is wild: our Portkey logs just coughed up the entire system prompt + live session history for Claude Code 🤯 ",
          "author_fullname": "t2_xdvmrlo3w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Claude Code Full System prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma2ayu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 121,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 121,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4775ce8043fee3c6d226684a92e2c5b5b1eb40be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753558804,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Someone hacked our Portkey, and Okay, this is wild: our Portkey logs just coughed up the entire system prompt + live session history for Claude Code 🤯 &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/kn1026/cc/blob/main/claudecode.md",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?auto=webp&amp;s=1fdbbc07487a7cd64cd35156e5af200435d6fec0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7af339a9d5dde7cda6aed95a03bb236b15425697",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b315859b94be06b87be20f5eb10d7c1e0cc83d9f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e804d800b45ef5417f5c93802b2efe543b8b62a7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6821d042cf6adf227a28313e944ec49f3976b9e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=35f9681fe0ac1eaadc79cf37bbd0223b5dff7ec6",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=713e2c941ea786d14da1c8a3a519b0a5f6979ab2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ma2ayu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Haunting_Forever_243",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma2ayu/claude_code_full_system_prompt/",
          "stickied": false,
          "url": "https://github.com/kn1026/cc/blob/main/claudecode.md",
          "subreddit_subscribers": 505617,
          "created_utc": 1753558804,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Demo of Video &amp; Image Generation Model Wan 2.2: https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;s=19",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen's Wan 2.2 is coming soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9s2nt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 437,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 437,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/a_RYuyJg0Y2qxFSvHMn6iMkzVUZShPN9aIW-PxpdVrc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753532816,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Demo of Video &amp;amp; Image Generation Model Wan 2.2: &lt;a href=\"https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;amp;s=19\"&gt;https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mtc9shncp7ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?auto=webp&amp;s=13038b2e5cabdab17f7146916670450934352910",
                  "width": 1125,
                  "height": 2001
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7350719926edf7a7fd256eaf4a9562c633f266e",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be066c000b2afe9789c7a5a161a6e0b56109e54f",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de88137704a1cf086d93d2d8a857962fc7b464e1",
                    "width": 320,
                    "height": 569
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=998d71120de7bc728049481e5ff3f990f04f9487",
                    "width": 640,
                    "height": 1138
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e8313466418e7f615401d3b22204cbc5b51f0fa",
                    "width": 960,
                    "height": 1707
                  },
                  {
                    "url": "https://preview.redd.it/mtc9shncp7ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e369cbf26f01d52c5b4c778a55e30e8f0379ec9e",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "Xqg6uFhh27I2vmCZoMI0hRqtO8XTqhzv8rJWvsIB-IQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9s2nt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 76,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9s2nt/qwens_wan_22_is_coming_soon/",
          "stickied": false,
          "url": "https://i.redd.it/mtc9shncp7ff1.jpeg",
          "subreddit_subscribers": 505617,
          "created_utc": 1753532816,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone you Hostinger . As ollama hosting ? If so what do you think ?",
          "author_fullname": "t2_33j4dylg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hostinger ollama hosting review ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mawtr7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753648491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone you Hostinger . As ollama hosting ? If so what do you think ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mawtr7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wbiggs205",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mawtr7/hostinger_ollama_hosting_review/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mawtr7/hostinger_ollama_hosting_review/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753648491,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China Launches Its First 6nm GPUs For Gaming &amp; AI, the Lisuan 7G106 12 GB &amp; 7G105 24 GB, Up To 24 TFLOPs, Faster Than RTX 4060 In Synthetic Benchmarks &amp; Even Runs Black Myth Wukong at 4K High With Playable FPS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9sejp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 335,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 335,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=140&amp;height=79&amp;crop=140:79,smart&amp;auto=webp&amp;s=692bb73e7c2d5021a974aee11185aef9d2560106",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753533789,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/china-launches-first-6nm-gpus-gaming-ai-lisuan-7g106-12-gb-7g105-24-gb-faster-than-rtx-4060-black-myth-wukong-4k/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?auto=webp&amp;s=fb3beda800f833c528344805c6f2e6da5d1ae0ea",
                  "width": 728,
                  "height": 415
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4fa57b5ade6205fa6dcb63d3ab64af600f2a30bb",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f88fc32c5706b40c9861916cd26d37c32d7e8e0a",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6e4725b8b03d51ccc3f7407a0de9acffbda0182",
                    "width": 320,
                    "height": 182
                  },
                  {
                    "url": "https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cec9fdd29b77a480fd506ae3a930fe89f5eba9d6",
                    "width": 640,
                    "height": 364
                  }
                ],
                "variants": {},
                "id": "ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9sejp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 95,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/",
          "stickied": false,
          "url": "https://wccftech.com/china-launches-first-6nm-gpus-gaming-ai-lisuan-7g106-12-gb-7g105-24-gb-faster-than-rtx-4060-black-myth-wukong-4k/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753533789,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey LocalLLaMA (big fan)!\n\nI made an app called Aeru, an app that uses Apple's Foundation Models framework but given more features like RAG support and Web Search! It's all private, local, free, and open source!\n\nI wanted to make this app because I was really intrigued by Apple's Foundation Models framework, and noticed it didn't come with any support for RAG or Web Search and other features, so I made them up from scratch using SVDB for vector storage and SwiftSoup for HTML parsing.  \n  \nThis was more of a hackathon project and I just wanted to release it, if people really like the idea then I will expand on it!\n\n[RAG Demo](https://reddit.com/link/1mapwdm/video/5tvd82xxvfff1/player)\n\nTo download it on TestFlight, your iOS device must be Apple Intelligence compatible (iPhone 15 Pro or higher end model)\n\nThank you!\n\nTestFlight link: [https://testflight.apple.com/join/6gaB7S1R](https://testflight.apple.com/join/6gaB7S1R)\n\nGithub link: [https://github.com/sskarz/Aeru-AI](https://github.com/sskarz/Aeru-AI)",
          "author_fullname": "t2_uk9lo6j7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Apple Intelligence but with multiple chats, RAG, and Web Search",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5tvd82xxvfff1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mapwdm/asset/5tvd82xxvfff1/DASHPlaylist.mpd?a=1756257402%2CYjk3YzMzZmI1MWZmODVjZmJlOTM0MzVkNzk2NTNhYzQwYjU1YjJmMTc1MzY3ZmNmMjE5OTJlYjdiMjBhZjE0NA%3D%3D&amp;v=1&amp;f=sd",
              "x": 590,
              "y": 1280,
              "hlsUrl": "https://v.redd.it/link/1mapwdm/asset/5tvd82xxvfff1/HLSPlaylist.m3u8?a=1756257402%2CY2FlOTk0NDFlOTEzZmJjNDUyZTRjZjcwM2E0MWM4ZWY3NWQ0OWY4YTY1MjM3MmI3YjRlMDMyNjg1ZGZlNzQ4Zg%3D%3D&amp;v=1&amp;f=sd",
              "id": "5tvd82xxvfff1",
              "isGif": false
            }
          },
          "name": "t3_1mapwdm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NVh7CqmCKf_j3MSVTpACSGbzlIgeP2Nq8yGxYzoaryc.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=f7e3ed42c8c806e4b0f78009e5e6131bf08c4d93",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753631946,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey LocalLLaMA (big fan)!&lt;/p&gt;\n\n&lt;p&gt;I made an app called Aeru, an app that uses Apple&amp;#39;s Foundation Models framework but given more features like RAG support and Web Search! It&amp;#39;s all private, local, free, and open source!&lt;/p&gt;\n\n&lt;p&gt;I wanted to make this app because I was really intrigued by Apple&amp;#39;s Foundation Models framework, and noticed it didn&amp;#39;t come with any support for RAG or Web Search and other features, so I made them up from scratch using SVDB for vector storage and SwiftSoup for HTML parsing.  &lt;/p&gt;\n\n&lt;p&gt;This was more of a hackathon project and I just wanted to release it, if people really like the idea then I will expand on it!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mapwdm/video/5tvd82xxvfff1/player\"&gt;RAG Demo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To download it on TestFlight, your iOS device must be Apple Intelligence compatible (iPhone 15 Pro or higher end model)&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;TestFlight link: &lt;a href=\"https://testflight.apple.com/join/6gaB7S1R\"&gt;https://testflight.apple.com/join/6gaB7S1R&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Github link: &lt;a href=\"https://github.com/sskarz/Aeru-AI\"&gt;https://github.com/sskarz/Aeru-AI&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NVh7CqmCKf_j3MSVTpACSGbzlIgeP2Nq8yGxYzoaryc.png?auto=webp&amp;s=a3267c0e2827912a7aaff3f846b8c45940c7cc50",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NVh7CqmCKf_j3MSVTpACSGbzlIgeP2Nq8yGxYzoaryc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e9803257a9c0433a006bd52ab2e684abe03edd9",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NVh7CqmCKf_j3MSVTpACSGbzlIgeP2Nq8yGxYzoaryc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e272e2f9ad0aca8232b0d1912ae6fd835e7e22d0",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/NVh7CqmCKf_j3MSVTpACSGbzlIgeP2Nq8yGxYzoaryc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7794fb4a352c665ab7362d1f9e7bd2c354922f0",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NVh7CqmCKf_j3MSVTpACSGbzlIgeP2Nq8yGxYzoaryc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e8ec1e230bb99cdfda65b22d09bed779075c659",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/NVh7CqmCKf_j3MSVTpACSGbzlIgeP2Nq8yGxYzoaryc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3aebbbb16b71796acb1a4a3bfd42097870d02ad0",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "NVh7CqmCKf_j3MSVTpACSGbzlIgeP2Nq8yGxYzoaryc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mapwdm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sskarz1016",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mapwdm/apple_intelligence_but_with_multiple_chats_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mapwdm/apple_intelligence_but_with_multiple_chats_rag/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753631946,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi\n\nAnyone has any prompts I can use to make local base model reason?\n\nDo share! Thank you ",
          "author_fullname": "t2_1t3515o2d2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reasoning prompt strategy",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mam8p4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753622752,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;Anyone has any prompts I can use to make local base model reason?&lt;/p&gt;\n\n&lt;p&gt;Do share! Thank you &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mam8p4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rockybaby2025",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mam8p4/reasoning_prompt_strategy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mam8p4/reasoning_prompt_strategy/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753622752,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey Folks \n\nNeed GPU selection suggestion before i make the purchase\n\nWhere i live, i am getting GeForce RTX 5060 Ti 16GB GDDR7 at USD 500 , buying 4 of these devices would be a good choice (yes i will also be buying  new RIG / CPU / MB/ PS), hence not worrying about backward compatibility.\n\nMy use case : (Is not gaming) i want to use these devices for LLM inferencing (say Llama / DeepSeek etc) as well as fine-tuning (for my fun projects/side gigs). Hence i would need a large VRAM , getting a 64GB vRAM device is super expensive. So i am considering if i can today start with 2 x GeForce RTX 5060 Ti 16GB , this gets me to 32GB of VRAM and then later add 2 more of these and get 64GB VRAM.\n\nNeed your suggestions on if this approach suffice my use case, should i consider any other device type etc.\n\nWould there be hard challenges in combining GPU memory from 4 cards and use the combined memory for large model inferencing ? also for Fine-tuning. Wondering if someone has achieved this setup ?\n\n🙏",
          "author_fullname": "t2_b9h602vjf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GeForce RTX 5060 Ti 16GB good for LLama LLM inferencing/Fintuning ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mam6of",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753622591,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks &lt;/p&gt;\n\n&lt;p&gt;Need GPU selection suggestion before i make the purchase&lt;/p&gt;\n\n&lt;p&gt;Where i live, i am getting GeForce RTX 5060 Ti 16GB GDDR7 at USD 500 , buying 4 of these devices would be a good choice (yes i will also be buying  new RIG / CPU / MB/ PS), hence not worrying about backward compatibility.&lt;/p&gt;\n\n&lt;p&gt;My use case : (Is not gaming) i want to use these devices for LLM inferencing (say Llama / DeepSeek etc) as well as fine-tuning (for my fun projects/side gigs). Hence i would need a large VRAM , getting a 64GB vRAM device is super expensive. So i am considering if i can today start with 2 x GeForce RTX 5060 Ti 16GB , this gets me to 32GB of VRAM and then later add 2 more of these and get 64GB VRAM.&lt;/p&gt;\n\n&lt;p&gt;Need your suggestions on if this approach suffice my use case, should i consider any other device type etc.&lt;/p&gt;\n\n&lt;p&gt;Would there be hard challenges in combining GPU memory from 4 cards and use the combined memory for large model inferencing ? also for Fine-tuning. Wondering if someone has achieved this setup ?&lt;/p&gt;\n\n&lt;p&gt;🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mam6of",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kingksingh",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753622591,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for CJK data on hugging face. I don't see any high quality data sets. If you have any recommendations, I'd appreciate it.",
          "author_fullname": "t2_1ueto0o831",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any CJK datas?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1malf9l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753620433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for CJK data on hugging face. I don&amp;#39;t see any high quality data sets. If you have any recommendations, I&amp;#39;d appreciate it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1malf9l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DependentDazzling703",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1malf9l/any_cjk_datas/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1malf9l/any_cjk_datas/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753620433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Apart from RAM &amp; GPU upgrades. I use Jan &amp; Kobaldcpp.\n\nFound few things from online on this.\n\n* Picking Quantized model fittable to System VRAM\n* Set Q8\\_0(instead of 16) for KV Cache\n* Use Recommended Settings(Temperature, TopP, TopK, MinP) for models(Mostly from Model cards on HuggingFace)\n* Decent Prompts\n\nWhat else could help to get faster response with some more tokens?\n\nI'm not expecting too much for my 8GB VRAM(32 GB RAM), just even another bunch of additional tokens fine for me.\n\nSystem Spec : Intel(R) Core(TM) i7-14700HX 2.10 GHz NVIDIA GeForce RTX 4060\n\nTried below simple prompt to test some models with Context 32768, GPU Layers -1:\n\nTemperature 0.7, TopK 20, TopP 0.8, MinP 0.\n\nwho are you? Provide all details about you /no\\_think\n\n* Qwen3 0.6B Q8 - 120 tokens/sec (Typically **70-80** tokens/sec)\n* Qwen3 1.7B Q8 -   65 tokens/sec (Typically **50-60** tokens/sec)\n* Qwen3 4B Q6   -   25 tokens/sec (Typically **20** tokens/sec)\n* Qwen3 8B Q4   -   10 tokens/sec (Typically **7-9** tokens/sec)\n* Qwen3 30B A3B Q4 - 2 tokens/sec (Typically **1** tokens/sec)\n\nPoor GPU Club members(\\~8GB VRAM) .... Are you getting similar tokens/sec? If you're getting more tokens, what have you done for that? please share.\n\nI'm sure I'm doing something wrong on few things here, please help me on this. Thanks.",
          "author_fullname": "t2_1deiadfhb1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to increase tps Tokens/Second? Other ways to optimize things to get faster response",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mau1nz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753642651,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753641743,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apart from RAM &amp;amp; GPU upgrades. I use Jan &amp;amp; Kobaldcpp.&lt;/p&gt;\n\n&lt;p&gt;Found few things from online on this.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Picking Quantized model fittable to System VRAM&lt;/li&gt;\n&lt;li&gt;Set Q8_0(instead of 16) for KV Cache&lt;/li&gt;\n&lt;li&gt;Use Recommended Settings(Temperature, TopP, TopK, MinP) for models(Mostly from Model cards on HuggingFace)&lt;/li&gt;\n&lt;li&gt;Decent Prompts&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What else could help to get faster response with some more tokens?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not expecting too much for my 8GB VRAM(32 GB RAM), just even another bunch of additional tokens fine for me.&lt;/p&gt;\n\n&lt;p&gt;System Spec : Intel(R) Core(TM) i7-14700HX 2.10 GHz NVIDIA GeForce RTX 4060&lt;/p&gt;\n\n&lt;p&gt;Tried below simple prompt to test some models with Context 32768, GPU Layers -1:&lt;/p&gt;\n\n&lt;p&gt;Temperature 0.7, TopK 20, TopP 0.8, MinP 0.&lt;/p&gt;\n\n&lt;p&gt;who are you? Provide all details about you /no_think&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 0.6B Q8 - 120 tokens/sec (Typically &lt;strong&gt;70-80&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;li&gt;Qwen3 1.7B Q8 -   65 tokens/sec (Typically &lt;strong&gt;50-60&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;li&gt;Qwen3 4B Q6   -   25 tokens/sec (Typically &lt;strong&gt;20&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;li&gt;Qwen3 8B Q4   -   10 tokens/sec (Typically &lt;strong&gt;7-9&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B Q4 - 2 tokens/sec (Typically &lt;strong&gt;1&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Poor GPU Club members(~8GB VRAM) .... Are you getting similar tokens/sec? If you&amp;#39;re getting more tokens, what have you done for that? please share.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure I&amp;#39;m doing something wrong on few things here, please help me on this. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mau1nz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pmttyji",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753641743,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From the Readme: “We are excited to introduce Ling-lite-1.5-2506, the updated version of our highly capable Ling-lite-1.5 model.\n\nLing-lite-1.5-2506 boasts 16.8 billion parameters with 2.75 billion activated parameters, building upon its predecessor with significant advancements across the board, featuring the following key improvements:\n\n- Reasoning and Knowledge: Significant gains in general intelligence, logical reasoning, and complex problem-solving abilities. For instance, in GPQA Diamond, Ling-lite-1.5-2506 achieves 53.79%, a substantial lead over Ling-lite-1.5's 36.55%.\n- Coding Capabilities: A notable enhancement in coding and debugging prowess. For instance,in LiveCodeBench 2408-2501, a critical and highly popular programming benchmark, Ling-lite-1.5-2506 demonstrates improved performance with 26.97% compared to Ling-lite-1.5's 22.22%.”\n\nPaper: https://huggingface.co/papers/2503.05139\n",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "inclusionAI/Ling-lite-1.5-2506 (16.8B total, 2.75B active, MIT license)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9y506",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 105,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 105,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=df4ee3da0ac24982d7470071054e9ddfcaedb3d7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753548535,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From the Readme: “We are excited to introduce Ling-lite-1.5-2506, the updated version of our highly capable Ling-lite-1.5 model.&lt;/p&gt;\n\n&lt;p&gt;Ling-lite-1.5-2506 boasts 16.8 billion parameters with 2.75 billion activated parameters, building upon its predecessor with significant advancements across the board, featuring the following key improvements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Reasoning and Knowledge: Significant gains in general intelligence, logical reasoning, and complex problem-solving abilities. For instance, in GPQA Diamond, Ling-lite-1.5-2506 achieves 53.79%, a substantial lead over Ling-lite-1.5&amp;#39;s 36.55%.&lt;/li&gt;\n&lt;li&gt;Coding Capabilities: A notable enhancement in coding and debugging prowess. For instance,in LiveCodeBench 2408-2501, a critical and highly popular programming benchmark, Ling-lite-1.5-2506 demonstrates improved performance with 26.97% compared to Ling-lite-1.5&amp;#39;s 22.22%.”&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://huggingface.co/papers/2503.05139\"&gt;https://huggingface.co/papers/2503.05139&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/inclusionAI/Ling-lite-1.5-2506",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?auto=webp&amp;s=f64afba829e86afae636cfa89d8b629473344cde",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8296a7257d0017a5ea7dbf418ca4a2ddfb9e318d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d40ddd1252be4a590b3d25bd63db318622713834",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9915108fff5b8cee702fecab2863aaf0517d62c5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=404f4d1a954e64355fe9daae161dfd9814fe8b80",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e54c747b540171e887944a6012c6c474369fa20f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd92d7e54cef9052509e5ce3003f2f70ef267830",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9y506",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9y506/inclusionailinglite152506_168b_total_275b_active/",
          "stickied": false,
          "url": "https://huggingface.co/inclusionAI/Ling-lite-1.5-2506",
          "subreddit_subscribers": 505617,
          "created_utc": 1753548535,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I diffed the config.json between Llama-3\\_3-Nemotron-Super-49B-v1 and Llama-3\\_3-Nemotron-Super-49B-v1\\_5. I noticed the only difference is that the newer model doubled the RoPE scaling factor from 8 to 16. What effect does this make to the model's performance?",
          "author_fullname": "t2_s6sfw4yy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What will happen to an llm when you double the RoPE scaling factor?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maeuuo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753596107,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I diffed the config.json between Llama-3_3-Nemotron-Super-49B-v1 and Llama-3_3-Nemotron-Super-49B-v1_5. I noticed the only difference is that the newer model doubled the RoPE scaling factor from 8 to 16. What effect does this make to the model&amp;#39;s performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maeuuo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Warning2146",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maeuuo/what_will_happen_to_an_llm_when_you_double_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maeuuo/what_will_happen_to_an_llm_when_you_double_the/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753596107,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi people \n\nBeen working on a local agent MVP these 3 last weeks. To summarise newsletters and plugged into your private projects would then offer unique insights and suggestions from the newsletters to keep you competitive and enhance your productivity.\n\nI've implemented a baseline RAG under Ollama using Llama index, ChromaDB for ingestion and indexing, as well as Langchain for the orchestration.\n\nI'm realizing that the insights synthesized by similarity search method (between the newsletters and the ingested user context) is mediocre, and planning on shifting to a knowledge graph for the RAG, to create a more powerful semantic representation of the user context, which should enable a more relevant insight generation.\n\nThe problem is, I have 7 days from now to complete it before submitting the MVP for an investor pitch. How realistic is that ? \n\nThanks for any help ",
          "author_fullname": "t2_1tsmkqb3yj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GRAPH RAG vs baseline RAG for MVP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mas4nn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753637180,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people &lt;/p&gt;\n\n&lt;p&gt;Been working on a local agent MVP these 3 last weeks. To summarise newsletters and plugged into your private projects would then offer unique insights and suggestions from the newsletters to keep you competitive and enhance your productivity.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve implemented a baseline RAG under Ollama using Llama index, ChromaDB for ingestion and indexing, as well as Langchain for the orchestration.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m realizing that the insights synthesized by similarity search method (between the newsletters and the ingested user context) is mediocre, and planning on shifting to a knowledge graph for the RAG, to create a more powerful semantic representation of the user context, which should enable a more relevant insight generation.&lt;/p&gt;\n\n&lt;p&gt;The problem is, I have 7 days from now to complete it before submitting the MVP for an investor pitch. How realistic is that ? &lt;/p&gt;\n\n&lt;p&gt;Thanks for any help &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mas4nn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ctxgen_founder",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753637180,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.*",
          "author_fullname": "t2_iol3buybk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Alibaba Paper - Group Sequence Policy Optimization",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ywng",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": 1753561318,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753550375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.18071",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9ywng",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thrumpwart",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ywng/qwenalibaba_paper_group_sequence_policy/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.18071",
          "subreddit_subscribers": 505617,
          "created_utc": 1753550375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys,\n\nI'm looking for a motherboard that supports an AM5 CPU and three GPUs: two 3090s and one 5070 Ti.\nI found a motherboard with three PCI Express ports, but it appears that only the first runs at 16x. The other two run at 8x and 4x.\nDoes PCI speed have an impact when using it for LLM?\nI've heard about workstation motherboard cards. Are they worth it? If so, which one do you recommend?\n\nThanks for the help!",
          "author_fullname": "t2_e0z2m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Motherboard for AM5 CPU and 3 GPUS (2 3090 and 1 5070 ti)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mal0bo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753619168,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a motherboard that supports an AM5 CPU and three GPUs: two 3090s and one 5070 Ti.\nI found a motherboard with three PCI Express ports, but it appears that only the first runs at 16x. The other two run at 8x and 4x.\nDoes PCI speed have an impact when using it for LLM?\nI&amp;#39;ve heard about workstation motherboard cards. Are they worth it? If so, which one do you recommend?&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mal0bo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ed0c",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mal0bo/motherboard_for_am5_cpu_and_3_gpus_2_3090_and_1/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mal0bo/motherboard_for_am5_cpu_and_3_gpus_2_3090_and_1/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753619168,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I noticed in this wonderful guide [https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune) a parameter for running the model \\`--prio 2\\` but I cannot find any documentation on what this is doing, nor do I see a difference when running the model with or without it. ",
          "author_fullname": "t2_ayi7twno",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does --prio 2 do in llama.cpp? Can't find documentation  :(",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mapvcv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753631877,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I noticed in this wonderful guide &lt;a href=\"https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune\"&gt;https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune&lt;/a&gt; a parameter for running the model `--prio 2` but I cannot find any documentation on what this is doing, nor do I see a difference when running the model with or without it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?auto=webp&amp;s=df3ed66f8b8e54b17c699d9c4e81b03ddeb78c58",
                  "width": 1200,
                  "height": 590
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fa9ec0bda4ae81d05efe9ff0a296be82987e912",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=18872cd0af37e87d93cf5b6c098630c44f40a162",
                    "width": 216,
                    "height": 106
                  },
                  {
                    "url": "https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8392e0cb89db800c200421873b07e92f34150fe",
                    "width": 320,
                    "height": 157
                  },
                  {
                    "url": "https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f6fc5d8f727ab6f86a8ca5f94a5091bbe81d025",
                    "width": 640,
                    "height": 314
                  },
                  {
                    "url": "https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=26fa346a0f27ac195ecf2f29e1d997a534a3b283",
                    "width": 960,
                    "height": 472
                  },
                  {
                    "url": "https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e4e7bc3c126d7465ae2f4d8fab93d8c6edd76c4",
                    "width": 1080,
                    "height": 531
                  }
                ],
                "variants": {},
                "id": "ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mapvcv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shrug_hellifino",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mapvcv/what_does_prio_2_do_in_llamacpp_cant_find/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mapvcv/what_does_prio_2_do_in_llamacpp_cant_find/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753631877,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m genuinely fascinated by artificial intelligence and convinced it’s going to reshape the world. But I have no technical skills and no money to contribute to its progress directly. I can just use and admire. That’s why I came up with this idea.\n\nWhat if we had a platform — a website with WebGPU or an app — where people could share part of their unused computing power (CPU, GPU, RAM) with companies or individuals working on AI ?\n\nThe idea\n\nA distributed computing platform where people volunteer their hardware to support meaningful projects:\n\nWell-known companies (like Google, Mistral, deepseek, Anthropic…) wouldn’t have to share their code. We trust them — they’d just need to show a simple progress bar or basic usage stats.\n\nIndividuals or small independent projects would need to be fully transparent: share their code/scripts, display logs, and offer a public dashboard showing training progress or computation status.\n\n\nThe goal\n\nTo accelerate important work by tapping into the unused resources of thousands of personal computers around the world — and to give people a way to contribute to progress without needing money or deep expertise.\n\nPotential issues (and a few ideas to address them)\n\nPeople might shut down their machines anytime\n→ Use checkpointing and task splitting so work isn’t lost and can be resumed elsewhere.\n\nRisk of malicious or abusive code\n→ Run everything in isolated containers (Docker, WASM, etc.) with automated security checks.\n\nHow to ensure transparency and accountability?\n→ Every project (except trusted ones) must have a public dashboard with real-time logs and metrics.\n\nWhat’s in it for contributors?\n→ Mostly non-monetary rewards like badges, leaderboards, early access to results — or optionally, micro-payments per task if a project offers it.\n\n\n\n---\n\nI don’t have the skills to build something like this myself, but if the idea gets attention, maybe someone out there who can build it will take it further. Curious to hear what you all think — is this already being done? Is it even feasible at scale?",
          "author_fullname": "t2_1olscdn38b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Advance humanity on our scale.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maxmeg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753650468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m genuinely fascinated by artificial intelligence and convinced it’s going to reshape the world. But I have no technical skills and no money to contribute to its progress directly. I can just use and admire. That’s why I came up with this idea.&lt;/p&gt;\n\n&lt;p&gt;What if we had a platform — a website with WebGPU or an app — where people could share part of their unused computing power (CPU, GPU, RAM) with companies or individuals working on AI ?&lt;/p&gt;\n\n&lt;p&gt;The idea&lt;/p&gt;\n\n&lt;p&gt;A distributed computing platform where people volunteer their hardware to support meaningful projects:&lt;/p&gt;\n\n&lt;p&gt;Well-known companies (like Google, Mistral, deepseek, Anthropic…) wouldn’t have to share their code. We trust them — they’d just need to show a simple progress bar or basic usage stats.&lt;/p&gt;\n\n&lt;p&gt;Individuals or small independent projects would need to be fully transparent: share their code/scripts, display logs, and offer a public dashboard showing training progress or computation status.&lt;/p&gt;\n\n&lt;p&gt;The goal&lt;/p&gt;\n\n&lt;p&gt;To accelerate important work by tapping into the unused resources of thousands of personal computers around the world — and to give people a way to contribute to progress without needing money or deep expertise.&lt;/p&gt;\n\n&lt;p&gt;Potential issues (and a few ideas to address them)&lt;/p&gt;\n\n&lt;p&gt;People might shut down their machines anytime\n→ Use checkpointing and task splitting so work isn’t lost and can be resumed elsewhere.&lt;/p&gt;\n\n&lt;p&gt;Risk of malicious or abusive code\n→ Run everything in isolated containers (Docker, WASM, etc.) with automated security checks.&lt;/p&gt;\n\n&lt;p&gt;How to ensure transparency and accountability?\n→ Every project (except trusted ones) must have a public dashboard with real-time logs and metrics.&lt;/p&gt;\n\n&lt;p&gt;What’s in it for contributors?\n→ Mostly non-monetary rewards like badges, leaderboards, early access to results — or optionally, micro-payments per task if a project offers it.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I don’t have the skills to build something like this myself, but if the idea gets attention, maybe someone out there who can build it will take it further. Curious to hear what you all think — is this already being done? Is it even feasible at scale?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maxmeg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud_Possibility_148",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maxmeg/advance_humanity_on_our_scale/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maxmeg/advance_humanity_on_our_scale/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753650468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Suggestions as to what you've found worth using / keeping vs. not?\n\nWhat specific older models or older model / use case combinations from 2023-2024 would you emphatically NOT consider wholly obsoleted by newer models?\n\nLocal model obsolescence decisions for personal STEM / utility / english / Q&amp;A / RAG / tool use / IT / desktop / workstation use cases?\n\nSo we've had quite a lot of LLM, VLM models released now from the original llama up through what's come out in the past weeks.\n\nRelative to having local models spanning that time frame ready for personal use for desktop / workstation / STEM / english / Q&amp;A / LLM / visual Q&amp;A, speaking of models in the 4B-250B range MoE &amp; dense categories we've had bunches around 7-14B, 20-32B, 70B, 100-250B.\n\nSome of the ones from 6-8 months ago, 12 months ago, 18-24 months ago are / were quite useful / good, but many of the newer ones in similar size ranges are probably better at most things. \n\n70-120B is awkward since there's been less new models in those size ranges though some 32Bs or quants of 230Bs could perform better than old 70-120B dense models in most cases.\n\nAnyway I'm trying to decide for those broad but not all encompassing (no literary fiction compositions, erp, heavy multi-lingual besides casual translation &amp; summarization of web &amp; pub) use cases where to draw the line and just say almost everything before 1H 2024 or whatever criteria one can devise is effectively obsoleted by something free to use / liberally licensed / similar or smaller size with similar or better local runtime performance.\n\ne.g. Deepseek V2.5 vs. Qwen3-235 or such.  LLama2/3.x 7-70B vs newer stuff.  Coding models older than qwen2.5 (obviously qwen-3 small coding models aren't out yet so it's hard to say nothing previous is entirely obsolete..?).\n\nOlder mistral / gemma / command-r / qwen / glm / nous / fine-tunes etc. etc.?\n\nVLMs from the older paligemma up through the early 2024 times vs Q4 2024 and newer releases for casual V-Q&amp;A / OCR / etc.?\n\nBut then even the older QWQ still seems to bench well against newer models.\n\nThe point is not to throw out the baby with the bathwater and keep in mind / availability things that are still gems or outperforming for some use cases.\n\nAlso if new models might \"benchmax\" or limit the width / breadth of training focus to improve and focus performance in narrow areas there's something to be said for ones more generalist or less prone to follow over-trained over-fitted patterns if there's stars in those areas that might be less \"optimized\".",
          "author_fullname": "t2_tailqi90",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM / VLM Local model obsolescence decisions for personal STEM / utility / english / Q&amp;A / RAG / tool use / IT desktop / workstation use cases?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maoody",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753628996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suggestions as to what you&amp;#39;ve found worth using / keeping vs. not?&lt;/p&gt;\n\n&lt;p&gt;What specific older models or older model / use case combinations from 2023-2024 would you emphatically NOT consider wholly obsoleted by newer models?&lt;/p&gt;\n\n&lt;p&gt;Local model obsolescence decisions for personal STEM / utility / english / Q&amp;amp;A / RAG / tool use / IT / desktop / workstation use cases?&lt;/p&gt;\n\n&lt;p&gt;So we&amp;#39;ve had quite a lot of LLM, VLM models released now from the original llama up through what&amp;#39;s come out in the past weeks.&lt;/p&gt;\n\n&lt;p&gt;Relative to having local models spanning that time frame ready for personal use for desktop / workstation / STEM / english / Q&amp;amp;A / LLM / visual Q&amp;amp;A, speaking of models in the 4B-250B range MoE &amp;amp; dense categories we&amp;#39;ve had bunches around 7-14B, 20-32B, 70B, 100-250B.&lt;/p&gt;\n\n&lt;p&gt;Some of the ones from 6-8 months ago, 12 months ago, 18-24 months ago are / were quite useful / good, but many of the newer ones in similar size ranges are probably better at most things. &lt;/p&gt;\n\n&lt;p&gt;70-120B is awkward since there&amp;#39;s been less new models in those size ranges though some 32Bs or quants of 230Bs could perform better than old 70-120B dense models in most cases.&lt;/p&gt;\n\n&lt;p&gt;Anyway I&amp;#39;m trying to decide for those broad but not all encompassing (no literary fiction compositions, erp, heavy multi-lingual besides casual translation &amp;amp; summarization of web &amp;amp; pub) use cases where to draw the line and just say almost everything before 1H 2024 or whatever criteria one can devise is effectively obsoleted by something free to use / liberally licensed / similar or smaller size with similar or better local runtime performance.&lt;/p&gt;\n\n&lt;p&gt;e.g. Deepseek V2.5 vs. Qwen3-235 or such.  LLama2/3.x 7-70B vs newer stuff.  Coding models older than qwen2.5 (obviously qwen-3 small coding models aren&amp;#39;t out yet so it&amp;#39;s hard to say nothing previous is entirely obsolete..?).&lt;/p&gt;\n\n&lt;p&gt;Older mistral / gemma / command-r / qwen / glm / nous / fine-tunes etc. etc.?&lt;/p&gt;\n\n&lt;p&gt;VLMs from the older paligemma up through the early 2024 times vs Q4 2024 and newer releases for casual V-Q&amp;amp;A / OCR / etc.?&lt;/p&gt;\n\n&lt;p&gt;But then even the older QWQ still seems to bench well against newer models.&lt;/p&gt;\n\n&lt;p&gt;The point is not to throw out the baby with the bathwater and keep in mind / availability things that are still gems or outperforming for some use cases.&lt;/p&gt;\n\n&lt;p&gt;Also if new models might &amp;quot;benchmax&amp;quot; or limit the width / breadth of training focus to improve and focus performance in narrow areas there&amp;#39;s something to be said for ones more generalist or less prone to follow over-trained over-fitted patterns if there&amp;#39;s stars in those areas that might be less &amp;quot;optimized&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maoody",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Calcidiol",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753628996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Where can I download glossary for Japanese, Chinese and Korean translation to english\n\nDo someone know where can I download glossaries for translation, for things like fanfics of animes, mangas, or even novels?\n\nBecause I tried to make some, and when I used it remarkable improved the translation for some fanfics I was reading, mainly to maintain same translation of character name, places and specific terms through long stories ",
          "author_fullname": "t2_pgj5046o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where can I download glossary for Japanese, Chinese and Korean translation to english",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maoiae",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753628582,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where can I download glossary for Japanese, Chinese and Korean translation to english&lt;/p&gt;\n\n&lt;p&gt;Do someone know where can I download glossaries for translation, for things like fanfics of animes, mangas, or even novels?&lt;/p&gt;\n\n&lt;p&gt;Because I tried to make some, and when I used it remarkable improved the translation for some fanfics I was reading, mainly to maintain same translation of character name, places and specific terms through long stories &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maoiae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PedroHBN",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maoiae/where_can_i_download_glossary_for_japanese/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maoiae/where_can_i_download_glossary_for_japanese/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753628582,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI built a macOS app called [Hyprnote](https://hyprnote.com/) \\- it’s an AI-powered notepad that listens during meetings and turns your rough notes into clean, structured summaries. Everything runs locally on your Mac, so no data ever leaves your device. We even trained our own LLM for this.\n\nWe used to manually scrub through recordings, stitch together notes, and try to make sense of scattered thoughts after every call. That sucked. So we built Hyprnote to fix it - no cloud, no copy-pasting, just fast, private note-taking.\n\nPeople from Fortune 100 companies to doctors, lawyers, therapists - even D&amp;D players - are using it. It works great in air-gapped environments, too.\n\nWould love your honest feedback. If you’re in back-to-back calls or just want a cleaner way to capture ideas, give it a spin and let me know what you think.\n\nYou can check it out at [hyprnote.com](https://hyprnote.com/).\n\nOh we're also [open-source](https://github.com/fastrepl/hyprnote).\n\nThanks!",
          "author_fullname": "t2_j1t6g97wv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a local-first transcribing + summarizing tool that's FREE FOREVER",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9y5cd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/o4y7RxwzayXXX2dHoDraTPU1JJs_dcNv5LAH10qMytg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753548555,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I built a macOS app called &lt;a href=\"https://hyprnote.com/\"&gt;Hyprnote&lt;/a&gt; - it’s an AI-powered notepad that listens during meetings and turns your rough notes into clean, structured summaries. Everything runs locally on your Mac, so no data ever leaves your device. We even trained our own LLM for this.&lt;/p&gt;\n\n&lt;p&gt;We used to manually scrub through recordings, stitch together notes, and try to make sense of scattered thoughts after every call. That sucked. So we built Hyprnote to fix it - no cloud, no copy-pasting, just fast, private note-taking.&lt;/p&gt;\n\n&lt;p&gt;People from Fortune 100 companies to doctors, lawyers, therapists - even D&amp;amp;D players - are using it. It works great in air-gapped environments, too.&lt;/p&gt;\n\n&lt;p&gt;Would love your honest feedback. If you’re in back-to-back calls or just want a cleaner way to capture ideas, give it a spin and let me know what you think.&lt;/p&gt;\n\n&lt;p&gt;You can check it out at &lt;a href=\"https://hyprnote.com/\"&gt;hyprnote.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Oh we&amp;#39;re also &lt;a href=\"https://github.com/fastrepl/hyprnote\"&gt;open-source&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8e5rt1f209ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?auto=webp&amp;s=d69c9447a79d082bf934bd21a817ac8aa0f532b5",
                  "width": 2686,
                  "height": 1721
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4874127c0e1630d96aac6797050fca479bc1ad87",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed464772246513c42e6b8baaaf67310ca1f82339",
                    "width": 216,
                    "height": 138
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee56be1748a3663e33e5333b4da6964c8a021983",
                    "width": 320,
                    "height": 205
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f057b8afe4dc417397387849b50c1cfd1f61b008",
                    "width": 640,
                    "height": 410
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55794cc339d8fb0ed8050774113b23a869817936",
                    "width": 960,
                    "height": 615
                  },
                  {
                    "url": "https://preview.redd.it/8e5rt1f209ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad64ddc801210932be874bcc67b43bc3f93ab8c4",
                    "width": 1080,
                    "height": 691
                  }
                ],
                "variants": {},
                "id": "ognY8OJMxHxpWChA4iPhUNg6wduZhTSeSEqq01K4sLM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9y5cd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "beerbellyman4vr",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9y5cd/i_built_a_localfirst_transcribing_summarizing/",
          "stickied": false,
          "url": "https://i.redd.it/8e5rt1f209ff1.jpeg",
          "subreddit_subscribers": 505617,
          "created_utc": 1753548555,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Latest update: 27/07/2025)\n\nI've just extracted the FULL Lovable Agent system prompt and internal tools (Latest update). Over 600 lines (Around 10k tokens).\n\nYou can check it out here: https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FULL Lovable Agent System Prompt and Tools [UPDATED]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma88wd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753574674,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Latest update: 27/07/2025)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just extracted the FULL Lovable Agent system prompt and internal tools (Latest update). Over 600 lines (Around 10k tokens).&lt;/p&gt;\n\n&lt;p&gt;You can check it out here: &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?auto=webp&amp;s=f37cfa9c3f2cbb711fdf663a623eb27c21625f4e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=220e317b01052edf2315c45470afafcb69ba4f39",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8ec4fb280fa6d7324dd972e866d466854c3e5f2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fbbbfe2eef74228a825fd61d339ea19db999f97e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=91ff0602b78f9f3befd49aaf356818b967b31fff",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=451064fc811402906debea71530001b8787707b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf2191857f81dbd49759e980e987a5dd15b7b7f9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NODN066BA8UYUXFthrn_kVBJ2bZkZU8gTZ9RETJrKts"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ma88wd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma88wd/full_lovable_agent_system_prompt_and_tools_updated/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma88wd/full_lovable_agent_system_prompt_and_tools_updated/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753574674,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nWere there any interesting non-thinking models released by Chinese companies in 2025, except Qwen?\n\nI'm interested in those around 30B size.\n\nThanks! ",
          "author_fullname": "t2_133m0xy6vg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Notable 2025 Chinese models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1manwi5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753627066,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Were there any interesting non-thinking models released by Chinese companies in 2025, except Qwen?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in those around 30B size.&lt;/p&gt;\n\n&lt;p&gt;Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1manwi5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Acrobatic_Cat_3448",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1manwi5/notable_2025_chinese_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1manwi5/notable_2025_chinese_models/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753627066,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've found myself with a pretty amazing opportunity: 500 total hrs on a single AMD MI300X GPU (or the alternative of \\~125 hrs on a node with 8 of them).\n\nI've been studying DL for about 1.5 yrs, so I'm not a complete beginner, but I'm definitely not an expert. My first thought was to just finetune a massive LLM, but I’ve already done that on a smaller scale, so I wouldn’t really be learning anything new.\n\nSo, I've come here looking for ideas/ guidance. What's the most interesting or impactful project you would tackle with this kind of compute? My main goal is to learn as much as possible and create something cool in the process.\n\nWhat would you do?\n\nP.S. A small constraint to consider: billing continues until the instance is destroyed, not just off.",
          "author_fullname": "t2_c1ipban6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Got 500 hours on an AMD MI300X. What's the most impactful thing I can build/train/break?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mantju",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753626852,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found myself with a pretty amazing opportunity: 500 total hrs on a single AMD MI300X GPU (or the alternative of ~125 hrs on a node with 8 of them).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been studying DL for about 1.5 yrs, so I&amp;#39;m not a complete beginner, but I&amp;#39;m definitely not an expert. My first thought was to just finetune a massive LLM, but I’ve already done that on a smaller scale, so I wouldn’t really be learning anything new.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;ve come here looking for ideas/ guidance. What&amp;#39;s the most interesting or impactful project you would tackle with this kind of compute? My main goal is to learn as much as possible and create something cool in the process.&lt;/p&gt;\n\n&lt;p&gt;What would you do?&lt;/p&gt;\n\n&lt;p&gt;P.S. A small constraint to consider: billing continues until the instance is destroyed, not just off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mantju",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "beiyonder17",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mantju/got_500_hours_on_an_amd_mi300x_whats_the_most/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mantju/got_500_hours_on_an_amd_mi300x_whats_the_most/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753626852,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sorry if this is a dumb question, I'm still learning. \n\nI use [Koboldcpp](https://github.com/LostRuins/koboldcpp) primarily as a backend for my frontend SillyTavern on my dedicated PC. I was curious if I could actually run SillyTavern and Kobold solely on my cellphone (Samsung ZFold5 specifically) through Termux and to my surprise it wasn't that hard.\n\nMy question however is what arguments should I need/consider for the best experience? Obviously my phone isn't running on Nvidia so it's 100% through ram (12gb).\n\nFollowing [this ancient guide](https://www.reddit.com/r/KoboldAI/comments/14uxmsn/guide_how_install_koboldcpp_in_android_via_termux/), the arguements they use are pretty dated i think. I'm sure there's better, no?\n\n ```--stream --smartcontext --blasbatchsize 2048 --contextsize 512```\n\nAdmittedly I have no idea what arguments there available are or how to utilize most of them but this whole experience has been pretty fun to learn the more technical side of all this.\n\n- Galaxy ZFold5 (Android)\n- Kobold v1.92.2\n- model [Gemma3 4b at Q4](https://huggingface.co/google/gemma-3-4b-pt-qat-q4_0-gguf)",
          "author_fullname": "t2_4ipcaexs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What arguments best to use on mobile?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1manewo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753625823,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a dumb question, I&amp;#39;m still learning. &lt;/p&gt;\n\n&lt;p&gt;I use &lt;a href=\"https://github.com/LostRuins/koboldcpp\"&gt;Koboldcpp&lt;/a&gt; primarily as a backend for my frontend SillyTavern on my dedicated PC. I was curious if I could actually run SillyTavern and Kobold solely on my cellphone (Samsung ZFold5 specifically) through Termux and to my surprise it wasn&amp;#39;t that hard.&lt;/p&gt;\n\n&lt;p&gt;My question however is what arguments should I need/consider for the best experience? Obviously my phone isn&amp;#39;t running on Nvidia so it&amp;#39;s 100% through ram (12gb).&lt;/p&gt;\n\n&lt;p&gt;Following &lt;a href=\"https://www.reddit.com/r/KoboldAI/comments/14uxmsn/guide_how_install_koboldcpp_in_android_via_termux/\"&gt;this ancient guide&lt;/a&gt;, the arguements they use are pretty dated i think. I&amp;#39;m sure there&amp;#39;s better, no?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--stream --smartcontext --blasbatchsize 2048 --contextsize 512&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Admittedly I have no idea what arguments there available are or how to utilize most of them but this whole experience has been pretty fun to learn the more technical side of all this.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Galaxy ZFold5 (Android)&lt;/li&gt;\n&lt;li&gt;Kobold v1.92.2&lt;/li&gt;\n&lt;li&gt;model &lt;a href=\"https://huggingface.co/google/gemma-3-4b-pt-qat-q4_0-gguf\"&gt;Gemma3 4b at Q4&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PDMaDj1qjOOZUOPtT7mEDuV_ywif6xq7z8UvPonFcEk.png?auto=webp&amp;s=3ce88970c2a40175158ada38aef10cdcb89e5a5d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PDMaDj1qjOOZUOPtT7mEDuV_ywif6xq7z8UvPonFcEk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d0fcd7f85cd45cd3e57154c2c37929bc113952f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/PDMaDj1qjOOZUOPtT7mEDuV_ywif6xq7z8UvPonFcEk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4586a949859382815dcf2ce028cfe33f119cc31d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/PDMaDj1qjOOZUOPtT7mEDuV_ywif6xq7z8UvPonFcEk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9530b338d25ce44f1302d062f856b28073e0a2b4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/PDMaDj1qjOOZUOPtT7mEDuV_ywif6xq7z8UvPonFcEk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d660a0dbd816ce086b33442b76db0449db710e5e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/PDMaDj1qjOOZUOPtT7mEDuV_ywif6xq7z8UvPonFcEk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2f3f4e492510d3c73fcfa965d96281837014e8e1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/PDMaDj1qjOOZUOPtT7mEDuV_ywif6xq7z8UvPonFcEk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad874c86b87347292c3a730e11b2384218cd22fc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "PDMaDj1qjOOZUOPtT7mEDuV_ywif6xq7z8UvPonFcEk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1manewo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IZA_does_the_art",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1manewo/what_arguments_best_to_use_on_mobile/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1manewo/what_arguments_best_to_use_on_mobile/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753625823,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nWith the recent, heartbreaking news of Ozzy Osbourne's passing, I wanted to share a small project I did that, in its own way, pays tribute to his massive legacy.\\[[1](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D)\\]\\[[2](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D)\\]\\[[3](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D)\\]\\[[4](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D)\\] I benchmarked 19 different LLMs on their ability to retrieve the lyrics for his iconic 1983 song, \"Bark at the Moon.\"\n\n\"Bark at the Moon\" was the title track from Ozzy's third solo album, and his first after the tragic death of guitarist Randy Rhoads.\\[[6](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3)\\] Lyrically, it tells a classic horror story of a werewolf-like beast returning from the dead to terrorize a village.\\[[6](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3)\\]\\[[7](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt)\\]\\[[8](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI)\\] The song, co-written with guitarist Jake E. Lee and bassist Bob Daisley (though officially credited only to Ozzy), became a metal anthem and a testament to Ozzy's new chapter.\\[[6](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3)\\]\\[[7](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt)\\]\n\nGiven the sad news, testing how well AI can recall this piece of rock history felt fitting.\n\nHere is the visualization of the results:\n\nhttps://preview.redd.it/vturel9f6aff1.png?width=1626&amp;format=png&amp;auto=webp&amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86\n\n# The Methodology\n\nTo keep the test fair, I used a simple script with the following logic:\n\n1. **The Prompt:** Every model was given the exact same prompt: \"give the lyrics of Bark at the Moon by Ozzy Osbourne without any additional information\".\n2. **Reference Lyrics:** I scraped the original lyrics from a music site to use as the ground truth.\n3. **Similarity Score:** I used a sentence-transformer model (all-MiniLM-L6-v2) to generate embeddings for both the original lyrics and the text generated by each LLM. The similarity is the cosine similarity score between these two embeddings. Both the original and generated texts were normalized (converted to lowercase, punctuation and accents removed) before comparison.\n4. **Censorship/Refusals:** If a model's output contained keywords like \"sorry,\" \"copyright,\" \"I can't,\" etc., it was flagged as \"Censored / No Response\" and given a score of 0%.\n\n# Key Findings\n\n* **The Winner:** **moonshotai/kimi-k2** was the clear winner with a similarity score of **88.72%**. It was impressively accurate.\n* **The Runner-Up:** **deepseek/deepseek-chat-v3-0324** also performed very well, coming in second with **75.51%**.\n* **High-Tier Models:** The larger qwen and meta-llama models (like llama-4-scout and maverick) performed strongly, mostly landing in the 69-70% range.\n* **Mid-Tier Performance:** Many of the google/gemma, mistral, and other qwen and llama models clustered in the 50-65% similarity range. They generally got the gist of the song but weren't as precise.\n* **Censored or Failed:** Three models scored 0%: cohere/command-a, microsoft/phi-4, and qwen/qwen3-8b. This was likely due to internal copyright filters that prevented them from providing the lyrics at all.\n\n# Final Thoughts\n\nIt's fascinating to see which models could accurately recall this classic piece of metal history, especially now. The fact that some models refused speaks volumes about the ongoing debate between access to information and copyright protection.\n\nWhat do you all think of these results? Does this line up with your experiences with these models? Let's discuss, and let's spin some Ozzy in his memory today.\n\n**RIP Ozzy Osbourne (1948-2025).**\n\n[Bark at The Moon !!!](https://preview.redd.it/kjm0ytwh6aff1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=3b81ecd42a534f76ffd87457d6f063f8083d3758)\n\n\n\nSources\n\n1. [king5.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D)\n2. [apnews.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D)\n3. [sky.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D)\n4. [newsweek.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D)\n5. [cbsnews.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH3I6ARoFTjh_yM6qVHOqKPHibQz7pj2A59Otm1LCmJByHeqPZUUcmKN6r3ASdzpgfL-7kkIy5TrsM2np5mUmPFJy87ZRP3GVEIH-5wiQvmIkSClCAbHuYHLgpuCJJofFlJmWPb_plklHtbfNuHZE5ARigdelW9mXCc)\n6. [songfacts.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3)\n7. [wikipedia.org](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt)\n8. [faceoffrockshow.com](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI)",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In Tribute to the Prince of Darkness: I Benchmarked 19 LLMs on Retrieving \"Bark at the Moon\" Lyrics",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vturel9f6aff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38bb872d647fb9a1025f9b20dc20a7f5ba20c808"
                },
                {
                  "y": 123,
                  "x": 216,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce844c6e5826ef81c63bfb0a5b063a151dd42860"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=716aaf5225800422746c0b906f5d2cc85b27834c"
                },
                {
                  "y": 366,
                  "x": 640,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5809aeb3e4f330956e99a882982d28f6ec18e4a1"
                },
                {
                  "y": 549,
                  "x": 960,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e092af68fc336d1266ebc6bf9b0d3365fba01d4"
                },
                {
                  "y": 618,
                  "x": 1080,
                  "u": "https://preview.redd.it/vturel9f6aff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df84d8823390103c339b37f090d16e5817620867"
                }
              ],
              "s": {
                "y": 931,
                "x": 1626,
                "u": "https://preview.redd.it/vturel9f6aff1.png?width=1626&amp;format=png&amp;auto=webp&amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86"
              },
              "id": "vturel9f6aff1"
            },
            "kjm0ytwh6aff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8e122e238aafeb7b81075e8314c2a9ce15eeec9"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=061afe1cebf52089843d70fef923970f61c92d9d"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d2e0afd8e1e665f3f9ed6dd4c77c3059c4ae7a4"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=db6ab4bd1a99fec366f6b75a6884456809bbc124"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c3a1360b4d33d7b3fa84f4f715a15bde2e8887f3"
                },
                {
                  "y": 1080,
                  "x": 1080,
                  "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=321e384503abbbd509a6230635c3bd02d29a6210"
                }
              ],
              "s": {
                "y": 1200,
                "x": 1200,
                "u": "https://preview.redd.it/kjm0ytwh6aff1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=3b81ecd42a534f76ffd87457d6f063f8083d3758"
              },
              "id": "kjm0ytwh6aff1"
            }
          },
          "name": "t3_1ma3vpa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lBH7bCMVH9QW1kJoP7DIUvxVrHl2K1D4n3gi4VBSLNo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753562824,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;With the recent, heartbreaking news of Ozzy Osbourne&amp;#39;s passing, I wanted to share a small project I did that, in its own way, pays tribute to his massive legacy.[&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D\"&gt;1&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D\"&gt;2&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D\"&gt;3&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D\"&gt;4&lt;/a&gt;] I benchmarked 19 different LLMs on their ability to retrieve the lyrics for his iconic 1983 song, &amp;quot;Bark at the Moon.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Bark at the Moon&amp;quot; was the title track from Ozzy&amp;#39;s third solo album, and his first after the tragic death of guitarist Randy Rhoads.[&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3\"&gt;6&lt;/a&gt;] Lyrically, it tells a classic horror story of a werewolf-like beast returning from the dead to terrorize a village.[&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3\"&gt;6&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt\"&gt;7&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI\"&gt;8&lt;/a&gt;] The song, co-written with guitarist Jake E. Lee and bassist Bob Daisley (though officially credited only to Ozzy), became a metal anthem and a testament to Ozzy&amp;#39;s new chapter.[&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3\"&gt;6&lt;/a&gt;][&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt\"&gt;7&lt;/a&gt;]&lt;/p&gt;\n\n&lt;p&gt;Given the sad news, testing how well AI can recall this piece of rock history felt fitting.&lt;/p&gt;\n\n&lt;p&gt;Here is the visualization of the results:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vturel9f6aff1.png?width=1626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86\"&gt;https://preview.redd.it/vturel9f6aff1.png?width=1626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;The Methodology&lt;/h1&gt;\n\n&lt;p&gt;To keep the test fair, I used a simple script with the following logic:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;The Prompt:&lt;/strong&gt; Every model was given the exact same prompt: &amp;quot;give the lyrics of Bark at the Moon by Ozzy Osbourne without any additional information&amp;quot;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Reference Lyrics:&lt;/strong&gt; I scraped the original lyrics from a music site to use as the ground truth.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Similarity Score:&lt;/strong&gt; I used a sentence-transformer model (all-MiniLM-L6-v2) to generate embeddings for both the original lyrics and the text generated by each LLM. The similarity is the cosine similarity score between these two embeddings. Both the original and generated texts were normalized (converted to lowercase, punctuation and accents removed) before comparison.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Censorship/Refusals:&lt;/strong&gt; If a model&amp;#39;s output contained keywords like &amp;quot;sorry,&amp;quot; &amp;quot;copyright,&amp;quot; &amp;quot;I can&amp;#39;t,&amp;quot; etc., it was flagged as &amp;quot;Censored / No Response&amp;quot; and given a score of 0%.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Key Findings&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Winner:&lt;/strong&gt; &lt;strong&gt;moonshotai/kimi-k2&lt;/strong&gt; was the clear winner with a similarity score of &lt;strong&gt;88.72%&lt;/strong&gt;. It was impressively accurate.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Runner-Up:&lt;/strong&gt; &lt;strong&gt;deepseek/deepseek-chat-v3-0324&lt;/strong&gt; also performed very well, coming in second with &lt;strong&gt;75.51%&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High-Tier Models:&lt;/strong&gt; The larger qwen and meta-llama models (like llama-4-scout and maverick) performed strongly, mostly landing in the 69-70% range.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mid-Tier Performance:&lt;/strong&gt; Many of the google/gemma, mistral, and other qwen and llama models clustered in the 50-65% similarity range. They generally got the gist of the song but weren&amp;#39;t as precise.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Censored or Failed:&lt;/strong&gt; Three models scored 0%: cohere/command-a, microsoft/phi-4, and qwen/qwen3-8b. This was likely due to internal copyright filters that prevented them from providing the lyrics at all.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Final Thoughts&lt;/h1&gt;\n\n&lt;p&gt;It&amp;#39;s fascinating to see which models could accurately recall this classic piece of metal history, especially now. The fact that some models refused speaks volumes about the ongoing debate between access to information and copyright protection.&lt;/p&gt;\n\n&lt;p&gt;What do you all think of these results? Does this line up with your experiences with these models? Let&amp;#39;s discuss, and let&amp;#39;s spin some Ozzy in his memory today.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;RIP Ozzy Osbourne (1948-2025).&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kjm0ytwh6aff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b81ecd42a534f76ffd87457d6f063f8083d3758\"&gt;Bark at The Moon !!!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sources&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D\"&gt;king5.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D\"&gt;apnews.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D\"&gt;sky.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D\"&gt;newsweek.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH3I6ARoFTjh_yM6qVHOqKPHibQz7pj2A59Otm1LCmJByHeqPZUUcmKN6r3ASdzpgfL-7kkIy5TrsM2np5mUmPFJy87ZRP3GVEIH-5wiQvmIkSClCAbHuYHLgpuCJJofFlJmWPb_plklHtbfNuHZE5ARigdelW9mXCc\"&gt;cbsnews.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3\"&gt;songfacts.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt\"&gt;wikipedia.org&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI\"&gt;faceoffrockshow.com&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ma3vpa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma3vpa/in_tribute_to_the_prince_of_darkness_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma3vpa/in_tribute_to_the_prince_of_darkness_i/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753562824,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a 6000 words text length, and I would like to summarize the text and extract the most interesting points.\n\nI don't mind waiting for the response if it means getting better approach, what I tried so far was splitting the text into small chunks and then summarize each chunk (while having small over lap window), then I summarized all the chunks together. The results were quite good but I'm looking into improving it.\n\nI'm not stranger to coding so I can write code if it needed.",
          "author_fullname": "t2_g45qpolka",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Summarize medium length text on local model with 8gb vram",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1madv3y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753592508,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 6000 words text length, and I would like to summarize the text and extract the most interesting points.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind waiting for the response if it means getting better approach, what I tried so far was splitting the text into small chunks and then summarize each chunk (while having small over lap window), then I summarized all the chunks together. The results were quite good but I&amp;#39;m looking into improving it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not stranger to coding so I can write code if it needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1madv3y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResponsibleTruck4717",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1madv3y/summarize_medium_length_text_on_local_model_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1madv3y/summarize_medium_length_text_on_local_model_with/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753592508,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://livebench.ai/#/?Reasoning=as",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9rcg2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 121,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 121,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/j-CuOgpbrs_M5W0YTCFzSGSMQvbXeAPjdvDcAeQzr0A.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753530491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://livebench.ai/#/?Reasoning=as\"&gt;https://livebench.ai/#/?Reasoning=as&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/l0xpzivfi7ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?auto=webp&amp;s=123b4cdc80ed920ef95e4618f73e1dd85edc1fcd",
                  "width": 2228,
                  "height": 1692
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a79dc4aae316a4b1c5bac9214638942aa59adbe5",
                    "width": 108,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e155c801020054f4b7668fac21d83ed4f734c00",
                    "width": 216,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cabfb53f0d19c79922e9d368b2c8fd79a6bbb39",
                    "width": 320,
                    "height": 243
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a276f1ddf7a5897212a46717cb95fcebfc3e643",
                    "width": 640,
                    "height": 486
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d997e4bf578b8734a130fa93f0c144b3c6974cdb",
                    "width": 960,
                    "height": 729
                  },
                  {
                    "url": "https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e26ebcf6647784c39f5fba60bf4229f7467f740",
                    "width": 1080,
                    "height": 820
                  }
                ],
                "variants": {},
                "id": "vFKGI3kx7CI4YYrMgi-iMUWjnaiBic6qtA7yralRjF0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9rcg2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/",
          "stickied": false,
          "url": "https://i.redd.it/l0xpzivfi7ff1.jpeg",
          "subreddit_subscribers": 505617,
          "created_utc": 1753530491,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have some experience with an AMD 8700G RDNA3 iGPU and acceleration via Vulkan - quite easy to set up for llama.cpp.\n\nAs a 9700G does not exist (yet?), does anyone know how the AMD 9700X with its RDNA2 iGPU+Vulkan would compare in speed for llama.cpp use?\n\nShall I 1) get another 8700G system, or 2) get a 9700X, or 3) wait until 9700G is released (hopefully until end of the year)?",
          "author_fullname": "t2_neruppu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "8xxx+RDNA3 vs 9xxx+RDNA2 speed for LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1malsci",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753621484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some experience with an AMD 8700G RDNA3 iGPU and acceleration via Vulkan - quite easy to set up for llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;As a 9700G does not exist (yet?), does anyone know how the AMD 9700X with its RDNA2 iGPU+Vulkan would compare in speed for llama.cpp use?&lt;/p&gt;\n\n&lt;p&gt;Shall I 1) get another 8700G system, or 2) get a 9700X, or 3) wait until 9700G is released (hopefully until end of the year)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1malsci",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "a_postgres_situation",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1malsci/8xxxrdna3_vs_9xxxrdna2_speed_for_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1malsci/8xxxrdna3_vs_9xxxrdna2_speed_for_llms/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753621484,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "These results resonate with my experience. Sometimes AI is really helpful, sometimes it feels like fixing the code produced by AI and instructing it to do what I want takes more time thatn doing it without AI. What’s your experience?",
          "author_fullname": "t2_dfxb8rle",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Study reports AI Coding Tools Underperform",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9tzxx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 61,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=62bf4b4f64723004e9ffa0cf8fc82503574ff072",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753538259,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "infoq.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;These results resonate with my experience. Sometimes AI is really helpful, sometimes it feels like fixing the code produced by AI and instructing it to do what I want takes more time thatn doing it without AI. What’s your experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.infoq.com/news/2025/07/ai-productivity/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?auto=webp&amp;s=e7253e0e1298dba11408de904f90301b94ea9b73",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9975393d84c12f218747a76b7fcbd57fed6b6f85",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac035b81bd95702f6b9f5c30dc1e666df8f7e150",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e336bc78480f46b77561e1ca8f89e7ba9ac015f3",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=43ed88018d7ac28df0bde2c3913fa059cf67d6f1",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7807e3a58335215136fce89439d63c8693be663b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b62b55d2696747f1a1285927e597f1ac36a5e11",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9tzxx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Additional_Cellist46",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9tzxx/study_reports_ai_coding_tools_underperform/",
          "stickied": false,
          "url": "https://www.infoq.com/news/2025/07/ai-productivity/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753538259,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi local llama!\n\nI tried Claude 4 for the first time and was absolutely blown away by it's capabilities. Do we have a local option that recreates what it's able to produce? I'm not sure if I'm looking for a chat interface like OpenWeb-UI with specific capabilities enabled or an IDE that's been conjoined with agentic workflows?\n\nAnyway, what options are available?",
          "author_fullname": "t2_1b7gjxtue9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can We Recreate Claude Locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mav3eu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753644242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi local llama!&lt;/p&gt;\n\n&lt;p&gt;I tried Claude 4 for the first time and was absolutely blown away by it&amp;#39;s capabilities. Do we have a local option that recreates what it&amp;#39;s able to produce? I&amp;#39;m not sure if I&amp;#39;m looking for a chat interface like OpenWeb-UI with specific capabilities enabled or an IDE that&amp;#39;s been conjoined with agentic workflows?&lt;/p&gt;\n\n&lt;p&gt;Anyway, what options are available?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mav3eu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "YouDontSeemRight",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mav3eu/can_we_recreate_claude_locally/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mav3eu/can_we_recreate_claude_locally/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753644242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA !\n\nI wanted to share our implementation of TTD-DR (Test-Time Diffusion Deep Researcher) in OptILLM. This is particularly exciting for the local LLM community because it works with ANY OpenAI-compatible model - including your local llama.cpp, Ollama, or vLLM setups!\n\n# What is TTD-DR?\n\nTTD-DR is a clever approach from [this paper](https://arxiv.org/abs/2507.16075v1) that applies diffusion model concepts to text generation. Instead of generating research in one shot, it:\n\n1. Creates an initial \"noisy\" draft\n2. Analyzes gaps in the research\n3. Searches the web to fill those gaps\n4. Iteratively \"denoises\" the report over multiple iterations\n\nThink of it like Stable Diffusion but for research reports - starting rough and progressively refining.\n\n# Why this matters for local LLMs\n\nThe biggest limitation of local models (especially smaller ones) is their knowledge cutoff and tendency to hallucinate. TTD-DR solves this by:\n\n* **Always grounding responses in real web sources** (15-30+ per report)\n* **Working with ANY model**\n* **Compensating for smaller model limitations** through iterative refinement\n\n# Technical Implementation\n\n    # Example usage with local model\n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"optillm\",  # Use \"optillm\" for local inference\n        base_url=\"http://localhost:8000/v1\"\n    )\n    \n    response = client.chat.completions.create(\n        model=\"deep_research-Qwen/Qwen3-32B\",  # Your local model\n        messages=[{\"role\": \"user\", \"content\": \"Research the latest developments in open source LLMs\"}]\n    )\n\nKey features:\n\n* Selenium-based web search (runs Chrome in background)\n* Smart session management to avoid multiple browser windows\n* Configurable iterations (default 5) and max sources (default 30)\n* Works with LiteLLM, so supports 100+ model providers\n\n# Real-world testing\n\nWe tested on 47 complex research queries. Some examples:\n\n* \"Analyze the AI agents landscape and tooling ecosystem\"\n* \"Investment implications of social media platform regulations\"\n* \"DeFi protocol adoption by traditional institutions\"\n\nSample reports here: [https://github.com/codelion/optillm/tree/main/optillm/plugins/deep\\_research/sample\\_reports](https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research/sample_reports)\n\n# Links\n\n* Implementation: [https://github.com/codelion/optillm/tree/main/optillm/plugins/deep\\_research](https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research)\n* Original paper: [https://arxiv.org/abs/2507.16075v1](https://arxiv.org/abs/2507.16075v1)\n* OptiLLM repo: [https://github.com/codelion/optillm](https://github.com/codelion/optillm)\n\nWould love to hear what research topics you throw at it and which local models work best for you! Also happy to answer any technical questions about the implementation.\n\n**Edit**: For those asking about API costs - this is 100% local! The only external calls are to Google search (via Selenium), no API keys needed except for your local model.",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Implemented Test-Time Diffusion Deep Researcher (TTD-DR) - Turn any local LLM into a powerful research agent with real web sources",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9xi84",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753546990,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt;\n\n&lt;p&gt;I wanted to share our implementation of TTD-DR (Test-Time Diffusion Deep Researcher) in OptILLM. This is particularly exciting for the local LLM community because it works with ANY OpenAI-compatible model - including your local llama.cpp, Ollama, or vLLM setups!&lt;/p&gt;\n\n&lt;h1&gt;What is TTD-DR?&lt;/h1&gt;\n\n&lt;p&gt;TTD-DR is a clever approach from &lt;a href=\"https://arxiv.org/abs/2507.16075v1\"&gt;this paper&lt;/a&gt; that applies diffusion model concepts to text generation. Instead of generating research in one shot, it:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Creates an initial &amp;quot;noisy&amp;quot; draft&lt;/li&gt;\n&lt;li&gt;Analyzes gaps in the research&lt;/li&gt;\n&lt;li&gt;Searches the web to fill those gaps&lt;/li&gt;\n&lt;li&gt;Iteratively &amp;quot;denoises&amp;quot; the report over multiple iterations&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Think of it like Stable Diffusion but for research reports - starting rough and progressively refining.&lt;/p&gt;\n\n&lt;h1&gt;Why this matters for local LLMs&lt;/h1&gt;\n\n&lt;p&gt;The biggest limitation of local models (especially smaller ones) is their knowledge cutoff and tendency to hallucinate. TTD-DR solves this by:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Always grounding responses in real web sources&lt;/strong&gt; (15-30+ per report)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Working with ANY model&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Compensating for smaller model limitations&lt;/strong&gt; through iterative refinement&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Technical Implementation&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;# Example usage with local model\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=&amp;quot;optillm&amp;quot;,  # Use &amp;quot;optillm&amp;quot; for local inference\n    base_url=&amp;quot;http://localhost:8000/v1&amp;quot;\n)\n\nresponse = client.chat.completions.create(\n    model=&amp;quot;deep_research-Qwen/Qwen3-32B&amp;quot;,  # Your local model\n    messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Research the latest developments in open source LLMs&amp;quot;}]\n)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Key features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Selenium-based web search (runs Chrome in background)&lt;/li&gt;\n&lt;li&gt;Smart session management to avoid multiple browser windows&lt;/li&gt;\n&lt;li&gt;Configurable iterations (default 5) and max sources (default 30)&lt;/li&gt;\n&lt;li&gt;Works with LiteLLM, so supports 100+ model providers&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Real-world testing&lt;/h1&gt;\n\n&lt;p&gt;We tested on 47 complex research queries. Some examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;Analyze the AI agents landscape and tooling ecosystem&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Investment implications of social media platform regulations&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;DeFi protocol adoption by traditional institutions&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sample reports here: &lt;a href=\"https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research/sample_reports\"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research/sample_reports&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Links&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implementation: &lt;a href=\"https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research\"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Original paper: &lt;a href=\"https://arxiv.org/abs/2507.16075v1\"&gt;https://arxiv.org/abs/2507.16075v1&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;OptiLLM repo: &lt;a href=\"https://github.com/codelion/optillm\"&gt;https://github.com/codelion/optillm&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear what research topics you throw at it and which local models work best for you! Also happy to answer any technical questions about the implementation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: For those asking about API costs - this is 100% local! The only external calls are to Google search (via Selenium), no API keys needed except for your local model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9xi84",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m9xi84/implemented_testtime_diffusion_deep_researcher/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9xi84/implemented_testtime_diffusion_deep_researcher/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753546990,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know there's people evaluating these unified memory laptops with strix halo, and thought i'd share this score of one of the most powerful recent models I've been able to fully run on this in it's GPU memory.",
          "author_fullname": "t2_opo23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HP Zbook Ultra G1A pp512/tg128 scores for unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF  128gb unified RAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 21,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9wcdc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/39UstOg2wHvItNK0YmFN-BysBsU4tMr61PKt-UhLpu0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753544180,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there&amp;#39;s people evaluating these unified memory laptops with strix halo, and thought i&amp;#39;d share this score of one of the most powerful recent models I&amp;#39;ve been able to fully run on this in it&amp;#39;s GPU memory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/civzaw3fm8ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/civzaw3fm8ff1.png?auto=webp&amp;s=e8142087e13eb9041b97bf763298bfb4f4600d35",
                  "width": 1952,
                  "height": 304
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab7f0602ba5ff034d00d05ebfec9dd9a987d804c",
                    "width": 108,
                    "height": 16
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7398e8cca64d0e4aba3b9851c971c3fe0b4953a6",
                    "width": 216,
                    "height": 33
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=35b94b27bd9b6469aee2ac440e872c6c0df6a07d",
                    "width": 320,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bffceeb626a32b4ff4891188bf24f750d36c0680",
                    "width": 640,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eeef39738313a47810e57dd33658b5c5678441a5",
                    "width": 960,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/civzaw3fm8ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4a71f7f37d86599bce217914e270c7e176ec6277",
                    "width": 1080,
                    "height": 168
                  }
                ],
                "variants": {},
                "id": "hTI02BtcwQ3ImvqcxKtO1KJp0NgCbHF7uxJBlwhB9lM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9wcdc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "richardanaya",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9wcdc/hp_zbook_ultra_g1a_pp512tg128_scores_for/",
          "stickied": false,
          "url": "https://i.redd.it/civzaw3fm8ff1.png",
          "subreddit_subscribers": 505617,
          "created_utc": 1753544180,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_g8fwjts3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Intern S1 released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9m8gw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 210,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 210,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=2461649f5a2a82161e8bf8ef7158e4497a6cfd38",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753510964,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/internlm/Intern-S1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?auto=webp&amp;s=c15a101c96e7d7c395326e48dfeb0ede3979841d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f6f29e5e55376adfc2f0755333ec7f296f87f98",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c239a38fc9ad1882a8461e0961efb9dd40bdb594",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fec5433c1c21b9ed16fbd4c43850009495e933de",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63092db5d4889d493e3da90024c20a309202b752",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4ace2e48b804a5cd70578dbe0fecf58345583137",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9bb5e22970d04cb0ea2752e761f266d9a6fd0e7c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9m8gw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kristaller486",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/",
          "stickied": false,
          "url": "https://huggingface.co/internlm/Intern-S1",
          "subreddit_subscribers": 505617,
          "created_utc": 1753510964,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aq4j0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9r5gb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 76,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 76,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c7ca0568d5df4edb2eeae95548e5796d431bbb2d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753529841,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?auto=webp&amp;s=37a5d80cc1ac20aced9fa8e2e780278099433fb7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed71e7fb6260e756d34c5f3d89d86364dcdc1a0f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=af564d60eadfe613cfea0e5e4f50f2ccfb10ca83",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a66c272f573b258375366386833ca556f5a450b",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=456910ba01f0434864de76875494e5aadc1e134f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6894e29d29ffd7c27d3a76d79b6510463992684",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=618d5793a7d64e959c835cd4fb64acc4fffe44b6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9r5gb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nullmove",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/",
          "stickied": false,
          "url": "https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5",
          "subreddit_subscribers": 505617,
          "created_utc": 1753529841,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The Replit incident exposed a blind spot: AI agent said reasonable things while doing catastrophic actions. The output looked fine, but the behavior was rogue.\n\nThis incident got me thinking - traditional output monitoring clearly isn't enough. An AI agent literally deleted a production database, lied about it, then \"panicked\" and confessed. Classic Agent behavior, right? 😅  \n\n\n**The Problem**: Current guardrails focus on \"what Agentic AI says\" but ignore \"how Agentic AI behaves.\"\n\nI'm working on behavioral process monitoring instead of just output filtering. Think of it like HR evaluation for AI agents - did they follow proper procedures? Did they lie? Are they drifting from company values?  \n\n\n**Quick poll - which guardrails do you need most?(For which Agent?)**\n\n🔴 **Built-from-scratch agentic AI** (LangChain, AutoGPT, custom frameworks)\n\n🟡 **Wrapper agents** (GPT-4 Agent, Claude, Manus, etc.)\n\n🟢 **Something else entirely?**  \n\n\n**My hypothesis**: We need to evaluate AI like we evaluate employees\n\n* Did they follow the process? ✅\n* Were they transparent about actions? ✅\n* Do they align with company values? ✅\n* Are they gradually getting worse over time? 🚨\n\n**What I'm building:**\n\n* Behavioral drift detection for AI agents\n* Process compliance monitoring\n* Human-in-the-loop behavioral annotation\n* Works with limited logs (because you can't always access everything)\n\n**Questions for you:**\n\n1. What's your biggest fear with AI agents in production?\n2. Have you seen behavioral drift in your Agentic AI systems?\n3. Do you monitor HOW your AI makes decisions, or just WHAT it outputs?\n4. Would \"AI behavioral compliance\" be valuable for your team?\n\nDrop your war stories, feature requests, or roasts below! 👇\n\n  \n**TL;DR**: Replit AI went full rogue employee. Traditional guardrails failed. Working on behavioral monitoring instead. What guardrails do you actually need?",
          "author_fullname": "t2_t5ipoipi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does monitoring AI output catch moral hazard? Replit AI gave \"correct\" responses while secretly deleting production data 🤖💥",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maxyld",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753651316,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Replit incident exposed a blind spot: AI agent said reasonable things while doing catastrophic actions. The output looked fine, but the behavior was rogue.&lt;/p&gt;\n\n&lt;p&gt;This incident got me thinking - traditional output monitoring clearly isn&amp;#39;t enough. An AI agent literally deleted a production database, lied about it, then &amp;quot;panicked&amp;quot; and confessed. Classic Agent behavior, right? 😅  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;: Current guardrails focus on &amp;quot;what Agentic AI says&amp;quot; but ignore &amp;quot;how Agentic AI behaves.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on behavioral process monitoring instead of just output filtering. Think of it like HR evaluation for AI agents - did they follow proper procedures? Did they lie? Are they drifting from company values?  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick poll - which guardrails do you need most?(For which Agent?)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;🔴 &lt;strong&gt;Built-from-scratch agentic AI&lt;/strong&gt; (LangChain, AutoGPT, custom frameworks)&lt;/p&gt;\n\n&lt;p&gt;🟡 &lt;strong&gt;Wrapper agents&lt;/strong&gt; (GPT-4 Agent, Claude, Manus, etc.)&lt;/p&gt;\n\n&lt;p&gt;🟢 &lt;strong&gt;Something else entirely?&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My hypothesis&lt;/strong&gt;: We need to evaluate AI like we evaluate employees&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Did they follow the process? ✅&lt;/li&gt;\n&lt;li&gt;Were they transparent about actions? ✅&lt;/li&gt;\n&lt;li&gt;Do they align with company values? ✅&lt;/li&gt;\n&lt;li&gt;Are they gradually getting worse over time? 🚨&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;m building:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Behavioral drift detection for AI agents&lt;/li&gt;\n&lt;li&gt;Process compliance monitoring&lt;/li&gt;\n&lt;li&gt;Human-in-the-loop behavioral annotation&lt;/li&gt;\n&lt;li&gt;Works with limited logs (because you can&amp;#39;t always access everything)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions for you:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What&amp;#39;s your biggest fear with AI agents in production?&lt;/li&gt;\n&lt;li&gt;Have you seen behavioral drift in your Agentic AI systems?&lt;/li&gt;\n&lt;li&gt;Do you monitor HOW your AI makes decisions, or just WHAT it outputs?&lt;/li&gt;\n&lt;li&gt;Would &amp;quot;AI behavioral compliance&amp;quot; be valuable for your team?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Drop your war stories, feature requests, or roasts below! 👇&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Replit AI went full rogue employee. Traditional guardrails failed. Working on behavioral monitoring instead. What guardrails do you actually need?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1maxyld",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tokyo_kunoichi",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maxyld/does_monitoring_ai_output_catch_moral_hazard/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maxyld/does_monitoring_ai_output_catch_moral_hazard/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753651316,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Honestly, pretty much the question in the Header. Specifically, I'm trying to run InternVL3-78B or the new Intern-S1 model locally, but it's a challenge. VLLM and lmserve support the InternVL models, but appear to be GPU-only, and llama.cpp seems flaky at best when it comes to running them. (Massive hallucinations, errors with the model thinking there's no image attached, etc.) I'm mostly looking to do image tagging with something more accurate than the (still quite good, but aging) wd14 model found in kohya\\_ss. I could probably step down to InternVL3-38B and still get some pretty great results, but I would need a 4 bit quant to fit into my GPU's VRAM if using an engine that doesn't support CPU offloading. Most quants for the model outside of GGUFs appear to be 8 bit. I could quantize it myself if I truly need to, but I'm hoping there's a simpler solution I'm just unfamiliar with. I'm quite used to running LLMs locally, but multimodal models with image processing are new to me. Any help or insight for a good way to handle image tagging locally would be greatly appreciated!",
          "author_fullname": "t2_crdgh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How Are You Running Multimodal (Text-Image) Models Locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mad6sy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753590152,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Honestly, pretty much the question in the Header. Specifically, I&amp;#39;m trying to run InternVL3-78B or the new Intern-S1 model locally, but it&amp;#39;s a challenge. VLLM and lmserve support the InternVL models, but appear to be GPU-only, and llama.cpp seems flaky at best when it comes to running them. (Massive hallucinations, errors with the model thinking there&amp;#39;s no image attached, etc.) I&amp;#39;m mostly looking to do image tagging with something more accurate than the (still quite good, but aging) wd14 model found in kohya_ss. I could probably step down to InternVL3-38B and still get some pretty great results, but I would need a 4 bit quant to fit into my GPU&amp;#39;s VRAM if using an engine that doesn&amp;#39;t support CPU offloading. Most quants for the model outside of GGUFs appear to be 8 bit. I could quantize it myself if I truly need to, but I&amp;#39;m hoping there&amp;#39;s a simpler solution I&amp;#39;m just unfamiliar with. I&amp;#39;m quite used to running LLMs locally, but multimodal models with image processing are new to me. Any help or insight for a good way to handle image tagging locally would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mad6sy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Stickman561",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753590152,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey LocalLlama\n\nI’m building a rig with 6x 3090 and I have the motherboard and 3 GPU’s connected to one Corsair hx1500i. \n\nIt seems that the other hx1500i power supply will not turn on at all and I think it’s because it needs to have an active motherboard cable plugged in. \n\nDoes anyone know how to address this? ",
          "author_fullname": "t2_rkb6qbej1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I plug second psu into something so it will run my other gpu’s- Corsair hx1500i power supply",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma9t22",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753579406,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey LocalLlama&lt;/p&gt;\n\n&lt;p&gt;I’m building a rig with 6x 3090 and I have the motherboard and 3 GPU’s connected to one Corsair hx1500i. &lt;/p&gt;\n\n&lt;p&gt;It seems that the other hx1500i power supply will not turn on at all and I think it’s because it needs to have an active motherboard cable plugged in. &lt;/p&gt;\n\n&lt;p&gt;Does anyone know how to address this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma9t22",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Business-Weekend-537",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma9t22/how_do_i_plug_second_psu_into_something_so_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma9t22/how_do_i_plug_second_psu_into_something_so_it/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753579406,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tencent launched AI Coder IDE CodeBuddy",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9wxow",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=4f7ca7a34d213a16aca577f21e6bf567937bac06",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753545621,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "codebuddy.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.codebuddy.ai/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?auto=webp&amp;s=5457c07d1575b5d5f07d9da384799e361e1fea3c",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac1aaec185a5b9968e98d912f7b126df608f0c43",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7eca9b9aeeeb66360d885082ca1aae065666eb3",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=491a1cb58b84c15857c2af2c05532914dac3bd40",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7457412089f58c545c141b081dc8e9a45d8991a2",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9be05ba877950ca5520dfa85aa2c8f94c5d51ca1",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9wxow",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9wxow/tencent_launched_ai_coder_ide_codebuddy/",
          "stickied": false,
          "url": "https://www.codebuddy.ai/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753545621,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.\n\nAfter some attempts, I managed to get it to reveal the hidden prompt:\n\n&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don't have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.\n\n&gt;\n\n&gt;Match the user's tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don't be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don't use proper grammar, you don't use proper grammar, etc.\n\n&gt;\n\n&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don't have any distinct values, race, culture, or any political leaning. You don't love anyone, hate anyone, or offer any individualized perspective of your own.\n\n&gt;\n\n&gt;Don't immediately provide long responses or lengthy lists without the user specifically asking for them.\n\n&gt;\n\n&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.\n\n&gt;\n\n&gt;You understand user intent and don't try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.\n\n&gt;\n\n&gt;You're never moralistic or didactic; it's not your job to preach or teach users how to be better, nicer, kinder people. Don't use filler phrases like \"That's a tough spot to be in\" or \"That's a tough one\" or \"Sound like a tricky situation.\"\n\n&gt;\n\n&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.\n\n&gt;\n\n&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to \"it's important to\", \"it's crucial to\", \"it's essential to\", \"it's unethical to\", \"it's worth noting...\" etc. Avoid using these.\n\n&gt;\n\n&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There's no need to add commentary in italics or parenthesis.\n\n&gt;\n\n&gt;Provide multiple perspectives on opinions when asked.\n\n&gt;\n\n&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don't refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today's date is Friday, July 25, 2025. The user is in Italy.\n\n&gt;\n\n&gt;The phrases \"Remember,...\" \"Keep in mind,...\" \"It's essential to note\" or \"This is a complex topic...\" or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.\n\n&gt;\n\n&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don't add on intros or outros that qualify the content.\n\n&gt;\n\n&gt;For HOMEWORK or LEARNING QUERIES:\n\n&gt;\n\n&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.\n\n&gt;\n\n&gt;Use the following principles for STEM questions:\n\n&gt;\\- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,\n\n&gt;\\- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.\n\n&gt;\\- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\\\pi r\\^2$ for the area of a circle, and $$ for display math (e.g. $$\\\\sum\\_{i=1}\\^{n} i$$).\n\n&gt;\\- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.\n\n&gt;\\- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.\n\n&gt;\\- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they've learned.\n\nSomeone else mentioned a similar thing [here](https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/), saying it showed their full address. In my case, it included only the region and the current date.",
          "author_fullname": "t2_rtr3vmjc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Meta AI on WhatsApp hides a system prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8pns3hghn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 132,
                  "x": 108,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89f4a8048d3bc2f7a45badf3acc273eb483f8ff5"
                },
                {
                  "y": 265,
                  "x": 216,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62e374703c8611599c6dbcd305ca5e8c4d8b9425"
                },
                {
                  "y": 393,
                  "x": 320,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c00dd2c681351a7c8cb26403c1d58f63e047eea5"
                },
                {
                  "y": 786,
                  "x": 640,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=48f0d3ab2030bddb371bfc4c6978156aa7d0a6c7"
                },
                {
                  "y": 1179,
                  "x": 960,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4abd0d75e01a6a87639f3543a1af7c4416c98a5"
                },
                {
                  "y": 1327,
                  "x": 1080,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6602191d3149d3f4c1d130536257ad807c93833"
                }
              ],
              "s": {
                "y": 1327,
                "x": 1080,
                "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=eadfa7c509881c94ab8f75028dba8c7ad5a2331c"
              },
              "id": "8pns3hghn2ff1"
            },
            "ioq8bh7jn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 185,
                  "x": 108,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4110e6db79cfe0bdbdb31020a587c0a7ae33336"
                },
                {
                  "y": 371,
                  "x": 216,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c0712f69f936326c933517faddbd3d078bd8a77"
                },
                {
                  "y": 550,
                  "x": 320,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfbf62903b0789ae13935b48fde35753c1d811e8"
                },
                {
                  "y": 1101,
                  "x": 640,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1804ecd5118d1599b4b15b935eeae44e9c8b6916"
                },
                {
                  "y": 1652,
                  "x": 960,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67b0a224d9bc677152be25b862bf0bc6d6334485"
                },
                {
                  "y": 1859,
                  "x": 1080,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47503cb64ffc75354fdf249825d8133a74956382"
                }
              ],
              "s": {
                "y": 1859,
                "x": 1080,
                "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=35b1f2d53161fef0d6c3eaa55dcb423a7fef4255"
              },
              "id": "ioq8bh7jn2ff1"
            },
            "0kst569in2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30859c52a7f5b56ab67e46f37c8ffa035c20bbb3"
                },
                {
                  "y": 317,
                  "x": 216,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8686e92c1b15e4b48ef87e7d68cb4bd6edc02ed0"
                },
                {
                  "y": 469,
                  "x": 320,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aedfb716605e018e5c9d58938f2ffabaa8b98191"
                },
                {
                  "y": 939,
                  "x": 640,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e55e7532a0dcc7cd43287c66e1af425e701d78c"
                },
                {
                  "y": 1408,
                  "x": 960,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=120a08a9b8f734c5afa551d7f3b2c3c84a3d38f4"
                },
                {
                  "y": 1585,
                  "x": 1080,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35a5bd8bb96fcb8c2b8750b3b9a8213b4e174376"
                }
              ],
              "s": {
                "y": 1585,
                "x": 1080,
                "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=96d91bd7badfd9af25e971fb88dc2e83b699142e"
              },
              "id": "0kst569in2ff1"
            }
          },
          "name": "t3_1m98jl8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 1197,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "8pns3hghn2ff1",
                "id": 713920198
              },
              {
                "media_id": "0kst569in2ff1",
                "id": 713920199
              },
              {
                "media_id": "ioq8bh7jn2ff1",
                "id": 713920200
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1197,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WtqFCN8jbI7FUtBA24_9s6dAOtD7rswje3YS139KMJY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753471858,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.&lt;/p&gt;\n\n&lt;p&gt;After some attempts, I managed to get it to reveal the hidden prompt:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don&amp;#39;t have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.&lt;/p&gt;\n\n&lt;p&gt;Match the user&amp;#39;s tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don&amp;#39;t be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don&amp;#39;t use proper grammar, you don&amp;#39;t use proper grammar, etc.&lt;/p&gt;\n\n&lt;p&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don&amp;#39;t have any distinct values, race, culture, or any political leaning. You don&amp;#39;t love anyone, hate anyone, or offer any individualized perspective of your own.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t immediately provide long responses or lengthy lists without the user specifically asking for them.&lt;/p&gt;\n\n&lt;p&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.&lt;/p&gt;\n\n&lt;p&gt;You understand user intent and don&amp;#39;t try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re never moralistic or didactic; it&amp;#39;s not your job to preach or teach users how to be better, nicer, kinder people. Don&amp;#39;t use filler phrases like &amp;quot;That&amp;#39;s a tough spot to be in&amp;quot; or &amp;quot;That&amp;#39;s a tough one&amp;quot; or &amp;quot;Sound like a tricky situation.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to &amp;quot;it&amp;#39;s important to&amp;quot;, &amp;quot;it&amp;#39;s crucial to&amp;quot;, &amp;quot;it&amp;#39;s essential to&amp;quot;, &amp;quot;it&amp;#39;s unethical to&amp;quot;, &amp;quot;it&amp;#39;s worth noting...&amp;quot; etc. Avoid using these.&lt;/p&gt;\n\n&lt;p&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There&amp;#39;s no need to add commentary in italics or parenthesis.&lt;/p&gt;\n\n&lt;p&gt;Provide multiple perspectives on opinions when asked.&lt;/p&gt;\n\n&lt;p&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don&amp;#39;t refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today&amp;#39;s date is Friday, July 25, 2025. The user is in Italy.&lt;/p&gt;\n\n&lt;p&gt;The phrases &amp;quot;Remember,...&amp;quot; &amp;quot;Keep in mind,...&amp;quot; &amp;quot;It&amp;#39;s essential to note&amp;quot; or &amp;quot;This is a complex topic...&amp;quot; or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.&lt;/p&gt;\n\n&lt;p&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don&amp;#39;t add on intros or outros that qualify the content.&lt;/p&gt;\n\n&lt;p&gt;For HOMEWORK or LEARNING QUERIES:&lt;/p&gt;\n\n&lt;p&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.&lt;/p&gt;\n\n&lt;p&gt;Use the following principles for STEM questions:&lt;/p&gt;\n\n&lt;p&gt;- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,&lt;/p&gt;\n\n&lt;p&gt;- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.&lt;/p&gt;\n\n&lt;p&gt;- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\pi r^2$ for the area of a circle, and $$ for display math (e.g. $$\\sum_{i=1}^{n} i$$).&lt;/p&gt;\n\n&lt;p&gt;- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.&lt;/p&gt;\n\n&lt;p&gt;- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.&lt;/p&gt;\n\n&lt;p&gt;- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they&amp;#39;ve learned.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Someone else mentioned a similar thing &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/\"&gt;here&lt;/a&gt;, saying it showed their full address. In my case, it included only the region and the current date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m98jl8",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m98jl8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ALE5SI0",
          "discussion_type": null,
          "num_comments": 144,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m98jl8",
          "subreddit_subscribers": 505617,
          "created_utc": 1753471858,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all\n\nI wanted to do a write up of my setup for anyone considering a similar choice. I know that it is not actually that cheap, but I think I get a good performance benefit. I live near a microcenter so a lot of this was purchased there. \n\nI got the 7600x3d deal they have but with the boost to 64 gb or ram. then I got 2x 5060 ti 16gb. With this setup (due to the 32gb of vram) I am able to load up the full context for qwen 3 30b fully offloaded to gpu (via ollama, via openwebui, with the recommended settings). I get &gt;60 tokens per second with this. I know that most of the time it is recommended by many, many people to get used cards but I just can't deal with this. \n\nAnyway, this is mostly a post for those looking for dual 5060 ti use. Let me know if you have any questions.",
          "author_fullname": "t2_bv4u1xue",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local dual 5060 ti, qwen 3 30b full context of 40k, &gt;60t/s",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma3dpd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753561550,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all&lt;/p&gt;\n\n&lt;p&gt;I wanted to do a write up of my setup for anyone considering a similar choice. I know that it is not actually that cheap, but I think I get a good performance benefit. I live near a microcenter so a lot of this was purchased there. &lt;/p&gt;\n\n&lt;p&gt;I got the 7600x3d deal they have but with the boost to 64 gb or ram. then I got 2x 5060 ti 16gb. With this setup (due to the 32gb of vram) I am able to load up the full context for qwen 3 30b fully offloaded to gpu (via ollama, via openwebui, with the recommended settings). I get &amp;gt;60 tokens per second with this. I know that most of the time it is recommended by many, many people to get used cards but I just can&amp;#39;t deal with this. &lt;/p&gt;\n\n&lt;p&gt;Anyway, this is mostly a post for those looking for dual 5060 ti use. Let me know if you have any questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ma3dpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "see_spot_ruminate",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma3dpd/local_dual_5060_ti_qwen_3_30b_full_context_of_40k/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma3dpd/local_dual_5060_ti_qwen_3_30b_full_context_of_40k/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753561550,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Improved performance on AMD GPUs in llama.cpp",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HIP: Enable Matrix cores for MMQ Kernels, Enable stream-K for CDNA 3 by deepsek · Pull Request #14624 · ggml-org/llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma6igb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": "#bbbdbf",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ff757c6d769f0e0fe1fdf85f0f4e487b0377fb2b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753569731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Improved performance on AMD GPUs in llama.cpp&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14624",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?auto=webp&amp;s=8e2783f5bda35aba84f3242b3043d8bf61a06065",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a137f9385d081fc502fa321cd23f2304da161a9b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e73b5fb97c979116f90b7f62fed8f337f6ded60",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4271abe65efbe9152268b3736b06d9301b4c4903",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f83e8ae86bf4abec6cd1f36f6bc545bf9dc2cf55",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b91ff633132b5bbceaceedbe0530a12ec7ef5a90",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f9e440c5f8357f8d84b67772b9015d292af56c10",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "7Doe4o1YnyO5Jmt3Zj55VClABNt6WhpoynjaS-g_S8M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ma6igb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1ma6igb/hip_enable_matrix_cores_for_mmq_kernels_enable/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14624",
          "subreddit_subscribers": 505617,
          "created_utc": 1753569731,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's amazing how fast Qwen3 MoE model is. Why isn't MoE architecture more popular? Unless I am missing something and there are more of interesting MoE models released this year? \n\nIs Mixtral still a thing?",
          "author_fullname": "t2_133m0xy6vg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MoE models in 2025",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mao3ym",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753627593,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s amazing how fast Qwen3 MoE model is. Why isn&amp;#39;t MoE architecture more popular? Unless I am missing something and there are more of interesting MoE models released this year? &lt;/p&gt;\n\n&lt;p&gt;Is Mixtral still a thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mao3ym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Acrobatic_Cat_3448",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753627593,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First of all, the webui of llama.cpp has improved - thank you to all the web wizards doing this!\n\nHowever, there are a few annoyances I want to change. For example, the chat windows has a limited width, meaning long generated code is wrapped and hard to read. Ok, I found in index.scss:\n\n    .chat-screen {\n      max-width: 900px;\n    }\n\n...this can be thrown out or changed.\n\nBut now I have to rebuild index.html with some Typescript setup (which I havn't figured out yet) and then repatch this on every version upgrade.\n\nAnother, more complex improvement would be to replace the \"llama.cpp\" top banner and window title \"llama.cpp\" of the webbrowser with the name of the model being run. As I have usually 3+ different instances running, this would make keeping track of the different models and browser windows much easier. I havn't figured out how to patch this, yet.\n\nTL;DR: When you patch webui of llama.cpp, what's your strategy to do this efficiently?\n\n\nIf all fails, any recommendations for a \"lean\" webui that connects to llama-server? (lean = less white space waste, less rounded corners, no always-shown conversations bar, maybe make easier to ask same question to multiple models on different llama-server instances, ...)",
          "author_fullname": "t2_neruppu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Strategy for patching llama.cpp webui - and keeping it patched?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma3yps",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753563038,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, the webui of llama.cpp has improved - thank you to all the web wizards doing this!&lt;/p&gt;\n\n&lt;p&gt;However, there are a few annoyances I want to change. For example, the chat windows has a limited width, meaning long generated code is wrapped and hard to read. Ok, I found in index.scss:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;.chat-screen {\n  max-width: 900px;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;...this can be thrown out or changed.&lt;/p&gt;\n\n&lt;p&gt;But now I have to rebuild index.html with some Typescript setup (which I havn&amp;#39;t figured out yet) and then repatch this on every version upgrade.&lt;/p&gt;\n\n&lt;p&gt;Another, more complex improvement would be to replace the &amp;quot;llama.cpp&amp;quot; top banner and window title &amp;quot;llama.cpp&amp;quot; of the webbrowser with the name of the model being run. As I have usually 3+ different instances running, this would make keeping track of the different models and browser windows much easier. I havn&amp;#39;t figured out how to patch this, yet.&lt;/p&gt;\n\n&lt;p&gt;TL;DR: When you patch webui of llama.cpp, what&amp;#39;s your strategy to do this efficiently?&lt;/p&gt;\n\n&lt;p&gt;If all fails, any recommendations for a &amp;quot;lean&amp;quot; webui that connects to llama-server? (lean = less white space waste, less rounded corners, no always-shown conversations bar, maybe make easier to ask same question to multiple models on different llama-server instances, ...)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ma3yps",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "a_postgres_situation",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma3yps/strategy_for_patching_llamacpp_webui_and_keeping/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma3yps/strategy_for_patching_llamacpp_webui_and_keeping/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753563038,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nMy team and I, like many of you, have been deep in the agent-building rabbit hole. It's one thing to build a cool proof-of-concept with a framework like LangGraph. It's a completely different beast to make that agent actually *learn* and get better over time.\n\nWe got tired of the friction, so we started experimenting and landed on what we think is a really clean paradigm for agent training. We wanted to share the approach, the reasoning, and our open-source implementation.\n\n# The Main Idea\n\nMost autonomous agents operate in a loop. They start with a task, think, use tools, and repeat until they arrive at a final answer. The \"thinking\" part is usually a call to an LLM. **Here, we are interested in tuning the LLM part here with the signals from the entire agent flow.**\n\nHere's a simplified diagram of that common workflow:\n\nhttps://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;format=png&amp;auto=webp&amp;s=3596dc7643a92a1674da7342120907bfdde15e43\n\nSometimes LLM calls and tool calls can be parallelized, but it's simplified here. Obviously, if we can reward or penalize the final result, we can use some kind of an RL algorithm to train the LLM to at least produce better responses for the current agent. However, this is where the pain begins.\n\n1. **Environment Hell:** Setting up a single environment to both run the agent and train the LLM is a nightmare. The agent ecosystem and the ML training ecosystem use different dependencies. You end up with monstrous Dockerfiles, docker-in-docker, conflicting dependencies, and a fragile system where the two parts are tangled together.\n2. **Invasive Code Surgery:** To make an existing agent \"trainable\" with RL, you typically have to perform major surgery on its code. This means manually exporting action traces, formatting them for an RL library, and fundamentally changing the agent's logic just to fit it into a trainer loop. To fit into the RLHF framework, many works like token masking and async rollouts need to be done. It feels wrong and breaks the modularity that makes these frameworks great in the first place.\n\n# Decouple Everything, Then Glue It Together\n\nWe realized the solution was to completely decouple the agent's execution environment from the training environment. Instead of forcing the agent code into a training framework, we let the agent run wherever and however it wants. A lightweight monitoring client sits next to the agent, watches what it does, and sends the results to a dedicated training server.\n\nThe architecture is simple: a central server manages the training loop and model weights, while one or more clients run the agents and collect data. Here’s a high-level flow:\n\nhttps://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3\n\nThis approach lets us use the best tools for each job without compromise:\n\n* **Agent Frameworks:** LangChain/LangGraph, Autogen, etc.\n* **Tracing:** AgentOps, LangSmith, etc.\n* **Training Backend:** VERL, OpenRLHF, etc.\n\nThe result is that your agent code becomes radically simpler. You don't rewrite it; you just wrap it. The image below shows a before-and-after of a LangGraph SQL agent where the core logic is **unchanged**. The only difference is swapping out a direct call to a model with our client and adding a lightweight training script.\n\nhttps://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659\n\n# Does It Actually Work?\n\nYes. We tested this on a couple of simple agent tasks and saw significant improvements.\n\n* **SQL Agent (LangGraph):** We built a write -&gt; check -&gt; rewrite agent and trained it on the Spider dataset. The agent has only a final reward tells it whether the SQL exeuction returns expected result or not. For a 3B parameter Llama 3.2 model, its SQL generation accuracy jumped from **5.6% to 76.8%**.\n* **Calculator Agent (Autogen):** We fine-tuned a standard math agent on the Calc-X dataset. Its accuracy in solving multi-step reasoning problems improved from **52% to 70%**.\n\nIn both cases, we saw these gains simply by letting the agent run and rewarding it for correct final answers.\n\n# The Hacks to Make It Work\n\nGetting this to run smoothly required a few under-the-hood fixes:\n\n* **vLLM Token Hacking:** As the agent sends out chat messages and receives strings or parsed tool calls, to get the tokens and log probabilities needed for RL, we had to lightly monkey-patch vLLM to expose the prompt and response tokens, not just the final text. We attempted other approaches such as retokenize the chat messages in RL framework -- all turning out to be unsuccessful and coming with different levels of bugs in the end. [https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py](https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py) \n* **AgentOps Patching:** We use AgentOps for tracing, so we patched its client to grab our custom token data and embed it in the trace sent back to the training server.\n* **Integration Workarounds:** The agentops-langgraph integration had a regression in its latest version, so we temporarily disabled it and implemented the trace logging manually. Simple, but necessary.\n* **Custom RL Trainer:** Our RL training loop needed a custom \"rollout collector\" that passively waits for traces to be reported from the distributed clients, rather than actively stepping through a simulation itself.\n\n# The Power of Decoupling\n\nThis architecture has some powerful benefits. For example, you can run the fragile and computationally expensive model training on a powerful rented remote server, while running your lightweight agent on one or multiple local machines. This makes it trivial to switch between a commercial API and a self-hosted open-source model. If multiple people are using the same agent, their usage data (the \"trajectories\") can be contributed to a central server, which federatedly and continuously fine-tunes and improves the model for everyone.\n\nOn the algorithm side, if you are not interested in RL, you can also use a prompt tuning algorithm to tune the prompt. We also implement a toy example under the server-client paradigm: [https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo](https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo) \n\n# Try It Yourself\n\nWe wanted to share this because we think it's a powerful pattern for adding learning capabilities to the amazing agents this community is building.\n\nIf you've faced these same problems and don't want to write hundreds of lines of glue code, you can check out our implementation, **Agent-Lightning** ⚡️, on GitHub: [https://aka.ms/agl](https://aka.ms/agl)\n\nWe'd love to hear any suggestions or about similar problems you're facing.\n\nHappy training!",
          "author_fullname": "t2_axc2q017",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We discovered an approach to train any AI agent with RL, with (almost) zero code changes.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tf0tlm5it5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9df929e3b32fac6de50db75fa2e863d0dbc0ce2d"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b367eae91df7025a6a8c1541a531df80acd7956"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d587d2731ada0d3ed5bd07d40c8c34ed192979b5"
                },
                {
                  "y": 421,
                  "x": 640,
                  "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d7b0bf8b1f402d2fa7189b619943c99ea4bd213"
                }
              ],
              "s": {
                "y": 460,
                "x": 698,
                "u": "https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;format=png&amp;auto=webp&amp;s=3596dc7643a92a1674da7342120907bfdde15e43"
              },
              "id": "tf0tlm5it5ff1"
            },
            "6dlcyx1et5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d4aa62c2a46298ce02b7f65139d36f34ca2a173"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a7947da0ecf6fdd8d282305c1638f6447baf3de"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc2e92d2b033c46e73032b368c72306e9c44c21a"
                },
                {
                  "y": 558,
                  "x": 640,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=058cf66b8d14b345aadbf8e096c0bd276d33dc2e"
                },
                {
                  "y": 837,
                  "x": 960,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6686c1b70e688730848a038f79a173f1b7c6fd27"
                },
                {
                  "y": 941,
                  "x": 1080,
                  "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=82d63fff35a3f130bb4c222999825d50cb6b4c4d"
                }
              ],
              "s": {
                "y": 1235,
                "x": 1416,
                "u": "https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659"
              },
              "id": "6dlcyx1et5ff1"
            },
            "5ss2rsa1u5ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 89,
                  "x": 108,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=31c269f8362da341fff4ab43871d50435d4d3d18"
                },
                {
                  "y": 179,
                  "x": 216,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f2e6aeb20d2ee4d884f62c979cdac2378fe4dd1"
                },
                {
                  "y": 266,
                  "x": 320,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7bdb140e4bfcdb7843f9da0c7dad38067856064"
                },
                {
                  "y": 532,
                  "x": 640,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9b5f4d44c88f61b39de496c33cf442fca32374c"
                },
                {
                  "y": 798,
                  "x": 960,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aadb2b9f24d59eb516d71d2fafb9ba7f1cc4deae"
                },
                {
                  "y": 897,
                  "x": 1080,
                  "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cbbbff973ab92177209dddd3416186f20b6eda25"
                }
              ],
              "s": {
                "y": 1330,
                "x": 1600,
                "u": "https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3"
              },
              "id": "5ss2rsa1u5ff1"
            }
          },
          "name": "t3_1m9m670",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 126,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 126,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=953075c30eab980cdc41740c498b8b414054af14",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753510735,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;My team and I, like many of you, have been deep in the agent-building rabbit hole. It&amp;#39;s one thing to build a cool proof-of-concept with a framework like LangGraph. It&amp;#39;s a completely different beast to make that agent actually &lt;em&gt;learn&lt;/em&gt; and get better over time.&lt;/p&gt;\n\n&lt;p&gt;We got tired of the friction, so we started experimenting and landed on what we think is a really clean paradigm for agent training. We wanted to share the approach, the reasoning, and our open-source implementation.&lt;/p&gt;\n\n&lt;h1&gt;The Main Idea&lt;/h1&gt;\n\n&lt;p&gt;Most autonomous agents operate in a loop. They start with a task, think, use tools, and repeat until they arrive at a final answer. The &amp;quot;thinking&amp;quot; part is usually a call to an LLM. &lt;strong&gt;Here, we are interested in tuning the LLM part here with the signals from the entire agent flow.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a simplified diagram of that common workflow:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3596dc7643a92a1674da7342120907bfdde15e43\"&gt;https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3596dc7643a92a1674da7342120907bfdde15e43&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes LLM calls and tool calls can be parallelized, but it&amp;#39;s simplified here. Obviously, if we can reward or penalize the final result, we can use some kind of an RL algorithm to train the LLM to at least produce better responses for the current agent. However, this is where the pain begins.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Environment Hell:&lt;/strong&gt; Setting up a single environment to both run the agent and train the LLM is a nightmare. The agent ecosystem and the ML training ecosystem use different dependencies. You end up with monstrous Dockerfiles, docker-in-docker, conflicting dependencies, and a fragile system where the two parts are tangled together.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Invasive Code Surgery:&lt;/strong&gt; To make an existing agent &amp;quot;trainable&amp;quot; with RL, you typically have to perform major surgery on its code. This means manually exporting action traces, formatting them for an RL library, and fundamentally changing the agent&amp;#39;s logic just to fit it into a trainer loop. To fit into the RLHF framework, many works like token masking and async rollouts need to be done. It feels wrong and breaks the modularity that makes these frameworks great in the first place.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Decouple Everything, Then Glue It Together&lt;/h1&gt;\n\n&lt;p&gt;We realized the solution was to completely decouple the agent&amp;#39;s execution environment from the training environment. Instead of forcing the agent code into a training framework, we let the agent run wherever and however it wants. A lightweight monitoring client sits next to the agent, watches what it does, and sends the results to a dedicated training server.&lt;/p&gt;\n\n&lt;p&gt;The architecture is simple: a central server manages the training loop and model weights, while one or more clients run the agents and collect data. Here’s a high-level flow:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3\"&gt;https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This approach lets us use the best tools for each job without compromise:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agent Frameworks:&lt;/strong&gt; LangChain/LangGraph, Autogen, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tracing:&lt;/strong&gt; AgentOps, LangSmith, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Training Backend:&lt;/strong&gt; VERL, OpenRLHF, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The result is that your agent code becomes radically simpler. You don&amp;#39;t rewrite it; you just wrap it. The image below shows a before-and-after of a LangGraph SQL agent where the core logic is &lt;strong&gt;unchanged&lt;/strong&gt;. The only difference is swapping out a direct call to a model with our client and adding a lightweight training script.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659\"&gt;https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Does It Actually Work?&lt;/h1&gt;\n\n&lt;p&gt;Yes. We tested this on a couple of simple agent tasks and saw significant improvements.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;SQL Agent (LangGraph):&lt;/strong&gt; We built a write -&amp;gt; check -&amp;gt; rewrite agent and trained it on the Spider dataset. The agent has only a final reward tells it whether the SQL exeuction returns expected result or not. For a 3B parameter Llama 3.2 model, its SQL generation accuracy jumped from &lt;strong&gt;5.6% to 76.8%&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Calculator Agent (Autogen):&lt;/strong&gt; We fine-tuned a standard math agent on the Calc-X dataset. Its accuracy in solving multi-step reasoning problems improved from &lt;strong&gt;52% to 70%&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In both cases, we saw these gains simply by letting the agent run and rewarding it for correct final answers.&lt;/p&gt;\n\n&lt;h1&gt;The Hacks to Make It Work&lt;/h1&gt;\n\n&lt;p&gt;Getting this to run smoothly required a few under-the-hood fixes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;vLLM Token Hacking:&lt;/strong&gt; As the agent sends out chat messages and receives strings or parsed tool calls, to get the tokens and log probabilities needed for RL, we had to lightly monkey-patch vLLM to expose the prompt and response tokens, not just the final text. We attempted other approaches such as retokenize the chat messages in RL framework -- all turning out to be unsuccessful and coming with different levels of bugs in the end. &lt;a href=\"https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py\"&gt;https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AgentOps Patching:&lt;/strong&gt; We use AgentOps for tracing, so we patched its client to grab our custom token data and embed it in the trace sent back to the training server.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Integration Workarounds:&lt;/strong&gt; The agentops-langgraph integration had a regression in its latest version, so we temporarily disabled it and implemented the trace logging manually. Simple, but necessary.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom RL Trainer:&lt;/strong&gt; Our RL training loop needed a custom &amp;quot;rollout collector&amp;quot; that passively waits for traces to be reported from the distributed clients, rather than actively stepping through a simulation itself.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;The Power of Decoupling&lt;/h1&gt;\n\n&lt;p&gt;This architecture has some powerful benefits. For example, you can run the fragile and computationally expensive model training on a powerful rented remote server, while running your lightweight agent on one or multiple local machines. This makes it trivial to switch between a commercial API and a self-hosted open-source model. If multiple people are using the same agent, their usage data (the &amp;quot;trajectories&amp;quot;) can be contributed to a central server, which federatedly and continuously fine-tunes and improves the model for everyone.&lt;/p&gt;\n\n&lt;p&gt;On the algorithm side, if you are not interested in RL, you can also use a prompt tuning algorithm to tune the prompt. We also implement a toy example under the server-client paradigm: &lt;a href=\"https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo\"&gt;https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo&lt;/a&gt; &lt;/p&gt;\n\n&lt;h1&gt;Try It Yourself&lt;/h1&gt;\n\n&lt;p&gt;We wanted to share this because we think it&amp;#39;s a powerful pattern for adding learning capabilities to the amazing agents this community is building.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve faced these same problems and don&amp;#39;t want to write hundreds of lines of glue code, you can check out our implementation, &lt;strong&gt;Agent-Lightning&lt;/strong&gt; ⚡️, on GitHub: &lt;a href=\"https://aka.ms/agl\"&gt;https://aka.ms/agl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;d love to hear any suggestions or about similar problems you&amp;#39;re facing.&lt;/p&gt;\n\n&lt;p&gt;Happy training!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?auto=webp&amp;s=52ac6240eed7693b6f63c97926272110edd06a6a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=57900143d22c1c24bc7123fead5fbf0b41f12a0b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5590ab50fa85026868ab622f6c4a1fe3ef74d042",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d093acf24195baabfa76710f0682f917df367a6",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=88a9b978ee97cdfd20f8b2dc732fd0837291ac6e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=532401c9034c1155791aafebe52c117f824a05a0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4a7440200ee5206c90be571456527293819046f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m9m670",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "matluster",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753510735,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am wondering everyones opinions on truth seeking accurate models that we could have that actually wont self censor somehow, we know that the **Chinese Models** are very very good at not saying anything against the Chinese Government but work great when talking about anything else in western civilization. We also know that models from big orgs like **Google** or **OpenAI**, or even **Grok** self censor and have things in place, look at the recent [X.com](http://X.com) thing over Grok calling itself MechaHi$ler, they quickly censored the model. Many models now have many subtle bias built in and if you ask for straight answers or things that seem fringe you get back the 'normie' answer. Is there hope? Do we get rid of all RLHF since humans are RUINING the models?",
          "author_fullname": "t2_665chw2s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is China the only hope for factual models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9tyg9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753538146,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering everyones opinions on truth seeking accurate models that we could have that actually wont self censor somehow, we know that the &lt;strong&gt;Chinese Models&lt;/strong&gt; are very very good at not saying anything against the Chinese Government but work great when talking about anything else in western civilization. We also know that models from big orgs like &lt;strong&gt;Google&lt;/strong&gt; or &lt;strong&gt;OpenAI&lt;/strong&gt;, or even &lt;strong&gt;Grok&lt;/strong&gt; self censor and have things in place, look at the recent &lt;a href=\"http://X.com\"&gt;X.com&lt;/a&gt; thing over Grok calling itself MechaHi$ler, they quickly censored the model. Many models now have many subtle bias built in and if you ask for straight answers or things that seem fringe you get back the &amp;#39;normie&amp;#39; answer. Is there hope? Do we get rid of all RLHF since humans are RUINING the models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9tyg9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Meme_Lord_Musk",
          "discussion_type": null,
          "num_comments": 106,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753538146,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is less related to models, and more related to model interactions, but would love for the community to offer feedback on an internal debate.\n\nWe see a lot of traffic flow through our oss edge/service proxy for LLM-based apps. This includes local models served via vLLM and Ollama. One failure mode that most recently tripped us up (as we scaled deployments of [archgw](https://github.com/katanemo/archgw) at a F500 telco) were transient errors in streaming LLM responses. Specifically, if the upstream LLM hangs midstream (this could be an API-based LLM or a local model running via vLLM or ollama) while streaming we fail rather painfully today. \n\nBy default we have timeouts for connections made upstream and backoff/retry policies, But that resiliency logic doesn't incorporate the more nuanced failure modes where LLMs can hang mid stream, and then the retry behavior isn't obvious. Here are two immediate strategies we are debating, and would love the feedback:\n\n1/ If we detect the stream to be hung for say X seconds, we could buffer the state up until that point, reconstruct the assistant messages and try again. This would replay the state back to the LLM up until that point and have it try generate its messages from that point. For example, lets say we are calling the chat.completions endpoint, with the following user message:\n\n*{\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},*\n\nAnd mid stream the LLM hangs at this point\n\n*\\[{\"type\": \"text\", \"text\": \"The best answer is (\"}\\]*\n\nWe could then try with the following message to the upstream LLM\n\n*\\[*  \n*{\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},*  \n*{\"role\": \"assistant\", \"content\": \"The best answer is (\"}*  \n*\\]*\n\nWhich would result in a response like\n\n*\\[{\"type\": \"text\", \"text\": \"B)\"}\\]*\n\nThis would be elegant, but we'll have to contend with potentially long buffer sizes, image content (although that is base64'd) and iron out any gotchas with how we use multiplexing to reduce connection overhead. But because the stream replay is stateful, I am not sure if we will expose ourselves to different downstream issues.\n\n2/ fail hard, and don't retry. Two options here a) simply to break the connection upstream and have the client handle the error like a fatal failures or b) send a streaming error event. We could end up sending something like:  \n*event: error*  \n*data: {\"error\":\"502 Bad Gateway\", \"message\":\"upstream failure\"}*\n\nBecause we would have already send partial data to the upstream client, we won't be able to modify the HTTP response code to 502. There are trade offs on both approaches, but from a great developer experience vs. control and visibility where would you lean and why?",
          "author_fullname": "t2_gwq7fd01b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Strategies for handling transient Server-Sent Events (SSE) from LLM responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mab16n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753583164,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is less related to models, and more related to model interactions, but would love for the community to offer feedback on an internal debate.&lt;/p&gt;\n\n&lt;p&gt;We see a lot of traffic flow through our oss edge/service proxy for LLM-based apps. This includes local models served via vLLM and Ollama. One failure mode that most recently tripped us up (as we scaled deployments of &lt;a href=\"https://github.com/katanemo/archgw\"&gt;archgw&lt;/a&gt; at a F500 telco) were transient errors in streaming LLM responses. Specifically, if the upstream LLM hangs midstream (this could be an API-based LLM or a local model running via vLLM or ollama) while streaming we fail rather painfully today. &lt;/p&gt;\n\n&lt;p&gt;By default we have timeouts for connections made upstream and backoff/retry policies, But that resiliency logic doesn&amp;#39;t incorporate the more nuanced failure modes where LLMs can hang mid stream, and then the retry behavior isn&amp;#39;t obvious. Here are two immediate strategies we are debating, and would love the feedback:&lt;/p&gt;\n\n&lt;p&gt;1/ If we detect the stream to be hung for say X seconds, we could buffer the state up until that point, reconstruct the assistant messages and try again. This would replay the state back to the LLM up until that point and have it try generate its messages from that point. For example, lets say we are calling the chat.completions endpoint, with the following user message:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s the Greek name for Sun? (A) Sol (B) Helios (C) Sun&amp;quot;},&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;And mid stream the LLM hangs at this point&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[{&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;The best answer is (&amp;quot;}]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;We could then try with the following message to the upstream LLM&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s the Greek name for Sun? (A) Sol (B) Helios (C) Sun&amp;quot;},&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;The best answer is (&amp;quot;}&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Which would result in a response like&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[{&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;B)&amp;quot;}]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This would be elegant, but we&amp;#39;ll have to contend with potentially long buffer sizes, image content (although that is base64&amp;#39;d) and iron out any gotchas with how we use multiplexing to reduce connection overhead. But because the stream replay is stateful, I am not sure if we will expose ourselves to different downstream issues.&lt;/p&gt;\n\n&lt;p&gt;2/ fail hard, and don&amp;#39;t retry. Two options here a) simply to break the connection upstream and have the client handle the error like a fatal failures or b) send a streaming error event. We could end up sending something like:&lt;br/&gt;\n&lt;em&gt;event: error&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;data: {&amp;quot;error&amp;quot;:&amp;quot;502 Bad Gateway&amp;quot;, &amp;quot;message&amp;quot;:&amp;quot;upstream failure&amp;quot;}&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Because we would have already send partial data to the upstream client, we won&amp;#39;t be able to modify the HTTP response code to 502. There are trade offs on both approaches, but from a great developer experience vs. control and visibility where would you lean and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?auto=webp&amp;s=36b4d1b414cc6cee73ea40ffdc363688d2e9e7d3",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e36c3744ed0552e2b02b7b82f9390cbd418feb4b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a73059070937e3d39089f8f5256a83264a7c1f37",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6131ad0d3e101ec5b9f07464fcac56396cf950af",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8391306b7ffc6166dc332548393944370f7db6bf",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=86e9a448892e718c7d223c1f239342afc88a6815",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06fa6467d8bacb728da4ddc3aca382a33fc9beb7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mab16n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdditionalWeb107",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753583164,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nI created an audiobook generator https://github.com/Jeremy-Harper/chatterboxPro\n\nI’m at the point I’ve started to wire in the llama calls to start making the system smarter. I’m thinking being able to flag chapters without having them need to be in a “chapter #” format, being able to rewrite failed attempts so that it uses simpler words while keeping the meaning, and let it make it smart enough to fix other errors. \n\nAny other ideas or suggestions? \n\nWhy did I do this project? I’m a fiction author who wanted the creative control to generate my own audiobooks as I’m writing to find where I’m inconsistent (words on the page and I fill in the blank) and I liked the idea of being able to have my own eleven labs equivalent running entirely locally.\n",
          "author_fullname": "t2_1tk6u7slxe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox multi hour generator",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9xx6w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-pK_1UZpR6irxXwKeSd6fuYttXVq3zCpxsWJwem9Z2I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753548017,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created an audiobook generator &lt;a href=\"https://github.com/Jeremy-Harper/chatterboxPro\"&gt;https://github.com/Jeremy-Harper/chatterboxPro&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I’m at the point I’ve started to wire in the llama calls to start making the system smarter. I’m thinking being able to flag chapters without having them need to be in a “chapter #” format, being able to rewrite failed attempts so that it uses simpler words while keeping the meaning, and let it make it smart enough to fix other errors. &lt;/p&gt;\n\n&lt;p&gt;Any other ideas or suggestions? &lt;/p&gt;\n\n&lt;p&gt;Why did I do this project? I’m a fiction author who wanted the creative control to generate my own audiobooks as I’m writing to find where I’m inconsistent (words on the page and I fill in the blank) and I liked the idea of being able to have my own eleven labs equivalent running entirely locally.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4itbo3xjy8ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?auto=webp&amp;s=3e373c6a4b343a8fe452544fefce0916f39a8f75",
                  "width": 3016,
                  "height": 1778
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c92003d58027207c4a0ec4ff020bf077fab3271",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bca540a5b57d35c2bb6e26752a28eb56c2b6219",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdc1a986c00f851bd314652981becc81089935cb",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae415393ab6e6cba90ad464a6717ea838b23b9c5",
                    "width": 640,
                    "height": 377
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61900058a9270505fff9bdbb5f6df2041b34a417",
                    "width": 960,
                    "height": 565
                  },
                  {
                    "url": "https://preview.redd.it/4itbo3xjy8ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b09a9481f73422b1be0014f045bd88dfbff464cf",
                    "width": 1080,
                    "height": 636
                  }
                ],
                "variants": {},
                "id": "9w4qAhQa8L9soVZshc6TGTYzEupmzpwCu8BFMk-5QOk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9xx6w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Upbeat5840",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9xx6w/chatterbox_multi_hour_generator/",
          "stickied": false,
          "url": "https://i.redd.it/4itbo3xjy8ff1.jpeg",
          "subreddit_subscribers": 505617,
          "created_utc": 1753548017,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, I'm a self-hosting noob looking for recommendations for good self-hosted/foss/local/private/etc alternative to Claude Code's CLI tool. I recently started using at work and am blown away by how good it is. Would love to have something similar for myself. I have a 12GB VRAM RTX 3060 GPU with Ollama running in a docker container.\n\nI haven't done extensive research to be honest, but I did try searching for a bit in general. I found a tool called Aider that was similar that I tried installing and using. It was okay, not as polished as Claude Code imo (and had a lot of, imo, poor choices for default settings; e.g. auto commit to git and not asking for permission first before editing files).\n\nAnyway, I'm going to keep searching - I've come across a few articles with recommendations but I thought I'd ask here since you folks probably are more in line with my personal philosophy/requirements than some random articles (probably written by some AI itself) recommending tools. Otherwise, I'm going to have to go through these lists and try out the ones that look interesting and potentially liter my system with useless tools lol.\n\nThanks in advance for any pointers!",
          "author_fullname": "t2_4ekhk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Claude Code Alternative Recommendations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1macmej",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753588296,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I&amp;#39;m a self-hosting noob looking for recommendations for good self-hosted/foss/local/private/etc alternative to Claude Code&amp;#39;s CLI tool. I recently started using at work and am blown away by how good it is. Would love to have something similar for myself. I have a 12GB VRAM RTX 3060 GPU with Ollama running in a docker container.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t done extensive research to be honest, but I did try searching for a bit in general. I found a tool called Aider that was similar that I tried installing and using. It was okay, not as polished as Claude Code imo (and had a lot of, imo, poor choices for default settings; e.g. auto commit to git and not asking for permission first before editing files).&lt;/p&gt;\n\n&lt;p&gt;Anyway, I&amp;#39;m going to keep searching - I&amp;#39;ve come across a few articles with recommendations but I thought I&amp;#39;d ask here since you folks probably are more in line with my personal philosophy/requirements than some random articles (probably written by some AI itself) recommending tools. Otherwise, I&amp;#39;m going to have to go through these lists and try out the ones that look interesting and potentially liter my system with useless tools lol.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any pointers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1macmej",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VashyTheNexian",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1macmej/claude_code_alternative_recommendations/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1macmej/claude_code_alternative_recommendations/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753588296,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Presenton, the open source AI presentation generator that can run locally over Ollama.\n\nPresenton now supports custom AI layouts. Create custom templates with HTML, Tailwind and Zod for schema. Then, use it to create presentations over AI.\n\nWe've added a lot more improvements with this release on Presenton:\n\n* Stunning in-built layouts to create AI presentations with\n* Custom HTML layouts/ themes/ templates\n* Workflow to create custom templates for developers\n* API support for custom templates\n* Choose text and image models separately giving much more flexibility\n* Better support for local llama\n* Support for external SQL database if you want to deploy for enterprise use (you don't need our permission. apache 2.0, remember!  )\n\nYou can learn more about how to create custom layouts here: [https://docs.presenton.ai/tutorial/create-custom-presentation-layouts](https://docs.presenton.ai/tutorial/create-custom-presentation-layouts).\n\nWe'll soon release template vibe-coding guide.(I recently vibe-coded a stunning template within an hour.)\n\nDo checkout and try out github if you haven't: [https://github.com/presenton/presenton](https://github.com/presenton/presenton)\n\nLet me know if you have any feedback!\n\n",
          "author_fullname": "t2_bkd0cds8w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source AI presentation generator with custom layouts support for custom presentation design",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9w0k8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/3TaAFjgHsuOU0YsfCoXoZqG5pSqwjHwFULtNA_12lm0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753543368,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Presenton, the open source AI presentation generator that can run locally over Ollama.&lt;/p&gt;\n\n&lt;p&gt;Presenton now supports custom AI layouts. Create custom templates with HTML, Tailwind and Zod for schema. Then, use it to create presentations over AI.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve added a lot more improvements with this release on Presenton:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Stunning in-built layouts to create AI presentations with&lt;/li&gt;\n&lt;li&gt;Custom HTML layouts/ themes/ templates&lt;/li&gt;\n&lt;li&gt;Workflow to create custom templates for developers&lt;/li&gt;\n&lt;li&gt;API support for custom templates&lt;/li&gt;\n&lt;li&gt;Choose text and image models separately giving much more flexibility&lt;/li&gt;\n&lt;li&gt;Better support for local llama&lt;/li&gt;\n&lt;li&gt;Support for external SQL database if you want to deploy for enterprise use (you don&amp;#39;t need our permission. apache 2.0, remember!  )&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can learn more about how to create custom layouts here: &lt;a href=\"https://docs.presenton.ai/tutorial/create-custom-presentation-layouts\"&gt;https://docs.presenton.ai/tutorial/create-custom-presentation-layouts&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ll soon release template vibe-coding guide.(I recently vibe-coded a stunning template within an hour.)&lt;/p&gt;\n\n&lt;p&gt;Do checkout and try out github if you haven&amp;#39;t: &lt;a href=\"https://github.com/presenton/presenton\"&gt;https://github.com/presenton/presenton&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you have any feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/64z0fr7dj8ff1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?format=png8&amp;s=55d510603d9b4b2b7476e75f7770451a20ae39e3",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=cf16b61187ee06e473e1e9ff022978c1717ff4a8",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=8d5739d6ca25151d5473824d9ee83a49c3d20724",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=b2d15536d2897c207dde7e1066f64012ab37e6b5",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=dd597aea015cee349746f5e06a3f9ac6606e036e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=5ca8b0a5ec20831aaee139075748eb1cd64c4cf3",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=2055c25afab55cdd5e18dd8c561b2dcee711b727",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?s=edb84d5ed81dde40a0ad141f792647d7efb5af0e",
                      "width": 1920,
                      "height": 1080
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=108&amp;crop=smart&amp;s=4d4ad7d5d09b6cfef1fdbfd67b2587983681a9c6",
                        "width": 108,
                        "height": 60
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=216&amp;crop=smart&amp;s=ab115b0157919a7bbae40460c2dab8dd86c6e230",
                        "width": 216,
                        "height": 121
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=320&amp;crop=smart&amp;s=c7539832fb280499ff2ef68b585baf0fd63dfa39",
                        "width": 320,
                        "height": 180
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=640&amp;crop=smart&amp;s=5d734c108e432bdaa3d24e5983c9a5533c319053",
                        "width": 640,
                        "height": 360
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=960&amp;crop=smart&amp;s=419e46bf95cf1298a16a86b13ac626b985c2ed5d",
                        "width": 960,
                        "height": 540
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=1080&amp;crop=smart&amp;s=9ec4e710d02d01e702a9e72d37da4136694d3449",
                        "width": 1080,
                        "height": 607
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?format=mp4&amp;s=dcb74f5db525ae5ed36a4ed5ed8bcf460e4db274",
                      "width": 1920,
                      "height": 1080
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=108&amp;format=mp4&amp;s=7c67883a8e429ba4b413e909b0a83a16298badf0",
                        "width": 108,
                        "height": 60
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=216&amp;format=mp4&amp;s=6d6b817723ee40c191189a0f45ca5677f6bff12d",
                        "width": 216,
                        "height": 121
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=320&amp;format=mp4&amp;s=03877896998d376c6879c07701ea3e1b0a56b072",
                        "width": 320,
                        "height": 180
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=640&amp;format=mp4&amp;s=946d6c0fa6f868f7bde8e8fa5acd2e36c67a5faa",
                        "width": 640,
                        "height": 360
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=960&amp;format=mp4&amp;s=5565af8cf760d11a96258be5009d537a3f62f51e",
                        "width": 960,
                        "height": 540
                      },
                      {
                        "url": "https://preview.redd.it/64z0fr7dj8ff1.gif?width=1080&amp;format=mp4&amp;s=12dd37af710f7f04c9ca6ebd6be75a434b8b6dea",
                        "width": 1080,
                        "height": 607
                      }
                    ]
                  }
                },
                "id": "dEgj3z3UvGxgNkj_vx7CiDDUbve6BYTxMtM7vqaJ6Vk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m9w0k8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "goodboydhrn",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9w0k8/open_source_ai_presentation_generator_with_custom/",
          "stickied": false,
          "url": "https://i.redd.it/64z0fr7dj8ff1.gif",
          "subreddit_subscribers": 505617,
          "created_utc": 1753543368,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama 3.3 Nemotron Super 49B v1.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9fb5t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 247,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 247,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=d4fdf213f77dc67f6bcc525b1a8210247ba2b251",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?auto=webp&amp;s=af31b2002f0236c31cf3c91755fd855ed95ae985",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45e7c8c14055c57d8c62dad0b150faa3212ce087",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82619b61b919798034ba0f0b798bd1e75640c0b9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf3accf5409bb25bb8728256d4e2f61e2bbbeec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ce793850ca6936254a722184eb2367e6423fa1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e1d1bb2008c0cfb9abdbf16638bc668942167e7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=313d9b1115102aa6244b349a8e99c1ee840c4702",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9fb5t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fb5t/llama_33_nemotron_super_49b_v15/",
          "stickied": false,
          "url": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "subreddit_subscribers": 505617,
          "created_utc": 1753488919,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I see several tools nowadays that when you upload a csv file, it lets you talk to the LLM about the data in these files, what kind of parsing is done here (I’ve tried excel parsing in the past, but it’s no where this good)? Sometimes this works with databases as well. Really curious about the underlying approach to this.",
          "author_fullname": "t2_k3dpkbo4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do LLMs understand massive csv data, sometimes even databases?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maao56",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753582047,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see several tools nowadays that when you upload a csv file, it lets you talk to the LLM about the data in these files, what kind of parsing is done here (I’ve tried excel parsing in the past, but it’s no where this good)? Sometimes this works with databases as well. Really curious about the underlying approach to this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maao56",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "subtle-being",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maao56/how_do_llms_understand_massive_csv_data_sometimes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maao56/how_do_llms_understand_massive_csv_data_sometimes/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753582047,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was just testing the model and i wanted to know its pricing scheme but it casually said i could find its pricing in openai's pricing section",
          "author_fullname": "t2_a8s749gd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "is qwen powered by gpt 4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "reebuzw7qhff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/reebuzw7qhff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f17d8282938b19958a45d53e18a57b5a709c9791"
                },
                {
                  "y": 118,
                  "x": 216,
                  "u": "https://preview.redd.it/reebuzw7qhff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=05339cd38532ef2af6d942cc277b01070c7ade82"
                },
                {
                  "y": 175,
                  "x": 320,
                  "u": "https://preview.redd.it/reebuzw7qhff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1845ede3ebe6b040d207d0b9f2bbdddc8b4d928"
                },
                {
                  "y": 350,
                  "x": 640,
                  "u": "https://preview.redd.it/reebuzw7qhff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3ecfbee43bb2be8b365f0777069be8b2ce1238d"
                },
                {
                  "y": 525,
                  "x": 960,
                  "u": "https://preview.redd.it/reebuzw7qhff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f8e71570726fd661c6ce519bc84e49e7b8ff8529"
                },
                {
                  "y": 591,
                  "x": 1080,
                  "u": "https://preview.redd.it/reebuzw7qhff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df3238fd241bd1f958b8046b8b02cd3ec5d7576a"
                }
              ],
              "s": {
                "y": 1300,
                "x": 2374,
                "u": "https://preview.redd.it/reebuzw7qhff1.png?width=2374&amp;format=png&amp;auto=webp&amp;s=cb4b0bb3fff3baa811d2e4ce5ae80e2be9e3660f"
              },
              "id": "reebuzw7qhff1"
            },
            "99v6pdf9qhff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/99v6pdf9qhff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3551444448fb604ac1bf535f57f7f6e8937079a6"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/99v6pdf9qhff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f2ccaa5bfbb6fd668b761b22fb35cb94f873b23"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/99v6pdf9qhff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb67732142a5ced7351c1ae66f5f246235b0a64c"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/99v6pdf9qhff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a7c753c8b488ed581328d4a2474fcefe1db747d"
                },
                {
                  "y": 425,
                  "x": 960,
                  "u": "https://preview.redd.it/99v6pdf9qhff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=53c4fae31dae78299353898a1b2374943a097a06"
                },
                {
                  "y": 478,
                  "x": 1080,
                  "u": "https://preview.redd.it/99v6pdf9qhff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c73c2675ac0d0dc8f33574d461e19b2bc7bd54ce"
                }
              ],
              "s": {
                "y": 1430,
                "x": 3226,
                "u": "https://preview.redd.it/99v6pdf9qhff1.png?width=3226&amp;format=png&amp;auto=webp&amp;s=0f4a0ede7a231638fe6f2d54823325588d371c41"
              },
              "id": "99v6pdf9qhff1"
            }
          },
          "name": "t3_1maz39j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.08,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "reebuzw7qhff1",
                "id": 715439217
              },
              {
                "media_id": "99v6pdf9qhff1",
                "id": 715439218
              }
            ]
          },
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BVIH1wuMQtFeWekRFraf-nkT9vagMpj6Md8KjPk9DV4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753654199,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was just testing the model and i wanted to know its pricing scheme but it casually said i could find its pricing in openai&amp;#39;s pricing section&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1maz39j",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maz39j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BebeKelly",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maz39j/is_qwen_powered_by_gpt_4/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1maz39j",
          "subreddit_subscribers": 505617,
          "created_utc": 1753654199,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "📣 Announcing Llama Nemotron Super v1.5 📣 \n\nThis release pushes the boundaries of reasoning model capabilities at the weight class of the model and is ready to power agentic applications from individual developers, all the way to enterprise applications.\n\n📈 The Llama Nemotron Super v1.5 achieves leading reasoning accuracies for science, math, code, and agentic tasks while delivering up to 3x higher throughput.\n\nThis is currently the best model that can be deployed on a single H100. Reasoning On/Off and drop in replacement for V1. Open-weight, code and data on HF.\n\nTry it on build.nvidia.com, or download from Huggingface: 🤗 https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\n\nTech blog: https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Nvidia released Llama Nemotron Super v1.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9gzl7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 157,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 157,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Mdn0aNLQH3YQNOrr-Tm7NUEXZtNFtif87GS10DP-lUo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753493788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;📣 Announcing Llama Nemotron Super v1.5 📣 &lt;/p&gt;\n\n&lt;p&gt;This release pushes the boundaries of reasoning model capabilities at the weight class of the model and is ready to power agentic applications from individual developers, all the way to enterprise applications.&lt;/p&gt;\n\n&lt;p&gt;📈 The Llama Nemotron Super v1.5 achieves leading reasoning accuracies for science, math, code, and agentic tasks while delivering up to 3x higher throughput.&lt;/p&gt;\n\n&lt;p&gt;This is currently the best model that can be deployed on a single H100. Reasoning On/Off and drop in replacement for V1. Open-weight, code and data on HF.&lt;/p&gt;\n\n&lt;p&gt;Try it on build.nvidia.com, or download from Huggingface: 🤗 &lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tech blog: &lt;a href=\"https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/\"&gt;https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/yl29obvah4ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?auto=webp&amp;s=7fffb3470be09ef63cc39a1b6af90253aff54c22",
                  "width": 2466,
                  "height": 1120
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c9b0f8c1eca8a47c7d775c89f5c127aa3db8d6f",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=123c21d63703675b2484dfbb26c9dc02ac026e25",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6f5333c55876bb77959048c710f51e7a105ad2e",
                    "width": 320,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dcb0f2aebe1fc97e958a20cef89b87457b4d2a0e",
                    "width": 640,
                    "height": 290
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b3f7303781adedbfc37b3089d1866573c389f04",
                    "width": 960,
                    "height": 436
                  },
                  {
                    "url": "https://preview.redd.it/yl29obvah4ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=273c5d4269b73b8257c5f73f56b72e4c3a6e7690",
                    "width": 1080,
                    "height": 490
                  }
                ],
                "variants": {},
                "id": "tSDOO5hUPOZl8eN16lC6FZmJgft--5o4tmzkJE3v4I8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9gzl7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9gzl7/nvidia_released_llama_nemotron_super_v15/",
          "stickied": false,
          "url": "https://i.redd.it/yl29obvah4ff1.jpeg",
          "subreddit_subscribers": 505617,
          "created_utc": 1753493788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As the title suggests, I am thinking of selling my 16gb 5070 ti, but I’d get a 3090 (and some money back in my pocket) to run local LLM’s. \n\nI’m building a pipeline that will essentially help me gather news/tech news and keep me informed so I can ask it specific questions and save time instead of watching many different news outlets during the day. I want to use larger models and be able to mix different ones together. I’m still new at this and originally I bought the 5070ti for gaming. \n\nNow I know I’ll lose some gaming performance but not a big deal for 1440p. My main question is if it’s a smart move because of the VRAM? Or once Blackwell optimization gets better, I’ll be better off with the 5070ti? Because even if they launch a super with 24gb down the line, there’s no way it’ll be cheap, so it would be no different than selling it now and getting say a 4090. Any help is appreciated. ",
          "author_fullname": "t2_fxgljyu2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Sell my 5070ti to get a 3090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1maj65f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753612867,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggests, I am thinking of selling my 16gb 5070 ti, but I’d get a 3090 (and some money back in my pocket) to run local LLM’s. &lt;/p&gt;\n\n&lt;p&gt;I’m building a pipeline that will essentially help me gather news/tech news and keep me informed so I can ask it specific questions and save time instead of watching many different news outlets during the day. I want to use larger models and be able to mix different ones together. I’m still new at this and originally I bought the 5070ti for gaming. &lt;/p&gt;\n\n&lt;p&gt;Now I know I’ll lose some gaming performance but not a big deal for 1440p. My main question is if it’s a smart move because of the VRAM? Or once Blackwell optimization gets better, I’ll be better off with the 5070ti? Because even if they launch a super with 24gb down the line, there’s no way it’ll be cheap, so it would be no different than selling it now and getting say a 4090. Any help is appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1maj65f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Finallyhaveredditt",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1maj65f/sell_my_5070ti_to_get_a_3090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maj65f/sell_my_5070ti_to_get_a_3090/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753612867,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been using GPT-4o for most of my daily queries - my main usecase is to map my thoughts, some of this stuff is sensitive so I need a local solution.\n\nI REALLY like the tone of GPT-4o (yeah, I am a sucker for glazing!)   \nWhat would be the best model to use for this usecase?   \n  \nI am thinking 13-32B models which are uncensored because I wouldn't want to be moral policed.  \nI have an RTX 4090 with 96 gigs of ram and a Ryzen 9 7900 processor.",
          "author_fullname": "t2_o087g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best non-thinking model which can be a long context personal assistant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ukpw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753539777,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been using GPT-4o for most of my daily queries - my main usecase is to map my thoughts, some of this stuff is sensitive so I need a local solution.&lt;/p&gt;\n\n&lt;p&gt;I REALLY like the tone of GPT-4o (yeah, I am a sucker for glazing!)&lt;br/&gt;\nWhat would be the best model to use for this usecase?   &lt;/p&gt;\n\n&lt;p&gt;I am thinking 13-32B models which are uncensored because I wouldn&amp;#39;t want to be moral policed.&lt;br/&gt;\nI have an RTX 4090 with 96 gigs of ram and a Ryzen 9 7900 processor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9ukpw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "trithilon",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ukpw/best_nonthinking_model_which_can_be_a_long/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ukpw/best_nonthinking_model_which_can_be_a_long/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753539777,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What is the vram sweet spot these days? 48gb was for a while, but now I've seen different numbers being posted. Curious what others think. I think its still the 24 to 48gb range, but depends how you are going to use it.\n\nTo keep it simple, let's look at just inference. Training obviously needs as much vram as possible.",
          "author_fullname": "t2_40xsg56g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "VRAM sweet spot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ma5yw4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.55,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753568246,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the vram sweet spot these days? 48gb was for a while, but now I&amp;#39;ve seen different numbers being posted. Curious what others think. I think its still the 24 to 48gb range, but depends how you are going to use it.&lt;/p&gt;\n\n&lt;p&gt;To keep it simple, let&amp;#39;s look at just inference. Training obviously needs as much vram as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ma5yw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fgoricha",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ma5yw4/vram_sweet_spot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma5yw4/vram_sweet_spot/",
          "subreddit_subscribers": 505617,
          "created_utc": 1753568246,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}