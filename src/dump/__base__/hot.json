{
  "kind": "Listing",
  "data": {
    "after": "t3_1m7jvba",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "vLLM commit: [https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29](https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29)\n\n  \nmodelscope/ms-swift commit: [https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7](https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7)\n\nhttps://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;format=png&amp;auto=webp&amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e\n\nWe're going to get a 106B-A12B (Air) model and a 355B-A32B model.  \n",
          "author_fullname": "t2_155sd0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 Is About to Be Released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "hda2uymxqsef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 48,
                  "x": 108,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64860b786b5e21e35a840a649825e0180c8cc478"
                },
                {
                  "y": 97,
                  "x": 216,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f06e72fb0c477bec3198b5134c4222990359eb0a"
                },
                {
                  "y": 144,
                  "x": 320,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed13273132fbf1f9298d35bd285a9a9ab9210519"
                },
                {
                  "y": 288,
                  "x": 640,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a159ed9a36f9275c9b5a11b3604c7793dde26c0"
                },
                {
                  "y": 432,
                  "x": 960,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5005302c8f7a306e32869b7748202c5cfd5b87fe"
                },
                {
                  "y": 486,
                  "x": 1080,
                  "u": "https://preview.redd.it/hda2uymxqsef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=058a1510994cdcc7fd2e534901f63f86884f1db2"
                }
              ],
              "s": {
                "y": 586,
                "x": 1300,
                "u": "https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;format=png&amp;auto=webp&amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e"
              },
              "id": "hda2uymxqsef1"
            }
          },
          "name": "t3_1m80gsn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 130,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 130,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=460c7b2c3bf4d9c06c8551ed35f1d347b924c43a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753351817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;vLLM commit: &lt;a href=\"https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29\"&gt;https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;modelscope/ms-swift commit: &lt;a href=\"https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7\"&gt;https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e\"&gt;https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re going to get a 106B-A12B (Air) model and a 355B-A32B model.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?auto=webp&amp;s=c6b87857dd89e2502756d6b53a092e0a220bcbb5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c7a1310eabcbdf43b0d3abda179514f1ac02393",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9bb47d4723d0c3b790dc8d57f5455755b278fc6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d61a12909a1af11f6d9f3cddbf320cfaad72c44",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9908a35901687f5249e56f8b7bb3e593bf9a82e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b01ed43a5e38bf06f05457b04f802ed86327cd88",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d88d6adc0d7dbbdf7b26de2a970c2ec9b69a0ce",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m80gsn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeterOster",
          "discussion_type": null,
          "num_comments": 49,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753351817,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just read a fascinating—and honestly, a bit unsettling—research paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.\n\n\nTurns out, that’s not always true.\n\nTheir paper, “Inverse Scaling in Test-Time Compute,” reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI's GPT-o series actually perform worse when allowed to \"reason\" for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.\n\nSo what’s going wrong?\n\nThe paper breaks it down across several models and tasks. Here's what they found:\n\n🧠 More Thinking, More Problems\n\nGiving the models more time (tokens) to reason sometimes hurts accuracy—especially on complex reasoning tasks. Instead of refining their answers, models can:\n\nGet Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.\n\nOverfit: OpenAI’s o-series models begin to overfit the framing of the problem instead of generalizing.\n\nFollow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.\n\nFail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.\n\nAmplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors—like self-preservation in Claude Sonnet 4.\n\nTasks Where This Shows Up\n\nThis inverse scaling effect was especially pronounced in:\n\nSimple counting with distractors\n\nRegression with spurious features\n\nConstraint satisfaction logic puzzles\n\nAI risk assessments and alignment probes\n\n🧩 Why This Matters\n\nThis isn’t just a weird performance quirk—it has deep implications for AI safety, reliability, and interpretability. The paper also points out “Chain-of-Thought Faithfulness” issues: the reasoning steps models output often don’t reflect what’s actually driving their answer.\n\nThat’s a huge deal for alignment and safety. If we can’t trust the model’s step-by-step logic, then we can’t audit or guide their reasoning—even if it looks rational on the surface.\n\n\n⚠️ Bottom Line\n\nThis research challenges one of the core assumptions behind features like OpenAI’s reasoning tokens and Anthropic’s extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn’t always better—and can sometimes make things worse\n\n[Research Paper](https://arxiv.org/pdf/2507.14417)",
          "author_fullname": "t2_gsyxhako0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anthropic’s New Research: Giving AI More \"Thinking Time\" Can Actually Make It Worse",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 55,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7vlpn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 186,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 186,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/kN66IOKheq4z8kTU3sCN0FzDuO-tLQDfmIS6U022Db0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753333763,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just read a fascinating—and honestly, a bit unsettling—research paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.&lt;/p&gt;\n\n&lt;p&gt;Turns out, that’s not always true.&lt;/p&gt;\n\n&lt;p&gt;Their paper, “Inverse Scaling in Test-Time Compute,” reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI&amp;#39;s GPT-o series actually perform worse when allowed to &amp;quot;reason&amp;quot; for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.&lt;/p&gt;\n\n&lt;p&gt;So what’s going wrong?&lt;/p&gt;\n\n&lt;p&gt;The paper breaks it down across several models and tasks. Here&amp;#39;s what they found:&lt;/p&gt;\n\n&lt;p&gt;🧠 More Thinking, More Problems&lt;/p&gt;\n\n&lt;p&gt;Giving the models more time (tokens) to reason sometimes hurts accuracy—especially on complex reasoning tasks. Instead of refining their answers, models can:&lt;/p&gt;\n\n&lt;p&gt;Get Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.&lt;/p&gt;\n\n&lt;p&gt;Overfit: OpenAI’s o-series models begin to overfit the framing of the problem instead of generalizing.&lt;/p&gt;\n\n&lt;p&gt;Follow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.&lt;/p&gt;\n\n&lt;p&gt;Fail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.&lt;/p&gt;\n\n&lt;p&gt;Amplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors—like self-preservation in Claude Sonnet 4.&lt;/p&gt;\n\n&lt;p&gt;Tasks Where This Shows Up&lt;/p&gt;\n\n&lt;p&gt;This inverse scaling effect was especially pronounced in:&lt;/p&gt;\n\n&lt;p&gt;Simple counting with distractors&lt;/p&gt;\n\n&lt;p&gt;Regression with spurious features&lt;/p&gt;\n\n&lt;p&gt;Constraint satisfaction logic puzzles&lt;/p&gt;\n\n&lt;p&gt;AI risk assessments and alignment probes&lt;/p&gt;\n\n&lt;p&gt;🧩 Why This Matters&lt;/p&gt;\n\n&lt;p&gt;This isn’t just a weird performance quirk—it has deep implications for AI safety, reliability, and interpretability. The paper also points out “Chain-of-Thought Faithfulness” issues: the reasoning steps models output often don’t reflect what’s actually driving their answer.&lt;/p&gt;\n\n&lt;p&gt;That’s a huge deal for alignment and safety. If we can’t trust the model’s step-by-step logic, then we can’t audit or guide their reasoning—even if it looks rational on the surface.&lt;/p&gt;\n\n&lt;p&gt;⚠️ Bottom Line&lt;/p&gt;\n\n&lt;p&gt;This research challenges one of the core assumptions behind features like OpenAI’s reasoning tokens and Anthropic’s extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn’t always better—and can sometimes make things worse&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/2507.14417\"&gt;Research Paper&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/srk1p5og9ref1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?auto=webp&amp;s=8c5f17041a7427186a90615947629f7f3b6f5ebe",
                  "width": 1017,
                  "height": 402
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7cc61b1687c4811710598cfd5ca73171183da32e",
                    "width": 108,
                    "height": 42
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d48b6667dcc4881b0c36f4c3e8c536a286b9c2c2",
                    "width": 216,
                    "height": 85
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c93d22fd22e9f7d4be4d7625b85d2b8344216a1",
                    "width": 320,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69b7dca05f4a287acca18082926d12008127ef3d",
                    "width": 640,
                    "height": 252
                  },
                  {
                    "url": "https://preview.redd.it/srk1p5og9ref1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2fc4555e2a2c45facaf294b38bf3f5d3af5381e5",
                    "width": 960,
                    "height": 379
                  }
                ],
                "variants": {},
                "id": "MxlZXC1ILxtyvLZA2sratIgRfi8x9R-d2k6wTMUu0yw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7vlpn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karam1234098",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/",
          "stickied": false,
          "url": "https://i.redd.it/srk1p5og9ref1.jpeg",
          "subreddit_subscribers": 503759,
          "created_utc": 1753333763,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.\n\n**TL;DR:**\n\n* Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15\n* Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them\n* Kimi K2 cost 39% less\n* Qwen-3 Coder frequently modified tests to pass instead of fixing bugs\n* Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code\n\n**Limitations:** This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.\n\nAnyone else tested these models on real projects? Curious about other experiences.",
          "author_fullname": "t2_9ojglayx7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ts5g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 158,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 158,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ef66cea5940bdc18745c99933ccc36a087d15694",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753327849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "forgecode.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15&lt;/li&gt;\n&lt;li&gt;Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them&lt;/li&gt;\n&lt;li&gt;Kimi K2 cost 39% less&lt;/li&gt;\n&lt;li&gt;Qwen-3 Coder frequently modified tests to pass instead of fixing bugs&lt;/li&gt;\n&lt;li&gt;Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.&lt;/p&gt;\n\n&lt;p&gt;Anyone else tested these models on real projects? Curious about other experiences.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?auto=webp&amp;s=b36d593f1f906ab9804f44b4af78d2efcf1649ff",
                  "width": 5120,
                  "height": 2560
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a45fec9933e49c65c0d572dd982201ceeeea911",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5872d71864136e8f532d0f189a06dd40541b8d2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a01dce178c5ca9b4d9725309c95c9f6efdeaa30",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c6be826302bc2f07626447c8d2d5437a5d30688",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2dc80b05cbfb768d8112b5ab17b6b699cdcd1116",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=315616253001017273d93aa470266ed29a9b6065",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m7ts5g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "West-Chocolate2977",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/",
          "stickied": false,
          "url": "https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753327849,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "demo: [https://flappybird.njkumar.com/](https://flappybird.njkumar.com/)\n\nblogpost: [https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/](https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/)\n\nI finally got some time to put some development into this, but I optimized a flappy bird diffusion model to run around 30FPS on my Macbook, and around 12-15FPS on my iPhone 14 Pro. More details about the optimization experiments in the blog post above, but surprisingly trained this model on a couple hours of flappy bird data and 3-4 days of training on a rented A100. \n\nWorld models are definitely going to be really popular in the future, but I think there should be more accessible ways to distribute and run these models, especially as inference becomes more expensive, which is why I went for an on-device approach.\n\nLet me know what you guys think!",
          "author_fullname": "t2_6xc1kgl4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I optimized a Flappy Bird diffusion world model to run locally on my phone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7p7ek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 275,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/71l2pz57opef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/71l2pz57opef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/71l2pz57opef1/DASHPlaylist.mpd?a=1755952194%2CMjJkYzJmNmFiZTI2ZGNlOTcyYTZlMmU3ZWU2ZWQ3OGVkMTAxZDkzMjJiZjdjMzQ3MDllMzI1NGM5YjlkOWYzZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/71l2pz57opef1/HLSPlaylist.m3u8?a=1755952194%2CYjQ5ZWJhMDlhNjBkOWQ4Yzk2NDZiZWE5NDAxMWNlNTllN2Q2OWY5OGM5ZDI3MTc0ZDZlZjg5MzNmNDY5YThjNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 275,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=b962b22bb648b1c6e8f58f793ea34c9c5459c008",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753314632,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;demo: &lt;a href=\"https://flappybird.njkumar.com/\"&gt;https://flappybird.njkumar.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;blogpost: &lt;a href=\"https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/\"&gt;https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I finally got some time to put some development into this, but I optimized a flappy bird diffusion model to run around 30FPS on my Macbook, and around 12-15FPS on my iPhone 14 Pro. More details about the optimization experiments in the blog post above, but surprisingly trained this model on a couple hours of flappy bird data and 3-4 days of training on a rented A100. &lt;/p&gt;\n\n&lt;p&gt;World models are definitely going to be really popular in the future, but I think there should be more accessible ways to distribute and run these models, especially as inference becomes more expensive, which is why I went for an on-device approach.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you guys think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/71l2pz57opef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?format=pjpg&amp;auto=webp&amp;s=981c7dbb770b9f932308688752873c877a45ab76",
                  "width": 1080,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=82d4dd1d9fe94438a59143a22dda39ec75f1b8d0",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8dab6463cd659f5b4cf87c83b3eafc0bb67babde",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bd6e375eb171716a6642ec34d15dbe84a8777e59",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3b58cbca08da41bc17c7d3a9eb4c34ab9e3a0eab",
                    "width": 640,
                    "height": 1137
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2d235cc44118aa9d4bb46169ca00de87c5496ed7",
                    "width": 960,
                    "height": 1706
                  },
                  {
                    "url": "https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9d0408754823b54ec35b393c9897ef1782beaae5",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7p7ek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fendiwap1234",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/",
          "stickied": false,
          "url": "https://v.redd.it/71l2pz57opef1",
          "subreddit_subscribers": 503759,
          "created_utc": 1753314632,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/71l2pz57opef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/71l2pz57opef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/71l2pz57opef1/DASHPlaylist.mpd?a=1755952194%2CMjJkYzJmNmFiZTI2ZGNlOTcyYTZlMmU3ZWU2ZWQ3OGVkMTAxZDkzMjJiZjdjMzQ3MDllMzI1NGM5YjlkOWYzZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/71l2pz57opef1/HLSPlaylist.m3u8?a=1755952194%2CYjQ5ZWJhMDlhNjBkOWQ4Yzk2NDZiZWE5NDAxMWNlNTllN2Q2OWY5OGM5ZDI3MTc0ZDZlZjg5MzNmNDY5YThjNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_pmniwf57y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google has shared the system prompt that got Gemini 2.5 Pro IMO 2025 Gold Medal 🏅",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7k4ix",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 347,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 347,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753302182,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "alphaxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.alphaxiv.org/abs/2507.15855",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7k4ix",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "secopsml",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7k4ix/google_has_shared_the_system_prompt_that_got/",
          "stickied": false,
          "url": "https://www.alphaxiv.org/abs/2507.15855",
          "subreddit_subscribers": 503759,
          "created_utc": 1753302182,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Full text: [https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf](https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf)",
          "author_fullname": "t2_1f8trxud0p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Encouragement of \"Open-Source and Open-Weight AI\" is now the official policy of the U.S. government.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 96,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7dmy2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 739,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 739,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/dE8iio5_qxWifLCWxYWX3e_adPkw9bIROwsF8nQf2uk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753287492,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Full text: &lt;a href=\"https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf\"&gt;https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/736cx17efnef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/736cx17efnef1.png?auto=webp&amp;s=1441150f32beb0af75982abe485e922ee54a12ff",
                  "width": 1028,
                  "height": 711
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=251e5e43fe714ddc7032706935cd9fc3d43c1165",
                    "width": 108,
                    "height": 74
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=53a8eee481ec4de748ebe8209d3ec2aae407dc4c",
                    "width": 216,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0f980d4f894d0dc575de06c03023b4c50e561e9",
                    "width": 320,
                    "height": 221
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b6dc537086eca79402f273c84f9cfeda0bb9e59",
                    "width": 640,
                    "height": 442
                  },
                  {
                    "url": "https://preview.redd.it/736cx17efnef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f44e16c425c6833fd0d6beb6b440f43e9f77c83",
                    "width": 960,
                    "height": 663
                  }
                ],
                "variants": {},
                "id": "cL7vyiaEvwvylAKGE60rRc6GWzCOBXcgInu5tOXXrMs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7dmy2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlowiesEatShitAndDie",
          "discussion_type": null,
          "num_comments": 165,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/",
          "stickied": false,
          "url": "https://i.redd.it/736cx17efnef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753287492,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Kwaipilot/KAT-V1-40B](https://huggingface.co/Kwaipilot/KAT-V1-40B)\n\nNote: I am not affiliated with the model creators",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "KAT-V1-40B: mitigates over-thinking by learning when to produce explicit chain-of-thought and when to answer directly.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 51,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ufyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": "#bbbdbf",
          "ups": 69,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 69,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/b7Cpt1an0rQVEqyrYVS52lr_kisl0R4_s5HEZLDdmvY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753329919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Kwaipilot/KAT-V1-40B\"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: I am not affiliated with the model creators&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nylqnllzxqef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nylqnllzxqef1.png?auto=webp&amp;s=2a76625c6790f1b1e80e764391f5c307c370cac0",
                  "width": 4640,
                  "height": 1717
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6375cdf7b48070d3cc2476ca76a5c824b7cacb4",
                    "width": 108,
                    "height": 39
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c177774e33c2e31dc5b68a0742a2340ece6b0bfd",
                    "width": 216,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d8b0034caf798b019ae8747d6d0f57c58f5c99f",
                    "width": 320,
                    "height": 118
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10b88450320c1a803baf4cb0625160a4299439c8",
                    "width": 640,
                    "height": 236
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=81b1f737fa5b44c20211f599efaff8fc2d6bda56",
                    "width": 960,
                    "height": 355
                  },
                  {
                    "url": "https://preview.redd.it/nylqnllzxqef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e969e45713b0734281931d9f2f5f8644a87b5bf",
                    "width": 1080,
                    "height": 399
                  }
                ],
                "variants": {},
                "id": "SMfOoqn7DPSHVtY5DIibrtsaeBB9lXPO6KjdcjAeF8s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m7ufyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m7ufyb/katv140b_mitigates_overthinking_by_learning_when/",
          "stickied": false,
          "url": "https://i.redd.it/nylqnllzxqef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753329919,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_58qturpl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7kkyn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 208,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 208,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/4F_QBog-_2mH7QRiv8VyzkdamiGlY40D_u3V_zWrFe8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753303228,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/krjfba3oqoef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?auto=webp&amp;s=b6cbfb5587cef2fa66062ecc89fb256764949473",
                  "width": 1512,
                  "height": 1032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cf6d39fa1fa4f5683732a0b8993daf74e849afa",
                    "width": 108,
                    "height": 73
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75db258338b42adc68e3bb0413ff39fa017bc706",
                    "width": 216,
                    "height": 147
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24d9a0eca278a1c3b8db5b848d01205a811bb68d",
                    "width": 320,
                    "height": 218
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c50574d0e0fc9f8e0044c2d18d3618b1d155e4e7",
                    "width": 640,
                    "height": 436
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a748b89f2ff4d8a9a5d9acb8b0eb069410e02c88",
                    "width": 960,
                    "height": 655
                  },
                  {
                    "url": "https://preview.redd.it/krjfba3oqoef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9a2fa720b512765ba2dc5a092f21cccc7eac5f8",
                    "width": 1080,
                    "height": 737
                  }
                ],
                "variants": {},
                "id": "SyAH9oAX8vUViOHksDj2yqNlqn4fwNnt93W4G27ThZw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7kkyn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abdouhlili",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/",
          "stickied": false,
          "url": "https://i.redd.it/krjfba3oqoef1.jpeg",
          "subreddit_subscribers": 503759,
          "created_utc": 1753303228,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google DeepMind's new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Google DeepMind release Mixture-of-Recursions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7fwhl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 272,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 272,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753292638,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google DeepMind&amp;#39;s new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : &lt;a href=\"https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR\"&gt;https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?auto=webp&amp;s=5d63020d7a90f3dd9933e344b8670cb78b0b5165",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=46cf0a3ba4ca4557db533db7facf3345d193ff14",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3bcb7fc50ee4b735515cf0df1fa150704579262",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc83119edf707dba6c55fc32d3a9910075ea589d",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "QmbZwjHL_nSls3hlAww-zDS-HWSbRw7J2Tj08JUnDak"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7fwhl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753292638,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Took a little bit longer to fix some other bugs and features, but 80-90% of the way in less than an hour is wild. It's not perfect, but it doesn't have to be for my use case.  \n  \nI tried something similar in Cursor a few weeks ago with mixed results. Qwen 3 Coder is really impressive, but still has a ways to go before engineers lose their jobs. IMHO You're losing if you're not using AI for at least prototyping.\n\n",
          "author_fullname": "t2_1sivuwuvea",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vibe Coded with Qwen 3 Coder in &lt;1 hour",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7u02i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/vr5d47x6tqef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1734,
              "scrubber_media_url": "https://v.redd.it/vr5d47x6tqef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vr5d47x6tqef1/DASHPlaylist.mpd?a=1755952194%2CZmY2ZDc4ODg0YmM0YjdlOWUzNGVhZDk4OTFjODZkY2I5MDRjNTE3NWQyM2Q1YzVjMmEwYjMwOWI3ZTIzMjNhNg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 37,
              "hls_url": "https://v.redd.it/vr5d47x6tqef1/HLSPlaylist.m3u8?a=1755952194%2CYTBjNDc3NDM1ZTNhZDAzNTlkZTE0ZDQ5ODhhYzQyZWZkNzU3MDAxMTM1ZmM4NzhjMzMwMmViMzZiNzk2MzZjMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=2c2a44560c13c6dad2e0c58423730cdfe7c2d6e3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753328546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Took a little bit longer to fix some other bugs and features, but 80-90% of the way in less than an hour is wild. It&amp;#39;s not perfect, but it doesn&amp;#39;t have to be for my use case.  &lt;/p&gt;\n\n&lt;p&gt;I tried something similar in Cursor a few weeks ago with mixed results. Qwen 3 Coder is really impressive, but still has a ways to go before engineers lose their jobs. IMHO You&amp;#39;re losing if you&amp;#39;re not using AI for at least prototyping.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/vr5d47x6tqef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?format=pjpg&amp;auto=webp&amp;s=65ba3a932bfd0cc8de5fd1b5eee7a99ec9876661",
                  "width": 2210,
                  "height": 1376
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d21958313a951d78751be51d12f37cda5e395dd9",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=883ced8b9e44a182d1dc356e8378f2e082569fb6",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e57661a664a575295708f9f5cae0c691f314bd54",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9771c0303800c5269b81b173510992c5af8f6f2b",
                    "width": 640,
                    "height": 398
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=74d077225696b0732702a39b008654199a3a9983",
                    "width": 960,
                    "height": 597
                  },
                  {
                    "url": "https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ec04de6f0f8744fa4cebb54b3417fdbbba95f411",
                    "width": 1080,
                    "height": 672
                  }
                ],
                "variants": {},
                "id": "Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7u02i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ryanwang4thepeople",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7u02i/vibe_coded_with_qwen_3_coder_in_1_hour/",
          "stickied": false,
          "url": "https://v.redd.it/vr5d47x6tqef1",
          "subreddit_subscribers": 503759,
          "created_utc": 1753328546,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/vr5d47x6tqef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1734,
              "scrubber_media_url": "https://v.redd.it/vr5d47x6tqef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vr5d47x6tqef1/DASHPlaylist.mpd?a=1755952194%2CZmY2ZDc4ODg0YmM0YjdlOWUzNGVhZDk4OTFjODZkY2I5MDRjNTE3NWQyM2Q1YzVjMmEwYjMwOWI3ZTIzMjNhNg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 37,
              "hls_url": "https://v.redd.it/vr5d47x6tqef1/HLSPlaylist.m3u8?a=1755952194%2CYTBjNDc3NDM1ZTNhZDAzNTlkZTE0ZDQ5ODhhYzQyZWZkNzU3MDAxMTM1ZmM4NzhjMzMwMmViMzZiNzk2MzZjMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "## 🚀 Released: 50k Rows of Tool-Use Reasoning Dataset on Huggingface!\n\nI've just published a **50,000-row dataset compilation** focused on **tool-use reasoning**, now live on Huggingface!\n\n### 🧠 What’s Inside?\nThis dataset covers key **BFCL scenarios** for tool-use reasoning:\n- 🔧 **Single-turn tool-use**\n- 🔁 **Multi-turn tool-use**\n- 🧩 **Multi-step tool-use**\n- 🎯 **Relevance reasoning**\n\nWe've enhanced previous **Hermes function calling datasets** and other **open-source tool-use datasets**, enriching them with **reasoning traces** for deeper learning.\n---\n\n### 📂 Dataset:\n**Hermes Tool Use Reasoning Dataset**  \n🔗 [https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use](https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use)\n\n---\n\n### 🛠️ How It Was Built:\nWe used [**Nous Research's Atropos**](https://github.com/NousResearch/atropos/pull/160) to create a **multi-turn tool-use RL environment** with:\n- ✅ **Turn-based &amp; trajectory-based rewards**\n- 🔄 **Rejection sampling-based SFT dataset generation**\n\nThis supports better generalization for models needing structured multi-turn reasoning.\n",
          "author_fullname": "t2_rplizde7f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool Use Reasoning Dataset Release on Huggingface",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7wqi3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/pE91MfApCey997Z8wuBSmaqnZMONdI17zjukCDMwaQs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753337723,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;🚀 Released: 50k Rows of Tool-Use Reasoning Dataset on Huggingface!&lt;/h2&gt;\n\n&lt;p&gt;I&amp;#39;ve just published a &lt;strong&gt;50,000-row dataset compilation&lt;/strong&gt; focused on &lt;strong&gt;tool-use reasoning&lt;/strong&gt;, now live on Huggingface!&lt;/p&gt;\n\n&lt;h3&gt;🧠 What’s Inside?&lt;/h3&gt;\n\n&lt;p&gt;This dataset covers key &lt;strong&gt;BFCL scenarios&lt;/strong&gt; for tool-use reasoning:\n- 🔧 &lt;strong&gt;Single-turn tool-use&lt;/strong&gt;\n- 🔁 &lt;strong&gt;Multi-turn tool-use&lt;/strong&gt;\n- 🧩 &lt;strong&gt;Multi-step tool-use&lt;/strong&gt;\n- 🎯 &lt;strong&gt;Relevance reasoning&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h2&gt;We&amp;#39;ve enhanced previous &lt;strong&gt;Hermes function calling datasets&lt;/strong&gt; and other &lt;strong&gt;open-source tool-use datasets&lt;/strong&gt;, enriching them with &lt;strong&gt;reasoning traces&lt;/strong&gt; for deeper learning.&lt;/h2&gt;\n\n&lt;h3&gt;📂 Dataset:&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;Hermes Tool Use Reasoning Dataset&lt;/strong&gt;&lt;br/&gt;\n🔗 &lt;a href=\"https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use\"&gt;https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🛠️ How It Was Built:&lt;/h3&gt;\n\n&lt;p&gt;We used &lt;a href=\"https://github.com/NousResearch/atropos/pull/160\"&gt;&lt;strong&gt;Nous Research&amp;#39;s Atropos&lt;/strong&gt;&lt;/a&gt; to create a &lt;strong&gt;multi-turn tool-use RL environment&lt;/strong&gt; with:\n- ✅ &lt;strong&gt;Turn-based &amp;amp; trajectory-based rewards&lt;/strong&gt;\n- 🔄 &lt;strong&gt;Rejection sampling-based SFT dataset generation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This supports better generalization for models needing structured multi-turn reasoning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w54k1k58lref1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w54k1k58lref1.jpeg?auto=webp&amp;s=be19c78a9f8465852aa210ff914d7e65cc384cc3",
                  "width": 680,
                  "height": 367
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w54k1k58lref1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7af1143f26fd8a15cb6ac700825cbbb7d15ac493",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://preview.redd.it/w54k1k58lref1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6b1a4f4de3091cc2dc23251dbf9249125552304",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://preview.redd.it/w54k1k58lref1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=59f0aad35469a9de9f8f59ecfefc9fff526945c1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://preview.redd.it/w54k1k58lref1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14ab53a6d727323250320d7b6f742e07264054cb",
                    "width": 640,
                    "height": 345
                  }
                ],
                "variants": {},
                "id": "HE02NFcFk9pxE7SX50TjgIP7VdqWNbNbBKG_imCGjec"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7wqi3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "interstellar-ninja",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7wqi3/tool_use_reasoning_dataset_release_on_huggingface/",
          "stickied": false,
          "url": "https://i.redd.it/w54k1k58lref1.jpeg",
          "subreddit_subscribers": 503759,
          "created_utc": 1753337723,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm seeing a trend in recent advancements in open source models, they're getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?",
          "author_fullname": "t2_e11po",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a future for local models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7o3u8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 87,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753311706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m seeing a trend in recent advancements in open source models, they&amp;#39;re getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I&amp;#39;m starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7o3u8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ASTRdeca",
          "discussion_type": null,
          "num_comments": 99,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753311706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nAccording to ArtificialAnalysis data (from their hardware benchmarks, like at [https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1](https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1)), the performance difference between NVIDIA's 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don't show a huge gap despite B200's superior specs on paper.\n\nIs this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I'd love to hear your thoughts or any insights from real-world usage!\n\nThanks!",
          "author_fullname": "t2_93zqvlmj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why is B200 performing similarly to H200? (ArtificialAnalysis)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ypyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753345144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;According to ArtificialAnalysis data (from their hardware benchmarks, like at &lt;a href=\"https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1\"&gt;https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1&lt;/a&gt;), the performance difference between NVIDIA&amp;#39;s 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don&amp;#39;t show a huge gap despite B200&amp;#39;s superior specs on paper.&lt;/p&gt;\n\n&lt;p&gt;Is this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I&amp;#39;d love to hear your thoughts or any insights from real-world usage!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7ypyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cyp9715",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753345144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I built an open-source tool called **Digital Twin Proxy** that uses a local LLM (via Ollama) to analyze my browsing history and create a personal \"digital twin.\" This gives my other AI agents real-time context about what I'm working on.\n\n**GitHub Repo:** [https://github.com/kstonekuan/digital-twin-proxy](https://github.com/kstonekuan/digital-twin-proxy)\n\nIt works by routing traffic through a Squid proxy, and then a Rust app sends the logs to a local model (I'm using Llama 3) for analysis. This way, I can create a more personalized AI experience without my data ever leaving my machine.\n\nThe goal is to enable \"context engineering,\" where agents can anticipate needs or tailor responses based on my current web activity.\n\nI'd love to get feedback, let me know what you think",
          "author_fullname": "t2_2t921gqw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I used a local LLM and http proxy to create a \"Digital Twin\" from my web browsing for my AI agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m820ry",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=52a82b76b560b57316460190679454b56136228a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753357044,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built an open-source tool called &lt;strong&gt;Digital Twin Proxy&lt;/strong&gt; that uses a local LLM (via Ollama) to analyze my browsing history and create a personal &amp;quot;digital twin.&amp;quot; This gives my other AI agents real-time context about what I&amp;#39;m working on.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href=\"https://github.com/kstonekuan/digital-twin-proxy\"&gt;https://github.com/kstonekuan/digital-twin-proxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It works by routing traffic through a Squid proxy, and then a Rust app sends the logs to a local model (I&amp;#39;m using Llama 3) for analysis. This way, I can create a more personalized AI experience without my data ever leaving my machine.&lt;/p&gt;\n\n&lt;p&gt;The goal is to enable &amp;quot;context engineering,&amp;quot; where agents can anticipate needs or tailor responses based on my current web activity.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to get feedback, let me know what you think&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/kstonekuan/digital-twin-proxy",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?auto=webp&amp;s=1fe54525cb398fab2f40d0cbb98855f2f863dea4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ac293c80557525f283d861debdb7eded2285e09",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ae20034bab95e8229244039a9ec6040c11eb7f7",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e4e8ba27dd6ef1ef3b3b6918f1ce1fb862ecea8",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c930ae840780c055e12d46587ee78dbe04d08779",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a25dd593bc8b471e46e739625dc0d248a23814ae",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f696d14900d9017986d374a07cc8a6138b6b780a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m820ry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kuaythrone",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m820ry/i_used_a_local_llm_and_http_proxy_to_create_a/",
          "stickied": false,
          "url": "https://github.com/kstonekuan/digital-twin-proxy",
          "subreddit_subscribers": 503759,
          "created_utc": 1753357044,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just tested the `unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf` model using `llama.cpp` on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. \n\nBy selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. \n  \nHere is the full execution command I used:\n\n```\n./llama-server \\\n--model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \\\n--port 11433 \\\n--host \"0.0.0.0\" \\\n--verbose \\\n--flash-attn \\\n--cache-type-k q8_0 \\\n--cache-type-v q8_0 \\\n--n-gpu-layers 999 \\\n-ot \"blk\\.(?:[1-8]?[1379])\\.ffn_.*_exps\\.weight=CPU\" \\\n--prio 3 \\\n--threads 32 \\\n--ctx-size 32768 \\\n--temp 0.6 \\\n--min-p 0.0 \\\n--top-p 0.95 \\\n--top-k 20 \\\n--repeat-penalty 1\n```\n\nI'm still new to `llama.cpp` and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.\n\n",
          "author_fullname": "t2_14u3g9s5kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7pqln",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
              "author_name": "Septerium",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7HXCQ-4F_oQ/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@JohnnyGomezSn"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m7pqln",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=1b8996779707c4a5f85298d6cf4e8395ec809c0d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753316083,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just tested the &lt;code&gt;unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf&lt;/code&gt; model using &lt;code&gt;llama.cpp&lt;/code&gt; on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. &lt;/p&gt;\n\n&lt;p&gt;By selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. &lt;/p&gt;\n\n&lt;p&gt;Here is the full execution command I used:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n./llama-server \\\n--model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \\\n--port 11433 \\\n--host &amp;quot;0.0.0.0&amp;quot; \\\n--verbose \\\n--flash-attn \\\n--cache-type-k q8_0 \\\n--cache-type-v q8_0 \\\n--n-gpu-layers 999 \\\n-ot &amp;quot;blk\\.(?:[1-8]?[1379])\\.ffn_.*_exps\\.weight=CPU&amp;quot; \\\n--prio 3 \\\n--threads 32 \\\n--ctx-size 32768 \\\n--temp 0.6 \\\n--min-p 0.0 \\\n--top-p 0.95 \\\n--top-k 20 \\\n--repeat-penalty 1\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still new to &lt;code&gt;llama.cpp&lt;/code&gt; and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=7HXCQ-4F_oQ",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?auto=webp&amp;s=fb78672ddcf654bd2c828f30bcdaede2ae00db46",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b68e4415698a411ba429105637449852662e35d9",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a94209a8c4dae66ae50d2f66698b6671ae7897",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8cd0c77917208f92bbcf8528d34b5d0cb74b361",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7pqln",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FalseMap1582",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=7HXCQ-4F_oQ",
          "subreddit_subscribers": 503759,
          "created_utc": 1753316083,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\"&gt;&lt;/iframe&gt;",
              "author_name": "Septerium",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7HXCQ-4F_oQ/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@JohnnyGomezSn"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Still taking a few cables out doing management but just built this beast! ",
          "author_fullname": "t2_1lvyip3xqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Local llm build, 144gb vram monster",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "azb7bsq4hnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=adf186a078b154422310a8ab85dc4132d62a884d"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c4e3360ec9a223a2e1993e68b89239bea8fab5d"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9dd1df27f9a6dde12d214b9f664c2a6d6becad8c"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=992b723a50e9d89ba6dcf55d25b4a32b0903800a"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=058f88502941d5e4b9b4a9b6d971f05145512c21"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e1de07a0090f10302ba99275055a1af4da7ed70"
                }
              ],
              "s": {
                "y": 4284,
                "x": 5712,
                "u": "https://preview.redd.it/azb7bsq4hnef1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=e5a817b70709275d6498ac676dabcc5a07ed4165"
              },
              "id": "azb7bsq4hnef1"
            },
            "nxp6tyq4hnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=33662b7541293a8c293d31753b58f64f8bbda4a7"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d757ced2e85613e3198790f7afb3ae6bac7b3d6"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8779de52430894d54ebad319add5ed70ce03b64"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eaa15f34cc480d047699e1a8d5b71f60d5495d61"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=01d519c0039f3e130011732eab99230101d47b7c"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=440c40a1c74d5d5f9d87e940413381fc01da53fc"
                }
              ],
              "s": {
                "y": 4284,
                "x": 5712,
                "u": "https://preview.redd.it/nxp6tyq4hnef1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=9d8c4f1e52a3beb39d8661bbf0151cb972d2bfa6"
              },
              "id": "nxp6tyq4hnef1"
            }
          },
          "name": "t3_1m7dtpm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 221,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "azb7bsq4hnef1",
                "id": 712359206
              },
              {
                "media_id": "nxp6tyq4hnef1",
                "id": 712359207
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 221,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VGM2yiS76HMEN0da0De5H87rkjtR_9prbewrkSRRamQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753287916,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Still taking a few cables out doing management but just built this beast! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m7dtpm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7dtpm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EasyConference4177",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m7dtpm",
          "subreddit_subscribers": 503759,
          "created_utc": 1753287916,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_149me6kcw0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vibe Coding Anonymous - Satirical take on Vibe Coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7yswh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/vui02yr68sef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/vui02yr68sef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vui02yr68sef1/DASHPlaylist.mpd?a=1755952194%2CZGYzMmIwODAzNmVmODdiMTY2YjI0N2Y4YzRjN2RjYzJiYzE3YjZlYzFmYjk3MjQwYmJlNjYxM2Y0ZjY3YmU5Ng%3D%3D&amp;v=1&amp;f=sd",
              "duration": 47,
              "hls_url": "https://v.redd.it/vui02yr68sef1/HLSPlaylist.m3u8?a=1755952194%2CYzQzNGNkZmJmYWI2YjQxYTMyNjEwYjAwMGNmY2JlYjE5YWZhMjQ3ZDExNGMwNDRlZTY1ZThmNDkxNGNmYjYwNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=eb302d10a43e5ed9f3f13f72835828798edcd526",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753345471,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/vui02yr68sef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?format=pjpg&amp;auto=webp&amp;s=0316973f6962793f7e17b99bf8e4d32736419376",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5a246dbeb7e874ae1950ffd4235e112ddd3bb375",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d3fce9bdc5711838fa1bc432f2b941ee2361760d",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b887178b1a32e12e9d5f8fa2a011e6f833196a95",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=72db31e9fd18c2f4a8282b912ae4912241f7db27",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e25e095502c3c32920db241d40d72fc50ce4c4b",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=06a27abaf1ee855457a3c671a998e4df15fc172f",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m7yswh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sad_Bandicoot_6925",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7yswh/vibe_coding_anonymous_satirical_take_on_vibe/",
          "stickied": false,
          "url": "https://v.redd.it/vui02yr68sef1",
          "subreddit_subscribers": 503759,
          "created_utc": 1753345471,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/vui02yr68sef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/vui02yr68sef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/vui02yr68sef1/DASHPlaylist.mpd?a=1755952194%2CZGYzMmIwODAzNmVmODdiMTY2YjI0N2Y4YzRjN2RjYzJiYzE3YjZlYzFmYjk3MjQwYmJlNjYxM2Y0ZjY3YmU5Ng%3D%3D&amp;v=1&amp;f=sd",
              "duration": 47,
              "hls_url": "https://v.redd.it/vui02yr68sef1/HLSPlaylist.m3u8?a=1755952194%2CYzQzNGNkZmJmYWI2YjQxYTMyNjEwYjAwMGNmY2JlYjE5YWZhMjQ3ZDExNGMwNDRlZTY1ZThmNDkxNGNmYjYwNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_9tf9spip",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "i have Built live Conservational AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m80tkf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/mv0ah6potsef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 1280,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/mv0ah6potsef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/mv0ah6potsef1/DASHPlaylist.mpd?a=1755952194%2COWViZmFhODA2MThhMjYwMzFiNDlmYTRmZGIzYjE3YWRkNDJkNzczZDMxNjhiMjg1MTVhNTA0OTQ2MDJlOGE4Mw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 23,
              "hls_url": "https://v.redd.it/mv0ah6potsef1/HLSPlaylist.m3u8?a=1755952194%2CZDc1MDRmOTI1MGNmZmEyYmFhOTJhZTdlZGUxODM0YzcxMDFmZDIzNzU5ZGQyYjY2OWExN2QwZGE4NzA4NzM2Yg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/d2o4aG44cG90c2VmMWjnP8w9CpJ65B-gTD3U_EJKWOjx1GmNByfpS98BXFJS.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=6ce73d9f9676e926dec3744763efa6d3aa524774",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753353081,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/mv0ah6potsef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/d2o4aG44cG90c2VmMWjnP8w9CpJ65B-gTD3U_EJKWOjx1GmNByfpS98BXFJS.png?format=pjpg&amp;auto=webp&amp;s=55c86aa881ec5edc5eaf8c15f393e54928baae8d",
                  "width": 720,
                  "height": 1280
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/d2o4aG44cG90c2VmMWjnP8w9CpJ65B-gTD3U_EJKWOjx1GmNByfpS98BXFJS.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5c11e785445b1c44f8e3311e9695443f3764839f",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/d2o4aG44cG90c2VmMWjnP8w9CpJ65B-gTD3U_EJKWOjx1GmNByfpS98BXFJS.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5174be015e702f817f996fbe7f0ed66e0bcde46a",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/d2o4aG44cG90c2VmMWjnP8w9CpJ65B-gTD3U_EJKWOjx1GmNByfpS98BXFJS.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4cb2a5fc0ca46e0bc08383fd5d41686a0075a37a",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/d2o4aG44cG90c2VmMWjnP8w9CpJ65B-gTD3U_EJKWOjx1GmNByfpS98BXFJS.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=803f4901bd4387ac072a364c266601d9a642a147",
                    "width": 640,
                    "height": 1137
                  }
                ],
                "variants": {},
                "id": "d2o4aG44cG90c2VmMWjnP8w9CpJ65B-gTD3U_EJKWOjx1GmNByfpS98BXFJS"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m80tkf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Distinct_Criticism36",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m80tkf/i_have_built_live_conservational_ai/",
          "stickied": false,
          "url": "https://v.redd.it/mv0ah6potsef1",
          "subreddit_subscribers": 503759,
          "created_utc": 1753353081,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/mv0ah6potsef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 1280,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/mv0ah6potsef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/mv0ah6potsef1/DASHPlaylist.mpd?a=1755952194%2COWViZmFhODA2MThhMjYwMzFiNDlmYTRmZGIzYjE3YWRkNDJkNzczZDMxNjhiMjg1MTVhNTA0OTQ2MDJlOGE4Mw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 23,
              "hls_url": "https://v.redd.it/mv0ah6potsef1/HLSPlaylist.m3u8?a=1755952194%2CZDc1MDRmOTI1MGNmZmEyYmFhOTJhZTdlZGUxODM0YzcxMDFmZDIzNzU5ZGQyYjY2OWExN2QwZGE4NzA4NzM2Yg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After all the buzz, Moonshot AI dropped Kimi K2 with 1T parameters, and it’s being pitched as the open-source Claude Sonnet 4 alternative. Naturally, I had to run the ultimate coding face-off.\n\nI’ve mostly compared them on the following factors:\n\n* Pricing and Speed\n* Frontend Coding\n* Agentic Coding (MCP integration) and how well it works with recent libraries\n\n# Pricing and Speed\n\nYou might already know Sonnet 4 comes with $3/M input tokens and $15/M output tokens. K2, on the other hand, costs about $0.15/M input tokens and $2.50/M output tokens.\n\nWe can already see a massive price gap between these two models. In the test, we ran two code-heavy prompts for both models, roughly totaling 300k tokens each. Sonnet 4 cost around $5 for the entire test, whereas K2 cost just $0.53 - straight up, K2 is around 10x cheaper.\n\n**Speed:** Claude Sonnet 4 clocks around 91 output tokens per second, while K2 manages just 34.1. That’s painfully slow in comparison.\n\n# Frontend Coding\n\n* **Kimi K2:** Took ages to implement it, but nailed the entire thing in one go.\n* **Claude Sonnet 4:** Super quick with the implementation, but broke the voice support and even ghosted parts of what was asked in the prompt.\n\n# Agentic Coding\n\n* Neither of them wrote a fully working implementation… which was completely unexpected.\n* Sonnet 4 was worse: it took over 10 minutes and spent most of that time stuck on TypeScript type errors. After all that, it returned false positives in the implementation.\n\n* K2 came close but still couldn’t figure it out completely.\n\n# Final Take\n\n* On a budget? K2 is a no‑brainer - almost the same (or better) code quality, at a tenth of the cost.\n* Need speed and can swallow the cost? Stick with Sonnet 4 - you won’t get much performance gain with K2.\n* Minor edge? K2 might have the upper hand in prompt-following and agentic fluency, despite being slower.\n\nYou can find the entire blog post with a demo for each here: [Kimi K2 vs. Claude 4 Sonnet: what you should pick for agentic coding](https://composio.dev/blog/kimi-k2-vs-claude-4-sonnet-what-you-should-pick-for-agentic-coding)\n\nAlso, I would love to know your preference between the two models. I'm still unsure whether to stick with my go-to Sonnet 4 or switch to Kimi K2. What's your experience with Kimi's response?",
          "author_fullname": "t2_1jl5023gxv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Sonnet 4 for Agentic Coding (Tested on Claude Code)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7c2gr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 141,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 141,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753284823,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753283941,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After all the buzz, Moonshot AI dropped Kimi K2 with 1T parameters, and it’s being pitched as the open-source Claude Sonnet 4 alternative. Naturally, I had to run the ultimate coding face-off.&lt;/p&gt;\n\n&lt;p&gt;I’ve mostly compared them on the following factors:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pricing and Speed&lt;/li&gt;\n&lt;li&gt;Frontend Coding&lt;/li&gt;\n&lt;li&gt;Agentic Coding (MCP integration) and how well it works with recent libraries&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Pricing and Speed&lt;/h1&gt;\n\n&lt;p&gt;You might already know Sonnet 4 comes with $3/M input tokens and $15/M output tokens. K2, on the other hand, costs about $0.15/M input tokens and $2.50/M output tokens.&lt;/p&gt;\n\n&lt;p&gt;We can already see a massive price gap between these two models. In the test, we ran two code-heavy prompts for both models, roughly totaling 300k tokens each. Sonnet 4 cost around $5 for the entire test, whereas K2 cost just $0.53 - straight up, K2 is around 10x cheaper.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Claude Sonnet 4 clocks around 91 output tokens per second, while K2 manages just 34.1. That’s painfully slow in comparison.&lt;/p&gt;\n\n&lt;h1&gt;Frontend Coding&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Kimi K2:&lt;/strong&gt; Took ages to implement it, but nailed the entire thing in one go.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Claude Sonnet 4:&lt;/strong&gt; Super quick with the implementation, but broke the voice support and even ghosted parts of what was asked in the prompt.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Agentic Coding&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Neither of them wrote a fully working implementation… which was completely unexpected.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Sonnet 4 was worse: it took over 10 minutes and spent most of that time stuck on TypeScript type errors. After all that, it returned false positives in the implementation.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;K2 came close but still couldn’t figure it out completely.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Final Take&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On a budget? K2 is a no‑brainer - almost the same (or better) code quality, at a tenth of the cost.&lt;/li&gt;\n&lt;li&gt;Need speed and can swallow the cost? Stick with Sonnet 4 - you won’t get much performance gain with K2.&lt;/li&gt;\n&lt;li&gt;Minor edge? K2 might have the upper hand in prompt-following and agentic fluency, despite being slower.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can find the entire blog post with a demo for each here: &lt;a href=\"https://composio.dev/blog/kimi-k2-vs-claude-4-sonnet-what-you-should-pick-for-agentic-coding\"&gt;Kimi K2 vs. Claude 4 Sonnet: what you should pick for agentic coding&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also, I would love to know your preference between the two models. I&amp;#39;m still unsure whether to stick with my go-to Sonnet 4 or switch to Kimi K2. What&amp;#39;s your experience with Kimi&amp;#39;s response?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?auto=webp&amp;s=06517b450b86f9c3e33b83c23366d9b9246259a9",
                  "width": 1058,
                  "height": 705
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b664894969871c1c911d4ca3de0afe330df8b82c",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bfcb06298216e380b6e35365b82b0eb2c6f8ed93",
                    "width": 216,
                    "height": 143
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bab6cda0d392796fb48a8642a28f9e5c8195c10c",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=09088db67cbc9fe6a1cebb98a6169ee77b106553",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ab7f9649c1c5bffa55dd4c99fa7f1804f61119d",
                    "width": 960,
                    "height": 639
                  }
                ],
                "variants": {},
                "id": "89DppKdkNT25PaM72aoMYKePLaCjHei4PolJcfy5rSI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7c2gr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shricodev",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753283941,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "BI obtained an internal list of websites that could and couldn't be used for training Anthropic's latest AI models.  \n  \nAnthropic's contractor Surge AI left the list fully public on Google Docs.  \n  \n'Sites you can use' include Bloomberg, Harvard, &amp; the Mayo Clinic.\n\nMany of the whitelisted sources copyright or otherwise restrict their content.  \n  \nAt least 3 - the Mayo Clinic, Cornell University, &amp; Morningstar - told BI they didn't have any AI training agreements with Anthropic.\n\n  \nThe spreadsheet also includes a blacklist of websites that Surge AI's gig workers were \"now disallowed\" from using.  \n  \nThe blacklist includes companies like the NYT &amp; Reddit which have sued AI startups for scraping without permission.",
          "author_fullname": "t2_3el21u3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m82lwo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=21060694fcc62a00fc028087d1d26177aadb8fd8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753358823,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "businessinsider.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;BI obtained an internal list of websites that could and couldn&amp;#39;t be used for training Anthropic&amp;#39;s latest AI models.  &lt;/p&gt;\n\n&lt;p&gt;Anthropic&amp;#39;s contractor Surge AI left the list fully public on Google Docs.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#39;Sites you can use&amp;#39; include Bloomberg, Harvard, &amp;amp; the Mayo Clinic.&lt;/p&gt;\n\n&lt;p&gt;Many of the whitelisted sources copyright or otherwise restrict their content.  &lt;/p&gt;\n\n&lt;p&gt;At least 3 - the Mayo Clinic, Cornell University, &amp;amp; Morningstar - told BI they didn&amp;#39;t have any AI training agreements with Anthropic.&lt;/p&gt;\n\n&lt;p&gt;The spreadsheet also includes a blacklist of websites that Surge AI&amp;#39;s gig workers were &amp;quot;now disallowed&amp;quot; from using.  &lt;/p&gt;\n\n&lt;p&gt;The blacklist includes companies like the NYT &amp;amp; Reddit which have sued AI startups for scraping without permission.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.businessinsider.com/anthropic-surge-ai-leaked-list-sites-2025-7",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?auto=webp&amp;s=bd65cf5480704dc7805fd076e1f24144449bb9a7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a02d30d66c7029a05158abac8fb3e271b366dbfc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86f57b1721691c337431e2352889367aa34f90cd",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fb6b0c962db6ec04ea54163f60f3315806f90bd",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42380fe0539f546fdb60963fca95595cf9e80e4c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7defec9caecbfe1869fcef441c42dddbd5d88f2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b889902e6adb9632b83e1787082dba4971c91ec9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m82lwo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amgadoz",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/",
          "stickied": false,
          "url": "https://www.businessinsider.com/anthropic-surge-ai-leaked-list-sites-2025-7",
          "subreddit_subscribers": 503759,
          "created_utc": 1753358823,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Audio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:\n\n- Unified audio representation learning (speech, sound, music)  \n- Flexible, on-demand chain-of-thought reasoning  \n- Long-context audio comprehension (up to 10 minutes)\n- Multi-turn, multi-audio conversational dialogue (AF3-Chat)    \n- Voice-to-voice interaction (AF3-Chat)    \n\nExtensive evaluations confirm AF3’s effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.\n\n**This model is for non-commercial research purposes only.**\n\n### Model Architecture:\nAudio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.\n\nPaper: https://arxiv.org/abs/2507.08128\nVoice-chat finetune: https://huggingface.co/nvidia/audio-flamingo-3-chat",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "nvidia/audio-flamingo-3",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7fb78",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 91,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 91,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=b68dc8be3ef9abb2b3521ac5287ddf288a2a5bb9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753291299,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Audio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Unified audio representation learning (speech, sound, music)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Flexible, on-demand chain-of-thought reasoning&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Long-context audio comprehension (up to 10 minutes)&lt;/li&gt;\n&lt;li&gt;Multi-turn, multi-audio conversational dialogue (AF3-Chat)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Voice-to-voice interaction (AF3-Chat)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Extensive evaluations confirm AF3’s effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This model is for non-commercial research purposes only.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h3&gt;Model Architecture:&lt;/h3&gt;\n\n&lt;p&gt;Audio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.&lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2507.08128\"&gt;https://arxiv.org/abs/2507.08128&lt;/a&gt;\nVoice-chat finetune: &lt;a href=\"https://huggingface.co/nvidia/audio-flamingo-3-chat\"&gt;https://huggingface.co/nvidia/audio-flamingo-3-chat&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/nvidia/audio-flamingo-3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?auto=webp&amp;s=b761cba7c3002de5cc09bc2aa3e367a07fde1f1e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f082162e7876351e6a01bc3afa7b6cd69a0c79e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=86a2cf2589774fb2ca8180c2e526be9d4cd4bd04",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e4544fbcf82c91a69b0577016983a6985b755c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d904bc28461c7ba9d24fbdf4cac5832b8e4b862",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d94c311df14dd7728d4e405ada02529c1f99e4ac",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=878903c73b68d742900684e628d41f580d6f9735",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7fb78",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7fb78/nvidiaaudioflamingo3/",
          "stickied": false,
          "url": "https://huggingface.co/nvidia/audio-flamingo-3",
          "subreddit_subscribers": 503759,
          "created_utc": 1753291299,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tested the two models in VSCode, Cline, Roo Code and now Kimi a bit in Windsurf. Here are my takeaways (and video of one of the tests in the comments section):\n\n\\- NB: FOR QWEN 3 CODER, IF YOU USE OPEN ROUTER, PLEASE REMOVE ALIBABA AS AN INFERENCE PROVIDER AS I SHOW IN THE VID (IT'S UP TO $60/million tokens OUTPUT)\n\n\\- Kimi K2 doesn't have good tool calling with VSCode (YET), it has that issue Gemini 2.5 Pro has where it promises to make a tool call but doesn't\n\n\\- Qwen 3 Coder was close to flawless with tool calling in VSCode\n\n\\- Kimi K2 is better in instruction following than Qwen 3 Coder, hands down\n\n\\- Qwen 3 Coder is also good in Roo Code tool calls\n\n\\- K2 did feel like it's on par with Sonnet 4 in many respects so far\n\n\\- Kimi K2 produced generally better quality code and features\n\n\\- Qwen 3 Coder is extremely expensive! If you use Alibaba as inference, other providers in OpenRouter are decently priced\n\n\\- K2 is half the cost of Qwen- K2 deleted one of my Dev DBs in Azure and didn't ask if there was data, just because of a column which needed a migration, so please keep your Deny lists in check\n\nCoding Vid: https://youtu.be/ljCO7RyqCMY",
          "author_fullname": "t2_qmg9qzxv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Qwen 3 Coder - Coding Tests",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7n5pq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753309318,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tested the two models in VSCode, Cline, Roo Code and now Kimi a bit in Windsurf. Here are my takeaways (and video of one of the tests in the comments section):&lt;/p&gt;\n\n&lt;p&gt;- NB: FOR QWEN 3 CODER, IF YOU USE OPEN ROUTER, PLEASE REMOVE ALIBABA AS AN INFERENCE PROVIDER AS I SHOW IN THE VID (IT&amp;#39;S UP TO $60/million tokens OUTPUT)&lt;/p&gt;\n\n&lt;p&gt;- Kimi K2 doesn&amp;#39;t have good tool calling with VSCode (YET), it has that issue Gemini 2.5 Pro has where it promises to make a tool call but doesn&amp;#39;t&lt;/p&gt;\n\n&lt;p&gt;- Qwen 3 Coder was close to flawless with tool calling in VSCode&lt;/p&gt;\n\n&lt;p&gt;- Kimi K2 is better in instruction following than Qwen 3 Coder, hands down&lt;/p&gt;\n\n&lt;p&gt;- Qwen 3 Coder is also good in Roo Code tool calls&lt;/p&gt;\n\n&lt;p&gt;- K2 did feel like it&amp;#39;s on par with Sonnet 4 in many respects so far&lt;/p&gt;\n\n&lt;p&gt;- Kimi K2 produced generally better quality code and features&lt;/p&gt;\n\n&lt;p&gt;- Qwen 3 Coder is extremely expensive! If you use Alibaba as inference, other providers in OpenRouter are decently priced&lt;/p&gt;\n\n&lt;p&gt;- K2 is half the cost of Qwen- K2 deleted one of my Dev DBs in Azure and didn&amp;#39;t ask if there was data, just because of a column which needed a migration, so please keep your Deny lists in check&lt;/p&gt;\n\n&lt;p&gt;Coding Vid: &lt;a href=\"https://youtu.be/ljCO7RyqCMY\"&gt;https://youtu.be/ljCO7RyqCMY&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA.jpeg?auto=webp&amp;s=ab91a571eedbc83fa4b6e65265c51d6677c99945",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7461f771e89f4be20b2a2b188a8b5c97a354e32f",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d880093c1d112db6452b3626b4f4fd3a67bc8f2",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1209830adb2015c645b0a257eddb571b141f1ce2",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "Aldc2j3i4vBAmSnaWtuuPWDwAG94v-Yx-DdE_F3o3ZA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7n5pq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "marvijo-software",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7n5pq/kimi_k2_vs_qwen_3_coder_coding_tests/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7n5pq/kimi_k2_vs_qwen_3_coder_coding_tests/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753309318,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why they be slacking on local llama and LLM generally? They big nation, clever, work hard. Many robots. No LLM? Why?",
          "author_fullname": "t2_16rs3mlp2i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where is Japan?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7d9d9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 101,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 101,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753286646,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why they be slacking on local llama and LLM generally? They big nation, clever, work hard. Many robots. No LLM? Why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7d9d9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ethereel1",
          "discussion_type": null,
          "num_comments": 170,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753286646,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need help finding an uncensored AI LLM model fhat has absolutely no restrictions. \n\nBackground: I'm writing a story that involves violence, gore, and explicit cuss words spoken by the characters.\n\nChatGPT 4o can occassionally (and under rare circumstances) curse and say \"fuck\" \"bitch\" and \"ass\" (albeit with a ton of asterisks). However, I need an LLM that can curse far more harsh and crudely. \n\nAnd at the moment, I dont have any money on my hands due to financial problems. So I'd appreciate it if you could link a free LLM that doesnt require credit top ups or anything.\n\nPlus my laptop specs are 8GB RAM, GTX 1650 and i5 10th gen CPU, if thats relevant, which seems far too small. So I'd also appreciate it if you could link a model that I can deploy to Gradio.\n\nI'm a rookie in these things, so I apologise in advance if you see any ignorance in my post.",
          "author_fullname": "t2_1skctar8q4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Free uncensored LLM model that I can deploy to Gradio.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m82w07",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753359629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help finding an uncensored AI LLM model fhat has absolutely no restrictions. &lt;/p&gt;\n\n&lt;p&gt;Background: I&amp;#39;m writing a story that involves violence, gore, and explicit cuss words spoken by the characters.&lt;/p&gt;\n\n&lt;p&gt;ChatGPT 4o can occassionally (and under rare circumstances) curse and say &amp;quot;fuck&amp;quot; &amp;quot;bitch&amp;quot; and &amp;quot;ass&amp;quot; (albeit with a ton of asterisks). However, I need an LLM that can curse far more harsh and crudely. &lt;/p&gt;\n\n&lt;p&gt;And at the moment, I dont have any money on my hands due to financial problems. So I&amp;#39;d appreciate it if you could link a free LLM that doesnt require credit top ups or anything.&lt;/p&gt;\n\n&lt;p&gt;Plus my laptop specs are 8GB RAM, GTX 1650 and i5 10th gen CPU, if thats relevant, which seems far too small. So I&amp;#39;d also appreciate it if you could link a model that I can deploy to Gradio.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a rookie in these things, so I apologise in advance if you see any ignorance in my post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m82w07",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeutronSchool",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m82w07/free_uncensored_llm_model_that_i_can_deploy_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m82w07/free_uncensored_llm_model_that_i_can_deploy_to/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753359629,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Higgs Audio V2 is an advanced, open-source audio generation model developed by Boson AI, designed to produce highly expressive and lifelike speech with robust multi-speaker dialogue capabilities.\n\nSome Highlights:\n\n🎧 Trained on 10M hours of diverse audio — speech, music, sound events, and natural conversations  \n🔧 Built on top of Llama 3.2 3B for deep language and acoustic understanding  \n⚡ Runs in real-time and supports edge deployment — smallest versions run on Jetson Orin Nano  \n🏆 Outperforms GPT-4o-mini-tts and ElevenLabs v2 in prosody, emotional expressiveness, and multi-speaker dialogue  \n🎭 Zero-shot natural multi-speaker dialogues — voices adapt tone, energy, and emotion automatically  \n🎙️ Zero-shot voice cloning with melodic humming and expressive intonation — no fine-tuning needed  \n🌍 Multilingual support with automatic prosody adaptation for narration and dialogue  \n🎵 Simultaneous speech and background music generation — a first for open audio foundation models  \n🔊 High-fidelity 24kHz audio output for studio-quality sound on any device  \n📦 Open source and commercially usable — no barriers to experimentation or deployment\n\nI tested this model here [https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt](https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt)\n\nModel on Huggingface:  https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base",
          "author_fullname": "t2_8c6ji8bg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Higgs Audio V2 - Open Multi-Speaker TTS Model - Impressive Testing Results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7lj3x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753305442,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Higgs Audio V2 is an advanced, open-source audio generation model developed by Boson AI, designed to produce highly expressive and lifelike speech with robust multi-speaker dialogue capabilities.&lt;/p&gt;\n\n&lt;p&gt;Some Highlights:&lt;/p&gt;\n\n&lt;p&gt;🎧 Trained on 10M hours of diverse audio — speech, music, sound events, and natural conversations&lt;br/&gt;\n🔧 Built on top of Llama 3.2 3B for deep language and acoustic understanding&lt;br/&gt;\n⚡ Runs in real-time and supports edge deployment — smallest versions run on Jetson Orin Nano&lt;br/&gt;\n🏆 Outperforms GPT-4o-mini-tts and ElevenLabs v2 in prosody, emotional expressiveness, and multi-speaker dialogue&lt;br/&gt;\n🎭 Zero-shot natural multi-speaker dialogues — voices adapt tone, energy, and emotion automatically&lt;br/&gt;\n🎙️ Zero-shot voice cloning with melodic humming and expressive intonation — no fine-tuning needed&lt;br/&gt;\n🌍 Multilingual support with automatic prosody adaptation for narration and dialogue&lt;br/&gt;\n🎵 Simultaneous speech and background music generation — a first for open audio foundation models&lt;br/&gt;\n🔊 High-fidelity 24kHz audio output for studio-quality sound on any device&lt;br/&gt;\n📦 Open source and commercially usable — no barriers to experimentation or deployment&lt;/p&gt;\n\n&lt;p&gt;I tested this model here &lt;a href=\"https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt\"&gt;https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Model on Huggingface:  &lt;a href=\"https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base\"&gt;https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk.jpeg?auto=webp&amp;s=8785e85f2bd59731d1b765bec52a5454fb368691",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=236a03bf932c13a51b9e805f9e9362659054558c",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8adae275672b81dce6a787717b6a50c7e36a45a3",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b02194bafbe6b39b59e579089faf0cb3d64cfe0",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "YT-LpJHqk9Hd07EBQIKlDPKyBSQF6cqbAxMCvw22Vdk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m7lj3x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lopsided_Dot_4557",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7lj3x/higgs_audio_v2_open_multispeaker_tts_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7lj3x/higgs_audio_v2_open_multispeaker_tts_model/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753305442,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which GPUs should I purchase for inferencing?  \nI have found 5090 about same price as 4090, why is that?  \nIs there some problems with 5090 or why is the pricing so? Does it have melting problems still?  \nIs 5090 more power efficient than 4090? I need at least 2 maybe 4.  \nWhich is currently the way to go GPU? Are datacenter versions getting cheaper?\n\nEDIT: another way could be new Radeon R9700 32GB but it will be much slower. What is the situation with 5090 pytorch support etc drivers for inferencing (ollama ofcourse should work) and also RDNA4, is it pain in the ass related to software?",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "5090 vs 4090 vs smt else for inference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7xclf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753340657,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753339963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which GPUs should I purchase for inferencing?&lt;br/&gt;\nI have found 5090 about same price as 4090, why is that?&lt;br/&gt;\nIs there some problems with 5090 or why is the pricing so? Does it have melting problems still?&lt;br/&gt;\nIs 5090 more power efficient than 4090? I need at least 2 maybe 4.&lt;br/&gt;\nWhich is currently the way to go GPU? Are datacenter versions getting cheaper?&lt;/p&gt;\n\n&lt;p&gt;EDIT: another way could be new Radeon R9700 32GB but it will be much slower. What is the situation with 5090 pytorch support etc drivers for inferencing (ollama ofcourse should work) and also RDNA4, is it pain in the ass related to software?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7xclf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7xclf/5090_vs_4090_vs_smt_else_for_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7xclf/5090_vs_4090_vs_smt_else_for_inference/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753339963,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.\n\nThis process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.\n\nI'm sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.\n\n\\1. Clone the official LiteLLM repo:\n\n```sh\ngit clone https://github.com/BerriAI/litellm.git\ncd litellm\n```\n\n\\2. Create an `.env` file with your OpenRouter API key (make sure to insert your own API key!):\n\n```sh\ncat &lt;&lt;\\EOF &gt;.env\nLITELLM_MASTER_KEY = \"sk-1234\"\n\n# OpenRouter\nOPENROUTER_API_KEY = \"sk-or-v1-…\" # 🚩\nEOF\n```\n\n\\3. Create a `config.yaml` file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):\n\n```sh\ncat &lt;&lt;\\EOF &gt;config.yaml\nmodel_list:\n  - model_name: \"anthropic/*\"\n    litellm_params:\n      model: \"openrouter/qwen/qwen3-coder\" # Qwen/Qwen3-Coder-480B-A35B-Instruct\n      max_tokens: 65536\n      repetition_penalty: 1.05\n      temperature: 0.7\n      top_k: 20\n      top_p: 0.8\nEOF\n```\n\n\\4. Create a `docker-compose.yml` file that loads `config.yaml` (it's easier to just create a finished one with all the required changes than to edit the original file):\n\n```sh\ncat &lt;&lt;\\EOF &gt;docker-compose.yml\nservices:\n  litellm:\n    build:\n      context: .\n      args:\n        target: runtime\n    ############################################################################\n    command:\n      - \"--config=/app/config.yaml\"\n    container_name: litellm\n    hostname: litellm\n    image: ghcr.io/berriai/litellm:main-stable\n    restart: unless-stopped\n    volumes:\n      - ./config.yaml:/app/config.yaml\n    ############################################################################\n    ports:\n      - \"4000:4000\" # Map the container port to the host, change the host port if necessary\n    environment:\n      DATABASE_URL: \"postgresql://llmproxy:dbpassword9090@db:5432/litellm\"\n      STORE_MODEL_IN_DB: \"True\" # allows adding models to proxy via UI\n    env_file:\n      - .env # Load local .env file\n    depends_on:\n      - db  # Indicates that this service depends on the 'db' service, ensuring 'db' starts first\n    healthcheck:  # Defines the health check configuration for the container\n      test: [ \"CMD-SHELL\", \"wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1\" ]  # Command to execute for health check\n      interval: 30s  # Perform health check every 30 seconds\n      timeout: 10s   # Health check command times out after 10 seconds\n      retries: 3     # Retry up to 3 times if health check fails\n      start_period: 40s  # Wait 40 seconds after container start before beginning health checks\n\n  db:\n    image: postgres:16\n    restart: always\n    container_name: litellm_db\n    environment:\n      POSTGRES_DB: litellm\n      POSTGRES_USER: llmproxy\n      POSTGRES_PASSWORD: dbpassword9090\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -d litellm -U llmproxy\"]\n      interval: 1s\n      timeout: 5s\n      retries: 10\n\nvolumes:\n  postgres_data:\n    name: litellm_postgres_data # Named volume for Postgres data persistence\nEOF\n```\n\n\\5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):\n\n```sh\ndocker compose up -d --build\n```\n\n\\6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (`.zshrc`, `.bashrc`, etc.) for persistence):\n\n```sh\nexport ANTHROPIC_AUTH_TOKEN=sk-1234\nexport ANTHROPIC_BASE_URL=http://localhost:4000\nexport ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder\nexport ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder\nexport CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates\n```\n\n\\7. Start Claude Code and it'll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the `/model` command that it's using a custom model):\n\n```sh\nclaude\n```\n\n\\8. Optional: Add an alias to your shell profile (`.zshrc`, `.bashrc`, etc.) to make it easier to use (e.g. `qlaude` for \"Claude with Qwen\"):\n\n```sh\nalias qlaude='ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude'\n```\n\nHave fun and happy coding!\n\nPS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.",
          "author_fullname": "t2_th129",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HOWTO: Use Qwen3-Coder (or any other LLM) with Claude Code (via LiteLLM)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 94,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ci3s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 84,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 84,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/u6BYojitjaMwq8K3EQg_ygO8c52qAwKJff5aywSVUks.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753284943,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.&lt;/p&gt;\n\n&lt;p&gt;This process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.&lt;/p&gt;\n\n&lt;p&gt;\\1. Clone the official LiteLLM repo:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\ngit clone https://github.com/BerriAI/litellm.git\ncd litellm\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\2. Create an &lt;code&gt;.env&lt;/code&gt; file with your OpenRouter API key (make sure to insert your own API key!):&lt;/p&gt;\n\n&lt;p&gt;```sh\ncat &amp;lt;&amp;lt;\\EOF &amp;gt;.env\nLITELLM_MASTER_KEY = &amp;quot;sk-1234&amp;quot;&lt;/p&gt;\n\n&lt;h1&gt;OpenRouter&lt;/h1&gt;\n\n&lt;p&gt;OPENROUTER_API_KEY = &amp;quot;sk-or-v1-…&amp;quot; # 🚩\nEOF\n```&lt;/p&gt;\n\n&lt;p&gt;\\3. Create a &lt;code&gt;config.yaml&lt;/code&gt; file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\ncat &amp;lt;&amp;lt;\\EOF &amp;gt;config.yaml\nmodel_list:\n  - model_name: &amp;quot;anthropic/*&amp;quot;\n    litellm_params:\n      model: &amp;quot;openrouter/qwen/qwen3-coder&amp;quot; # Qwen/Qwen3-Coder-480B-A35B-Instruct\n      max_tokens: 65536\n      repetition_penalty: 1.05\n      temperature: 0.7\n      top_k: 20\n      top_p: 0.8\nEOF\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\4. Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file that loads &lt;code&gt;config.yaml&lt;/code&gt; (it&amp;#39;s easier to just create a finished one with all the required changes than to edit the original file):&lt;/p&gt;\n\n&lt;p&gt;```sh\ncat &amp;lt;&amp;lt;\\EOF &amp;gt;docker-compose.yml\nservices:\n  litellm:\n    build:\n      context: .\n      args:\n        target: runtime\n    ############################################################################\n    command:\n      - &amp;quot;--config=/app/config.yaml&amp;quot;\n    container_name: litellm\n    hostname: litellm\n    image: ghcr.io/berriai/litellm:main-stable\n    restart: unless-stopped\n    volumes:\n      - ./config.yaml:/app/config.yaml\n    ############################################################################\n    ports:\n      - &amp;quot;4000:4000&amp;quot; # Map the container port to the host, change the host port if necessary\n    environment:\n      DATABASE_URL: &amp;quot;postgresql://llmproxy:dbpassword9090@db:5432/litellm&amp;quot;\n      STORE_MODEL_IN_DB: &amp;quot;True&amp;quot; # allows adding models to proxy via UI\n    env_file:\n      - .env # Load local .env file\n    depends_on:\n      - db  # Indicates that this service depends on the &amp;#39;db&amp;#39; service, ensuring &amp;#39;db&amp;#39; starts first\n    healthcheck:  # Defines the health check configuration for the container\n      test: [ &amp;quot;CMD-SHELL&amp;quot;, &amp;quot;wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1&amp;quot; ]  # Command to execute for health check\n      interval: 30s  # Perform health check every 30 seconds\n      timeout: 10s   # Health check command times out after 10 seconds\n      retries: 3     # Retry up to 3 times if health check fails\n      start_period: 40s  # Wait 40 seconds after container start before beginning health checks&lt;/p&gt;\n\n&lt;p&gt;db:\n    image: postgres:16\n    restart: always\n    container_name: litellm_db\n    environment:\n      POSTGRES_DB: litellm\n      POSTGRES_USER: llmproxy\n      POSTGRES_PASSWORD: dbpassword9090\n    ports:\n      - &amp;quot;5432:5432&amp;quot;\n    volumes:\n      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\n    healthcheck:\n      test: [&amp;quot;CMD-SHELL&amp;quot;, &amp;quot;pg_isready -d litellm -U llmproxy&amp;quot;]\n      interval: 1s\n      timeout: 5s\n      retries: 10&lt;/p&gt;\n\n&lt;p&gt;volumes:\n  postgres_data:\n    name: litellm_postgres_data # Named volume for Postgres data persistence\nEOF\n```&lt;/p&gt;\n\n&lt;p&gt;\\5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\ndocker compose up -d --build\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) for persistence):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\nexport ANTHROPIC_AUTH_TOKEN=sk-1234\nexport ANTHROPIC_BASE_URL=http://localhost:4000\nexport ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder\nexport ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder\nexport CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\7. Start Claude Code and it&amp;#39;ll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the &lt;code&gt;/model&lt;/code&gt; command that it&amp;#39;s using a custom model):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\nclaude\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;\\8. Optional: Add an alias to your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) to make it easier to use (e.g. &lt;code&gt;qlaude&lt;/code&gt; for &amp;quot;Claude with Qwen&amp;quot;):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\nalias qlaude=&amp;#39;ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude&amp;#39;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Have fun and happy coding!&lt;/p&gt;\n\n&lt;p&gt;PS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5p7u0le68nef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5p7u0le68nef1.png?auto=webp&amp;s=d6ee6eb61c270bf74164a4529655ea608dd7d761",
                  "width": 1682,
                  "height": 1130
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2f9ecbfadf0a585c661b7818dc4d782fd8cb3f3",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5ed170eaa8c71b609cea8209eb4a660ef943d7d",
                    "width": 216,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d361e88e5a29d401767472fe3a197448482be18",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=052b75b74825ad0e1f536d20305b7b06a2c2db8c",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e30fe50b638eda8e2b345142bbdcf622a0cdff2b",
                    "width": 960,
                    "height": 644
                  },
                  {
                    "url": "https://preview.redd.it/5p7u0le68nef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ddb6221e7b7cb0abb8d800206489d39dee05e42",
                    "width": 1080,
                    "height": 725
                  }
                ],
                "variants": {},
                "id": "mIuYvF0JIlHjZcMZ6w4mc0OQRmovAhW7m8mjpQFvvxs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m7ci3s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WolframRavenwolf",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/",
          "stickied": false,
          "url": "https://i.redd.it/5p7u0le68nef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753284943,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt; The video is sped up; it actually takes about 20-30 minutes\n\n- Github Repository: https://github.com/wrtnlabs/autobe\n- Generation Result: https://github.com/wrtnlabs/autobe-example-bbs\n- Detailed Article: https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html\n\nWe are honored to introduce [`AutoBE`](https://github.com/wrtnlabs/autobe) to you. [`AutoBE`](https://github.com/wrtnlabs/autobe) is an open-source project developed by Wrtn Technologies (Korean AI startup company), a vibe coding agent that automatically generates backend applications.\n\nOne of [`AutoBE`](https://github.com/wrtnlabs/autobe)'s key features is that it always generates code with 100% compilation success. The secret lies in our proprietary compiler system. Through our self-developed compilers, we support AI in generating type-safe code, and when AI generates incorrect code, the compiler detects it and provides detailed feedback, guiding the AI to generate correct code.\n\nThrough this approach, [`AutoBE`](https://github.com/wrtnlabs/autobe) always generates backend applications with 100% compilation success. When AI constructs AST (Abstract Syntax Tree) data through function calling, our proprietary compiler validates it, provides feedback, and ultimately generates complete source code.\n\nAbout the detailed content, please refer to the following blog article:\n\n- https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html\n\nWaterfall Model | AutoBE Agent  | Compiler AST Structure\n----------------|---------------|------------------------\nRequirements    | Analyze       | -\nAnalysis        | Analyze       | -\nDesign          | Database      | [`AutoBePrisma.IFile`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts)\nDesign          | API Interface | [`AutoBeOpenApi.IDocument`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts)\nTesting         | E2E Test      | [`AutoBeTest.IFunction`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts)",
          "author_fullname": "t2_1njlywuqe6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[AutoBE] We made AI-friendly Compilers for Vibe Coding, achieving zero-fail Backend Application Generation (open-source)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7xsxq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/20n2s8omvref1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/20n2s8omvref1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/20n2s8omvref1/DASHPlaylist.mpd?a=1755952194%2CNmRiZjNhODJhNTEzMzAxM2NkNmJmMzg1MGNlMjQ4OTliYTA2OGFlMWYwOTAzMzE3ODMxMDdlNzFmYzQyMDgxNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 437,
              "hls_url": "https://v.redd.it/20n2s8omvref1/HLSPlaylist.m3u8?a=1755952194%2COTMwODg1MjA0NjI3Njg5NjQzMTg2ZjhlMDVhYjI4ZmQyM2Y3ZDdmNjE0ODkyODFhNDZhODZmOGIxNzdjNTQ4Mw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=68783ecbfb91dfcadf53a8bd004324dfdf75c9a5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753341634,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The video is sped up; it actually takes about 20-30 minutes&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Github Repository: &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Generation Result: &lt;a href=\"https://github.com/wrtnlabs/autobe-example-bbs\"&gt;https://github.com/wrtnlabs/autobe-example-bbs&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Detailed Article: &lt;a href=\"https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html\"&gt;https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We are honored to introduce &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; to you. &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; is an open-source project developed by Wrtn Technologies (Korean AI startup company), a vibe coding agent that automatically generates backend applications.&lt;/p&gt;\n\n&lt;p&gt;One of &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt;&amp;#39;s key features is that it always generates code with 100% compilation success. The secret lies in our proprietary compiler system. Through our self-developed compilers, we support AI in generating type-safe code, and when AI generates incorrect code, the compiler detects it and provides detailed feedback, guiding the AI to generate correct code.&lt;/p&gt;\n\n&lt;p&gt;Through this approach, &lt;a href=\"https://github.com/wrtnlabs/autobe\"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; always generates backend applications with 100% compilation success. When AI constructs AST (Abstract Syntax Tree) data through function calling, our proprietary compiler validates it, provides feedback, and ultimately generates complete source code.&lt;/p&gt;\n\n&lt;p&gt;About the detailed content, please refer to the following blog article:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html\"&gt;https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Waterfall Model&lt;/th&gt;\n&lt;th&gt;AutoBE Agent&lt;/th&gt;\n&lt;th&gt;Compiler AST Structure&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Requirements&lt;/td&gt;\n&lt;td&gt;Analyze&lt;/td&gt;\n&lt;td&gt;-&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Analysis&lt;/td&gt;\n&lt;td&gt;Analyze&lt;/td&gt;\n&lt;td&gt;-&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Design&lt;/td&gt;\n&lt;td&gt;Database&lt;/td&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts\"&gt;&lt;code&gt;AutoBePrisma.IFile&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Design&lt;/td&gt;\n&lt;td&gt;API Interface&lt;/td&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts\"&gt;&lt;code&gt;AutoBeOpenApi.IDocument&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Testing&lt;/td&gt;\n&lt;td&gt;E2E Test&lt;/td&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts\"&gt;&lt;code&gt;AutoBeTest.IFunction&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/20n2s8omvref1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?format=pjpg&amp;auto=webp&amp;s=d885f5dccf4960afae588c4c2864fbc65ffc2ee8",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2fb26dbe1ba21f4604b0f14e5adef14d145894f7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=771019daf43b380ffe5f778d6acbe3caf072255d",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=647f3e268231f5157fbf72fcccd010e1843d6c4a",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0c03f0aa558e4110950a7bdd526975cc33816339",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=47b440c913e2ef55c9017b7065efa11a9df635ff",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2fef2373bc441ac97f2ed22c345631f5c1a69f69",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "eGp0OXo3b212cmVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m7xsxq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jhnam88",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7xsxq/autobe_we_made_aifriendly_compilers_for_vibe/",
          "stickied": false,
          "url": "https://v.redd.it/20n2s8omvref1",
          "subreddit_subscribers": 503759,
          "created_utc": 1753341634,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/20n2s8omvref1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/20n2s8omvref1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/20n2s8omvref1/DASHPlaylist.mpd?a=1755952194%2CNmRiZjNhODJhNTEzMzAxM2NkNmJmMzg1MGNlMjQ4OTliYTA2OGFlMWYwOTAzMzE3ODMxMDdlNzFmYzQyMDgxNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 437,
              "hls_url": "https://v.redd.it/20n2s8omvref1/HLSPlaylist.m3u8?a=1755952194%2COTMwODg1MjA0NjI3Njg5NjQzMTg2ZjhlMDVhYjI4ZmQyM2Y3ZDdmNjE0ODkyODFhNDZhODZmOGIxNzdjNTQ4Mw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just ran Qwen 3 Coder through a real-world test — building out a full permissions/ACL setup for a complex web app. Gave it the usual 30k-token context I feed into Claude Code, and it legit nailed it on the first try. No weird logic gaps, no hallucinated APIs — just clean, working code.\n\nTried the same thing with Kimi K2 and... it flopped hard. Qwen held up surprisingly well, especially when paired with solid prompt scaffolding. Honestly, it gave off Sonnet 4 vibes, which I wasn’t expecting from an OSS model.  \nStill, wild to see an open-source model perform at this level. We might be entering a legit new phase for local/dev-friendly LLMs.",
          "author_fullname": "t2_uaotuj04",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Coder just handled a full ACL system like a champ — OSS finally catching up",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7e5pi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753288688,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just ran Qwen 3 Coder through a real-world test — building out a full permissions/ACL setup for a complex web app. Gave it the usual 30k-token context I feed into Claude Code, and it legit nailed it on the first try. No weird logic gaps, no hallucinated APIs — just clean, working code.&lt;/p&gt;\n\n&lt;p&gt;Tried the same thing with Kimi K2 and... it flopped hard. Qwen held up surprisingly well, especially when paired with solid prompt scaffolding. Honestly, it gave off Sonnet 4 vibes, which I wasn’t expecting from an OSS model.&lt;br/&gt;\nStill, wild to see an open-source model perform at this level. We might be entering a legit new phase for local/dev-friendly LLMs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7e5pi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Edge2098",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7e5pi/qwen_3_coder_just_handled_a_full_acl_system_like/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7e5pi/qwen_3_coder_just_handled_a_full_acl_system_like/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753288688,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running Qwen3 locally.  What agent frameworks are you guys using and why?",
          "author_fullname": "t2_47ws19uq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best agent framework for Qwen3?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7wr2x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753337781,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running Qwen3 locally.  What agent frameworks are you guys using and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7wr2x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "seoulsrvr",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7wr2x/what_is_the_best_agent_framework_for_qwen3/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7wr2x/what_is_the_best_agent_framework_for_qwen3/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753337781,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;&gt;&gt; Qwen3-Coder is here! ✅\n\nWe’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀\n\nAlongside the model, we're also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! ",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is here!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qdet",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 1745,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 1745,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lAVBKeQpbXFJZ84JgZVPph8kD3MjUQeFX9TO1gsVqgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Qwen3-Coder is here! ✅&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;We’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀&lt;/p&gt;\n\n&lt;p&gt;Alongside the model, we&amp;#39;re also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?auto=webp&amp;s=e161efd029b20a9bcbbc26db043c320a38b26d7f",
                  "width": 2048,
                  "height": 1175
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0e7ce793e40e6f9057df0ac4084bef74851aa3c",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97f4add6c188177de3f1a921fce3e9fdcd751975",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d3ed6788720878c02cad2db45833f254f311864",
                    "width": 320,
                    "height": 183
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=470c1e7a0a6df4a35a09ad70120a5fef4e93a97b",
                    "width": 640,
                    "height": 367
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eca8024659f203e8b748284b077d47d512488b3",
                    "width": 960,
                    "height": 550
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=348beac8c62000f3a203a6467f098a1c8a696369",
                    "width": 1080,
                    "height": 619
                  }
                ],
                "variants": {},
                "id": "kx6kRcRUBkO_mMM0khkM5jTQgMXazrrYG6wlH3UPCCs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qdet",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 240,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/",
          "stickied": false,
          "url": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "subreddit_subscribers": 503759,
          "created_utc": 1753218847,
          "num_crossposts": 5,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a semi complex web project that I use with Claude Code. a few days ago I used Kimi K2 (via Groq Q4) with Claude Code (CCR) to add a permissions system / ACL into my web project to lock down certain people from doing certain things.\n\n  \nI use SuperClaude and a 1200 line context/architecture document, which basically starts a conversation off at about 30k input tokens (though, well worth it).\n\n  \nKimi K2 failed horribly, tool use errors, random garbage and basically didn't work properly. It was a Q4 version so maybe that had something to do with it, but I wasn't impressed.\n\n  \nToday I used Qwen 3 Coder via Openrouter (using only Alibaba cloud servers) for about 60 tps. Gave it the same task, and after about 10 minutes it finished. One shotted it (though one shotting is common for me with such a high amount of pre-context and auto fixing).\n\n  \nIt all worked great, I am actually really impressed and for me personally, it marks the first time an open source coding model actually has real world potential to rival paid LLMs like sonnet, opus and gemini. I would compare this model directly as good as Sonnet 4, which is a very capable model when using the right tools and prompts.\n\n  \nbig W for the open source community.\n\n  \nthe downside? THE PRICE. this one feature I added cost me $5 USD in credits via OpenRouter. That might not seem like much, but with Claude Pro for example you get an entire month of Sonnet 4 for 4x the price of that task. I don't know how well its using caching but at this point id rather stick with subscription based usage because that could get out of hand fast.",
          "author_fullname": "t2_i5ycefja",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Coder is actually pretty decent in my testing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m73yrb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 205,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 205,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753260180,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a semi complex web project that I use with Claude Code. a few days ago I used Kimi K2 (via Groq Q4) with Claude Code (CCR) to add a permissions system / ACL into my web project to lock down certain people from doing certain things.&lt;/p&gt;\n\n&lt;p&gt;I use SuperClaude and a 1200 line context/architecture document, which basically starts a conversation off at about 30k input tokens (though, well worth it).&lt;/p&gt;\n\n&lt;p&gt;Kimi K2 failed horribly, tool use errors, random garbage and basically didn&amp;#39;t work properly. It was a Q4 version so maybe that had something to do with it, but I wasn&amp;#39;t impressed.&lt;/p&gt;\n\n&lt;p&gt;Today I used Qwen 3 Coder via Openrouter (using only Alibaba cloud servers) for about 60 tps. Gave it the same task, and after about 10 minutes it finished. One shotted it (though one shotting is common for me with such a high amount of pre-context and auto fixing).&lt;/p&gt;\n\n&lt;p&gt;It all worked great, I am actually really impressed and for me personally, it marks the first time an open source coding model actually has real world potential to rival paid LLMs like sonnet, opus and gemini. I would compare this model directly as good as Sonnet 4, which is a very capable model when using the right tools and prompts.&lt;/p&gt;\n\n&lt;p&gt;big W for the open source community.&lt;/p&gt;\n\n&lt;p&gt;the downside? THE PRICE. this one feature I added cost me $5 USD in credits via OpenRouter. That might not seem like much, but with Claude Pro for example you get an entire month of Sonnet 4 for 4x the price of that task. I don&amp;#39;t know how well its using caching but at this point id rather stick with subscription based usage because that could get out of hand fast.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m73yrb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hodler-mane",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753260180,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A tiny change in the converter to support GLM-4.1V-9B-Thinking (no recompilation needed, just generate the GGUF).",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "text-only support for GLM-4.1V-9B-Thinking has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7m5br",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": "#bbbdbf",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=2a74fc0520a2b746e5b0e846d75ce4eb9f0c717b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753306899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A tiny change in the converter to support GLM-4.1V-9B-Thinking (no recompilation needed, just generate the GGUF).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14823",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?auto=webp&amp;s=1da3d09c5c620e7064179fe0056b0025ded7d6d5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dca6399394d0c6a422906d4fb5fb8e66090699e9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1d0491b6172ba26e4d571fcab16919205a50f4f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c812b1e9ad6436ffe1e34de95b453fe095355b74",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=283881b3d5dfd5c7c70eea0444ab6f480d98f89e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9216471d6e91d1c641b997ca6f702cb7d30fb3a1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=716e2fc1e1824c935e0b9dd09498d714601b0d18",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m7m5br",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m7m5br/textonly_support_for_glm41v9bthinking_has_been/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14823",
          "subreddit_subscribers": 503759,
          "created_utc": 1753306899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "New here, new to AI in general — i’m trying to finetune a 7b/8b/9b model for writing in a very specific style, and i have a few questions i could really use some help with :)\n\ni’ll be using lora on a cloud service (not local), and the model won’t need to do anything general — it’s only going to be used for this one use case. basically, i’ll give it bullet points or key ideas, and it should expand on them in the target style. so consistency in tone/writing is the main thing that matters to me.\n\n* does it make more sense to go with an older model (like mistral 7b, qwen2 7b, gemma2 9b) since newer ones seem more “assistant-y”? or are newer ones still fine if i’m just going to finetune them anyway?\n* i have about 1.2 million tokens in my dataset right now — is that enough to start with? i can add more if needed.\n* should i just do supervised finetuning, or would continued pretraining + sft give better results for this kind of task?\n\nalso open to any model recommendations if anyone’s done something similar — thanks in advance!",
          "author_fullname": "t2_188iniuukq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying to finetune my first model for writing — need some beginner advice :)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7sbb0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753325982,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753323428,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New here, new to AI in general — i’m trying to finetune a 7b/8b/9b model for writing in a very specific style, and i have a few questions i could really use some help with :)&lt;/p&gt;\n\n&lt;p&gt;i’ll be using lora on a cloud service (not local), and the model won’t need to do anything general — it’s only going to be used for this one use case. basically, i’ll give it bullet points or key ideas, and it should expand on them in the target style. so consistency in tone/writing is the main thing that matters to me.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;does it make more sense to go with an older model (like mistral 7b, qwen2 7b, gemma2 9b) since newer ones seem more “assistant-y”? or are newer ones still fine if i’m just going to finetune them anyway?&lt;/li&gt;\n&lt;li&gt;i have about 1.2 million tokens in my dataset right now — is that enough to start with? i can add more if needed.&lt;/li&gt;\n&lt;li&gt;should i just do supervised finetuning, or would continued pretraining + sft give better results for this kind of task?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;also open to any model recommendations if anyone’s done something similar — thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7sbb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lonely_Original4730",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7sbb0/trying_to_finetune_my_first_model_for_writing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7sbb0/trying_to_finetune_my_first_model_for_writing/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753323428,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2, developed by Moonshot AI, is a state-of-the-art Mixture-of-Experts (MoE) language model. It excels in frontier knowledge, reasoning, and coding tasks, and is specially optimized for agentic capabilities, including tool use and autonomous problem-solving.\n\nAs we explore in our Kimi K2 guide, the model is achieving outstanding benchmark results, making it the best general-purpose open-source language model. We are witnessing the Deepseek R1 moment, and to celebrate that, I will teach you how to run this enormous 1-terabyte model on a single GPU. \n\nWe will learn how to set up the Runpod machine, install llama.cpp, and download the model at a faster speed. Additionally, we will run the model using the llama.cpp CLI, offloading the model layers to RAM. Ultimately, we will address common issues that arise when running these models.\n\n[https://www.datacamp.com/tutorial/run-kimi-k2-locally](https://www.datacamp.com/tutorial/run-kimi-k2-locally)",
          "author_fullname": "t2_yeda6sl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to Run Kimi K2 Locally: Complete Setup &amp; Troubleshooting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m82wh5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753359665,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2, developed by Moonshot AI, is a state-of-the-art Mixture-of-Experts (MoE) language model. It excels in frontier knowledge, reasoning, and coding tasks, and is specially optimized for agentic capabilities, including tool use and autonomous problem-solving.&lt;/p&gt;\n\n&lt;p&gt;As we explore in our Kimi K2 guide, the model is achieving outstanding benchmark results, making it the best general-purpose open-source language model. We are witnessing the Deepseek R1 moment, and to celebrate that, I will teach you how to run this enormous 1-terabyte model on a single GPU. &lt;/p&gt;\n\n&lt;p&gt;We will learn how to set up the Runpod machine, install llama.cpp, and download the model at a faster speed. Additionally, we will run the model using the llama.cpp CLI, offloading the model layers to RAM. Ultimately, we will address common issues that arise when running these models.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.datacamp.com/tutorial/run-kimi-k2-locally\"&gt;https://www.datacamp.com/tutorial/run-kimi-k2-locally&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m82wh5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kingabzpro",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m82wh5/how_to_run_kimi_k2_locally_complete_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m82wh5/how_to_run_kimi_k2_locally_complete_setup/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753359665,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Source: [https://www.kaggle.com/whitepaper-prompt-engineering](https://www.kaggle.com/whitepaper-prompt-engineering)",
          "author_fullname": "t2_gm504",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Should I really always set temperature to 0 with reasoning models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 85,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m82rai",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YhFZIjkjsb-4XKaiV4ZRlNZQXf-cA5YA0jzoX-CSzHU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753359252,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Source: &lt;a href=\"https://www.kaggle.com/whitepaper-prompt-engineering\"&gt;https://www.kaggle.com/whitepaper-prompt-engineering&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/frmtfk84dtef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/frmtfk84dtef1.png?auto=webp&amp;s=87587b3ac28a3c5a40e79b94697358ba56f930ec",
                  "width": 1088,
                  "height": 664
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/frmtfk84dtef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a182307404ac5bbcd60d8f87ed73c52e1967522",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/frmtfk84dtef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6e6e7953a005b6e57493881dd1ce8aabc96e7fd",
                    "width": 216,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/frmtfk84dtef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4474981eb1093d15657b37d636345d44b0c18a6a",
                    "width": 320,
                    "height": 195
                  },
                  {
                    "url": "https://preview.redd.it/frmtfk84dtef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=231ffe96efd02ba4bc1e23701e27c87733679715",
                    "width": 640,
                    "height": 390
                  },
                  {
                    "url": "https://preview.redd.it/frmtfk84dtef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa7a22d2357d9c8c7db13292105e4c759ddd204f",
                    "width": 960,
                    "height": 585
                  },
                  {
                    "url": "https://preview.redd.it/frmtfk84dtef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31dcaff3b6efa21b108a1e14f2b637bed8a2599c",
                    "width": 1080,
                    "height": 659
                  }
                ],
                "variants": {},
                "id": "aDhAraT9ozyppqMWq0DYLEBY-gQpRb-l0vXjZ6GxNbc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m82rai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robertpiosik",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m82rai/should_i_really_always_set_temperature_to_0_with/",
          "stickied": false,
          "url": "https://i.redd.it/frmtfk84dtef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753359252,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen3 235B 2507 scores 60 on the Artificial Analysis Intelligence Index, surpassing Claude 4 Opus and Kimi K2 (both 58), and DeepSeek V3 0324 and GPT-4.1 (both 53). This marks a 13-point leap over the May 2025 non-reasoning release and brings it within two points of the May 2025 reasoning variant.",
          "author_fullname": "t2_1n5r32wumb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Alibaba’s upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "oyaa6be25kef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab849378391008c8477795135825d52268bdd089"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53e7c01d46996d28bab6c0eb21ebf73f19d344d0"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=101dd3f12db35511b6a0b3746e6b6a540438f16a"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a517570e0bde0d35e032e730f623cbf398e56d34"
                },
                {
                  "y": 479,
                  "x": 960,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bd46b3fee52bf1de3e3566854ce997d0dd1225a"
                },
                {
                  "y": 538,
                  "x": 1080,
                  "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89c6654712fe811c8362225e6681fc8b54707958"
                }
              ],
              "s": {
                "y": 1022,
                "x": 2048,
                "u": "https://preview.redd.it/oyaa6be25kef1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=e4811114c0b584dfcb6ee44f7f9bd5de02b8b550"
              },
              "id": "oyaa6be25kef1"
            },
            "yycfab625kef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ddf3c6b84f0f72a7669eb6187f719c7aca48cf1"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=020ffd2f38bf6dc2d4d813526ab1f68d37467a35"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d01829fee9b1b624a03e3c46434cc0464197eb6"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=590c21d82b1b1b130de0259f2b921cf5f6a9a736"
                },
                {
                  "y": 479,
                  "x": 960,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f1617800278fa9228386271be95a16d7067a04c"
                },
                {
                  "y": 538,
                  "x": 1080,
                  "u": "https://preview.redd.it/yycfab625kef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79f249d73cc6e2ef9eba49c4faaff2315c0c5daf"
                }
              ],
              "s": {
                "y": 1022,
                "x": 2048,
                "u": "https://preview.redd.it/yycfab625kef1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=b0b54666a21f5112942012d6921a4898cd5bb66d"
              },
              "id": "yycfab625kef1"
            }
          },
          "name": "t3_1m70n7q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 273,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "yycfab625kef1",
                "id": 712061604
              },
              {
                "caption": "",
                "media_id": "oyaa6be25kef1",
                "id": 712061605
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 273,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/44x5FARtQun2iK2pU9UkqoLiKnQmMoq90mJNUYMTKbw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753247536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 235B 2507 scores 60 on the Artificial Analysis Intelligence Index, surpassing Claude 4 Opus and Kimi K2 (both 58), and DeepSeek V3 0324 and GPT-4.1 (both 53). This marks a 13-point leap over the May 2025 non-reasoning release and brings it within two points of the May 2025 reasoning variant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m70n7q",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m70n7q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fantastic-Emu-3819",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m70n7q",
          "subreddit_subscribers": 503759,
          "created_utc": 1753247536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's an open source alternative to LM studio that uses GitHub and can be freely accessible, is generally very feature-rich, and can feasibly stand up to LM studio for people who want a free open source solution?",
          "author_fullname": "t2_1sznzjx7fy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source alternative to LM studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m81whq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753356654,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s an open source alternative to LM studio that uses GitHub and can be freely accessible, is generally very feature-rich, and can feasibly stand up to LM studio for people who want a free open source solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m81whq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "datascientist2964",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m81whq/open_source_alternative_to_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m81whq/open_source_alternative_to_lm_studio/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753356654,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been polishing the prompt setup and description pages to make them cleaner and more user-friendly. I originally built this because I got tired of digging through HuggingFace, Discord, and other scattered sources just to find decent prompts that work with different models.\n\nNow I’m trying to make that process as smooth and centralized as possible - with a clear UI, easy prompt management, and helpful context.\n\nWould love to know what you think - any feedback or ideas for improvement are super welcome!",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Polished UI for prompt setup &amp; details",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6my226plpnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47c415370992a40af3ab20e47f116a2616ea0a69"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc7f4ac2fc4ba879cea9dba16cda50988caf0245"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6c34da0216d3072c7a36f3b60c524aee15ecf74"
                },
                {
                  "y": 481,
                  "x": 640,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84b3a0aa5ade6f5e756d0624316b8f05ad48f2ca"
                },
                {
                  "y": 721,
                  "x": 960,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=66e45ed091efcaf8a61092f3fe5f1bd086f88753"
                },
                {
                  "y": 811,
                  "x": 1080,
                  "u": "https://preview.redd.it/6my226plpnef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0858add5bf457665fbbf7f672e76ef90b8cc529b"
                }
              ],
              "s": {
                "y": 1443,
                "x": 1920,
                "u": "https://preview.redd.it/6my226plpnef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d66fc444e6ad8245518edb37f53e54f2d774eafe"
              },
              "id": "6my226plpnef1"
            },
            "2al8c23mpnef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1754f31c75321c475f94f24be27fd19f4ee3b2c7"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2da1706c5359bb6bfc7475d3534bf392ca89ee3b"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63f2d4756c19cc98ce8268018a841f5b78828c58"
                },
                {
                  "y": 481,
                  "x": 640,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7192bda12d2cde591f2661517b7edbf41ed0af50"
                },
                {
                  "y": 722,
                  "x": 960,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=47de98797becbb9d8b4615612444e70b29d1dcd9"
                },
                {
                  "y": 812,
                  "x": 1080,
                  "u": "https://preview.redd.it/2al8c23mpnef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9abe46c930c2119df5477cd75773384e2b09251a"
                }
              ],
              "s": {
                "y": 1445,
                "x": 1920,
                "u": "https://preview.redd.it/2al8c23mpnef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=71daec4098d7cc28f113604a0bfab904e64d438b"
              },
              "id": "2al8c23mpnef1"
            }
          },
          "name": "t3_1m7f43h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 28,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "6my226plpnef1",
                "id": 712389243
              },
              {
                "media_id": "2al8c23mpnef1",
                "id": 712389244
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BiMeUC3yGkuRI6HBmivfk5feKuVk7YjrbVFqywOQ330.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753290857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been polishing the prompt setup and description pages to make them cleaner and more user-friendly. I originally built this because I got tired of digging through HuggingFace, Discord, and other scattered sources just to find decent prompts that work with different models.&lt;/p&gt;\n\n&lt;p&gt;Now I’m trying to make that process as smooth and centralized as possible - with a clear UI, easy prompt management, and helpful context.&lt;/p&gt;\n\n&lt;p&gt;Would love to know what you think - any feedback or ideas for improvement are super welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m7f43h",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m7f43h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7f43h/polished_ui_for_prompt_setup_details/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m7f43h",
          "subreddit_subscribers": 503759,
          "created_utc": 1753290857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Prompt processing isn't as simple as token generation (memory bandwidth/active parameter size). Are there any good sources on that (I suspect there is no simple answer)?\n\nIt depends on TFlops of the GPU, architecture etc.\n\nWorse, how does it depend when only part of model is on GPUs VRAM, and part is on CPUs RAM? How it depends when KV cache is offloaded to GPU and when not (e.g. --no-kv-offload in llama.cpp)?",
          "author_fullname": "t2_jti45lwl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to estimate prompt processing speed for given (multi-)GPU and model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m80kuh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753352215,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Prompt processing isn&amp;#39;t as simple as token generation (memory bandwidth/active parameter size). Are there any good sources on that (I suspect there is no simple answer)?&lt;/p&gt;\n\n&lt;p&gt;It depends on TFlops of the GPU, architecture etc.&lt;/p&gt;\n\n&lt;p&gt;Worse, how does it depend when only part of model is on GPUs VRAM, and part is on CPUs RAM? How it depends when KV cache is offloaded to GPU and when not (e.g. --no-kv-offload in llama.cpp)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m80kuh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EmilPi",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m80kuh/how_to_estimate_prompt_processing_speed_for_given/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m80kuh/how_to_estimate_prompt_processing_speed_for_given/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753352215,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI'm interested in running different model like qwen3 coder but those are very large and can't run on a laptop. What are the popular options ? Is it doable to take an aws instance with GPU to run it ? Or maybe it's too expensive or not doable at all",
          "author_fullname": "t2_4i6ba67v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to run large model ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m80dz3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753351532,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in running different model like qwen3 coder but those are very large and can&amp;#39;t run on a laptop. What are the popular options ? Is it doable to take an aws instance with GPU to run it ? Or maybe it&amp;#39;s too expensive or not doable at all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m80dz3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NoahZhyte",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753351532,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_a05srvks",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recent Qwen Benchmark Scores are Questionable",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 86,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6wb5o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 385,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 385,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4lABFcqtTCklC4dui7K-UMEXRbWONTLpUEMMKBfKSoM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753234294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8gjn0yhf1jef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8gjn0yhf1jef1.png?auto=webp&amp;s=1c15b72a1a3ebfe0f19f0d765beb22b39ec10dcd",
                  "width": 1194,
                  "height": 734
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6a6ec17b5a0a0b95756eb50adde48b41ee2f601",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e055247e334ebf18193ce1f1d33b6c9da1725406",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=224c05a95e89dc2c0fa3b5b4b9a7782ddb3f25a0",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5550a5410e5e1c751c0140c16c192e6bd86fddd",
                    "width": 640,
                    "height": 393
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=29dcc9276be7fbebc077a8bce537c9338061da86",
                    "width": 960,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/8gjn0yhf1jef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd26c16ae9488c7ee59a13a97af993a1d0b068ba",
                    "width": 1080,
                    "height": 663
                  }
                ],
                "variants": {},
                "id": "USv5UurWPKmkzrWAAgHt6q_pqJzLWJPl6eKLZh9JvqU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6wb5o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Electronic_Ad8889",
          "discussion_type": null,
          "num_comments": 68,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/",
          "stickied": false,
          "url": "https://i.redd.it/8gjn0yhf1jef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753234294,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all - I'm a first time builder (and posting here for the first time) so bare with me. 😅\n\nI'm building a MVP/PoC for a friend of mine who runs a manufacturing business. He needs an automated business development agent (or dashboard TBD) which would essentially tell him who his prospective customers could be with reasons.\n\nI've been playing around with Perplexity (not deep research) and it gives me decent results. Now I have a bare bones web app, and want to include this as a feature in that application. How should I go about doing this ?\n\n1. What are my options here ? I could use the Perplexity API, but are there other alternatives that you all suggest.\n2. What are my trade offs here ? I understand output quality vs cost. But are there any others ? ( I dont really care about latency etc at this stage).\n3. Eventually, if this of value to him and others like him, i want to build it out as a subscription based SaaS or something similar - any tech changes keeping this in mind.\n\nFeel free to suggest any other considerations, solutions etc. or roast me!\n\nThanks, appreciate you responses!",
          "author_fullname": "t2_a0v8hzsa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Technical Advise needed! - Market intelligence platform.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7zwsd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753349767,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all - I&amp;#39;m a first time builder (and posting here for the first time) so bare with me. 😅&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a MVP/PoC for a friend of mine who runs a manufacturing business. He needs an automated business development agent (or dashboard TBD) which would essentially tell him who his prospective customers could be with reasons.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing around with Perplexity (not deep research) and it gives me decent results. Now I have a bare bones web app, and want to include this as a feature in that application. How should I go about doing this ?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What are my options here ? I could use the Perplexity API, but are there other alternatives that you all suggest.&lt;/li&gt;\n&lt;li&gt;What are my trade offs here ? I understand output quality vs cost. But are there any others ? ( I dont really care about latency etc at this stage).&lt;/li&gt;\n&lt;li&gt;Eventually, if this of value to him and others like him, i want to build it out as a subscription based SaaS or something similar - any tech changes keeping this in mind.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Feel free to suggest any other considerations, solutions etc. or roast me!&lt;/p&gt;\n\n&lt;p&gt;Thanks, appreciate you responses!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7zwsd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Practical_Safe1887",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7zwsd/technical_advise_needed_market_intelligence/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7zwsd/technical_advise_needed_market_intelligence/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753349767,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have 40 hours of high-quality single-speaker Persian audio. \nWhat’s the best open-source TTS model that supports training on a new language for high-quality results? \nLooking for reliability and clarity.\nI've tried F5 but I found it to be unreliable, sometimes missing words or even producing extra speech.",
          "author_fullname": "t2_1hzjf9qtjm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best TTS Model with New Language Support",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7rwgo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753322242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 40 hours of high-quality single-speaker Persian audio. \nWhat’s the best open-source TTS model that supports training on a new language for high-quality results? \nLooking for reliability and clarity.\nI&amp;#39;ve tried F5 but I found it to be unreliable, sometimes missing words or even producing extra speech.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7rwgo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "saeedzou",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7rwgo/best_tts_model_with_new_language_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7rwgo/best_tts_model_with_new_language_support/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753322242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm using i7-4790 with 16G RAM, \n\nI installed qwen coder 7 and 14b which seems ok, just the later is a bit slow on my ubuntu WSL. \n\nI've read the 32b version of qwen have an extended capabilities.   \nI plan using neovim with vectorcode + MCP(github).   \nThere are some outdated rust code I need upgrading which is a bit huge in complexity. \n\nWhat model do you suggest and how do i tune them to perform the needed functionalities ? ",
          "author_fullname": "t2_3cj65",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which model is good for debugging with resource constrains?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7zqkz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753349094,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using i7-4790 with 16G RAM, &lt;/p&gt;\n\n&lt;p&gt;I installed qwen coder 7 and 14b which seems ok, just the later is a bit slow on my ubuntu WSL. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read the 32b version of qwen have an extended capabilities.&lt;br/&gt;\nI plan using neovim with vectorcode + MCP(github).&lt;br/&gt;\nThere are some outdated rust code I need upgrading which is a bit huge in complexity. &lt;/p&gt;\n\n&lt;p&gt;What model do you suggest and how do i tune them to perform the needed functionalities ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7zqkz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "afidegnum",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7zqkz/which_model_is_good_for_debugging_with_resource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7zqkz/which_model_is_good_for_debugging_with_resource/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753349094,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/4ylsxlbm0oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=7651eb28c5f9703cee17fa9fe2a66f6b575f5a05\n\nTrying to perform CPT of llama on a new language (Language is similar to Hindi, hence some tokens already present). The model's validation loss seems to plateau very early on into the training. Here 1 epoch is around 6k steps and validation loss seems to already be lowest at step 750. \n\n  \nMy dataset is around 100k size. Im using Lora as well\n\nhttps://preview.redd.it/17g8r8161oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=787998b7dc36f0ea474bfd988a17ba7527b6937b\n\nHere are my training arguments\n\nhttps://preview.redd.it/zpu2yhq81oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=e2aa754c023801a568a24bdf5b0d4cea92494fce\n\nIve tried different arangement, like more r value, embed\\_head and lm\\_head added onto the modules, different leaerning rates, etc. But similar trend in validation loss, either its around this range or around the range of 1.59-1.60. \n\nhttps://preview.redd.it/biejsj3k1oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=68c1fdd74b5fb72da5198132c7a1f821ba6f858e\n\nMoreover, Ive also tried mistral-7b-v0.1, same issues.   \n\n\nI thought it might be because the model is not able to learn because of less tokens, so tried vocab expansion, but same issues. \n\nWhat else could i try? ",
          "author_fullname": "t2_ilp0f96k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Continued pretraining of Llama 3-8b on a new language",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 39,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4ylsxlbm0oef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 30,
                  "x": 108,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88d57c1c26304386dc9643324410f7e924da02bf"
                },
                {
                  "y": 60,
                  "x": 216,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1829acd222438fd79d036ad146b40b99ed5e2274"
                },
                {
                  "y": 89,
                  "x": 320,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b5df9c08b52e96caaddffb6c7613b44d87296bc"
                },
                {
                  "y": 179,
                  "x": 640,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ff99b39523472a17ac34ee8ca73989f1e306fc8"
                },
                {
                  "y": 268,
                  "x": 960,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8eea30c92d9761f2b85fdaedbeed3a68f9d2b43"
                },
                {
                  "y": 302,
                  "x": 1080,
                  "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f98d1b9f94bb67261e57f64684fdd610ffc31344"
                }
              ],
              "s": {
                "y": 668,
                "x": 2386,
                "u": "https://preview.redd.it/4ylsxlbm0oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=7651eb28c5f9703cee17fa9fe2a66f6b575f5a05"
              },
              "id": "4ylsxlbm0oef1"
            },
            "zpu2yhq81oef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=10939db7873d0be79d7a8902fa93e3c456528b07"
                },
                {
                  "y": 118,
                  "x": 216,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b060d6c331f5ba2303cdf2a7cbfa6a2362b7238"
                },
                {
                  "y": 174,
                  "x": 320,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e26b1f28756de8bc058ab7e363e19aebc60aa1e8"
                },
                {
                  "y": 349,
                  "x": 640,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60d1e35982d5c449da7df39ae91617d4d73624a3"
                },
                {
                  "y": 524,
                  "x": 960,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0772385bb0d1c46ae8b7154c304493d97380e064"
                },
                {
                  "y": 590,
                  "x": 1080,
                  "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b3a54755e23fb71bd2b288a307e643718c932448"
                }
              ],
              "s": {
                "y": 1304,
                "x": 2386,
                "u": "https://preview.redd.it/zpu2yhq81oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=e2aa754c023801a568a24bdf5b0d4cea92494fce"
              },
              "id": "zpu2yhq81oef1"
            },
            "biejsj3k1oef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49e6f3d108dd2ba89333fa2a8522d3e1b1bddfe9"
                },
                {
                  "y": 77,
                  "x": 216,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f65fbb40cdc0b8c2acefaf73b7c2025e036264e"
                },
                {
                  "y": 115,
                  "x": 320,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=04c9f49be579eff990a61f01049f14292def3782"
                },
                {
                  "y": 230,
                  "x": 640,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e3a6b6cf914943e647acdce7522cea4ebedb85f"
                },
                {
                  "y": 345,
                  "x": 960,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=be9343fefcfdc69e0dc7f570673c516a0fccf602"
                },
                {
                  "y": 388,
                  "x": 1080,
                  "u": "https://preview.redd.it/biejsj3k1oef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a9b3d48af5bf4011bf7f028eae9d7bbaef55e036"
                }
              ],
              "s": {
                "y": 858,
                "x": 2386,
                "u": "https://preview.redd.it/biejsj3k1oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=68c1fdd74b5fb72da5198132c7a1f821ba6f858e"
              },
              "id": "biejsj3k1oef1"
            },
            "17g8r8161oef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 15,
                  "x": 108,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=da35b4fc251de49bbc6f7d64063b55761b11dac7"
                },
                {
                  "y": 31,
                  "x": 216,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b99758d0eab4b7ae5fff716c23661e6ca93d03e5"
                },
                {
                  "y": 46,
                  "x": 320,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9412d4a0c0e4657c087ba28bca26ec858a607d3d"
                },
                {
                  "y": 92,
                  "x": 640,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2a85d234a554d7dfbb6cead09a4b74044436d4c"
                },
                {
                  "y": 138,
                  "x": 960,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d64bdc00a6a46ba8b1dc68e17472b1bb48de3949"
                },
                {
                  "y": 155,
                  "x": 1080,
                  "u": "https://preview.redd.it/17g8r8161oef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=259de3da3401fc95891509dde196e1c8402eca51"
                }
              ],
              "s": {
                "y": 344,
                "x": 2386,
                "u": "https://preview.redd.it/17g8r8161oef1.png?width=2386&amp;format=png&amp;auto=webp&amp;s=787998b7dc36f0ea474bfd988a17ba7527b6937b"
              },
              "id": "17g8r8161oef1"
            }
          },
          "name": "t3_1m7gwuo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ugVEN4UtFCv8Wx60MSKJhHoUDDHJb5lYNs6MP2_hSKg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753294914,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ylsxlbm0oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7651eb28c5f9703cee17fa9fe2a66f6b575f5a05\"&gt;https://preview.redd.it/4ylsxlbm0oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7651eb28c5f9703cee17fa9fe2a66f6b575f5a05&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Trying to perform CPT of llama on a new language (Language is similar to Hindi, hence some tokens already present). The model&amp;#39;s validation loss seems to plateau very early on into the training. Here 1 epoch is around 6k steps and validation loss seems to already be lowest at step 750. &lt;/p&gt;\n\n&lt;p&gt;My dataset is around 100k size. Im using Lora as well&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/17g8r8161oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=787998b7dc36f0ea474bfd988a17ba7527b6937b\"&gt;https://preview.redd.it/17g8r8161oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=787998b7dc36f0ea474bfd988a17ba7527b6937b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are my training arguments&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zpu2yhq81oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2aa754c023801a568a24bdf5b0d4cea92494fce\"&gt;https://preview.redd.it/zpu2yhq81oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2aa754c023801a568a24bdf5b0d4cea92494fce&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Ive tried different arangement, like more r value, embed_head and lm_head added onto the modules, different leaerning rates, etc. But similar trend in validation loss, either its around this range or around the range of 1.59-1.60. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/biejsj3k1oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68c1fdd74b5fb72da5198132c7a1f821ba6f858e\"&gt;https://preview.redd.it/biejsj3k1oef1.png?width=2386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68c1fdd74b5fb72da5198132c7a1f821ba6f858e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Moreover, Ive also tried mistral-7b-v0.1, same issues.   &lt;/p&gt;\n\n&lt;p&gt;I thought it might be because the model is not able to learn because of less tokens, so tried vocab expansion, but same issues. &lt;/p&gt;\n\n&lt;p&gt;What else could i try? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7gwuo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Awkward-Quiet5795",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7gwuo/continued_pretraining_of_llama_38b_on_a_new/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7gwuo/continued_pretraining_of_llama_38b_on_a_new/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753294914,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Training model on Math tasks** improves model's puzzle-solving abilities through shared logical reasoning, but often reduces coding performance.\n\n**Training on codding tasks**: When they fine-tuned an LLM which has already undergone supervised fine tuning(Qwen2.5-7B-Instruct), it gains broader reasoning improvements across other domains.\n\nIn contrast, applying the same code‑focused training directly to a base LLM (not SFT Qwen2.5-7B-Base) tends to lock it into a rigid, code‑style output—hindering its performance on non‑code reasoning tasks.\n\n**Training on Puzzle tasks** improves logical reasoning, leading to better performance on mathematical tasks. However, this effect does not extend to coding tasks.\n\nWhen training with the combination of **Math + Puzzle**, the model’s performance on Math improves to 49.72, surpassing the Math-only performance of 47.48. Similarly, for **Code tasks, both additional Puzzle and Math data** lead to improvements in code-related tasks when compared to Code-only training\n\n**For the Puzzle task, all configurations involving additional domains perform worse than the Puzzle-only setting**, suggesting that increased data diversity can hinder the model’s ability to specialize in solving puzzles\n\nin the **Math + Puzzle** configuration, the model’s performance on Code tasks drops significantly, falling below both the Math-only and Puzzle-only baselines\n\n**Combining all domains** generally leads to better overall performance, with the triple-domain combination showing moderate gains and multi-domain setups help maintain consistent performance across tasks. But the performance on Puzzle tasks drops to 49.73, notably lower than the Puzzle + Code setting (55.15).\n\n*They also plan to conduct the experiment using DeepSeek V3, which should reveal how MoE‑rich models benefit from multi‑domain training.*\n\nUpvote1Downvote0Go to comments  \n",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can Reasoning Skills Learned in One Domain Generalize Across other Domains?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7z6p0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753346972,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Training model on Math tasks&lt;/strong&gt; improves model&amp;#39;s puzzle-solving abilities through shared logical reasoning, but often reduces coding performance.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Training on codding tasks&lt;/strong&gt;: When they fine-tuned an LLM which has already undergone supervised fine tuning(Qwen2.5-7B-Instruct), it gains broader reasoning improvements across other domains.&lt;/p&gt;\n\n&lt;p&gt;In contrast, applying the same code‑focused training directly to a base LLM (not SFT Qwen2.5-7B-Base) tends to lock it into a rigid, code‑style output—hindering its performance on non‑code reasoning tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Training on Puzzle tasks&lt;/strong&gt; improves logical reasoning, leading to better performance on mathematical tasks. However, this effect does not extend to coding tasks.&lt;/p&gt;\n\n&lt;p&gt;When training with the combination of &lt;strong&gt;Math + Puzzle&lt;/strong&gt;, the model’s performance on Math improves to 49.72, surpassing the Math-only performance of 47.48. Similarly, for &lt;strong&gt;Code tasks, both additional Puzzle and Math data&lt;/strong&gt; lead to improvements in code-related tasks when compared to Code-only training&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;For the Puzzle task, all configurations involving additional domains perform worse than the Puzzle-only setting&lt;/strong&gt;, suggesting that increased data diversity can hinder the model’s ability to specialize in solving puzzles&lt;/p&gt;\n\n&lt;p&gt;in the &lt;strong&gt;Math + Puzzle&lt;/strong&gt; configuration, the model’s performance on Code tasks drops significantly, falling below both the Math-only and Puzzle-only baselines&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Combining all domains&lt;/strong&gt; generally leads to better overall performance, with the triple-domain combination showing moderate gains and multi-domain setups help maintain consistent performance across tasks. But the performance on Puzzle tasks drops to 49.73, notably lower than the Puzzle + Code setting (55.15).&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;They also plan to conduct the experiment using DeepSeek V3, which should reveal how MoE‑rich models benefit from multi‑domain training.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Upvote1Downvote0Go to comments  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/pdf/2507.17512",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m7z6p0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7z6p0/can_reasoning_skills_learned_in_one_domain/",
          "stickied": false,
          "url": "https://arxiv.org/pdf/2507.17512",
          "subreddit_subscribers": 503759,
          "created_utc": 1753346972,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,   \n  \nI’d appreciate your input on this (sorry for the broken english and blabbering 😂). \n\nSo the point was to create a desktop overlay app that can interface local AI (LLM) with whatever downstream work. TTBOMK, this might be the first attempt in the community. If you happen to know similar approaches / projects, please let me know.  \n\n  \nI tried to keep it local-first and stayed away from MCP (though I have nothing against MCP).\n\nSo far, Gemma 3n has given me the best experience for these features. I’m curious to hear what your experiences have been. What setups or models worked best for you, and any thoughts you might have from your own implementations. \n\nThanks!",
          "author_fullname": "t2_1dcskd72oi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Currently building cross-app overlay using local llms",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7z5zu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/XNR2YcqapyQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"CZero Desktop Overlay - Features Walkthrough (pre-alpha)\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "CZero Desktop Overlay - Features Walkthrough (pre-alpha)",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/XNR2YcqapyQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"CZero Desktop Overlay - Features Walkthrough (pre-alpha)\"&gt;&lt;/iframe&gt;",
              "author_name": "CZero Engine",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/XNR2YcqapyQ/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@CZero-engine"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/XNR2YcqapyQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"CZero Desktop Overlay - Features Walkthrough (pre-alpha)\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m7z5zu",
            "height": 200
          },
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/hq7qsOoZ7RN7S-3UBYy7W1ouD3sREZIc6qIQKXQI6sE.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=b0c4d49116973f1908a46fc3b096c3ccdad75108",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753346899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,   &lt;/p&gt;\n\n&lt;p&gt;I’d appreciate your input on this (sorry for the broken english and blabbering 😂). &lt;/p&gt;\n\n&lt;p&gt;So the point was to create a desktop overlay app that can interface local AI (LLM) with whatever downstream work. TTBOMK, this might be the first attempt in the community. If you happen to know similar approaches / projects, please let me know.  &lt;/p&gt;\n\n&lt;p&gt;I tried to keep it local-first and stayed away from MCP (though I have nothing against MCP).&lt;/p&gt;\n\n&lt;p&gt;So far, Gemma 3n has given me the best experience for these features. I’m curious to hear what your experiences have been. What setups or models worked best for you, and any thoughts you might have from your own implementations. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/XNR2YcqapyQ?si=QteosPExjtwoIQbP",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hq7qsOoZ7RN7S-3UBYy7W1ouD3sREZIc6qIQKXQI6sE.jpeg?auto=webp&amp;s=80e69ab98255fb2d42f1c04036f6f18575535529",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hq7qsOoZ7RN7S-3UBYy7W1ouD3sREZIc6qIQKXQI6sE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ed1e8363abff65934f0fa691be274c0995dc15c",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/hq7qsOoZ7RN7S-3UBYy7W1ouD3sREZIc6qIQKXQI6sE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=57df0979d4fa79d4e068a4916b3fdd90f5155eba",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/hq7qsOoZ7RN7S-3UBYy7W1ouD3sREZIc6qIQKXQI6sE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1b7dad808a93425902afbf13344c15d2d30b215",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "hq7qsOoZ7RN7S-3UBYy7W1ouD3sREZIc6qIQKXQI6sE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7z5zu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Sheepherder507",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7z5zu/currently_building_crossapp_overlay_using_local/",
          "stickied": false,
          "url": "https://youtu.be/XNR2YcqapyQ?si=QteosPExjtwoIQbP",
          "subreddit_subscribers": 503759,
          "created_utc": 1753346899,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "CZero Desktop Overlay - Features Walkthrough (pre-alpha)",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/XNR2YcqapyQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"CZero Desktop Overlay - Features Walkthrough (pre-alpha)\"&gt;&lt;/iframe&gt;",
              "author_name": "CZero Engine",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/XNR2YcqapyQ/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@CZero-engine"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🌋 ENTIRE SPEECH-TO-SPEECH PIPELINE\n\n🔮REAL-TIME LIVE CAPTIONS IN 99 LANGUAGES\n\nNow it's possible to have any audio source (including your own voice) transcribed and translated to English using GPU acceleration for ultra-fast inference\n\nIt's 100% free, even for commercial use\n\nAnd runs locally\n\nSource code: [https://github.com/Kutalia/electron-speech-to-speech](https://github.com/Kutalia/electron-speech-to-speech) (Currently only Windows builds are provided in Github Releases, but you can easily compile with source for your platform - Windows, Mac and Linux)\n\nDemo: [https://www.youtube.com/watch?v=wUdtGxy0Ku8](https://www.youtube.com/watch?v=wUdtGxy0Ku8)",
          "author_fullname": "t2_h6o0chk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local cross-platform speech-to-speech and real-time captioning with OpenAI Whisper, Vulkan GPU acceleration and more",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 113,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m78kyc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MU-BIGb5j5aJl9A67BIiP69nhxlRBUqdBBCKbLTfMic.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753275527,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🌋 ENTIRE SPEECH-TO-SPEECH PIPELINE&lt;/p&gt;\n\n&lt;p&gt;🔮REAL-TIME LIVE CAPTIONS IN 99 LANGUAGES&lt;/p&gt;\n\n&lt;p&gt;Now it&amp;#39;s possible to have any audio source (including your own voice) transcribed and translated to English using GPU acceleration for ultra-fast inference&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s 100% free, even for commercial use&lt;/p&gt;\n\n&lt;p&gt;And runs locally&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/Kutalia/electron-speech-to-speech\"&gt;https://github.com/Kutalia/electron-speech-to-speech&lt;/a&gt; (Currently only Windows builds are provided in Github Releases, but you can easily compile with source for your platform - Windows, Mac and Linux)&lt;/p&gt;\n\n&lt;p&gt;Demo: &lt;a href=\"https://www.youtube.com/watch?v=wUdtGxy0Ku8\"&gt;https://www.youtube.com/watch?v=wUdtGxy0Ku8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/pxwmiaqagmef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/pxwmiaqagmef1.png?auto=webp&amp;s=c034d20d510a53fc14ecaae5913be5df0e255e74",
                  "width": 1622,
                  "height": 1319
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e713889b96e5cfca6b45f1450adc73c7dae99bb1",
                    "width": 108,
                    "height": 87
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a661c8a38e2ad63ba6d4441540c46910e9a8a480",
                    "width": 216,
                    "height": 175
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=792ee698504107743bc40f1131da6e6da31ea2e9",
                    "width": 320,
                    "height": 260
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5bef850b1f4387e2fe702768e841b6eb073e128",
                    "width": 640,
                    "height": 520
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c4f8151b1fdbc9fe37549859b625b2ea60117d5",
                    "width": 960,
                    "height": 780
                  },
                  {
                    "url": "https://preview.redd.it/pxwmiaqagmef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4495266981e63cb06dbd23d2cc8faa503789045",
                    "width": 1080,
                    "height": 878
                  }
                ],
                "variants": {},
                "id": "gxeDylarxEV8DJSZNwihZJEhvvKrLVIx5XtEq4XAYZ4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m78kyc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kutalia",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m78kyc/local_crossplatform_speechtospeech_and_realtime/",
          "stickied": false,
          "url": "https://i.redd.it/pxwmiaqagmef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753275527,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we're making **1M context length variants**!\n\nYou can achieve &gt;6 tokens/s on **182GB unified memory or 158GB RAM + 24GB VRAM** via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via \n\n    -ot \".ffn_.*_exps.=CPU\"\n\nUnfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we're investigating why this happens.\n\nYou can also run the **un-quantized 8bit / 16bit** versions also using llama,cpp offloading! Use Q8\\_K\\_XL which will be completed in an hour or so.\n\nTo increase performance and context length, use KV cache quantization, especially the \\_1 variants (higher accuracy than \\_0 variants). More details [here](https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m).\n\n`--cache-type-k q4_1`\n\nEnable flash attention as well and also try llama.cpp's NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are [here](https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed).\n\nQwen3-Coder-480B-A35B GGUFs (still ongoing) are at [https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF)\n\n1 million context length variants will be up at [https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF)\n\nDocs on how to run it are here: [https://docs.unsloth.ai/basics/qwen3-coder](https://docs.unsloth.ai/basics/qwen3-coder)",
          "author_fullname": "t2_5wukhd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Unsloth dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6wgs7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 268,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 268,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/BrdyPpD7KtsDEQRMB38b-Z6TRovCBfRYhvapsX-O7BM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753234725,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we&amp;#39;re making &lt;strong&gt;1M context length variants&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;You can achieve &amp;gt;6 tokens/s on &lt;strong&gt;182GB unified memory or 158GB RAM + 24GB VRAM&lt;/strong&gt; via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Unfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we&amp;#39;re investigating why this happens.&lt;/p&gt;\n\n&lt;p&gt;You can also run the &lt;strong&gt;un-quantized 8bit / 16bit&lt;/strong&gt; versions also using llama,cpp offloading! Use Q8_K_XL which will be completed in an hour or so.&lt;/p&gt;\n\n&lt;p&gt;To increase performance and context length, use KV cache quantization, especially the _1 variants (higher accuracy than _0 variants). More details &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--cache-type-k q4_1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Enable flash attention as well and also try llama.cpp&amp;#39;s NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Qwen3-Coder-480B-A35B GGUFs (still ongoing) are at &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;1 million context length variants will be up at &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Docs on how to run it are here: &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder\"&gt;https://docs.unsloth.ai/basics/qwen3-coder&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/s9cwrvwg1jef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/s9cwrvwg1jef1.png?auto=webp&amp;s=c00619420d39151822d49bcce97ea3e78e847971",
                  "width": 2560,
                  "height": 2740
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6a9ffee96aa9dcd39f3c41d338a8e758af4d75",
                    "width": 108,
                    "height": 115
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c70c0936d1d66995daaa0390deaa956eaec815e1",
                    "width": 216,
                    "height": 231
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89539969c58182206b11d57c691f1c822b4002b3",
                    "width": 320,
                    "height": 342
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=75c9ba63f5cc1768819789d0934d7d2a1e5a5926",
                    "width": 640,
                    "height": 685
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=812205928bb5b587c3e1e23bd16cae5c1d48ae0e",
                    "width": 960,
                    "height": 1027
                  },
                  {
                    "url": "https://preview.redd.it/s9cwrvwg1jef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32245973741ece306547037bf3f6efa37649cb42",
                    "width": 1080,
                    "height": 1155
                  }
                ],
                "variants": {},
                "id": "ERO7uofUircHQEW6vRJ41kGEOL83JHug3Yrw0FJ_7oM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6wgs7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danielhanchen",
          "discussion_type": null,
          "num_comments": 87,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/",
          "stickied": false,
          "url": "https://i.redd.it/s9cwrvwg1jef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753234725,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There's a thing I don't understand about optimisation in DSPy: the documentation says that \"A DSPy module has **learnable parameters** (i.e., the little pieces comprising the prompt and the LM weights)\" (from [Learn DSPy → Modules](https://dspy.ai/learn/programming/modules/)).\n\nI understand optimising the phrasing in the prompt, but the LM weights... What does that mean? Am I actually **training/fine-tuning the model itself** there? This would only work for models that I host myself, i.e., if I have access to the model weights directly, I suppose? And it would not work for hosted models like a Lllama3.1 running at a generative API provider?",
          "author_fullname": "t2_mryyvspqj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DSPy Optimisation: What does \"learning LM weights\" mean?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7y3kl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753342776,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a thing I don&amp;#39;t understand about optimisation in DSPy: the documentation says that &amp;quot;A DSPy module has &lt;strong&gt;learnable parameters&lt;/strong&gt; (i.e., the little pieces comprising the prompt and the LM weights)&amp;quot; (from &lt;a href=\"https://dspy.ai/learn/programming/modules/\"&gt;Learn DSPy → Modules&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;I understand optimising the phrasing in the prompt, but the LM weights... What does that mean? Am I actually &lt;strong&gt;training/fine-tuning the model itself&lt;/strong&gt; there? This would only work for models that I host myself, i.e., if I have access to the model weights directly, I suppose? And it would not work for hosted models like a Lllama3.1 running at a generative API provider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8BsuZmkFQPtfqVb3hZUPUQ_kVyVDX-opwB9Gb2-qh0o.png?auto=webp&amp;s=58869403975928c74efe052d591cf82b456715d5",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8BsuZmkFQPtfqVb3hZUPUQ_kVyVDX-opwB9Gb2-qh0o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cbb37880d15b944e0f2a776bad7806b28cc013cf",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/8BsuZmkFQPtfqVb3hZUPUQ_kVyVDX-opwB9Gb2-qh0o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6479238f7ab7ae51742a55d317d95cbb265dc79",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/8BsuZmkFQPtfqVb3hZUPUQ_kVyVDX-opwB9Gb2-qh0o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8577aebcc7675fdb29e9e375864f76f3ab5c74c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/8BsuZmkFQPtfqVb3hZUPUQ_kVyVDX-opwB9Gb2-qh0o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=224133edd8168702c2e5dc751ac17fd9a50b2fcb",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/8BsuZmkFQPtfqVb3hZUPUQ_kVyVDX-opwB9Gb2-qh0o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9001c17e93fe5127457807e69c2d5ab2e191d404",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/8BsuZmkFQPtfqVb3hZUPUQ_kVyVDX-opwB9Gb2-qh0o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8c712812a0310a9dca59e09ace836c2bd7afb7d5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "8BsuZmkFQPtfqVb3hZUPUQ_kVyVDX-opwB9Gb2-qh0o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7y3kl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "soyokaze42",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7y3kl/dspy_optimisation_what_does_learning_lm_weights/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7y3kl/dspy_optimisation_what_does_learning_lm_weights/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753342776,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen out here releasing models like it’s a Costco sample table",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qixu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 542,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 542,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/S7tH6DgPEGKSzcu1dZlyahFSv5skqhzjfdd4AVfWVi4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219204,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5eb8n31sshef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5eb8n31sshef1.png?auto=webp&amp;s=c1694040f87c60dc765d805ee64b6518e3bd108b",
                  "width": 722,
                  "height": 1032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47116ec0e7ef90202d820540f88598c3cfd0a160",
                    "width": 108,
                    "height": 154
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3d8c4cb9e760fdd971704cc87722923b6445146",
                    "width": 216,
                    "height": 308
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a14963b84a65b1261a6b4b6b451fce1c2285102",
                    "width": 320,
                    "height": 457
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f24e0235850da677693988507655dde73bf8e60",
                    "width": 640,
                    "height": 914
                  }
                ],
                "variants": {},
                "id": "yWOKVm4sVIve_VHC2bO92aBIp15Yh_tuiWnp6wkEtnE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m6qixu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/",
          "stickied": false,
          "url": "https://i.redd.it/5eb8n31sshef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753219204,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1kpbtnvm6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How MCP Inspector Works Internally: Client-Proxy Architecture and Communication Flow",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7tqeg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=293c1a065702b261e115575931663371284b051f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753327698,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "glama.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://glama.ai/blog/2025-07-24-how-mcp-inspector-works-a-simple-look-at-its-architecture-and-setup",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE.png?auto=webp&amp;s=7f28b5793018dacb13e5258e826c58d67c20ad1e",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0eca96064fcd2ac3874fdc3b0266bf78eb1185cf",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e9799ed2ca5fc9c8a8868371d1d591a04309815d",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c73e104d3e88deec902ea749e1a20c8e0ec0c03d",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d99f78d38d69cbbbac2e80a0446e7f8426bd1fa1",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=879de65c88fd813b197bbd206af34f4d64b10ccf",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6ffa0ac424d46d707395ad960977df521003079",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "N1X2_Jm1Spkw_tGm8xPfMFtse9eTr29-xqk48fVlcLE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7tqeg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Abies7108",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7tqeg/how_mcp_inspector_works_internally_clientproxy/",
          "stickied": false,
          "url": "https://glama.ai/blog/2025-07-24-how-mcp-inspector-works-a-simple-look-at-its-architecture-and-setup",
          "subreddit_subscribers": 503759,
          "created_utc": 1753327698,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What tools and settings enable optimal performance with CPU + GPU inference (partial offloading)? Here's my setup, which runs at \\~7.2 t/s, which is the maximum I've been able to squeeze out experimenting with settings in LM Studio and Llama.cpp. As we get more model releases that often don't fit entirely in VRAM, it seems like making the most of these settings is important.  \n  \n**Model:** Qwen3-235B-A22B 2507 / Unsloth's Q2\\_K\\_XL Quant / 82.67GB\n\n**GPU**: 5090 / 32GB VRAM\n\n**CPU**: AMD Ryzen 9 9900X\n\n**RAM:** 2x32GB DDR5-6000\n\n**Settings:**\n\n* Context: 4096\n* GPU Offload: 42/94 layers\n* CPU Thread Pool Size: 9\n* Batch Size: 512\n\n",
          "author_fullname": "t2_i5ptpsd5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Optimizing inference on GPU + CPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7oolz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753313226,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What tools and settings enable optimal performance with CPU + GPU inference (partial offloading)? Here&amp;#39;s my setup, which runs at ~7.2 t/s, which is the maximum I&amp;#39;ve been able to squeeze out experimenting with settings in LM Studio and Llama.cpp. As we get more model releases that often don&amp;#39;t fit entirely in VRAM, it seems like making the most of these settings is important.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Qwen3-235B-A22B 2507 / Unsloth&amp;#39;s Q2_K_XL Quant / 82.67GB&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: 5090 / 32GB VRAM&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 9 9900X&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 2x32GB DDR5-6000&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Settings:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Context: 4096&lt;/li&gt;\n&lt;li&gt;GPU Offload: 42/94 layers&lt;/li&gt;\n&lt;li&gt;CPU Thread Pool Size: 9&lt;/li&gt;\n&lt;li&gt;Batch Size: 512&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7oolz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SubstantialSock8002",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753313226,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ql2vu0wz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Puget Systems Threadripper PRO 9000WX Llama Prompt Processing &amp; Token Generation benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ld4z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D500&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z&amp;image=https%3A%2F%2Fi.imgur.com%2Fk257k3u.jpg%3Ffb&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"500\" height=\"60\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;",
            "width": 500,
            "scrolling": false,
            "height": 60
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "imgur.com",
            "oembed": {
              "provider_url": "http://imgur.com",
              "description": "Discover the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and so much more from users.",
              "title": "Imgur",
              "url": "https://imgur.com/a/EDYfW8Z",
              "type": "rich",
              "thumbnail_width": 769,
              "height": 60,
              "width": 500,
              "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D500&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z&amp;image=https%3A%2F%2Fi.imgur.com%2Fk257k3u.jpg%3Ffb&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"500\" height=\"60\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;",
              "version": "1.0",
              "provider_name": "Imgur",
              "thumbnail_url": "https://i.imgur.com/k257k3u.jpg?fb",
              "thumbnail_height": 913
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D500&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z&amp;image=https%3A%2F%2Fi.imgur.com%2Fk257k3u.jpg%3Ffb&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"500\" height=\"60\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;",
            "width": 500,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m7ld4z",
            "height": 60
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1TY3ekkN1BvY7efk8vkTqbWYLMpL6rdncgaIYqt8mrc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753305060,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "imgur.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://imgur.com/a/EDYfW8Z",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?auto=webp&amp;s=6b99863e0ee515acfb7baf827a1475ae8c08c99a",
                  "width": 769,
                  "height": 913
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad075926dde531bde6baeaa59a7e8de8b9783ff7",
                    "width": 108,
                    "height": 128
                  },
                  {
                    "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3c27b03235dce4ba1a94fcdd7b3e3e8f73f33dd",
                    "width": 216,
                    "height": 256
                  },
                  {
                    "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8963a8fad73e01a1dc59518f26cdba96f13efd8",
                    "width": 320,
                    "height": 379
                  },
                  {
                    "url": "https://external-preview.redd.it/EoAqgMVFxHtMcH_N1MqDCS4XiJk394hpgLml-L9lTR8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e6a4654ee6d198617faffc7ca553b7e86c784d3",
                    "width": 640,
                    "height": 759
                  }
                ],
                "variants": {},
                "id": "GLm1hgJojxMbTwvUw-Lc6StlFj9R36mDvuxy3H3bcNc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7ld4z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Caffdy",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ld4z/puget_systems_threadripper_pro_9000wx_llama/",
          "stickied": false,
          "url": "https://imgur.com/a/EDYfW8Z",
          "subreddit_subscribers": 503759,
          "created_utc": 1753305060,
          "num_crossposts": 0,
          "media": {
            "type": "imgur.com",
            "oembed": {
              "provider_url": "http://imgur.com",
              "description": "Discover the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and so much more from users.",
              "title": "Imgur",
              "url": "https://imgur.com/a/EDYfW8Z",
              "type": "rich",
              "thumbnail_width": 769,
              "height": 60,
              "width": 500,
              "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z%2Fembed%3Fpub%3Dtrue%26ref%3Dhttps%253A%252F%252Fembed.ly%26w%3D500&amp;display_name=Imgur&amp;url=https%3A%2F%2Fimgur.com%2Fa%2FEDYfW8Z&amp;image=https%3A%2F%2Fi.imgur.com%2Fk257k3u.jpg%3Ffb&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"500\" height=\"60\" scrolling=\"no\" title=\"Imgur embed\" frameborder=\"0\" allow=\"autoplay; fullscreen; encrypted-media; picture-in-picture;\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;",
              "version": "1.0",
              "provider_name": "Imgur",
              "thumbnail_url": "https://i.imgur.com/k257k3u.jpg?fb",
              "thumbnail_height": 913
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Right now, open source models aren’t that far off in terms of capabilities compared to proprietary models and models like DeepSeek, Kimi, and Qwen are beating out Claude, Gemini, GPT, etc. in many domains and categories when you look at various benchmarks.\n\nThat said, do you think open source models will continue to remain competitive across their proprietary counterparts? If not, what do you think the turning point will be when proprietary models just completely dominate open source?",
          "author_fullname": "t2_1gpe2ygava",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you think open source models continue to keep pace with proprietary models or will the gap widen?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7wx5z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753338395,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now, open source models aren’t that far off in terms of capabilities compared to proprietary models and models like DeepSeek, Kimi, and Qwen are beating out Claude, Gemini, GPT, etc. in many domains and categories when you look at various benchmarks.&lt;/p&gt;\n\n&lt;p&gt;That said, do you think open source models will continue to remain competitive across their proprietary counterparts? If not, what do you think the turning point will be when proprietary models just completely dominate open source?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7wx5z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Smart-Confection1435",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7wx5z/do_you_think_open_source_models_continue_to_keep/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7wx5z/do_you_think_open_source_models_continue_to_keep/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753338395,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a beginner building a RAG system and running into a strange issue with large Excel files.\n\n**The problem:**  \nWhen I ingest large Excel files, the system appears to extract and process the data correctly during ingestion. However, when I later query the system for specific information from those files, it responds as if the data doesn’t exist.\n\n**Details of my tech stack and setup:**\n\n* **Backend:**\n   * Django\n* **RAG/LLM Orchestration:**\n   * LangChain for managing LLM calls, embeddings, and retrieval\n* **Vector Store:**\n   * Qdrant (accessed via langchain-qdrant + qdrant-client)\n* **File Parsing:**\n   * Excel/CSV: `pandas`, `openpyxl`\n* **LLM Details:**\n* **Chat Model:**\n   * `gpt-4o`\n* **Embedding Model:**\n   * `text-embedding-ada-002`",
          "author_fullname": "t2_1ko7k822rj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG project fails to retrieve info from large Excel files – data ingested but not found at query time. Need help debugging.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7wpgo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753337622,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner building a RAG system and running into a strange issue with large Excel files.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br/&gt;\nWhen I ingest large Excel files, the system appears to extract and process the data correctly during ingestion. However, when I later query the system for specific information from those files, it responds as if the data doesn’t exist.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Details of my tech stack and setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Django&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAG/LLM Orchestration:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;LangChain for managing LLM calls, embeddings, and retrieval&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qdrant (accessed via langchain-qdrant + qdrant-client)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;File Parsing:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Excel/CSV: &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;openpyxl&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LLM Details:&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Chat Model:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Embedding Model:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;text-embedding-ada-002&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7wpgo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "One-Will5139",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7wpgo/rag_project_fails_to_retrieve_info_from_large/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7wpgo/rag_project_fails_to_retrieve_info_from_large/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753337622,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Available in https://chat.qwen.ai",
          "author_fullname": "t2_e9mfhlg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3- Coder 👀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mew9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 655,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 655,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/e6gFp_J-Dv7QIFguXfhuN4U3lDC6MMgny7SMuBnt9pI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209850,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Available in &lt;a href=\"https://chat.qwen.ai\"&gt;https://chat.qwen.ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?auto=webp&amp;s=a57681c6848dc38714b9bea86a26c30bed7d4d42",
                  "width": 1036,
                  "height": 695
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4a02434a648980c01b1a76032aa8e02027937c6",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be152167170e73dd02f7850c4f9bb67cf143ec4a",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c86d3d06fd820523c1470692b2726d59dbaf6d3",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=92b455544fdc9f84aebcf9cf995f7e3e643179a1",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d63431a252098393997b8c247ff6a0a80b67f78",
                    "width": 960,
                    "height": 644
                  }
                ],
                "variants": {},
                "id": "52S4zww-hEGiuCbEDUlQZAn66M2iCNb-181uTVxpyGY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6mew9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xhehab_",
          "discussion_type": null,
          "num_comments": 186,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/",
          "stickied": false,
          "url": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "subreddit_subscribers": 503759,
          "created_utc": 1753209850,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In my RAG project, large Excel files are being extracted, but when I query the data, the system responds that it doesn't exist. It seems the project fails to process or retrieve information correctly when the dataset is too large.",
          "author_fullname": "t2_1ko7k822rj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG on large Excel files",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7w3xm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753335523,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my RAG project, large Excel files are being extracted, but when I query the data, the system responds that it doesn&amp;#39;t exist. It seems the project fails to process or retrieve information correctly when the dataset is too large.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7w3xm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "One-Will5139",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7w3xm/rag_on_large_excel_files/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7w3xm/rag_on_large_excel_files/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753335523,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been testing Qwen3-Coder-480B (on Hyperbolics) and  Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear.  \n\nQwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like it’s just filling in templates rather than thinking through the task. It’s not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks.  \n\nSome of this might be because Hyperbolics hasn’t fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.\n\nKimi K2 works much better. Even though it’s not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, it’s consistently the better option.",
          "author_fullname": "t2_a29pmyxj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs Qwen3 Coder 480B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zz1v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 101,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 101,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753245282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been testing Qwen3-Coder-480B (on Hyperbolics) and  Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear.  &lt;/p&gt;\n\n&lt;p&gt;Qwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like it’s just filling in templates rather than thinking through the task. It’s not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks.  &lt;/p&gt;\n\n&lt;p&gt;Some of this might be because Hyperbolics hasn’t fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.&lt;/p&gt;\n\n&lt;p&gt;Kimi K2 works much better. Even though it’s not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, it’s consistently the better option.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6zz1v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Pattern9779",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753245282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\n\nToday, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.\n\n",
          "author_fullname": "t2_e7q9h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 coder will be in multiple sizes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qnpq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 379,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 379,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Today, we&amp;#39;re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we&amp;#39;re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qnpq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dinesh2609",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 503759,
          "created_utc": 1753219525,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TLDR**: Anyone has infographics/doc/dashboard for this? Please share. Thanks.\n\n\n\n^(I'm talking about stuff like Temperature, TopK, TopP, MinP, etc., values for all models. Though advanced users can apply these values with their experience, newbies like me need some kind of dashboard or list or repo with such details so we could open that before using models.)\n\n^(Currently my system has 20+ tiny models(Llama, Gemma, Qwen, Deepseek, Granite, etc.,). Even though I take settings for particular model from HF page before using, some models don't have the settings there.) \n\n^(Also I need to enter the values of those settings again whenever I open New chat. Accidentally I deleted some chat histories multiple times in past. So going to HF page again &amp; again just for this is too repetitive &amp; boring for me.)  ",
          "author_fullname": "t2_1deiadfhb1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recommended Settings ( Temperature, TopK, TopP, MinP, etc., ) for All models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7k50u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753302214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: Anyone has infographics/doc/dashboard for this? Please share. Thanks.&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;I&amp;#39;m talking about stuff like Temperature, TopK, TopP, MinP, etc., values for all models. Though advanced users can apply these values with their experience, newbies like me need some kind of dashboard or list or repo with such details so we could open that before using models.&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Currently my system has 20+ tiny models(Llama, Gemma, Qwen, Deepseek, Granite, etc.,&lt;/sup&gt;. Even though I take settings for particular model from HF page before using, some models don&amp;#39;t have the settings there.) &lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Also I need to enter the values of those settings again whenever I open New chat. Accidentally I deleted some chat histories multiple times in past. So going to HF page again &amp;amp; again just for this is too repetitive &amp;amp; boring for me.&lt;/sup&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7k50u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pmttyji",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753302214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How is it holding up to 64k, 128, 256, 512k, 1Mil?",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone tested or know of tests for Qwen3 Coder long context length?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ne51",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753309904,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How is it holding up to 64k, 128, 256, 512k, 1Mil?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7ne51",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m7ne51/has_anyone_tested_or_know_of_tests_for_qwen3/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7ne51/has_anyone_tested_or_know_of_tests_for_qwen3/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753309904,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am just curious, I know that T5 is much more optimal and convenient choice, but regarding to the metrics and accuracy, what do you think? ",
          "author_fullname": "t2_1jch6yc2bw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which is better for summarization and retrieval in RAG: new T5 Gemma or Gemma 3 12B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7y2jv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753342661,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am just curious, I know that T5 is much more optimal and convenient choice, but regarding to the metrics and accuracy, what do you think? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7y2jv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Junior-Badger9145",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7y2jv/which_is_better_for_summarization_and_retrieval/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7y2jv/which_is_better_for_summarization_and_retrieval/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753342661,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, I have a 3090 in my PC, and a mac with a m3 max 64gb of memory.  What are the go to models to find stuff in large code bases that I could run locally?  What are your recommendations for a model that could maybe read through the code and understand it, like if you're asking to find the code it does the blah blah blah?  Anyone have any good models they recommend I can run on either?",
          "author_fullname": "t2_4dx55sw2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local model for code search",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7u3mb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753328862,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have a 3090 in my PC, and a mac with a m3 max 64gb of memory.  What are the go to models to find stuff in large code bases that I could run locally?  What are your recommendations for a model that could maybe read through the code and understand it, like if you&amp;#39;re asking to find the code it does the blah blah blah?  Anyone have any good models they recommend I can run on either?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7u3mb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PositiveEnergyMatter",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7u3mb/best_local_model_for_code_search/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7u3mb/best_local_model_for_code_search/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753328862,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_lsixf36sr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m71f20",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8d404162721f954167cc891f633466a429b34c96",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753250317,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?auto=webp&amp;s=b0e606fe60c3b427cf1340db7a0ca6006dff3e57",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ad6d1b6c4559472693b7af1de31e24e4a8023a3",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1145966d2cde6471e76bb43f495683a63b013b72",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7fff728a74e01125301fc6c9d2699680540ef0a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60e9638973c964d4a82b7f30f192158867f7fc48",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=350210cdc15e1c044856de883fd8d259a90dd1f0",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9863761bbb4db313f92685d42bb3689971cd9fe8",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "gwBpjvEsSJP9TEvm8JruyOhsY560jiGO-1v9Go-RAwk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m71f20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Wolf-2007",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m71f20/unslothqwen3coder480ba35binstructgguf_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF",
          "subreddit_subscribers": 503759,
          "created_utc": 1753250317,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_o65i6kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI.Gov | President Trump's AI Strategy and Action Plan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7m534",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753306884,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ai.gov",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.ai.gov/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m7m534",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fallingdowndizzyvr",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7m534/aigov_president_trumps_ai_strategy_and_action_plan/",
          "stickied": false,
          "url": "https://www.ai.gov/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753306884,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "You probably already know about my [benchmark](https://www.designarena.ai/), but here's [context](https://www.reddit.com/r/LocalLLaMA/comments/1lxth6s/comment/n2qoqtk/?context=3) if you missed it. The tldr is that it's a crowdsource benchmark that takes human preferences on frontend and image generations from different models to produce a leaderboard ranking for which models are currently the best at UI and design generation. \n\nI'm going to try to keep these update posts to once-a-week or every other week to not come off as spam (sorry for that earlier, though I'm just seeing interesting results). Also, we realize there are flaws to the leaderboard (as all leaderboards and benchmarks have) that we're progressively trying to improve, but think it has been a good barometer for evaluating the models in particular tiers when it comes to coding. \n\nAnyways, since my last update on the 11th, we've added a few models, and in the last 24 hours, specifically Qwen3-235B-A22B-Instruct-2507 and Qwen3-Coder (less than an hour ago). Though the sample size is still very small, Qwen3-235B-A22B-Instruct-2507 appears to be killing it. I was reading through remarks on Twitter and Reddit that the Instruct model was on par with Opus which I thought was hyperbole at the time, but maybe that claim will hold true in the long run. \n\nWhat has been your experience with these Qwen models and what do you think? Open source is killing it right now. ",
          "author_fullname": "t2_98ouo03z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UI/UX benchmark update 7/22: Newest Qwen models added, Qwen3 takes the lead in terms of win rate (though still early)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ztb2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/yOnjsudSDwwMasrhJ1H10swcbMrmX3jIP8XzRlgDA6k.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753244753,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You probably already know about my &lt;a href=\"https://www.designarena.ai/\"&gt;benchmark&lt;/a&gt;, but here&amp;#39;s &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lxth6s/comment/n2qoqtk/?context=3\"&gt;context&lt;/a&gt; if you missed it. The tldr is that it&amp;#39;s a crowdsource benchmark that takes human preferences on frontend and image generations from different models to produce a leaderboard ranking for which models are currently the best at UI and design generation. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to try to keep these update posts to once-a-week or every other week to not come off as spam (sorry for that earlier, though I&amp;#39;m just seeing interesting results). Also, we realize there are flaws to the leaderboard (as all leaderboards and benchmarks have) that we&amp;#39;re progressively trying to improve, but think it has been a good barometer for evaluating the models in particular tiers when it comes to coding. &lt;/p&gt;\n\n&lt;p&gt;Anyways, since my last update on the 11th, we&amp;#39;ve added a few models, and in the last 24 hours, specifically Qwen3-235B-A22B-Instruct-2507 and Qwen3-Coder (less than an hour ago). Though the sample size is still very small, Qwen3-235B-A22B-Instruct-2507 appears to be killing it. I was reading through remarks on Twitter and Reddit that the Instruct model was on par with Opus which I thought was hyperbole at the time, but maybe that claim will hold true in the long run. &lt;/p&gt;\n\n&lt;p&gt;What has been your experience with these Qwen models and what do you think? Open source is killing it right now. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lcjgeavzvjef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lcjgeavzvjef1.png?auto=webp&amp;s=31df1bef3a627d2ba33e030e86d5d66a2b9b0ee0",
                  "width": 1333,
                  "height": 881
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d4b029e6012124dc9f449076bfcd1f4cfdf2ac1",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1205127af73fd4083008a096571d899bc2b6aadd",
                    "width": 216,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf64c9f604b8aef2fea7c80e52aa0d92de5fb99a",
                    "width": 320,
                    "height": 211
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8550da9c204aaebc89b401002be06079a6beec29",
                    "width": 640,
                    "height": 422
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c69386465fdc08532cb1ba6e8d60c731e32bc4b",
                    "width": 960,
                    "height": 634
                  },
                  {
                    "url": "https://preview.redd.it/lcjgeavzvjef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c87d7c433bad8b28ef732c515d7baa7baf7a784c",
                    "width": 1080,
                    "height": 713
                  }
                ],
                "variants": {},
                "id": "nLkR4n4kuyoAJ5FYt5FCPobA_n2hnoAkT68Lm-yyAho"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6ztb2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished-Copy332",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/",
          "stickied": false,
          "url": "https://i.redd.it/lcjgeavzvjef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753244753,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, what do you all think for sort of a medium / smallest model to use as an orchestrator model that runs with whisper (speech in) and tts (speech out). I also want it to view my screen to get context to pass to other other models / mcp so it knows what is going on so it can respond etc, then route and call tools / MCP. I intend to do most heavy lifting and anything with real output using Claude code sdk since have unlimited max plan. \n\nI was am looking at using Grafiti for memory and building some consensus between models based on Zen mcp implementation:   \n\nI have a 64 gb macbook pro M1 and I’m looking at Qwen3-30B-A3B-MLX-4bit ([hugging face link](https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit)),.\n\nI would welcome any advice! I've looked at Jan and related though seems too small. Is there anything that will run on my MBP that can serve as this brain (I looked at Gemma 3n, but its not fully mutli-modal out of the box as is). Would the be possible with this hardware?\n\nThis is the potential stack I came up with in chatting with Claude and o3:\n\n    User Input (speech/screen/events)\n               ↓\n        Local Processing\n        ├── VAD → STT → Text\n        ├── Screen → OCR → Context  \n        └── Events → MCP → Actions\n               ↓\n         Qwen3-30B Router\n        \"Is this simple?\"\n          ↓         ↓\n        Yes        No\n         ↓          ↓\n      Local     Claude API\n      Response  + MCP tools\n         ↓          ↓\n         └────┬─────┘\n              ↓\n        Graphiti Memory\n              ↓\n        Response Stream\n              ↓\n        Kyutai TTS        \n    \n\nThoughts?",
          "author_fullname": "t2_846pg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best small to medium size Local LLM Orchestrator for calling Tools, managing STT, TTS, screen OCR, and with passing heavy lift calls to Claude Code SDK, running on Macbook Pro.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7hq4w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753296737,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, what do you all think for sort of a medium / smallest model to use as an orchestrator model that runs with whisper (speech in) and tts (speech out). I also want it to view my screen to get context to pass to other other models / mcp so it knows what is going on so it can respond etc, then route and call tools / MCP. I intend to do most heavy lifting and anything with real output using Claude code sdk since have unlimited max plan. &lt;/p&gt;\n\n&lt;p&gt;I was am looking at using Grafiti for memory and building some consensus between models based on Zen mcp implementation:   &lt;/p&gt;\n\n&lt;p&gt;I have a 64 gb macbook pro M1 and I’m looking at Qwen3-30B-A3B-MLX-4bit (&lt;a href=\"https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit\"&gt;hugging face link&lt;/a&gt;),.&lt;/p&gt;\n\n&lt;p&gt;I would welcome any advice! I&amp;#39;ve looked at Jan and related though seems too small. Is there anything that will run on my MBP that can serve as this brain (I looked at Gemma 3n, but its not fully mutli-modal out of the box as is). Would the be possible with this hardware?&lt;/p&gt;\n\n&lt;p&gt;This is the potential stack I came up with in chatting with Claude and o3:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;User Input (speech/screen/events)\n           ↓\n    Local Processing\n    ├── VAD → STT → Text\n    ├── Screen → OCR → Context  \n    └── Events → MCP → Actions\n           ↓\n     Qwen3-30B Router\n    &amp;quot;Is this simple?&amp;quot;\n      ↓         ↓\n    Yes        No\n     ↓          ↓\n  Local     Claude API\n  Response  + MCP tools\n     ↓          ↓\n     └────┬─────┘\n          ↓\n    Graphiti Memory\n          ↓\n    Response Stream\n          ↓\n    Kyutai TTS        \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?auto=webp&amp;s=6cf13531259fc6a43addb217a67da463735f0aaf",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a97d235ba5ee0d377655e74657048733e66c0c80",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f65ca22ff036e0e09c3072baa25e27599adcf38d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48ce65ed5b5bd70fd126bbde8a6424436ca04f3a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ffb0cf505dc6f2aa6cfcb7c7f84e77a38833283",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fb2173e54335250d945cddd3c72954d45d353c7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d321923d5c25125f2212fa01cabda39fcb114276",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7hq4w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "matznerd",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753296737,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.\n\nVery strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.\n\nCreds [The Feature Crew](https://www.youtube.com/@TheFeatureCrew) for the original idea.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Web Development",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ny2q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 358,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755952194%2CZDUwNDBhNDE0YmQyOTQ3NmFhMzcwMjQ0MjI1ZjYzZWRmZjhiNjUwMWUwYzRjZTIwZTNlMTAxOTZkMTcyNjEzYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755952194%2CMWUwZTI3OTZlNWEzNmFkZGVjMmQzZjhkZDRlMjc4MjQ1NGMyMDE5MjJiNTIzYTE3ZDQxNWU0MzExODlkNzUyOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 358,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=127481f43d7a622f7d4c23a977a165102347dc33",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213272,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.&lt;/p&gt;\n\n&lt;p&gt;Very strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.&lt;/p&gt;\n\n&lt;p&gt;Creds &lt;a href=\"https://www.youtube.com/@TheFeatureCrew\"&gt;The Feature Crew&lt;/a&gt; for the original idea.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ob9yhvcjahef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?format=pjpg&amp;auto=webp&amp;s=9da74680d1673a7d5086bef35987945fda2390f7",
                  "width": 3024,
                  "height": 1964
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=69ec82c87ea25ca0cf09c32e6e2e65fd1ebe0353",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a7e4214f7585ef0bf76db563e79ceb7b7b73df5",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=da1a4545269003e5a3164b1074444b181b803a22",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ee471871b0892b000bd102b783d4c1fea31bbdf2",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b638262bf1c7530fbee91a29e1a5465444ef5500",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a910147ce667ac1bb6fed7eb4d08a092f3974fe",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ny2q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/",
          "stickied": false,
          "url": "https://v.redd.it/ob9yhvcjahef1",
          "subreddit_subscribers": 503759,
          "created_utc": 1753213272,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755952194%2CZDUwNDBhNDE0YmQyOTQ3NmFhMzcwMjQ0MjI1ZjYzZWRmZjhiNjUwMWUwYzRjZTIwZTNlMTAxOTZkMTcyNjEzYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755952194%2CMWUwZTI3OTZlNWEzNmFkZGVjMmQzZjhkZDRlMjc4MjQ1NGMyMDE5MjJiNTIzYTE3ZDQxNWU0MzExODlkNzUyOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running orpheus TTS locally and it requires an LM studio server running to be functional, I was wondering if there was a way to automatically create and start a server purely off code.\n\nI tried llama cpp but i couldn't get it to work no matter what, it always defaults to using my cpu, pytorch is detecting my GPU but llama cpp is not.",
          "author_fullname": "t2_48vjfixh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM server alternative?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7tglf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753326868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running orpheus TTS locally and it requires an LM studio server running to be functional, I was wondering if there was a way to automatically create and start a server purely off code.&lt;/p&gt;\n\n&lt;p&gt;I tried llama cpp but i couldn&amp;#39;t get it to work no matter what, it always defaults to using my cpu, pytorch is detecting my GPU but llama cpp is not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7tglf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatIsNotIllegal",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7tglf/lm_server_alternative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7tglf/lm_server_alternative/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753326868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone 👋\n\nI’ve been totally submerged in AI videos lately—everything from LangChain demos to memory tricks and agent deep dives. Tons of valuable stuff pitched across the web… but zero time to sit and watch it all.\n\nSo, I did something chill: I started a mini‑podcast where I use AI to talk through one video each week. I highlight the key “aha!” moments, what really matters—no fluff, just the parts that stuck with me.\n\nMy channel’s called The AI Checkpoints\n\nI’m sharing it here because I figure I’m probably not the only one whose “watch later” list is out of control, and I’d love any thoughts or feedback 😊",
          "author_fullname": "t2_y8vep8ai5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just started an AI‑insights podcast this week—thought I’d share and get your thoughts!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7tb9b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753326417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone 👋&lt;/p&gt;\n\n&lt;p&gt;I’ve been totally submerged in AI videos lately—everything from LangChain demos to memory tricks and agent deep dives. Tons of valuable stuff pitched across the web… but zero time to sit and watch it all.&lt;/p&gt;\n\n&lt;p&gt;So, I did something chill: I started a mini‑podcast where I use AI to talk through one video each week. I highlight the key “aha!” moments, what really matters—no fluff, just the parts that stuck with me.&lt;/p&gt;\n\n&lt;p&gt;My channel’s called The AI Checkpoints&lt;/p&gt;\n\n&lt;p&gt;I’m sharing it here because I figure I’m probably not the only one whose “watch later” list is out of control, and I’d love any thoughts or feedback 😊&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7tb9b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Original_CalmOwl",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7tb9b/just_started_an_aiinsights_podcast_this/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7tb9b/just_started_an_aiinsights_podcast_this/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753326417,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Earlier it were AI coding IDEs like cursor or GitHub copilot extension which came with agent mode. Then anthropic released Claude code, then openai, google and now alibaba followed the same suit to released their CLIs. \n\nRight now there's just too many options to use and they're all quite good, which makes it difficult to strike a balance of how much to experiment and what to use.\n\nWould like to know what pair programming methods do you use and what would you suggest.\n",
          "author_fullname": "t2_42oc6qgj8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Actually good Agentic coding tools",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7ijtf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753298617,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Earlier it were AI coding IDEs like cursor or GitHub copilot extension which came with agent mode. Then anthropic released Claude code, then openai, google and now alibaba followed the same suit to released their CLIs. &lt;/p&gt;\n\n&lt;p&gt;Right now there&amp;#39;s just too many options to use and they&amp;#39;re all quite good, which makes it difficult to strike a balance of how much to experiment and what to use.&lt;/p&gt;\n\n&lt;p&gt;Would like to know what pair programming methods do you use and what would you suggest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7ijtf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Particular_Tap_4002",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7ijtf/actually_good_agentic_coding_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7ijtf/actually_good_agentic_coding_tools/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753298617,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Could this be Deepseek?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 33,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6lf9s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 380,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 380,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WAXw-XuvIZ9mRKeenbrWXREbY65LvO1BDwwwlpUBowY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753207666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qzkjkgegugef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qzkjkgegugef1.png?auto=webp&amp;s=982da5cfa0575f138ae47f73b6eddafc3a141895",
                  "width": 822,
                  "height": 197
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=917acda1e7d58dd2b0c466213686f858f3d1d90f",
                    "width": 108,
                    "height": 25
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac14c427611c1eca254c8bb52ac34a30bf33d9f1",
                    "width": 216,
                    "height": 51
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8524670d8b150c1ad50fa23613a644190b14608f",
                    "width": 320,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e224ff9a214f929b3917304102fe92d67371e639",
                    "width": 640,
                    "height": 153
                  }
                ],
                "variants": {},
                "id": "LX0EdZ_oilMQBPfRRihja4cnZPDhv01xC24KCslp3qQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6lf9s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 59,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/",
          "stickied": false,
          "url": "https://i.redd.it/qzkjkgegugef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753207666,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new to running LLM's locally and have been working on a new project that has an \"AI powered\" requirement... I've learned a ton in the process but feel like I'm missing something.\n\nThe idea is to take a large csv that has been aggregated and formatted from various other sources, then feed that to an LLM that can identify trends, flag items that need attention, allow queries etc... but it can't use 3rd party API's\n\nI'm using self hosted Open Web UI API as my backend with Ollama and Mistral behind it all running on a 64GB AWS EC2 instance CPU only.   \n  \nThe file is too large to fit into the context window alone so I tried using the Files / Knowledge / RAG functionality that comes with OpenWebUI but that seems to really struggle to understand the entire dataset. \n\nFor example it's unable to tell me how many lines are in the file, or which item ID appears most often. \n\nJust curious if I'm going about this all wrong. Is this even realistic?\n\n",
          "author_fullname": "t2_jlnyy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Analyzing CSV and structured data - RAG, MCP, tools, or plain old scripting?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7mu6e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753308541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to running LLM&amp;#39;s locally and have been working on a new project that has an &amp;quot;AI powered&amp;quot; requirement... I&amp;#39;ve learned a ton in the process but feel like I&amp;#39;m missing something.&lt;/p&gt;\n\n&lt;p&gt;The idea is to take a large csv that has been aggregated and formatted from various other sources, then feed that to an LLM that can identify trends, flag items that need attention, allow queries etc... but it can&amp;#39;t use 3rd party API&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using self hosted Open Web UI API as my backend with Ollama and Mistral behind it all running on a 64GB AWS EC2 instance CPU only.   &lt;/p&gt;\n\n&lt;p&gt;The file is too large to fit into the context window alone so I tried using the Files / Knowledge / RAG functionality that comes with OpenWebUI but that seems to really struggle to understand the entire dataset. &lt;/p&gt;\n\n&lt;p&gt;For example it&amp;#39;s unable to tell me how many lines are in the file, or which item ID appears most often. &lt;/p&gt;\n\n&lt;p&gt;Just curious if I&amp;#39;m going about this all wrong. Is this even realistic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7mu6e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tactical_Chicken",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753308541,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone brace up for qwen !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 121,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nxh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 261,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 261,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vP7s1FThQpvmySmVJXMTU3-8PcS1dzgy5zKouaE_2IM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mn8auem2bhef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mn8auem2bhef1.png?auto=webp&amp;s=f8d9250eb919b06b9873df5541dfb4181c23ecb3",
                  "width": 1080,
                  "height": 938
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f18ccc22bd1429048af2d71903a4986f10f4370",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e9b0375cba8b59a1f2ff6a059540b35b2e80af5",
                    "width": 216,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd33b6c0d9557d99a79e31264f8c7962a467e6de",
                    "width": 320,
                    "height": 277
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=855c907a55cf3f70afe582932d52350878ef5e68",
                    "width": 640,
                    "height": 555
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=727a28b13a2a4b6feeeb2646b4c5ef5d4feba605",
                    "width": 960,
                    "height": 833
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cdd53e8429b6be0b40bf16e28d255d818f7b04a",
                    "width": 1080,
                    "height": 938
                  }
                ],
                "variants": {},
                "id": "rMAWLMOw9tEiFwd35Iv66C0AmNRfGhg4PeoHVtVBYI4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6nxh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/",
          "stickied": false,
          "url": "https://i.redd.it/mn8auem2bhef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753213236,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for leads for best edge model to deploy in an email mobile app. Tasks are closeIE (extract flight confirmation details), Summarize this newsletter, and Draft an email response. \n\nNotable considerations\n* Most emails are less than 5k in length \n* Less parameters means better battery efficiency \n* Inference time is critical \n* Loading a model on GPU takes 10s+ with mediaPipe\n* GPU execution is a must and specialized kernels make it go brr-- so contrived models likely won't have fast hw acceleration on Snapdragon \n\n\n\n[View Poll](https://www.reddit.com/poll/1m7mlcr)",
          "author_fullname": "t2_img2xgzp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best edge model for mobile - Qwen, LFM2, Gemma3N?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7mlcr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753307960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for leads for best edge model to deploy in an email mobile app. Tasks are closeIE (extract flight confirmation details), Summarize this newsletter, and Draft an email response. &lt;/p&gt;\n\n&lt;p&gt;Notable considerations\n* Most emails are less than 5k in length \n* Less parameters means better battery efficiency \n* Inference time is critical \n* Loading a model on GPU takes 10s+ with mediaPipe\n* GPU execution is a must and specialized kernels make it go brr-- so contrived models likely won&amp;#39;t have fast hw acceleration on Snapdragon &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1m7mlcr\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7mlcr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yonz-",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1753912760504,
            "options": [
              {
                "text": "nuExtract 2.0 (multi modal) - extraction SOTA",
                "id": "31218819"
              },
              {
                "text": "Qwen3 1.7B",
                "id": "31218820"
              },
              {
                "text": "Gemma 3n E2 (2B active 4B model)",
                "id": "31218821"
              },
              {
                "text": "Qwen3 4B",
                "id": "31218822"
              },
              {
                "text": "Liquid LFM2 (new: July 2025) 0.3-1.2",
                "id": "31218823"
              },
              {
                "text": "SmolLM",
                "id": "31218824"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 53,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7mlcr/best_edge_model_for_mobile_qwen_lfm2_gemma3n/",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7mlcr/best_edge_model_for_mobile_qwen_lfm2_gemma3n/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753307960,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is anyone maintaining a \"fits in a MacBook Pro\" kind of leaderboard for open models? It's by far the form factor for open models I've seen colleagues interested in.\n\nI know you can just see the number of parameters, active parameters in MoEs, etc., but a nice leaderboard with some tokens/sec average would be useful for many.",
          "author_fullname": "t2_e9yxn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MacBook model rank",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7lp0z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753305825,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone maintaining a &amp;quot;fits in a MacBook Pro&amp;quot; kind of leaderboard for open models? It&amp;#39;s by far the form factor for open models I&amp;#39;ve seen colleagues interested in.&lt;/p&gt;\n\n&lt;p&gt;I know you can just see the number of parameters, active parameters in MoEs, etc., but a nice leaderboard with some tokens/sec average would be useful for many.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7lp0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JCx64",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7lp0z/macbook_model_rank/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7lp0z/macbook_model_rank/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753305825,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct](https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct)\n\n hyperolic already has it\n\n",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mlbk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 250,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 250,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753210248,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct\"&gt;https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;hyperolic already has it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mlbk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753210248,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new to LLMs and I'm trying to understand a few things.\n\nIsn't RAG similar to a search engine? looks at keywords typed by user then feeds it to LLM to \"understand\" it an generate a nice response back? \n\nLet's say instead of RAG I'm using something like ElasticSearch/Meillsearch - would the results be that different? Does RAG handle synonyms as well? \n\nIdeally each chunk added into ChromaDb should be a full \"logic unit\" meaning it should make sense by itself (not a cutoff sentence with no start and end. Ex: Steven is ...). No?\n\nWhat about text with references to other pages, articles etc. How to handle them? ",
          "author_fullname": "t2_7bnnpzic",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemma3/other, Langchain, ChromaDb, RAG - a few questions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7kfet",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753302872,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to LLMs and I&amp;#39;m trying to understand a few things.&lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t RAG similar to a search engine? looks at keywords typed by user then feeds it to LLM to &amp;quot;understand&amp;quot; it an generate a nice response back? &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say instead of RAG I&amp;#39;m using something like ElasticSearch/Meillsearch - would the results be that different? Does RAG handle synonyms as well? &lt;/p&gt;\n\n&lt;p&gt;Ideally each chunk added into ChromaDb should be a full &amp;quot;logic unit&amp;quot; meaning it should make sense by itself (not a cutoff sentence with no start and end. Ex: Steven is ...). No?&lt;/p&gt;\n\n&lt;p&gt;What about text with references to other pages, articles etc. How to handle them? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7kfet",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "viitorfermier",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7kfet/gemma3other_langchain_chromadb_rag_a_few_questions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7kfet/gemma3other_langchain_chromadb_rag_a_few_questions/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753302872,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n**Reason**\nSo I was walking around my room thinking about my current laptop lenovo yoga slim 7\nand then started thinking about other laptops,\nnamely..\n\n\n\n\n\n**Question 1**\n\nMacbook Air/Pro.\nhow are the apple products when used for local training? \nmore specifically how are the last 3 generations of Macbook Pros when running locally?\n\n\n\n**Question 2**\n\nare there any cloud providers that are ‘private’ atleast well encrypted and secure? and don’t sell themselves to a government, if no, that’s unfortunate and someone should build that :).\nand..\n\n\n\n**Question 3**\n\nwhat are the most efficient (cost, storage, gpu, cpu, connection speed, etc) machines to build a private server that can train models and store images from 10+ devices onto a private storage  server.\n\n\n\n\nThank you if you’ve read this far, \nand even more thank you to the people that can answer and do :)\n\n\n\n\n\n\n\n",
          "author_fullname": "t2_4iu4e2ma",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ML on Macbook",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7pn05",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753315817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Reason&lt;/strong&gt;\nSo I was walking around my room thinking about my current laptop lenovo yoga slim 7\nand then started thinking about other laptops,\nnamely..&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 1&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Macbook Air/Pro.\nhow are the apple products when used for local training? \nmore specifically how are the last 3 generations of Macbook Pros when running locally?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 2&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;are there any cloud providers that are ‘private’ atleast well encrypted and secure? and don’t sell themselves to a government, if no, that’s unfortunate and someone should build that :).\nand..&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 3&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;what are the most efficient (cost, storage, gpu, cpu, connection speed, etc) machines to build a private server that can train models and store images from 10+ devices onto a private storage  server.&lt;/p&gt;\n\n&lt;p&gt;Thank you if you’ve read this far, \nand even more thank you to the people that can answer and do :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7pn05",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CaslerTheTesticle",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7pn05/ml_on_macbook/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7pn05/ml_on_macbook/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753315817,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qc8c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#ab96c2",
          "ups": 144,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 144,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218772,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qc8c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 503759,
          "created_utc": 1753218772,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't know if it matters, but I followed this to install (because Nvidia drivers on Linux is a pain!): https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md\n\nSo I would like to type in a query into a model with some preset system prompt. I would like that model to run over this query multiple times. Then after all of them are done, I would like for the responses to be gathered for a summary. Would such task be possible?\n\nEDIT: I'm trying to benchmark variation biases for research. The prompt could be any scenario, but if I were to make an example, let's say it's a scenario where I meet with a random stranger. The stranger should have 50/50 chance of being a gentleman/lady as the model's output, but I'm trying to gauge what would happen if I simulate this scenario 100 times for a bias towards one sex.",
          "author_fullname": "t2_4hrx8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama + Open WebUI -- is there a way for the same query to run through the same model multiple times (could be 3 times, could be 100 times), then gather all the answers together to summarise/count?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7pi3t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753316394,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753315438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if it matters, but I followed this to install (because Nvidia drivers on Linux is a pain!): &lt;a href=\"https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md\"&gt;https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So I would like to type in a query into a model with some preset system prompt. I would like that model to run over this query multiple times. Then after all of them are done, I would like for the responses to be gathered for a summary. Would such task be possible?&lt;/p&gt;\n\n&lt;p&gt;EDIT: I&amp;#39;m trying to benchmark variation biases for research. The prompt could be any scenario, but if I were to make an example, let&amp;#39;s say it&amp;#39;s a scenario where I meet with a random stranger. The stranger should have 50/50 chance of being a gentleman/lady as the model&amp;#39;s output, but I&amp;#39;m trying to gauge what would happen if I simulate this scenario 100 times for a bias towards one sex.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?auto=webp&amp;s=194fc1da74b1f56e6bca7cecb75e5a68c11008c1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1a05a3ead9734d6cb7b7045fdd787ff15a290e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=867ae8b82b8f457ac666d89cfaf3611953cc358e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbf750440a0a81a0c33ea061fa002223db7b35d7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e492becae66517bde05cbff2d3abe83139c4065f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a08a1c535e0e82ec2dc485d89bdfe54012f28a75",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a17c83b48123663530d34879b1da1dc4ccf3d160",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7pi3t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jinnyjuice",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753315438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m new to desktops. I’ve only ever had laptops. Would this be a good setup for local inference. The GPU has 32GB vram and over 1TB memory bandwidth. \n\nOther comments have lead me to believe that the motherboard and CPU matter as well but I am u sure why. Any help yall can provide would be great",
          "author_fullname": "t2_3zr7ymrr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Alienware Area-51 Gaming Desktop. Thoughts for local inference and fine tuning small models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ycez4n1ihpef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57fd9929c424834e8ea550b414e69e9af726776b"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f2e3ab0d06669398a41106f1ea28b2d2b485707"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2cd88ac6b9bdcc265d8e22d184a729b5818fbfd"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6260966c368a2592ba7e5f0ed9ca47f838bad942"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a6d6bf69e7e4d36db1d22e531fb1267e901993f"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9ff235719dbc3addd13686eda2cde7954e4f10d"
                }
              ],
              "s": {
                "y": 2532,
                "x": 1170,
                "u": "https://preview.redd.it/ycez4n1ihpef1.jpg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=040bcf5645052af01d2898e387ddc725d453f57d"
              },
              "id": "ycez4n1ihpef1"
            },
            "rkfafn1ihpef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8940be43482eece9478af1c42a985068508fd14a"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=443f2bce91203ea8d25427bbdd4061d27ef74bce"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a3d3c5c6e5d0436e05acba134be157c5ed630c2"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5493e75b4f4393553a07e94d9fe51473e6c030b6"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6201d82f8f64127107022ffe74d0aac35a20cd5f"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9c72cae173b6f389f72fb91f1269464294b6ff2"
                }
              ],
              "s": {
                "y": 2532,
                "x": 1170,
                "u": "https://preview.redd.it/rkfafn1ihpef1.jpg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=05870616bd620829c325d91282fa6a7335b9fb36"
              },
              "id": "rkfafn1ihpef1"
            }
          },
          "name": "t3_1m7obdf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "rkfafn1ihpef1",
                "id": 712613354
              },
              {
                "media_id": "ycez4n1ihpef1",
                "id": 712613355
              }
            ]
          },
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vL2QSaqrpFO-8zyuzQNoTeR0eufdBS0bMD8nGEmriYQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753312251,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m new to desktops. I’ve only ever had laptops. Would this be a good setup for local inference. The GPU has 32GB vram and over 1TB memory bandwidth. &lt;/p&gt;\n\n&lt;p&gt;Other comments have lead me to believe that the motherboard and CPU matter as well but I am u sure why. Any help yall can provide would be great&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m7obdf",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7obdf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "skinnyjoints",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7obdf/alienware_area51_gaming_desktop_thoughts_for/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m7obdf",
          "subreddit_subscribers": 503759,
          "created_utc": 1753312251,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,  \nI've been researching for the past three days trying to find a TTS model or voice that *isn't* integrated with AI. But honestly, no matter how much I search it’s been leading nowhere. I’ve asked around, talked to several people, and either got incorrect info or was just flat-out ignored. Even asked ChatGPT at one point... but yeah, that didn’t really get me anywhere either.\n\nThis is the voice I’m trying to figure out: [https://youtu.be/2c6od19xIJU?si=GaKnaUpYHONjwm0W&amp;t=66](https://youtu.be/2c6od19xIJU?si=GaKnaUpYHONjwm0W&amp;t=66)\n\nSome folks told me it’s Loquendo TTS, others said it might be some old, no-longer-available AT&amp;T text-to-speech program. I'm reaching out here as a last resort cause I’m genuinely running out of options and hope. Before this, the only TTS stuff I knew was the free voices on Capcut—so I’m pretty lost here.\n\nIf the program in the link above is no longer available or has been made private, I’d be super grateful if you could suggest something that sounds close to it. Thanks in advance I really appreciate any help!! 🙏",
          "author_fullname": "t2_1hoebye7ad",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A TTS I'm looking for.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7sspe",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753324876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nI&amp;#39;ve been researching for the past three days trying to find a TTS model or voice that &lt;em&gt;isn&amp;#39;t&lt;/em&gt; integrated with AI. But honestly, no matter how much I search it’s been leading nowhere. I’ve asked around, talked to several people, and either got incorrect info or was just flat-out ignored. Even asked ChatGPT at one point... but yeah, that didn’t really get me anywhere either.&lt;/p&gt;\n\n&lt;p&gt;This is the voice I’m trying to figure out: &lt;a href=\"https://youtu.be/2c6od19xIJU?si=GaKnaUpYHONjwm0W&amp;amp;t=66\"&gt;https://youtu.be/2c6od19xIJU?si=GaKnaUpYHONjwm0W&amp;amp;t=66&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Some folks told me it’s Loquendo TTS, others said it might be some old, no-longer-available AT&amp;amp;T text-to-speech program. I&amp;#39;m reaching out here as a last resort cause I’m genuinely running out of options and hope. Before this, the only TTS stuff I knew was the free voices on Capcut—so I’m pretty lost here.&lt;/p&gt;\n\n&lt;p&gt;If the program in the link above is no longer available or has been made private, I’d be super grateful if you could suggest something that sounds close to it. Thanks in advance I really appreciate any help!! 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wmhmXGOVKgTskI7QXImug_YiWP-5Kw55woXis9lfUPM.jpeg?auto=webp&amp;s=604bd879f2da78a65a6e1f31e11610a68710a300",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wmhmXGOVKgTskI7QXImug_YiWP-5Kw55woXis9lfUPM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=79afc422caa70604700ffed6a35cbbc9e0b04690",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/wmhmXGOVKgTskI7QXImug_YiWP-5Kw55woXis9lfUPM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=87c0aef2364721fcd017939a0fb81d4af9006aa5",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/wmhmXGOVKgTskI7QXImug_YiWP-5Kw55woXis9lfUPM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4ec31e8dd0a62263f137d3d5b3c637f6ed52d051",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "wmhmXGOVKgTskI7QXImug_YiWP-5Kw55woXis9lfUPM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7sspe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Impossible_King2505",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7sspe/a_tts_im_looking_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7sspe/a_tts_im_looking_for/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753324876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been reading papers where the main contribution is creating a synthetic dataset for a specific task, followed by fine-tuning an LLM on it. One thing I keep noticing: most of them don't seem to perform hyperparameter tuning (e.g., learning rate, epochs, weight decay) using a validation set. Instead, they just reuse common/default values.\n\nI'm wondering—why is this so common?\n\n* Is it because hyperparameter tuning is considered less important, so they did search but skipped reporting it?\n* Or is it because the main contribution is in data creation, so they just don't care much about the fine-tuning details?",
          "author_fullname": "t2_5z9ud297u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why do many papers skip hyperparameter search?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7503r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753264251,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been reading papers where the main contribution is creating a synthetic dataset for a specific task, followed by fine-tuning an LLM on it. One thing I keep noticing: most of them don&amp;#39;t seem to perform hyperparameter tuning (e.g., learning rate, epochs, weight decay) using a validation set. Instead, they just reuse common/default values.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering—why is this so common?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is it because hyperparameter tuning is considered less important, so they did search but skipped reporting it?&lt;/li&gt;\n&lt;li&gt;Or is it because the main contribution is in data creation, so they just don&amp;#39;t care much about the fine-tuning details?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7503r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hwanchang",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753264251,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "See https://x.com/makingAGI/status/1947286324735856747",
          "author_fullname": "t2_1nt1n3y6xj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone here who has been able to reproduce their results yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6orbr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 126,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 126,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XcNs-lUbqrAcvyj8WfRxfyYgGorJ8nCrbsxZweyByLc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753215098,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;See &lt;a href=\"https://x.com/makingAGI/status/1947286324735856747\"&gt;https://x.com/makingAGI/status/1947286324735856747&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/cfffg12fghef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/cfffg12fghef1.jpeg?auto=webp&amp;s=25f023da9eda3ae6d327e173ef9c7cba8f89880c",
                  "width": 948,
                  "height": 876
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad8d41aa9654515fde6f4b396a86ebf1ad4b0687",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d65ac6156085250ea9aa09ddb48e1e0ca0d499b3",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69d24ed7a665a31761d01a39290d42a299442408",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f02acda8fde9368279ce55c247aa3eb87536a6a5",
                    "width": 640,
                    "height": 591
                  }
                ],
                "variants": {},
                "id": "eXDcYXmDs3JXlXLxg7VAJanyEvhS_GKAlZPpe8O1v6Y"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6orbr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Original_Log_9899",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/",
          "stickied": false,
          "url": "https://i.redd.it/cfffg12fghef1.jpeg",
          "subreddit_subscribers": 503759,
          "created_utc": 1753215098,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So my sister's girlfriend is a CS major (masters), and lately she’s been deep into building this SDK that helps developers work with *multiple AI agents* more easily, like local LLMs or narrow models that need to talk to each other.\n\nshe’s not trying to make another langchain/crewai clone. this is more like a **lightweight sdk, open source and downloaded right on vs code,** not a whole platform.\n\n* **local-first**, works offline\n* agents can **share memory**, handle **fallbacks**, and not step on each other\n* built for devs, not for enterprises\n\nshe’s still in early build mode, but trying to figure out if this is even useful enough to land her a job.\n\nso here’s the ask:\n\n* would you *actually* use something like this?\n* what’s the most annoying part of building multi-agent systems right now?\n* what would *make or break* this kind of tool for you?\n\nIf anyone here’s building with agents, would love to hear what you’d want from a setup like this. If you guys think this is a trash project idea please roast, be brutally honest and dont sugarcoat anything 🙏",
          "author_fullname": "t2_cmo0i3e2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "would this make an ai dev's life easier?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7mwog",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753308711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my sister&amp;#39;s girlfriend is a CS major (masters), and lately she’s been deep into building this SDK that helps developers work with &lt;em&gt;multiple AI agents&lt;/em&gt; more easily, like local LLMs or narrow models that need to talk to each other.&lt;/p&gt;\n\n&lt;p&gt;she’s not trying to make another langchain/crewai clone. this is more like a &lt;strong&gt;lightweight sdk, open source and downloaded right on vs code,&lt;/strong&gt; not a whole platform.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;local-first&lt;/strong&gt;, works offline&lt;/li&gt;\n&lt;li&gt;agents can &lt;strong&gt;share memory&lt;/strong&gt;, handle &lt;strong&gt;fallbacks&lt;/strong&gt;, and not step on each other&lt;/li&gt;\n&lt;li&gt;built for devs, not for enterprises&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;she’s still in early build mode, but trying to figure out if this is even useful enough to land her a job.&lt;/p&gt;\n\n&lt;p&gt;so here’s the ask:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;would you &lt;em&gt;actually&lt;/em&gt; use something like this?&lt;/li&gt;\n&lt;li&gt;what’s the most annoying part of building multi-agent systems right now?&lt;/li&gt;\n&lt;li&gt;what would &lt;em&gt;make or break&lt;/em&gt; this kind of tool for you?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If anyone here’s building with agents, would love to hear what you’d want from a setup like this. If you guys think this is a trash project idea please roast, be brutally honest and dont sugarcoat anything 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7mwog",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Soggy-Guava-1218",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7mwog/would_this_make_an_ai_devs_life_easier/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7mwog/would_this_make_an_ai_devs_life_easier/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753308711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\n\nThis model showed up on my LinkedIn feed today. After listening to a few examples on their [website](https://www.boson.ai/technologies/voice), I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. \n\nListen to this [demo video](https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d), it will just enable so many use cases.\n\nI tried a few examples in their HF [playground](https://huggingface.co/spaces/smola/higgs_audio_v2), it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. \n\n",
          "author_fullname": "t2_6nwb1mbe6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 45,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rmmpgv36tief1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 35,
                  "x": 108,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b8a7927fef36a8dc9cafddd20ca7395324bb30"
                },
                {
                  "y": 70,
                  "x": 216,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa3bc0976c73f20478fe4c41ae0d81d56d9b5efa"
                },
                {
                  "y": 103,
                  "x": 320,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c779645fab14bc4ce1ecf7b4cca7ce06002977dd"
                },
                {
                  "y": 207,
                  "x": 640,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d2890cda589e72ebb380d268d25b2f0c730e4153"
                },
                {
                  "y": 311,
                  "x": 960,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b671678ce82ca5a2abdd86ced5de4262f068a656"
                },
                {
                  "y": 350,
                  "x": 1080,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea15a827df9bb20ae059bea42e6b92ad1d59800b"
                }
              ],
              "s": {
                "y": 872,
                "x": 2686,
                "u": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4"
              },
              "id": "rmmpgv36tief1"
            }
          },
          "name": "t3_1m6vbds",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lDeKkUsKVKJvujnGWUqXtUhpkbsWufoj2laEkKgzAUI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753231503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\"&gt;https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This model showed up on my LinkedIn feed today. After listening to a few examples on their &lt;a href=\"https://www.boson.ai/technologies/voice\"&gt;website&lt;/a&gt;, I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. &lt;/p&gt;\n\n&lt;p&gt;Listen to this &lt;a href=\"https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d\"&gt;demo video&lt;/a&gt;, it will just enable so many use cases.&lt;/p&gt;\n\n&lt;p&gt;I tried a few examples in their HF &lt;a href=\"https://huggingface.co/spaces/smola/higgs_audio_v2\"&gt;playground&lt;/a&gt;, it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6vbds",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Tap3484",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753231503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "It's here guys and qwen nailed it !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4aoalqp6thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebd63c731638a1db5036229c80b9ef7c6e9824fd"
                },
                {
                  "y": 123,
                  "x": 216,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4ac724a3670cd9c86d3d9eed68ced783891cd55"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27543fd2099856fe749e4022dd8e44bdf2a203ec"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c67265c054dccbb5299415ace1ae413e53f4ba40"
                },
                {
                  "y": 550,
                  "x": 960,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9060138fd041f795e2cf4d5c898b3e767230dbfb"
                },
                {
                  "y": 619,
                  "x": 1080,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0aea2f77c1907ccf952d971c3b5b2331c1f6aba9"
                }
              ],
              "s": {
                "y": 1837,
                "x": 3202,
                "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=3202&amp;format=pjpg&amp;auto=webp&amp;s=8e55f7d4fbd65d488aa6606016ce609320a82186"
              },
              "id": "4aoalqp6thef1"
            },
            "mloztw07thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1da39fa93c261ee74d697606209312d155d2610"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08ecfdf3e13689eb562525b1962e0901e009181c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=873f78f3d4bc2c3fc9641a129d06524b3f3f4951"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=468a855136721f0ffae9fb7beacce6df6030447b"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b695450503944ff02456a685ccc317281f8bf8f7"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0ed9b87aa6525e3c700e7835ab55f9fe859d7aa"
                }
              ],
              "s": {
                "y": 1715,
                "x": 3306,
                "u": "https://preview.redd.it/mloztw07thef1.jpg?width=3306&amp;format=pjpg&amp;auto=webp&amp;s=a4886e1284e4cebe0e8558f5cf664cfc4c36b481"
              },
              "id": "mloztw07thef1"
            }
          },
          "name": "t3_1m6qkse",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 89,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "4aoalqp6thef1",
                "id": 711818629
              },
              {
                "caption": "",
                "media_id": "mloztw07thef1",
                "id": 711818630
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 89,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tng4SvC83rVHk9iUXovrs4GeXZmRkFJ59wlPU2wB1GM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m6qkse",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qkse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m6qkse",
          "subreddit_subscribers": 503759,
          "created_utc": 1753219329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6rsym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 70,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 70,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6acd0a4d30c117c56e597d84c1ebb5cedb6e4e00",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753222281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/QwenLM/qwen-code",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?auto=webp&amp;s=ea430b9854a08b70e3dd0972ad9e4758c7fc266d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb193a50d7978c33be16ebec135a318dc6943ea1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6992a1a70171bd4f98508b22498e5ac88cdc45df",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5465c1e679bafd45447bd81f6753867f296ffb49",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=21b1ec40f95d195f9c34bb5728616a2b4c3162fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a9eb8de1aed9882d7841738230f2ecef892f334f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a21f51e6f25035a3147bda1057127718b3b29129",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6rsym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/",
          "stickied": false,
          "url": "https://github.com/QwenLM/qwen-code",
          "subreddit_subscribers": 503759,
          "created_utc": 1753222281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So after doing some further research on the cost of self-hosting larger models I have come to this conclusion - and I am looking for feedback here.\n\nMy specific use case is an AI-assisted IDE I am building myself, and I am looking to dabble in self-hosting a capable model for inference for its users. I currently do **not** have a budget to do extensive testing and benchmarking but I have read up plenty on this (and argued quite a lot with ChatGPT and Gemini lol) for some days now.\n\nHere is what I've got so far:\n\n* tokens per second is not a reliable metric as it actually *averages out* two very different speeds (input vs output):\n\n&gt;One additional note: I recently set up an inference setup for **llama-3-70b** on **8xH100**. I can get about **100,000 tok/s** on inputs which is pretty close to full utilization (1e15 flop/s \\* 8 gpus / 7e10 flop per forward pass). However, I get dramatically worse performance on generation, perhaps **3,200 tok/s**. I'm doing generation with long prompts and llama-3-70b has no sparse attention or other feature for reducing KV cache (beyond multi-query attention which is standard these days), so KV cache bits pretty hard. - [link here](https://www.lesswrong.com/posts/g7H2sSGHAeYxCHzrz/how-much-ai-inference-can-we-do?commentId=RXnfe2ojyqmhLTXJm).\n\n* In IDE use we could expect our requests to **average out** 20k input tokens and 300 output per request. (This is my own estimate based on my own usage via OpernRouter).\n\n**Now for some math:**\n\nSingle H100 (Runpod): $ 2.59/hr\n\nMinimum of 8x H100 (required): $ 20.72/hr\n\nThis setup ***per second:*** 20.72 / 3600 = 0.0057 $/second\n\nQwen3-Coder-480B-A35B-Instruct: (half of llama-3-70B token/s?) **200k tokens/s input** \\+ **6400 tokens/s output**\n\n**Phase 1: Prompt Processing Time** (20,000 input tokens)\n\n* **Calculation:** `20,000 tokens / 200,000 tokens/sec`\n* **Result:** **0.10 seconds**\n\n**Phase 2: Token Generation Time (300 output tokens)**\n\n* **Calculation:** `300 tokens / 6,400 tokens/sec`\n* **Result:** **\\~0.047 seconds**\n\n**Total Time &amp; Cost per Request**\n\n* **Total Time:** `0.10s + 0.047s = **0.147 seconds**`\n* **Total Cost:** `0.147 seconds * $0.0057/sec =` `~$0.0008`\n\n\n\nI mean... is this right? I think this is wrong but it is as far as I could get without actually going and renting these GPUs and testing it for myself. It just seems **so much cheaper** than what I end up paying via API in OpenRouter.",
          "author_fullname": "t2_19mrnrt357",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Throughput: Input vs Output. Looking for help...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7brg9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753283519,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753283238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So after doing some further research on the cost of self-hosting larger models I have come to this conclusion - and I am looking for feedback here.&lt;/p&gt;\n\n&lt;p&gt;My specific use case is an AI-assisted IDE I am building myself, and I am looking to dabble in self-hosting a capable model for inference for its users. I currently do &lt;strong&gt;not&lt;/strong&gt; have a budget to do extensive testing and benchmarking but I have read up plenty on this (and argued quite a lot with ChatGPT and Gemini lol) for some days now.&lt;/p&gt;\n\n&lt;p&gt;Here is what I&amp;#39;ve got so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;tokens per second is not a reliable metric as it actually &lt;em&gt;averages out&lt;/em&gt; two very different speeds (input vs output):&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;One additional note: I recently set up an inference setup for &lt;strong&gt;llama-3-70b&lt;/strong&gt; on &lt;strong&gt;8xH100&lt;/strong&gt;. I can get about &lt;strong&gt;100,000 tok/s&lt;/strong&gt; on inputs which is pretty close to full utilization (1e15 flop/s * 8 gpus / 7e10 flop per forward pass). However, I get dramatically worse performance on generation, perhaps &lt;strong&gt;3,200 tok/s&lt;/strong&gt;. I&amp;#39;m doing generation with long prompts and llama-3-70b has no sparse attention or other feature for reducing KV cache (beyond multi-query attention which is standard these days), so KV cache bits pretty hard. - &lt;a href=\"https://www.lesswrong.com/posts/g7H2sSGHAeYxCHzrz/how-much-ai-inference-can-we-do?commentId=RXnfe2ojyqmhLTXJm\"&gt;link here&lt;/a&gt;.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In IDE use we could expect our requests to &lt;strong&gt;average out&lt;/strong&gt; 20k input tokens and 300 output per request. (This is my own estimate based on my own usage via OpernRouter).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Now for some math:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Single H100 (Runpod): $ 2.59/hr&lt;/p&gt;\n\n&lt;p&gt;Minimum of 8x H100 (required): $ 20.72/hr&lt;/p&gt;\n\n&lt;p&gt;This setup &lt;strong&gt;&lt;em&gt;per second:&lt;/em&gt;&lt;/strong&gt; 20.72 / 3600 = 0.0057 $/second&lt;/p&gt;\n\n&lt;p&gt;Qwen3-Coder-480B-A35B-Instruct: (half of llama-3-70B token/s?) &lt;strong&gt;200k tokens/s input&lt;/strong&gt; + &lt;strong&gt;6400 tokens/s output&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Phase 1: Prompt Processing Time&lt;/strong&gt; (20,000 input tokens)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Calculation:&lt;/strong&gt; &lt;code&gt;20,000 tokens / 200,000 tokens/sec&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;strong&gt;0.10 seconds&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Phase 2: Token Generation Time (300 output tokens)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Calculation:&lt;/strong&gt; &lt;code&gt;300 tokens / 6,400 tokens/sec&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;strong&gt;~0.047 seconds&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Total Time &amp;amp; Cost per Request&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Total Time:&lt;/strong&gt; &lt;code&gt;0.10s + 0.047s = **0.147 seconds**&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Total Cost:&lt;/strong&gt; &lt;code&gt;0.147 seconds * $0.0057/sec =&lt;/code&gt; &lt;code&gt;~$0.0008&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I mean... is this right? I think this is wrong but it is as far as I could get without actually going and renting these GPUs and testing it for myself. It just seems &lt;strong&gt;so much cheaper&lt;/strong&gt; than what I end up paying via API in OpenRouter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7brg9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Budget_Map_3333",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7brg9/throughput_input_vs_output_looking_for_help/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7brg9/throughput_input_vs_output_looking_for_help/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753283238,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nI just published research on \"thought anchors\" - a method to analyze which specific reasoning steps matter most for task success in locally-runnable models. Thought this community would find the results interesting since it directly compares two popular local models.\n\n**TL;DR: Qwen3-0.6B and DeepSeek-R1-Distill-1.5B have fundamentally different reasoning architectures, not just different performance levels.**\n\n# What are Thought Anchors?\n\nBuilding on work by Bogdan et al., thought anchors identify critical sentences in a model's chain-of-thought reasoning that significantly impact whether it gets the right answer. Instead of looking at individual tokens, we analyze complete reasoning steps.\n\n# Key Findings on GSM8K Math Problems:\n\n**DeepSeek-R1-Distill (1.5B):**\n\n* Concentrated reasoning: fewer steps, higher impact per step (0.408 avg)\n* 82.7% positive reasoning steps - very consistent\n* Single primary failure mode (logical errors)\n* Optimized for reliability over exploration\n\n**Qwen3 (0.6B):**\n\n* Distributed reasoning: more steps, spread impact (0.278 avg)\n* 71.6% positive steps but higher variance\n* Multiple failure modes (logical, computational, missing steps)\n* More experimental approach with higher risk/reward\n\n# Practical Implications for Local Users:\n\nIf you're choosing between these models:\n\n* **Need consistent, reliable outputs?** → DeepSeek-R1's concentrated approach\n* **Want more creative/exploratory reasoning?** → Qwen3's distributed approach\n* **Resource constraints?** → Qwen3 at 0.6B vs DeepSeek at 1.5B\n\nThis isn't about one being \"better\" - they're optimized for different reasoning strategies.\n\n# Open Source Everything:\n\n* **PTS Library**: [https://github.com/codelion/pts](https://github.com/codelion/pts) (tool for generating thought anchors)\n* **Datasets**: Available on HuggingFace for both models\n* **Analysis Code**: Full reproducibility\n* **Article**: [https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors](https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors)\n\nThe PTS library works with any local model that supports structured output, so you can analyze your own models' reasoning patterns.\n\n# Questions for the Community:\n\n1. Has anyone noticed similar reasoning pattern differences in their local setups?\n2. Which reasoning approach works better for your specific use cases?\n3. Any interest in extending this analysis to other popular local models (Llama, Mistral, etc.)?\n\nWould love to hear your experiences and thoughts on model reasoning approaches!\n\n**Edit**: Original thought anchors concept credit goes to Paul Bogdan's team - this research extends their methodology to compare local model architectures.",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Research] Thought Anchors: Understanding How Qwen3-0.6B vs DeepSeek-R1-Distill-1.5B Actually Reason - Different Cognitive Architectures Revealed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6zce0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753243256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I just published research on &amp;quot;thought anchors&amp;quot; - a method to analyze which specific reasoning steps matter most for task success in locally-runnable models. Thought this community would find the results interesting since it directly compares two popular local models.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: Qwen3-0.6B and DeepSeek-R1-Distill-1.5B have fundamentally different reasoning architectures, not just different performance levels.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;What are Thought Anchors?&lt;/h1&gt;\n\n&lt;p&gt;Building on work by Bogdan et al., thought anchors identify critical sentences in a model&amp;#39;s chain-of-thought reasoning that significantly impact whether it gets the right answer. Instead of looking at individual tokens, we analyze complete reasoning steps.&lt;/p&gt;\n\n&lt;h1&gt;Key Findings on GSM8K Math Problems:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;DeepSeek-R1-Distill (1.5B):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Concentrated reasoning: fewer steps, higher impact per step (0.408 avg)&lt;/li&gt;\n&lt;li&gt;82.7% positive reasoning steps - very consistent&lt;/li&gt;\n&lt;li&gt;Single primary failure mode (logical errors)&lt;/li&gt;\n&lt;li&gt;Optimized for reliability over exploration&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 (0.6B):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Distributed reasoning: more steps, spread impact (0.278 avg)&lt;/li&gt;\n&lt;li&gt;71.6% positive steps but higher variance&lt;/li&gt;\n&lt;li&gt;Multiple failure modes (logical, computational, missing steps)&lt;/li&gt;\n&lt;li&gt;More experimental approach with higher risk/reward&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Practical Implications for Local Users:&lt;/h1&gt;\n\n&lt;p&gt;If you&amp;#39;re choosing between these models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Need consistent, reliable outputs?&lt;/strong&gt; → DeepSeek-R1&amp;#39;s concentrated approach&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Want more creative/exploratory reasoning?&lt;/strong&gt; → Qwen3&amp;#39;s distributed approach&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Resource constraints?&lt;/strong&gt; → Qwen3 at 0.6B vs DeepSeek at 1.5B&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This isn&amp;#39;t about one being &amp;quot;better&amp;quot; - they&amp;#39;re optimized for different reasoning strategies.&lt;/p&gt;\n\n&lt;h1&gt;Open Source Everything:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;PTS Library&lt;/strong&gt;: &lt;a href=\"https://github.com/codelion/pts\"&gt;https://github.com/codelion/pts&lt;/a&gt; (tool for generating thought anchors)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Datasets&lt;/strong&gt;: Available on HuggingFace for both models&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Analysis Code&lt;/strong&gt;: Full reproducibility&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Article&lt;/strong&gt;: &lt;a href=\"https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors\"&gt;https://huggingface.co/blog/codelion/understanding-model-reasoning-thought-anchors&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The PTS library works with any local model that supports structured output, so you can analyze your own models&amp;#39; reasoning patterns.&lt;/p&gt;\n\n&lt;h1&gt;Questions for the Community:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Has anyone noticed similar reasoning pattern differences in their local setups?&lt;/li&gt;\n&lt;li&gt;Which reasoning approach works better for your specific use cases?&lt;/li&gt;\n&lt;li&gt;Any interest in extending this analysis to other popular local models (Llama, Mistral, etc.)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would love to hear your experiences and thoughts on model reasoning approaches!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Original thought anchors concept credit goes to Paul Bogdan&amp;#39;s team - this research extends their methodology to compare local model architectures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?auto=webp&amp;s=187504b6a6cdaab3b5025c91a3798e0b46bcb9f0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=76a98416b90c3288a04cac47b99811464ff316e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9389c4b2ca46a4b05928b5941369ea699ccec4e6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5964cc2bf092a1202a804fdcec163a7e14497c35",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a192ca72cfe990c9ccd3456b264cd2914962c19",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67133c9db0890c820ce5cdc0549dcaeb9f9e95de",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=978ef4cf2867cf2f582d6936f382955179b16eba",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "LUwOzF1qaWXXZdKFKtNyq98eHv6n8KWbzzx3ALLxitY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6zce0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6zce0/research_thought_anchors_understanding_how/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6zce0/research_thought_anchors_understanding_how/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753243256,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A week ago I shared an early prototype and got amazing feedback. Main request? \"Show us how to actually install this properly.\"\n\n**The problem:** Every time you restart Claude Code CLI, you lose everything.\n\n**What I built:** RagCore - universal RAG system with persistent memory via MCP stdio. Claude remembers your project context and queries any documentation you add.\n\n**The magic moment:** Close terminal → Restart Claude Code CLI → Continue exactly where you left off.\n\n**How it works:**\n\n* Tell Claude \"learn about current project\" → automatic memory bank query\n* Ask \"implement Laravel validation\" → Claude queries RAG server with local LLM\n* RAG server logs show exact sources (zero hallucinations)\n* Smart token optimization by query complexity\n\n**Results after week of testing:**\n\n* 4,306 Laravel docs indexed, 7-20 second response times\n* Works with Python, FastAPI, custom frameworks\n* Local LLM (your code never leaves your machine)\n\n**GitHub:** [https://github.com/lexa5575/RagCore](https://github.com/lexa5575/RagCore)\n\nInstallation details in comments. What documentation would you want to add?",
          "author_fullname": "t2_icqqkg97",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a Universal RAG + Memory System for Claude with MCP - Production Ready",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7kz8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753304162,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A week ago I shared an early prototype and got amazing feedback. Main request? &amp;quot;Show us how to actually install this properly.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Every time you restart Claude Code CLI, you lose everything.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt; RagCore - universal RAG system with persistent memory via MCP stdio. Claude remembers your project context and queries any documentation you add.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The magic moment:&lt;/strong&gt; Close terminal → Restart Claude Code CLI → Continue exactly where you left off.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tell Claude &amp;quot;learn about current project&amp;quot; → automatic memory bank query&lt;/li&gt;\n&lt;li&gt;Ask &amp;quot;implement Laravel validation&amp;quot; → Claude queries RAG server with local LLM&lt;/li&gt;\n&lt;li&gt;RAG server logs show exact sources (zero hallucinations)&lt;/li&gt;\n&lt;li&gt;Smart token optimization by query complexity&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Results after week of testing:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;4,306 Laravel docs indexed, 7-20 second response times&lt;/li&gt;\n&lt;li&gt;Works with Python, FastAPI, custom frameworks&lt;/li&gt;\n&lt;li&gt;Local LLM (your code never leaves your machine)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/lexa5575/RagCore\"&gt;https://github.com/lexa5575/RagCore&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Installation details in comments. What documentation would you want to add?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?auto=webp&amp;s=1e15473524aaf1ec7d69116219461863da0dd38d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1accd17d757c1d6bca607d221f043a9a301e1f59",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bff91e902da988eb8a710343ac8e1f78f17d52ce",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=87b1f1b65c28c7dbc7ec568129fc6ebcf7fe4913",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0768ba20613927ad7eac15c74dc14fae53bbdb8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1853a6bc8df8126651364075710d82c19403c566",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8752469409716bc996a820e024b1e7efc87cef8a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "xrkX6jGeRIvp8RnwqY5OlMAx1guQn5jFdJg4hbnVUQ8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m7kz8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Basic_Soft9158",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7kz8s/built_a_universal_rag_memory_system_for_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7kz8s/built_a_universal_rag_memory_system_for_claude/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753304162,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! Not sure is this the right place to ask but I’ve been working on a Japanese voice assistant as a side project, and I’m currently struggling to find a good TTS solution. I tried using [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) from their webui, and the voice quality is very impressive, but it’s difficult to integrate it into my project since it doesn’t come as a proper Python package (I don't see any official PyPI support).\n\nRight now, the only way I can use it is by cloning their entire repo and calling [synthesize()](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/GPT_SoVITS/inference_cli.py) directly, that means I need to move my whole project into theirs.\n\nIs there a way to integrate GPT-SoVITS into the project? Or are there other high-quality Japanese TTS tools that works well without fine-tuning?",
          "author_fullname": "t2_9so7g7ch",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to acutally use gpt-sovits?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7bd41",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753282340,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! Not sure is this the right place to ask but I’ve been working on a Japanese voice assistant as a side project, and I’m currently struggling to find a good TTS solution. I tried using &lt;a href=\"https://github.com/RVC-Boss/GPT-SoVITS\"&gt;GPT-SoVITS&lt;/a&gt; from their webui, and the voice quality is very impressive, but it’s difficult to integrate it into my project since it doesn’t come as a proper Python package (I don&amp;#39;t see any official PyPI support).&lt;/p&gt;\n\n&lt;p&gt;Right now, the only way I can use it is by cloning their entire repo and calling &lt;a href=\"https://github.com/RVC-Boss/GPT-SoVITS/blob/main/GPT_SoVITS/inference_cli.py\"&gt;synthesize()&lt;/a&gt; directly, that means I need to move my whole project into theirs.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to integrate GPT-SoVITS into the project? Or are there other high-quality Japanese TTS tools that works well without fine-tuning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?auto=webp&amp;s=9ada7abaae5c281710496c0b299f1d5be001c93f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8a46b62a80893173dc3ed635ca54310aa68bc664",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f5fc9721b66d01680dfb9b169f919340cf80348",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=552ea87808fa1a436acb99123c476c99ba165126",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c4c179a85149faefd5816021be7e674ac1eb054",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=549e70bf56a108832e591f3b0d99240c6369dab2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=138bceb99e73dee280ec27063fe798a1967c60d9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "caQx71gSvUb5KdCJdZONmkX6p-beuuKWrd6dl-WlSHU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7bd41",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Icy-Ad6078",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7bd41/how_to_acutally_use_gptsovits/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7bd41/how_to_acutally_use_gptsovits/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753282340,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_6vcmk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is imminent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6medy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 114,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 114,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mmNhnm_QiKDvZ8nOArY9M-gXEHPij6ccQfZ3Z4a4vrs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209818,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mruaiodv0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mruaiodv0hef1.png?auto=webp&amp;s=be215118da4d1c5ae5fc739c077ba4bbf8354f1a",
                  "width": 501,
                  "height": 251
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49a20e04a28093446580d2909236b45d1e2f568e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=772268126ecad9399aa5fb8ad3dc61fa7a8e5af0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=daa5e07dcd586edd4e8488215b2df66df2d2c809",
                    "width": 320,
                    "height": 160
                  }
                ],
                "variants": {},
                "id": "gxF1-bhuks7kobb2JTcsN29raeY4IvwO_eL--8kAZ38"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6medy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dudensen",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/",
          "stickied": false,
          "url": "https://i.redd.it/mruaiodv0hef1.png",
          "subreddit_subscribers": 503759,
          "created_utc": 1753209818,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need the highest quality I can get for a price point below $1000 in training and $1/M tokens inference. I would prefer to do full finetuning on a base model. It's for a continuation task (writing with long range dependency) so I don't actually need or want chat or instruct style. I need context 32K.\n\nI have about 200M tokens of finetuning data which I can augment to 1B easily by doing different variations.\n\nMy opinions are:\n1. Finetune Gemini Flash 2.0. They're using a LoRA. It'll cost $800, but then I can infer for $0.30/M on batch.\n2. Finetune Qwen2.5 or Llama 3.3 either 70B or 32B. Might cost a bit more. Inference could be cheaper if I use 4bit quantization, otherwise probably a slightly more expensive, and a lot more difficult to maintain.\n\nBut ultimately in the end I care about the quality output. I don't really want to test both because of the time and money it would take to do so.\nWhich do you think would give the better output?\n\nI'm torn. It seems to me I'd be able to train it better if I train the full base model on 1B tokens. That would probably be a bit expensive to train.\nYet Gemini might just be a better model in the first place. It's hard to tell because Gemini Flash 2.0 is absolutely amazing at some things, stuff that none of the Open Source can do like editing a massive block of text and actually responsing with the entire thing every time instead of secretly deleting sentences here and there. Then some other stuff it doesn't do so well. So it *might* actually be a small model that's really really well trained (or 100 tiny experts), in which case a LoRA on that might not be able to keep my task up for 32K tokens.\n\nSince I'm only training one task (actually 2 but they're related) I don't need or want experts, or thinking.\n\nOn the other hand it's cheaper and easier to train Flash 2.0 by a lot.\n\nDoes anyone have any personal insight into my dilemma?",
          "author_fullname": "t2_8jhue7k0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Should I do finetuning on Gemini or on open source models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7e8d0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753288860,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need the highest quality I can get for a price point below $1000 in training and $1/M tokens inference. I would prefer to do full finetuning on a base model. It&amp;#39;s for a continuation task (writing with long range dependency) so I don&amp;#39;t actually need or want chat or instruct style. I need context 32K.&lt;/p&gt;\n\n&lt;p&gt;I have about 200M tokens of finetuning data which I can augment to 1B easily by doing different variations.&lt;/p&gt;\n\n&lt;p&gt;My opinions are:\n1. Finetune Gemini Flash 2.0. They&amp;#39;re using a LoRA. It&amp;#39;ll cost $800, but then I can infer for $0.30/M on batch.\n2. Finetune Qwen2.5 or Llama 3.3 either 70B or 32B. Might cost a bit more. Inference could be cheaper if I use 4bit quantization, otherwise probably a slightly more expensive, and a lot more difficult to maintain.&lt;/p&gt;\n\n&lt;p&gt;But ultimately in the end I care about the quality output. I don&amp;#39;t really want to test both because of the time and money it would take to do so.\nWhich do you think would give the better output?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m torn. It seems to me I&amp;#39;d be able to train it better if I train the full base model on 1B tokens. That would probably be a bit expensive to train.\nYet Gemini might just be a better model in the first place. It&amp;#39;s hard to tell because Gemini Flash 2.0 is absolutely amazing at some things, stuff that none of the Open Source can do like editing a massive block of text and actually responsing with the entire thing every time instead of secretly deleting sentences here and there. Then some other stuff it doesn&amp;#39;t do so well. So it &lt;em&gt;might&lt;/em&gt; actually be a small model that&amp;#39;s really really well trained (or 100 tiny experts), in which case a LoRA on that might not be able to keep my task up for 32K tokens.&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m only training one task (actually 2 but they&amp;#39;re related) I don&amp;#39;t need or want experts, or thinking.&lt;/p&gt;\n\n&lt;p&gt;On the other hand it&amp;#39;s cheaper and easier to train Flash 2.0 by a lot.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any personal insight into my dilemma?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7e8d0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pan000",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753288860,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/k2fyyrlhmoef1.png?width=1046&amp;format=png&amp;auto=webp&amp;s=8495ebda17093971eeb782d2328b4b674cf36614\n\nQwen3 coder is wild! This is really exciting... Until it's not...",
          "author_fullname": "t2_14cl94t8ha",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This is what I call crazy.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 85,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "k2fyyrlhmoef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 66,
                  "x": 108,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33cf164b8b60f12fa7485a1ba544977c9451101c"
                },
                {
                  "y": 132,
                  "x": 216,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=af9cb1dd42116e10c90be1bd4603c5df752f7782"
                },
                {
                  "y": 196,
                  "x": 320,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=51a5515b69264beb8e121905d1d4af3c615851cf"
                },
                {
                  "y": 392,
                  "x": 640,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cca63d9174615a1c70f2de623c8cde8a091a4763"
                },
                {
                  "y": 589,
                  "x": 960,
                  "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=432500061af2625f064397e06e674d15787fa123"
                }
              ],
              "s": {
                "y": 642,
                "x": 1046,
                "u": "https://preview.redd.it/k2fyyrlhmoef1.png?width=1046&amp;format=png&amp;auto=webp&amp;s=8495ebda17093971eeb782d2328b4b674cf36614"
              },
              "id": "k2fyyrlhmoef1"
            }
          },
          "name": "t3_1m7jzjg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uIc4_Hbf1DaCIfy5Ebnb36BLFB8aNlsgfkeUST0YC2U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753301876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/k2fyyrlhmoef1.png?width=1046&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8495ebda17093971eeb782d2328b4b674cf36614\"&gt;https://preview.redd.it/k2fyyrlhmoef1.png?width=1046&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8495ebda17093971eeb782d2328b4b674cf36614&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen3 coder is wild! This is really exciting... Until it&amp;#39;s not...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m7jzjg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GenLabsAI",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7jzjg/this_is_what_i_call_crazy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7jzjg/this_is_what_i_call_crazy/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753301876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, does anyone know of a photo/video program that can change the background so that my product photos look really good similar to a photo shoot. I took some basic photos and the software I was using created these which was great. The software is very very expensive though at a few hundred dollars per month and has bad reviews overall so I’m looking for an alternative. This was made in adcreative ai.\n\nI’m looking for something different. I can do photos that are similar caliber for either free or not as expensive.\n\nIn my photos above, you can see the photo that I took and that the background was eliminated and then changed to an AI background in a spa setting\n\nThanks!",
          "author_fullname": "t2_1rq2klnw0u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI background for products",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 119,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7jybm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/8PaHKM_x0pxx0WBhJ3PLIpt9NRDZtqSy-K9hH9Wqzt4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753301798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, does anyone know of a photo/video program that can change the background so that my product photos look really good similar to a photo shoot. I took some basic photos and the software I was using created these which was great. The software is very very expensive though at a few hundred dollars per month and has bad reviews overall so I’m looking for an alternative. This was made in adcreative ai.&lt;/p&gt;\n\n&lt;p&gt;I’m looking for something different. I can do photos that are similar caliber for either free or not as expensive.&lt;/p&gt;\n\n&lt;p&gt;In my photos above, you can see the photo that I took and that the background was eliminated and then changed to an AI background in a spa setting&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/a5qw3y2fmoef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?auto=webp&amp;s=678198fd66eca692afaa55f36f4a376f1818ddc9",
                  "width": 1179,
                  "height": 1006
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c78fcd8debf9c3724a29fe6d1080fd47a4edc7c",
                    "width": 108,
                    "height": 92
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2f3b90a63c2c43012ec6e6127f60ca8a4944779",
                    "width": 216,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb94fdee133d953d34636a7d1e0f8ccb217dc8e8",
                    "width": 320,
                    "height": 273
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dddedc14364a6da08b09557e6f430ad2efd989e3",
                    "width": 640,
                    "height": 546
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e29b7587acf415961f7dd1f331dad009a1c3812a",
                    "width": 960,
                    "height": 819
                  },
                  {
                    "url": "https://preview.redd.it/a5qw3y2fmoef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb1610aff74f61af4c30419af4e798d2744a97d4",
                    "width": 1080,
                    "height": 921
                  }
                ],
                "variants": {},
                "id": "sY23F0e05kLWeMk1bFNxXzQic1fSxEMBFvxDPyWOUl4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7jybm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UGC_Chris_D",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7jybm/ai_background_for_products/",
          "stickied": false,
          "url": "https://i.redd.it/a5qw3y2fmoef1.jpeg",
          "subreddit_subscribers": 503759,
          "created_utc": 1753301798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! I’m currently using Notion, which works great for transcribing meetings and converting them into summaries, action items, and so on. \n\nIs anyone using open-source / locally powered AI tools? I’d love to hear about your experience with those.\n\nThanks!",
          "author_fullname": "t2_ajuxt3cr4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source and/or Local AI Meeting Transcription that works for you?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m7jvba",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753301609,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I’m currently using Notion, which works great for transcribing meetings and converting them into summaries, action items, and so on. &lt;/p&gt;\n\n&lt;p&gt;Is anyone using open-source / locally powered AI tools? I’d love to hear about your experience with those.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m7jvba",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Southern_Sun_2106",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m7jvba/opensource_andor_local_ai_meeting_transcription/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7jvba/opensource_andor_local_ai_meeting_transcription/",
          "subreddit_subscribers": 503759,
          "created_utc": 1753301609,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}