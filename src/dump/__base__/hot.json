{
  "kind": "Listing",
  "data": {
    "after": "t3_1m0jeyu",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Mine are:\n\n1. All the popular public benchmarks are nearly worthless when it comes to a model's general ability. Literaly the only good thing we get out of them is a rating for \"can the model regurgitate the answers to questions the devs made sure it was trained on repeatedly to get higher benchmarks, without fucking it up\", which does have some value. I think the people who maintain the benchmarks know this too, but we're all supposed to pretend like your MMLU score is indicative of the ability to help the user solve questions outside of those in your training data? Please. No one but hobbyists has enough integrity to keep their benchmark questions private? Bleak.\n\n2. Any ranker who has an LLM judge giving a rating to the \"writing style\" of another LLM is a hack who has no business ranking models. Please don't waste your time or ours. You clearly don't understand what an LLM is. Stop wasting carbon with your pointless inference.\n\n3. Every community finetune I've used is always far worse than the base model. They always reduce the coherency, it's just a matter of how much. That's because 99.9% of finetuners are clueless people just running training scripts on the latest random dataset they found, or doing random merges (of equally awful finetunes). They don't even try their own models, they just shit them out into the world and subject us to them. idk why they do it, is it narcissism, or resume-padding, or what? I wish HF would start charging money for storage just to discourage these people. YOU DON'T HAVE TO UPLOAD EVERY MODEL YOU MAKE. The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste.",
          "author_fullname": "t2_93yn32gx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Your unpopular takes on LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0z1zx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 361,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 361,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752627161,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mine are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;All the popular public benchmarks are nearly worthless when it comes to a model&amp;#39;s general ability. Literaly the only good thing we get out of them is a rating for &amp;quot;can the model regurgitate the answers to questions the devs made sure it was trained on repeatedly to get higher benchmarks, without fucking it up&amp;quot;, which does have some value. I think the people who maintain the benchmarks know this too, but we&amp;#39;re all supposed to pretend like your MMLU score is indicative of the ability to help the user solve questions outside of those in your training data? Please. No one but hobbyists has enough integrity to keep their benchmark questions private? Bleak.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any ranker who has an LLM judge giving a rating to the &amp;quot;writing style&amp;quot; of another LLM is a hack who has no business ranking models. Please don&amp;#39;t waste your time or ours. You clearly don&amp;#39;t understand what an LLM is. Stop wasting carbon with your pointless inference.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Every community finetune I&amp;#39;ve used is always far worse than the base model. They always reduce the coherency, it&amp;#39;s just a matter of how much. That&amp;#39;s because 99.9% of finetuners are clueless people just running training scripts on the latest random dataset they found, or doing random merges (of equally awful finetunes). They don&amp;#39;t even try their own models, they just shit them out into the world and subject us to them. idk why they do it, is it narcissism, or resume-padding, or what? I wish HF would start charging money for storage just to discourage these people. YOU DON&amp;#39;T HAVE TO UPLOAD EVERY MODEL YOU MAKE. The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0z1zx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dtdisapointingresult",
          "discussion_type": null,
          "num_comments": 249,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752627161,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta's new ASI team discussed about abandoning Meta's powerful Open-source and focus on developing close",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m14a9j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 114,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 114,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752643396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html\"&gt;https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?auto=webp&amp;s=211ff5c9d8860c633734a0f69515f881de8905e4",
                  "width": 1050,
                  "height": 550
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f431ed3ef795de81f0d9be2452ed2466f4727f88",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c728fd0b47256c06b7e53063606348710b74999",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a1dc415541c74d1ee2dd3620b8e6997e56ad7f2",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a0ebffa84a0071645409fce2ba2a7d33bd6a731",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=361e53f2aa166522931efd9533bd8c76685cfc5a",
                    "width": 960,
                    "height": 502
                  }
                ],
                "variants": {},
                "id": "62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m14a9j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m14a9j/metas_new_asi_team_discussed_about_abandoning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m14a9j/metas_new_asi_team_discussed_about_abandoning/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752643396,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Said it when this was presented that will have MSRP around RTX5080 since AMD decided to bench it against that card and not some workstation grade RTX.... 🥳\n\n",
          "author_fullname": "t2_viufiki6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO \"Blackwell\" With 24 GB VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m13eb2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 126,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 126,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=cc6436a72dfff74da11e97b61b0f451a9cbe9da4",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752640119,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Said it when this was presented that will have MSRP around RTX5080 since AMD decided to bench it against that card and not some workstation grade RTX.... 🥳&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-radeon-ai-pro-r9700-32-gb-gpu-listed-pricing-around-1250-half-price-nvidia-rtx-pro-blackwell-24-gb/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?auto=webp&amp;s=1b1cddd160cfd154f6028f96b8eadb26d54994ab",
                  "width": 1493,
                  "height": 837
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e7da05caae6d75f7cdd0e822908efc42a62fe6e",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82f7f189fbb4651232aba3b6bfdd435808cf3730",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f63d85f0ef5f4d1d42b85af8946dafa138067eaa",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=82787d21e7b0821fdce5a034706e0598040c7cc4",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2b9202a3e04226baa903ee3afe8ddf7a1398cc0",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2fc89c53afff247534a45b455b4d1ae5d01d6db7",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m13eb2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Repeat_22",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/",
          "stickied": false,
          "url": "https://wccftech.com/amd-radeon-ai-pro-r9700-32-gb-gpu-listed-pricing-around-1250-half-price-nvidia-rtx-pro-blackwell-24-gb/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752640119,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "ETH Zurich &amp; EPFL Public LLM – Technical Specs\n\t•\tRelease: Late summer 2025\n\t•\tDevelopers: EPFL, ETH Zurich, Swiss National Supercomputing Centre (CSCS), Swiss universities\n\t•\tModel sizes: 8B and 70B parameters (fully open weights and code, Apache 2.0 license)\n\t•\tMultilinguality: Fluency in 1,000+ languages (trained on &gt;1,500 languages; ~60% English, ~40% non-English; code and math included)\n\t•\tTraining data: &gt;15 trillion tokens, high-quality, transparent, reproducible, with web-crawling opt-outs respected\n\t•\tTraining hardware: Alps supercomputer (CSCS, Lugano), &gt;10,000 NVIDIA Grace Hopper Superchips, 100% carbon-neutral electricity\n\t•\tCompliance: Swiss data protection and copyright laws, EU AI Act transparency\n\t•\tIntended use: Science, society, industry; fully public download, detailed documentation on model architecture and training\n\t•\tInitiative: Swiss AI Initiative, 800+ researchers, 20M+ GPU hours/year, funded by ETH Board (2025–2028)",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0v9m1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 339,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 339,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=140&amp;height=69&amp;crop=140:69,smart&amp;auto=webp&amp;s=eb6d386e78fa6261648d91dd3aa26540b2de45a8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752617058,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ethz.ch",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ETH Zurich &amp;amp; EPFL Public LLM – Technical Specs\n    • Release: Late summer 2025\n    • Developers: EPFL, ETH Zurich, Swiss National Supercomputing Centre (CSCS), Swiss universities\n    • Model sizes: 8B and 70B parameters (fully open weights and code, Apache 2.0 license)\n    • Multilinguality: Fluency in 1,000+ languages (trained on &amp;gt;1,500 languages; ~60% English, ~40% non-English; code and math included)\n    • Training data: &amp;gt;15 trillion tokens, high-quality, transparent, reproducible, with web-crawling opt-outs respected\n    • Training hardware: Alps supercomputer (CSCS, Lugano), &amp;gt;10,000 NVIDIA Grace Hopper Superchips, 100% carbon-neutral electricity\n    • Compliance: Swiss data protection and copyright laws, EU AI Act transparency\n    • Intended use: Science, society, industry; fully public download, detailed documentation on model architecture and training\n    • Initiative: Swiss AI Initiative, 800+ researchers, 20M+ GPU hours/year, funded by ETH Board (2025–2028)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?auto=webp&amp;s=c444e4312b925e4518f67c13cd1d0d45a148e086",
                  "width": 1565,
                  "height": 782
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=00f03455ad8e9fc0d7ab20142af7d9f6c62b3273",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6c969005aff0c942b7f56ff899031ce58c131be",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f469ddb87f611d5472215734a3217eb0ca1795e4",
                    "width": 320,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3",
                    "width": 640,
                    "height": 319
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=869859917c64b3c7518aec8cee6c925a903fa611",
                    "width": 960,
                    "height": 479
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bd6e7e7d67e1ccb5d54f14559b874a510f09c70",
                    "width": 1080,
                    "height": 539
                  }
                ],
                "variants": {},
                "id": "TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0v9m1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/",
          "stickied": false,
          "url": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "subreddit_subscribers": 499772,
          "created_utc": 1752617058,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "T5Gemma released a new encoder-decoder model.",
          "author_fullname": "t2_8jqx3m14",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "T5Gemma: A new collection of encoder-decoder Gemma models- Google Developers Blog",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m16kdm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=cf3e921e188ce52410c6810f28f99be1a091917a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752651989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "developers.googleblog.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;T5Gemma released a new encoder-decoder model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://developers.googleblog.com/en/t5gemma/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k.png?auto=webp&amp;s=be53d214730046f4f70ef325e842c4f096c3e4a1",
                  "width": 860,
                  "height": 430
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=72b5316e353260b40cfddbb1814bf1be5854d7c9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec7568e111028071d64de2a43a06cb4ebc2bc70a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=80e400a3214a3712b67659f8c646e9780ab343cd",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbbccb5f228138b777b853e12fc432178fff5f50",
                    "width": 640,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m16kdm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeltaSqueezer",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m16kdm/t5gemma_a_new_collection_of_encoderdecoder_gemma/",
          "stickied": false,
          "url": "https://developers.googleblog.com/en/t5gemma/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752651989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Update to my [previous post](https://www.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/) — the repo is **finally public**!\n\n# 🔥 TL;DR\n\n* **GitHub**: [diptanshu1991/LoFT](https://github.com/diptanshu1991/LoFT)\n* **What you get**: 5 CLI commands: `loft finetune`, `merge`, `export`, `quantize`, `chat`\n* **Hardware**: Tested on 8GB MacBook Air  — peak RAM **330MB**\n* **Performance**: 300 Dolly samples, 2 epochs → 1.5 hrs total wall-time\n* **Inference speed**: 6.9 tok/sec (Q4\\_0) on CPU\n* **License**: MIT – 100% open-source\n\n# 🧠 What is LoFT?\n\n**LoFT CLI** is a lightweight, CPU-friendly toolkit that lets you:\n\n* ✅ Finetune 1–3B LLMs like TinyLlama using **QLoRA**\n* 🔄 Merge and export models to **GGUF**\n* 🧱 Quantize models (Q4\\_0, Q5\\_1, etc.)\n* 💬 Run **offline inference** using `llama.cpp`\n\nAll from a **command-line interface** on your **local laptop**. No Colab. No GPUs. No cloud.\n\n# 📊 Benchmarks (8GB MacBook Air)\n\n|Step|Output|Size|Peak RAM|Time|\n|:-|:-|:-|:-|:-|\n|Finetune|LoRA Adapter|4.3 MB|308 MB|23 min|\n|Merge|HF Model|4.2 GB|322 MB|4.7 min|\n|Export|GGUF (FP16)|2.1 GB|322 MB|83 sec|\n|Quantize|GGUF (Q4\\_0)|607 MB|322 MB|21 sec|\n|Chat|6.9 tok/sec|–|322 MB|79 sec|\n\n🧪 Trained on: 300 Dolly samples, 2 epochs → loss &lt; 1.0\n\n# 🧪 5-Command Lifecycle\n\nLoFT runs the complete LLM workflow — from training to chat — in just 5 commands:\n\n    loft finetune  \n    loft merge  \n    loft export  \n    loft quantize  \n    loft chat\n\n&gt;\n\n# 🧪 Coming Soon in LoFT\n\n**📦 Plug-and-Play Recipes**\n\n* Legal Q&amp;A bots (air-gapped, offline)\n* Customer support assistants\n* Contract summarizers\n\n**🌱 Early Experiments**\n\n* Multi-turn finetuning\n* Adapter-sharing for niche domains\n* Dataset templating tools\n\n&gt;\n\nLoFT is built for indie builders, researchers, and OSS devs who want **local GenAI without GPU constraints**. Would love your feedback on:\n\n* What models/datasets you would like to see supported next\n* Edge cases or bugs during install/training\n* Use cases where this unlocks new workflows\n\n🔗 GitHub: [https://github.com/diptanshu1991/LoFT](https://github.com/diptanshu1991/LoFT)  \n🪪 MIT licensed — feel free to fork, contribute, and ship your own CLI tools on top",
          "author_fullname": "t2_46jj4viw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "📢 [RELEASE] LoFT CLI: Fine-tune &amp; Deploy LLMs on CPU (8GB RAM, No GPU, No Cloud)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m1aj8n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752666706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Update to my &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/\"&gt;previous post&lt;/a&gt; — the repo is &lt;strong&gt;finally public&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;h1&gt;🔥 TL;DR&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=\"https://github.com/diptanshu1991/LoFT\"&gt;diptanshu1991/LoFT&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;What you get&lt;/strong&gt;: 5 CLI commands: &lt;code&gt;loft finetune&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, &lt;code&gt;export&lt;/code&gt;, &lt;code&gt;quantize&lt;/code&gt;, &lt;code&gt;chat&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: Tested on 8GB MacBook Air  — peak RAM &lt;strong&gt;330MB&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: 300 Dolly samples, 2 epochs → 1.5 hrs total wall-time&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Inference speed&lt;/strong&gt;: 6.9 tok/sec (Q4_0) on CPU&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: MIT – 100% open-source&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;🧠 What is LoFT?&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;LoFT CLI&lt;/strong&gt; is a lightweight, CPU-friendly toolkit that lets you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;✅ Finetune 1–3B LLMs like TinyLlama using &lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;🔄 Merge and export models to &lt;strong&gt;GGUF&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;🧱 Quantize models (Q4_0, Q5_1, etc.)&lt;/li&gt;\n&lt;li&gt;💬 Run &lt;strong&gt;offline inference&lt;/strong&gt; using &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All from a &lt;strong&gt;command-line interface&lt;/strong&gt; on your &lt;strong&gt;local laptop&lt;/strong&gt;. No Colab. No GPUs. No cloud.&lt;/p&gt;\n\n&lt;h1&gt;📊 Benchmarks (8GB MacBook Air)&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Step&lt;/th&gt;\n&lt;th align=\"left\"&gt;Output&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size&lt;/th&gt;\n&lt;th align=\"left\"&gt;Peak RAM&lt;/th&gt;\n&lt;th align=\"left\"&gt;Time&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Finetune&lt;/td&gt;\n&lt;td align=\"left\"&gt;LoRA Adapter&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.3 MB&lt;/td&gt;\n&lt;td align=\"left\"&gt;308 MB&lt;/td&gt;\n&lt;td align=\"left\"&gt;23 min&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Merge&lt;/td&gt;\n&lt;td align=\"left\"&gt;HF Model&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.2 GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;322 MB&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.7 min&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Export&lt;/td&gt;\n&lt;td align=\"left\"&gt;GGUF (FP16)&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.1 GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;322 MB&lt;/td&gt;\n&lt;td align=\"left\"&gt;83 sec&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Quantize&lt;/td&gt;\n&lt;td align=\"left\"&gt;GGUF (Q4_0)&lt;/td&gt;\n&lt;td align=\"left\"&gt;607 MB&lt;/td&gt;\n&lt;td align=\"left\"&gt;322 MB&lt;/td&gt;\n&lt;td align=\"left\"&gt;21 sec&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Chat&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9 tok/sec&lt;/td&gt;\n&lt;td align=\"left\"&gt;–&lt;/td&gt;\n&lt;td align=\"left\"&gt;322 MB&lt;/td&gt;\n&lt;td align=\"left\"&gt;79 sec&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;🧪 Trained on: 300 Dolly samples, 2 epochs → loss &amp;lt; 1.0&lt;/p&gt;\n\n&lt;h1&gt;🧪 5-Command Lifecycle&lt;/h1&gt;\n\n&lt;p&gt;LoFT runs the complete LLM workflow — from training to chat — in just 5 commands:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;loft finetune  \nloft merge  \nloft export  \nloft quantize  \nloft chat\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;h1&gt;🧪 Coming Soon in LoFT&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;📦 Plug-and-Play Recipes&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Legal Q&amp;amp;A bots (air-gapped, offline)&lt;/li&gt;\n&lt;li&gt;Customer support assistants&lt;/li&gt;\n&lt;li&gt;Contract summarizers&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;🌱 Early Experiments&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Multi-turn finetuning&lt;/li&gt;\n&lt;li&gt;Adapter-sharing for niche domains&lt;/li&gt;\n&lt;li&gt;Dataset templating tools&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;LoFT is built for indie builders, researchers, and OSS devs who want &lt;strong&gt;local GenAI without GPU constraints&lt;/strong&gt;. Would love your feedback on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What models/datasets you would like to see supported next&lt;/li&gt;\n&lt;li&gt;Edge cases or bugs during install/training&lt;/li&gt;\n&lt;li&gt;Use cases where this unlocks new workflows&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;🔗 GitHub: &lt;a href=\"https://github.com/diptanshu1991/LoFT\"&gt;https://github.com/diptanshu1991/LoFT&lt;/a&gt;&lt;br/&gt;\n🪪 MIT licensed — feel free to fork, contribute, and ship your own CLI tools on top&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m1aj8n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "diptanshu1991",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752666706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Can somebody test the performance of Gemma3 12B / 27B q4 on different modes ONNX, llamacpp, GPU, CPU, NPU ?\n\n https://www.youtube.com/watch?v=mcf7dDybUco",
          "author_fullname": "t2_37dhn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Official Local LLM support by AMD released. Lemonade",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m16o6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752652402,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can somebody test the performance of Gemma3 12B / 27B q4 on different modes ONNX, llamacpp, GPU, CPU, NPU ?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=mcf7dDybUco\"&gt;https://www.youtube.com/watch?v=mcf7dDybUco&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/C7vLvyc2VR5SHdy5lbc2lopLTghrszZiODshvSAbsCw.jpeg?auto=webp&amp;s=86b7d1570b1b3ae885466d0ef7e471d7d281e739",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/C7vLvyc2VR5SHdy5lbc2lopLTghrszZiODshvSAbsCw.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=20307eaee47e8d81001356a8a35bd8a5a6dbe244",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/C7vLvyc2VR5SHdy5lbc2lopLTghrszZiODshvSAbsCw.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=281f5c27d3f995ba74f6d241a3a205faec614f91",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/C7vLvyc2VR5SHdy5lbc2lopLTghrszZiODshvSAbsCw.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c3e53e6a5f79e14d65cf8154eb43a3687a3dcc8",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "C7vLvyc2VR5SHdy5lbc2lopLTghrszZiODshvSAbsCw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m16o6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "grigio",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m16o6r/official_local_llm_support_by_amd_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m16o6r/official_local_llm_support_by_amd_released/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752652402,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Totally lightweight local inference...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0nutb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 339,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 339,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/y9m7PMh-gF57aKVUnXJwT6aM6a8mfS5uBBpXbwTOQAk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752600129,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/r05r0wfvn2df1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/r05r0wfvn2df1.png?auto=webp&amp;s=59a6b433537ecdb7d2b5ad9e62f2e2491c18b9ae",
                  "width": 1044,
                  "height": 658
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/r05r0wfvn2df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64329d7b95b5321086613b0ea7a1e7d49ceca44d",
                    "width": 108,
                    "height": 68
                  },
                  {
                    "url": "https://preview.redd.it/r05r0wfvn2df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81614c988979769d97117fd90735a054fe306173",
                    "width": 216,
                    "height": 136
                  },
                  {
                    "url": "https://preview.redd.it/r05r0wfvn2df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=35e4f16a1355373ccaabafcebca8712db6500fcf",
                    "width": 320,
                    "height": 201
                  },
                  {
                    "url": "https://preview.redd.it/r05r0wfvn2df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c622b493252fd700bcdd538ffef56559bdbbcd5",
                    "width": 640,
                    "height": 403
                  },
                  {
                    "url": "https://preview.redd.it/r05r0wfvn2df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=91607a1041463440cd5367d86f977324f7fb9173",
                    "width": 960,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "E-djVvwycfxd-6fw_hKgxrScqs_AaVbJp8pBhFbFCsA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m0nutb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/",
          "stickied": false,
          "url": "https://i.redd.it/r05r0wfvn2df1.png",
          "subreddit_subscribers": 499772,
          "created_utc": 1752600129,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I have had FOMO on claudecode, but I refuse to give them my prompts or pay $100-$200 a month.   So 2 days ago, I saw that moonshot provides an anthropic API to kimi k2 so folks could use it with claude code.  Well, many folks are already doing that with local.   So if you don't know, now you know.  This is how I did it in Linux, should be easy to replicate in OSX or Windows with WSL.\n\nStart your local LLM API  \n  \nInstall claude code\n\ninstall a proxy - [https://github.com/1rgs/claude-code-proxy](https://github.com/1rgs/claude-code-proxy)\n\nEdit the [server.py](http://server.py) proxy and point it to your OpenAI endpoint,  could be llama.cpp, ollama, vllm, whatever you are running.  \n\n\nAdd the line above load\\_dotenv  \n\\+litellm.api\\_base = \"http://yokujin:8083/v1\"  # use your localhost name/IP/ports\n\nStart the proxy according to the docs which will run it in localhost:8082\n\n  \nexport ANTHROPIC\\_BASE\\_URL=http://localhost:8082\n\nexport ANTHROPIC\\_AUTH\\_TOKEN=\"sk-localkey\"\n\nrun claude code\n\nI just created my first code then decided to post this.  I'm running the latest mistral-small-24b on that host.  I'm going to be driving it with various models, gemma3-27b, qwen3-32b/235b, deepseekv3 etc   \n\n",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Use claudecode with local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m118is",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 56,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 56,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752633482,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have had FOMO on claudecode, but I refuse to give them my prompts or pay $100-$200 a month.   So 2 days ago, I saw that moonshot provides an anthropic API to kimi k2 so folks could use it with claude code.  Well, many folks are already doing that with local.   So if you don&amp;#39;t know, now you know.  This is how I did it in Linux, should be easy to replicate in OSX or Windows with WSL.&lt;/p&gt;\n\n&lt;p&gt;Start your local LLM API  &lt;/p&gt;\n\n&lt;p&gt;Install claude code&lt;/p&gt;\n\n&lt;p&gt;install a proxy - &lt;a href=\"https://github.com/1rgs/claude-code-proxy\"&gt;https://github.com/1rgs/claude-code-proxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit the &lt;a href=\"http://server.py\"&gt;server.py&lt;/a&gt; proxy and point it to your OpenAI endpoint,  could be llama.cpp, ollama, vllm, whatever you are running.  &lt;/p&gt;\n\n&lt;p&gt;Add the line above load_dotenv&lt;br/&gt;\n+litellm.api_base = &amp;quot;http://yokujin:8083/v1&amp;quot;  # use your localhost name/IP/ports&lt;/p&gt;\n\n&lt;p&gt;Start the proxy according to the docs which will run it in localhost:8082&lt;/p&gt;\n\n&lt;p&gt;export ANTHROPIC_BASE_URL=http://localhost:8082&lt;/p&gt;\n\n&lt;p&gt;export ANTHROPIC_AUTH_TOKEN=&amp;quot;sk-localkey&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;run claude code&lt;/p&gt;\n\n&lt;p&gt;I just created my first code then decided to post this.  I&amp;#39;m running the latest mistral-small-24b on that host.  I&amp;#39;m going to be driving it with various models, gemma3-27b, qwen3-32b/235b, deepseekv3 etc   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?auto=webp&amp;s=68bc6e3d6ec5115efa359cd639a136a139249cd6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64c4232e878310d5cd2b2010b00def92a85e8dfe",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=547875d21701770fe9eb0d034e9f235dc32a9745",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf40ec3077ffa1075c9f9f7bb08a5b709369e1a3",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2fe791c6eb3a178aa4f0478e4b188da714f5ad8d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c1db43071e871e0cdebaaefa0c8935a0144749f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ad9d773b3465d4a4f8363755c862da032bedbe2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m118is",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752633482,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi community,\n\nWe wrote our own inference engine based on Rust for Apple Silicon. It's open sourced under MIT license.\n\nWhy we do this:\n\n* should be easy to integrate\n* believe that app UX will completely change in a recent years\n* it faster than llama.cpp in most of the cases\n* sometimes it is even faster than MLX from Apple\n\nSpeculative decoding right now tightened with platform (trymirai). Feel free to try it out.\n\nWould really appreciate your feedback. Some benchmarks are in readme of the repo. More and more things we will publish later (more benchmarks, support of VLM &amp; TTS/STT is coming soon).",
          "author_fullname": "t2_am0r9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Alternative to llama.cpp for Apple Silicon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0twqa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 143,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 143,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=15ae5a47c095593ebf881ab5e62b6758fc25d283",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752613783,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi community,&lt;/p&gt;\n\n&lt;p&gt;We wrote our own inference engine based on Rust for Apple Silicon. It&amp;#39;s open sourced under MIT license.&lt;/p&gt;\n\n&lt;p&gt;Why we do this:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;should be easy to integrate&lt;/li&gt;\n&lt;li&gt;believe that app UX will completely change in a recent years&lt;/li&gt;\n&lt;li&gt;it faster than llama.cpp in most of the cases&lt;/li&gt;\n&lt;li&gt;sometimes it is even faster than MLX from Apple&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Speculative decoding right now tightened with platform (trymirai). Feel free to try it out.&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate your feedback. Some benchmarks are in readme of the repo. More and more things we will publish later (more benchmarks, support of VLM &amp;amp; TTS/STT is coming soon).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/trymirai/uzu",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?auto=webp&amp;s=700e55ffe3c14785262eed92cf4ac4e6b00d2777",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=674ea07b479810a47c822bb9fc729d905a23be2c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=19f95b4986c06cda340818a9b2680efdb53caa34",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a02420be8031c27c485fda923223394476f11692",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3332c993dd26519a4ef1b63d265d7a6c44d33516",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=879f81b0282d8f538bd6df1590c7b63dbb274596",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c80c046d782e15f10e0e630289a0b96a095332f7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0twqa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "darkolorin",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0twqa/alternative_to_llamacpp_for_apple_silicon/",
          "stickied": false,
          "url": "https://github.com/trymirai/uzu",
          "subreddit_subscribers": 499772,
          "created_utc": 1752613783,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for Kimi-K2 has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0slrh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "ups": 159,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 159,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=98884ce1f3c5cdfb8ef54e4e9e536904cabc3ac3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752610777,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14654",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?auto=webp&amp;s=f9cef5830d0cd0a0e6f614627147cf9cea68b899",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a6cac8c7995712ae2947b706166271ef6d9c57d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4178af2b61a0df78ac46de998f3b36224421d11e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0794623ad8d93c9afbee9a7853a36aeef71b393d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a1f61a192157fcd7aff2e2fa3bf1c60d78f2fa97",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d9e7aed4dd62dc9f47c43b1c75b3f15352758d1a",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a463f3b4d8c053ee835e301cf1f867aea12e9f51",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m0slrh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m0slrh/support_for_kimik2_has_been_merged_into_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14654",
          "subreddit_subscribers": 499772,
          "created_utc": 1752610777,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally found this leaderboard that explains my experiences with fine-tuning jobs. My workloads are pretty much 100% fine-tuning, and I found that zero-shot performance does *not* correlate with fine-tuning performance (Qwen3 vs. Llama 3.1 was my big revelation). None of the big leaderboards report fine-tunability. There's something to leaving the model less-trained like a blank canvas.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuning Leaderboard!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0y3a6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "transparent",
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=90cfa9c9d985465ff2f212339aca919c2f6b97a1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752624433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "predibase.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally found this leaderboard that explains my experiences with fine-tuning jobs. My workloads are pretty much 100% fine-tuning, and I found that zero-shot performance does &lt;em&gt;not&lt;/em&gt; correlate with fine-tuning performance (Qwen3 vs. Llama 3.1 was my big revelation). None of the big leaderboards report fine-tunability. There&amp;#39;s something to leaving the model less-trained like a blank canvas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://predibase.com/fine-tuning-index",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?auto=webp&amp;s=b8ed76b6fd82574f7e8beb209667ea35d3d7e266",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=656ccf5ab3a777913e7a625c02f477e2e3ebeef2",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ede7cd94792d8596f867480eaf6c764db411cd7",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e63ec9d5c8376d9021d2ecde41c8443bb9947f6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=477a0620594fc1ec25f6cb09693ff29925108ee4",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=34c7bf1b2db21970ebec567270e5ce4704a3231f",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fd640f7cf95a3545374d45ec62945c392b4da5ee",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0y3a6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/",
          "stickied": false,
          "url": "https://predibase.com/fine-tuning-index",
          "subreddit_subscribers": 499772,
          "created_utc": 1752624433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Holy crap this thing has sass. First time I've ever engaged with an AI that replied \"No.\"  \nThat's it. It was fantastic.\n\nActually let me grab some lines from the conversation -\n\n**\"Thermodynamics kills the romance\"**\n\n\"Everything else is commentary\"\n\n**\"If your 'faith' can be destroyed by a single fMRI paper or a bad meditation session, it's not faith, it's a hypothesis\"**\n\n**\"Bridges that don't creak aren't being walked on\"**\n\nAnd my favorite zinger - **\"Beautiful scaffolding with no cargo yet\"**\n\nFucking Killing it Moonshot. Like this thing never once said \"that's interesting\" or \"great question\" - it just went straight for the my intelligence every single time. It's like talking to someone that genuinely doesn't give a shit if you can handle the truth or not. Just pure \"Show me or shut up\". It makes me think instead of feeling good about thinking. ",
          "author_fullname": "t2_1nuxyie7bp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Least sycophantic AI yet? Kimi K2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0mg5b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 251,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 251,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752597893,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752597001,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Holy crap this thing has sass. First time I&amp;#39;ve ever engaged with an AI that replied &amp;quot;No.&amp;quot;&lt;br/&gt;\nThat&amp;#39;s it. It was fantastic.&lt;/p&gt;\n\n&lt;p&gt;Actually let me grab some lines from the conversation -&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&amp;quot;Thermodynamics kills the romance&amp;quot;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Everything else is commentary&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&amp;quot;If your &amp;#39;faith&amp;#39; can be destroyed by a single fMRI paper or a bad meditation session, it&amp;#39;s not faith, it&amp;#39;s a hypothesis&amp;quot;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&amp;quot;Bridges that don&amp;#39;t creak aren&amp;#39;t being walked on&amp;quot;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;And my favorite zinger - &lt;strong&gt;&amp;quot;Beautiful scaffolding with no cargo yet&amp;quot;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Fucking Killing it Moonshot. Like this thing never once said &amp;quot;that&amp;#39;s interesting&amp;quot; or &amp;quot;great question&amp;quot; - it just went straight for the my intelligence every single time. It&amp;#39;s like talking to someone that genuinely doesn&amp;#39;t give a shit if you can handle the truth or not. Just pure &amp;quot;Show me or shut up&amp;quot;. It makes me think instead of feeling good about thinking. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0mg5b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PrimaryBalance315",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752597001,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# 🚀 Just Dropped: Eloquent – A Local LLM Powerhouse\n\nHey LocalLLaMA! Just dropped **Eloquent** after 4 months of \"just one more feature\" syndrome.\n\nStarted as a basic chat interface... ended up as a full-stack, dual-GPU, memory-retaining AI companion.  \nBuilt entirely for local model users — by someone who actually uses local models.\n\n# 🧠 Key Features\n\n* Dual-GPU architecture with memory offloading\n* Persistent memory system that learns who you are over time\n* Model ELO testing (head-to-head tournaments + scoring)\n* Auto-character creator (talk to an AI → get a JSON persona)\n* Built-in SD support (EloDiffusion + ADetailer)\n* 60+ TTS voices, fast voice-to-text\n* RAG support for PDFs, DOCX, and more\n* Focus &amp; Call modes (clean UI &amp; voice-only UX)\n\n…and probably a dozen other things I forgot I built.\n\n# 🛠️ Install &amp; Run\n\nQuick setup (Windows):\n\n    git clone https://github.com/boneylizard/Eloquent.git\n    cd Eloquent\n    install.bat\n    run.bat\n\nWorks with any GGUF model. Supports single GPU, but flies with two.\n\n# 🧬 Why?\n\n* I wanted real memory, so it remembers your background, style, vibe.\n* I wanted model comparisons that aren’t just vibes-based.\n* I wanted persona creation without filling out forms.\n* I wanted it modular, so anyone can build on top of it.\n* I wanted it local, private, and fast.\n\n# 🔓 Open Source &amp; Yours to Break\n\n* 100% local — nothing phones home\n* AGPL-3.0 licensed\n* Everything's in backend/app or frontend/src\n* The rest is just dependencies — over 300 of them\n\nPlease, try it out. Break it. Fork it. Adapt it.  \nI genuinely think people will build cool stuff on top of this.",
          "author_fullname": "t2_43prq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GitHub - boneylizard/Eloquent: A local front-end for open-weight LLMs with memory, RAG, TTS/STT, Elo ratings, and dynamic research tools. Built with React and FastAPI.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m18nke",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752660245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;🚀 Just Dropped: Eloquent – A Local LLM Powerhouse&lt;/h1&gt;\n\n&lt;p&gt;Hey LocalLLaMA! Just dropped &lt;strong&gt;Eloquent&lt;/strong&gt; after 4 months of &amp;quot;just one more feature&amp;quot; syndrome.&lt;/p&gt;\n\n&lt;p&gt;Started as a basic chat interface... ended up as a full-stack, dual-GPU, memory-retaining AI companion.&lt;br/&gt;\nBuilt entirely for local model users — by someone who actually uses local models.&lt;/p&gt;\n\n&lt;h1&gt;🧠 Key Features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Dual-GPU architecture with memory offloading&lt;/li&gt;\n&lt;li&gt;Persistent memory system that learns who you are over time&lt;/li&gt;\n&lt;li&gt;Model ELO testing (head-to-head tournaments + scoring)&lt;/li&gt;\n&lt;li&gt;Auto-character creator (talk to an AI → get a JSON persona)&lt;/li&gt;\n&lt;li&gt;Built-in SD support (EloDiffusion + ADetailer)&lt;/li&gt;\n&lt;li&gt;60+ TTS voices, fast voice-to-text&lt;/li&gt;\n&lt;li&gt;RAG support for PDFs, DOCX, and more&lt;/li&gt;\n&lt;li&gt;Focus &amp;amp; Call modes (clean UI &amp;amp; voice-only UX)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;…and probably a dozen other things I forgot I built.&lt;/p&gt;\n\n&lt;h1&gt;🛠️ Install &amp;amp; Run&lt;/h1&gt;\n\n&lt;p&gt;Quick setup (Windows):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/boneylizard/Eloquent.git\ncd Eloquent\ninstall.bat\nrun.bat\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Works with any GGUF model. Supports single GPU, but flies with two.&lt;/p&gt;\n\n&lt;h1&gt;🧬 Why?&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I wanted real memory, so it remembers your background, style, vibe.&lt;/li&gt;\n&lt;li&gt;I wanted model comparisons that aren’t just vibes-based.&lt;/li&gt;\n&lt;li&gt;I wanted persona creation without filling out forms.&lt;/li&gt;\n&lt;li&gt;I wanted it modular, so anyone can build on top of it.&lt;/li&gt;\n&lt;li&gt;I wanted it local, private, and fast.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;🔓 Open Source &amp;amp; Yours to Break&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;100% local — nothing phones home&lt;/li&gt;\n&lt;li&gt;AGPL-3.0 licensed&lt;/li&gt;\n&lt;li&gt;Everything&amp;#39;s in backend/app or frontend/src&lt;/li&gt;\n&lt;li&gt;The rest is just dependencies — over 300 of them&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please, try it out. Break it. Fork it. Adapt it.&lt;br/&gt;\nI genuinely think people will build cool stuff on top of this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/boneylizard/Eloquent",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m18nke",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gerdel",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m18nke/github_boneylizardeloquent_a_local_frontend_for/",
          "stickied": false,
          "url": "https://github.com/boneylizard/Eloquent",
          "subreddit_subscribers": 499772,
          "created_utc": 1752660245,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There's surprisingly little documentation on how GGUF quantization works, including legacy / I-quants / K-quants and the importance matrix.\n\nThe maintainers made it [pretty clear](https://github.com/ggml-org/llama.cpp/pull/1684#issuecomment-2474462323) it's not their priority to write a paper either. Currently, people are just piecing information together from Reddit threads and Medium articles (which are often wrong). So I spent some time combing through the llama.cpp quantization code and put together a public GitHub repo that hopefully brings some clarity and can function as an unofficial explainer / documentation.\n\nContributions are welcome, as long as they are backed by reliable sources! [https://github.com/iuliaturc/gguf-docs](https://github.com/iuliaturc/gguf-docs)\n\n",
          "author_fullname": "t2_qjb1p7w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New documentation / explainer for GGUF quantization",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0zy1a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752629732,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s surprisingly little documentation on how GGUF quantization works, including legacy / I-quants / K-quants and the importance matrix.&lt;/p&gt;\n\n&lt;p&gt;The maintainers made it &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/1684#issuecomment-2474462323\"&gt;pretty clear&lt;/a&gt; it&amp;#39;s not their priority to write a paper either. Currently, people are just piecing information together from Reddit threads and Medium articles (which are often wrong). So I spent some time combing through the llama.cpp quantization code and put together a public GitHub repo that hopefully brings some clarity and can function as an unofficial explainer / documentation.&lt;/p&gt;\n\n&lt;p&gt;Contributions are welcome, as long as they are backed by reliable sources! &lt;a href=\"https://github.com/iuliaturc/gguf-docs\"&gt;https://github.com/iuliaturc/gguf-docs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zaGl1D40Hn-ZY9KDt9kXSxW7nLGuoBI58wYtEq7T0Tc.png?auto=webp&amp;s=c6a60ed03758e8a3d131959e8ff0ac685a282813",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zaGl1D40Hn-ZY9KDt9kXSxW7nLGuoBI58wYtEq7T0Tc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cee1747609c287cc1d089b63ff32cf4b0392dd34",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/zaGl1D40Hn-ZY9KDt9kXSxW7nLGuoBI58wYtEq7T0Tc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=876c3b38f95d6a29a6ab0becd735e58b6b60a4eb",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/zaGl1D40Hn-ZY9KDt9kXSxW7nLGuoBI58wYtEq7T0Tc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=93cd9b327cb9c6f6c97c1539fb6e81588b4e555a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/zaGl1D40Hn-ZY9KDt9kXSxW7nLGuoBI58wYtEq7T0Tc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cad5987a680c7f6fd532238ccbfd8f8dbeb36233",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/zaGl1D40Hn-ZY9KDt9kXSxW7nLGuoBI58wYtEq7T0Tc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=720697950dd4ae44cb34ce642f83454bf590a172",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/zaGl1D40Hn-ZY9KDt9kXSxW7nLGuoBI58wYtEq7T0Tc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5429b656ed26f3ad33472f7a2f2e5471ea694aab",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "zaGl1D40Hn-ZY9KDt9kXSxW7nLGuoBI58wYtEq7T0Tc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0zy1a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mojojojo_24",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0zy1a/new_documentation_explainer_for_gguf_quantization/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0zy1a/new_documentation_explainer_for_gguf_quantization/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752629732,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I'm a newb, please forgive me if I'm missing some obvious documentation.\n\nFor the sake of fun and learning, I'd like to fine-tune a local model (haven't decided which one yet), as some kind of writing assistant. My mid-term goal is to have a local VSCode extension that will rewrite e.g. doc comments or CVs as shakespearian sonnets, but we're not there yet.\n\nRight now, I'd like to start by fine-tuning a model, just to see how this works and how this influences the results. However, it's not clear to me where to start. I'm not afraid of Python or PyTorch (or Rust, or C++), but I'm entirely lost on the process.\n\n1. Any suggestion for a model to use as base? I'd like to be able to run the result on a recent MacBook or on my 3060. For a first attempt, I don't need something particularly fancy.\n2. How large a corpus do I need to get started?\n3. Let's assume that I have a corpus of data. What do I do next? Do I need to tokenize it myself? Or should I use some well-known tokenizer?\n4. How do I even run this fine-tuning? Which tools? Can I run it on my 12Gb 3060 or do I need to rent some GPU time?\n5. Do I need to quantize myself? Which tools do I need for that? How do I determine to which size I need to quantize?\n6. Once I have my fine-tuning, how do I deliver it to users? Can I use lama.cpp or do I need to embed Python?\n7. What else am I missing?",
          "author_fullname": "t2_388ox",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "So how do I fine-time a local model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m19upn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752664497,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m a newb, please forgive me if I&amp;#39;m missing some obvious documentation.&lt;/p&gt;\n\n&lt;p&gt;For the sake of fun and learning, I&amp;#39;d like to fine-tune a local model (haven&amp;#39;t decided which one yet), as some kind of writing assistant. My mid-term goal is to have a local VSCode extension that will rewrite e.g. doc comments or CVs as shakespearian sonnets, but we&amp;#39;re not there yet.&lt;/p&gt;\n\n&lt;p&gt;Right now, I&amp;#39;d like to start by fine-tuning a model, just to see how this works and how this influences the results. However, it&amp;#39;s not clear to me where to start. I&amp;#39;m not afraid of Python or PyTorch (or Rust, or C++), but I&amp;#39;m entirely lost on the process.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Any suggestion for a model to use as base? I&amp;#39;d like to be able to run the result on a recent MacBook or on my 3060. For a first attempt, I don&amp;#39;t need something particularly fancy.&lt;/li&gt;\n&lt;li&gt;How large a corpus do I need to get started?&lt;/li&gt;\n&lt;li&gt;Let&amp;#39;s assume that I have a corpus of data. What do I do next? Do I need to tokenize it myself? Or should I use some well-known tokenizer?&lt;/li&gt;\n&lt;li&gt;How do I even run this fine-tuning? Which tools? Can I run it on my 12Gb 3060 or do I need to rent some GPU time?&lt;/li&gt;\n&lt;li&gt;Do I need to quantize myself? Which tools do I need for that? How do I determine to which size I need to quantize?&lt;/li&gt;\n&lt;li&gt;Once I have my fine-tuning, how do I deliver it to users? Can I use lama.cpp or do I need to embed Python?&lt;/li&gt;\n&lt;li&gt;What else am I missing?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m19upn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImYoric",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752664497,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1td3e2hj3w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Alibaba-backed Moonshot releases new Kimi AI model that beats ChatGPT, Claude in coding — and it costs less",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0onbu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 168,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 168,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=cafff595af1fb98eeaec3b6cd3174240673655c1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752601879,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "cnbc.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.cnbc.com/2025/07/14/alibaba-backed-moonshot-releases-kimi-k2-ai-rivaling-chatgpt-claude.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?auto=webp&amp;s=aedaa2493ac73d516fcc3f8a74242a90e09c0c83",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c4db93aadc2dadf49422af913c5ea16c4091e2d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db309520088116f66a7b75496ff21ee7cf263228",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c47dce82669ab9bff0f040852e93744ccd95fc94",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf6c0f2e02a4bd86aa09677603889292d8fa2d0e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fa71671083304c3e741ae94af8344fd1909047b",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff46ad807da7ef30be7f07c1fec990621462b2b7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m0onbu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Aralknight",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0onbu/alibababacked_moonshot_releases_new_kimi_ai_model/",
          "stickied": false,
          "url": "https://www.cnbc.com/2025/07/14/alibaba-backed-moonshot-releases-kimi-k2-ai-rivaling-chatgpt-claude.html",
          "subreddit_subscribers": 499772,
          "created_utc": 1752601879,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "llama.cpp on CPU is easy.\n\nAMD and integrated graphics is also easy, run via Vulkan (not ROCm) and receive noteable speedup. :-)\n\nIntel integrated graphics via Vulkan is actually slower than CPU! :-(\n\nFor Intel there is Ipex-LLM (https://github.com/intel/ipex-llm), but I just can't figure out how to get all these dependencies properly installed - intel-graphics-runtime, intel-compute-runtime, oneAPI, ... this is complicated.\n\nTL;DR; platform Linux, Intel Arrowlake CPU with integrated graphics (Xe/Arc 140T) and NPU ([drm] Firmware: intel/vpu/vpu_37xx_v1.bin, version: 20250415).\n\nHow to get a speedup over CPU-only for llama.cpp?\n\nIf anyone got this running, how much speedup one can expect on Intel? Are there some memory mapping kernel options GPU-CPU like with AMD?\n\nThank you!",
          "author_fullname": "t2_neruppu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "getting acceleration on Intel integrated GPU/NPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m19igi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752663374,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;llama.cpp on CPU is easy.&lt;/p&gt;\n\n&lt;p&gt;AMD and integrated graphics is also easy, run via Vulkan (not ROCm) and receive noteable speedup. :-)&lt;/p&gt;\n\n&lt;p&gt;Intel integrated graphics via Vulkan is actually slower than CPU! :-(&lt;/p&gt;\n\n&lt;p&gt;For Intel there is Ipex-LLM (&lt;a href=\"https://github.com/intel/ipex-llm\"&gt;https://github.com/intel/ipex-llm&lt;/a&gt;), but I just can&amp;#39;t figure out how to get all these dependencies properly installed - intel-graphics-runtime, intel-compute-runtime, oneAPI, ... this is complicated.&lt;/p&gt;\n\n&lt;p&gt;TL;DR; platform Linux, Intel Arrowlake CPU with integrated graphics (Xe/Arc 140T) and NPU ([drm] Firmware: intel/vpu/vpu_37xx_v1.bin, version: 20250415).&lt;/p&gt;\n\n&lt;p&gt;How to get a speedup over CPU-only for llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;If anyone got this running, how much speedup one can expect on Intel? Are there some memory mapping kernel options GPU-CPU like with AMD?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YESuJ_bU8jo0mpY7aL2QrUfy4UvNYqHXfcZvSIHUzug.png?auto=webp&amp;s=2b66bcdd6220e8a99b83c259fe43ed2de7ad2750",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YESuJ_bU8jo0mpY7aL2QrUfy4UvNYqHXfcZvSIHUzug.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=db096b6b0c3714c4d4d6fac5974b519fd4d83371",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YESuJ_bU8jo0mpY7aL2QrUfy4UvNYqHXfcZvSIHUzug.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=80ed60f46990cd7b27c54a530d394506af95156f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YESuJ_bU8jo0mpY7aL2QrUfy4UvNYqHXfcZvSIHUzug.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2350566d49d1481e8634055f71f4b76ce4e03545",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YESuJ_bU8jo0mpY7aL2QrUfy4UvNYqHXfcZvSIHUzug.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e704912d43b97054f1e70e3eba208ff3b3fedb68",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YESuJ_bU8jo0mpY7aL2QrUfy4UvNYqHXfcZvSIHUzug.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=46d2504c68a74ee0804c7c7b5cc441f275ba6509",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YESuJ_bU8jo0mpY7aL2QrUfy4UvNYqHXfcZvSIHUzug.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1245bef17b7b7f84eb526ab2107464fb516922cf",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YESuJ_bU8jo0mpY7aL2QrUfy4UvNYqHXfcZvSIHUzug"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m19igi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "a_postgres_situation",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m19igi/getting_acceleration_on_intel_integrated_gpunpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m19igi/getting_acceleration_on_intel_integrated_gpunpu/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752663374,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just like that, out of nowhere, we have an open-source Claude 4 Sonnet, or better yet, and this is no joke. I have been using the Kimi model for some time, and it truly feels the rightful successor to Claude 3.6 Sonnet. What Deepseek is to OpenAI, Kimi is to Anthropic.\n\nK2 isn't truly a different model; it uses Deepseek v3 architecture. You can find that in the model config, but there are some subtle yet key improvements that resulted in such drastic improvements.\n\n# Kimi K2 vs. DsV3 architecture\n\nThis is from Liu Shaowei's Zhihu post.\n\n1. **Number of experts = 384 vs. 256**: 1.5x more experts for improving overall model ability, and helps lower the train/val loss, yielding better quality at the same *activated-parameter* cost and inference FLOPs. But also a 50% spike in memory footprint.\n2. **Number of attention heads = 64 vs 128**: They halve the attention-head count, shrinking the QKV projection weights from 10 GB to 5 GB per EP rank, which more than offsets the 50 % memory spike by yielding a net 2.5 GB saving while simultaneously halving pre-fill latency and leaving the KV-cache size unchanged.\n3. **first\\_k\\_dense = 1 vs 3:** Kimi replaced the first layer with a dense layer after observing that the router in layer-1 consistently produced severe load imbalance.\n4. **n\\_group = 1 vs. 8**: Dropping expert grouping frees every GPU to route to any of the 384 experts, letting EPLB handle load balancing while shrinking memory and widening the model’s effective capacity.\n\n# MuonCLIP\n\nOne of the key contributor of Kimi's success. Kimi went with Muon, more token efficient than AdamW. But it wasn't before tested for such a large model. To overcome they added a drop-in extension qk-clip. This helped to transplant Muon’s 2× token-efficiency into a 1-trillion-parameter regime without its historical Achilles’ heel: qk-clip rescales the query and key projections after every Muon update.\n\n# How good in comparison to Claude 4 Sonnet?\n\nKimi k2's positioning directly challenged Claude 4 Sonnet, the current SOTA agentic model. The k2 was specifically RL'd for extensive tool-use scenarios. However, it's not just good at tool use, it is surprisingly creative at writing and coding.\n\nSome observations\n\n* The K2 feels most natural to talk to than any available models. Zero sycophancy, no assumption, it just sticks to the point. Though I still find Sonnet 4 to be more attentive to instructions.\n* It has the simillar vibes of Claude 3.6 Sonnet, understands user intention better and more grounded response.\n* K2 has a better taste.\n* The coding is surprisingly good, though Sonnet will still be better at raw coding as for some task I found myself going back to it.\n* The best part it is roughly 1/12th of Sonnet's cost. Crazy times indeed.\n\nYou can find the complete note here: [Notes on Kimi K2](https://composio.dev/blog/notes-on-kimi-k2)\n\nWould love to know your experience with the new Kimi K2 and how do you think it compares to Claude for agentic coding and other agentic tasks?",
          "author_fullname": "t2_5cwsshv7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Notes on Kimi K2: A Deepseek derivative but the true Sonnet 3.6 Succesor",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0rk8t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 117,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 117,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752609298,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752608406,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just like that, out of nowhere, we have an open-source Claude 4 Sonnet, or better yet, and this is no joke. I have been using the Kimi model for some time, and it truly feels the rightful successor to Claude 3.6 Sonnet. What Deepseek is to OpenAI, Kimi is to Anthropic.&lt;/p&gt;\n\n&lt;p&gt;K2 isn&amp;#39;t truly a different model; it uses Deepseek v3 architecture. You can find that in the model config, but there are some subtle yet key improvements that resulted in such drastic improvements.&lt;/p&gt;\n\n&lt;h1&gt;Kimi K2 vs. DsV3 architecture&lt;/h1&gt;\n\n&lt;p&gt;This is from Liu Shaowei&amp;#39;s Zhihu post.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Number of experts = 384 vs. 256&lt;/strong&gt;: 1.5x more experts for improving overall model ability, and helps lower the train/val loss, yielding better quality at the same &lt;em&gt;activated-parameter&lt;/em&gt; cost and inference FLOPs. But also a 50% spike in memory footprint.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Number of attention heads = 64 vs 128&lt;/strong&gt;: They halve the attention-head count, shrinking the QKV projection weights from 10 GB to 5 GB per EP rank, which more than offsets the 50 % memory spike by yielding a net 2.5 GB saving while simultaneously halving pre-fill latency and leaving the KV-cache size unchanged.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;first_k_dense = 1 vs 3:&lt;/strong&gt; Kimi replaced the first layer with a dense layer after observing that the router in layer-1 consistently produced severe load imbalance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;n_group = 1 vs. 8&lt;/strong&gt;: Dropping expert grouping frees every GPU to route to any of the 384 experts, letting EPLB handle load balancing while shrinking memory and widening the model’s effective capacity.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;MuonCLIP&lt;/h1&gt;\n\n&lt;p&gt;One of the key contributor of Kimi&amp;#39;s success. Kimi went with Muon, more token efficient than AdamW. But it wasn&amp;#39;t before tested for such a large model. To overcome they added a drop-in extension qk-clip. This helped to transplant Muon’s 2× token-efficiency into a 1-trillion-parameter regime without its historical Achilles’ heel: qk-clip rescales the query and key projections after every Muon update.&lt;/p&gt;\n\n&lt;h1&gt;How good in comparison to Claude 4 Sonnet?&lt;/h1&gt;\n\n&lt;p&gt;Kimi k2&amp;#39;s positioning directly challenged Claude 4 Sonnet, the current SOTA agentic model. The k2 was specifically RL&amp;#39;d for extensive tool-use scenarios. However, it&amp;#39;s not just good at tool use, it is surprisingly creative at writing and coding.&lt;/p&gt;\n\n&lt;p&gt;Some observations&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The K2 feels most natural to talk to than any available models. Zero sycophancy, no assumption, it just sticks to the point. Though I still find Sonnet 4 to be more attentive to instructions.&lt;/li&gt;\n&lt;li&gt;It has the simillar vibes of Claude 3.6 Sonnet, understands user intention better and more grounded response.&lt;/li&gt;\n&lt;li&gt;K2 has a better taste.&lt;/li&gt;\n&lt;li&gt;The coding is surprisingly good, though Sonnet will still be better at raw coding as for some task I found myself going back to it.&lt;/li&gt;\n&lt;li&gt;The best part it is roughly 1/12th of Sonnet&amp;#39;s cost. Crazy times indeed.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can find the complete note here: &lt;a href=\"https://composio.dev/blog/notes-on-kimi-k2\"&gt;Notes on Kimi K2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to know your experience with the new Kimi K2 and how do you think it compares to Claude for agentic coding and other agentic tasks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zGgQbeMFTo_lv7lUQOHfoNHDMmbj1qkSBZum0vd4aW0.png?auto=webp&amp;s=be0f59a7d80ea9baf0f5765f28ae5ccb54405651",
                  "width": 957,
                  "height": 638
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zGgQbeMFTo_lv7lUQOHfoNHDMmbj1qkSBZum0vd4aW0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b0a6766afd7ad39a4f200b56a12c3aac0dd8217",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/zGgQbeMFTo_lv7lUQOHfoNHDMmbj1qkSBZum0vd4aW0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2938e3166381ec1896e700d2388d1776bb5dee0",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/zGgQbeMFTo_lv7lUQOHfoNHDMmbj1qkSBZum0vd4aW0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e9780dfd5ff0fb6bd81353eb93b97ab9ae7ad41b",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/zGgQbeMFTo_lv7lUQOHfoNHDMmbj1qkSBZum0vd4aW0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5de23db11f854a0136a52e55d26a3b94d5b22c84",
                    "width": 640,
                    "height": 426
                  }
                ],
                "variants": {},
                "id": "zGgQbeMFTo_lv7lUQOHfoNHDMmbj1qkSBZum0vd4aW0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0rk8t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SunilKumarDash",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0rk8t/notes_on_kimi_k2_a_deepseek_derivative_but_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0rk8t/notes_on_kimi_k2_a_deepseek_derivative_but_the/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752608406,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mistralai/Voxtral-Mini-3B-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0k22v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 330,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 330,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=cd77edaec4f8585417c31cc0da4b2dfdd34cc4c0",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752591620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mistralai/Voxtral-Mini-3B-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?auto=webp&amp;s=b60aa7dc8cbbd7e5f4f11d34f8a4d4c1f465c3a4",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7c79d7457e96e3a6bea5d1a2a1846d564067679",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=feab21f20e4d7e30de4c34f4ab6828660f821a94",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdb02c3fbc2730980b12aaad550d915114f1d0a1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=709cf21da4c4a0ff86a826909c9d3b8548549207",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=21a65f27202d731371723411d21fafcb6ee782d9",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4c945c23c165877410cc16badb1ceb6e44e99bb6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m0k22v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0k22v/mistralaivoxtralmini3b2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/mistralai/Voxtral-Mini-3B-2507",
          "subreddit_subscribers": 499772,
          "created_utc": 1752591620,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We're likely getting a closed source model instead ",
          "author_fullname": "t2_5luz9ozsa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Well, if anyone was waiting for Llama 4 Behemoth, it's gone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0g2mk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 426,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 426,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=860ae22badddb8fc2c343a9ccac78f148022a869",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752581342,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "analyticsindiamag.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re likely getting a closed source model instead &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://analyticsindiamag.com/global-tech/meta-plans-to-abandon-llama-4-behemoth-but-why/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?auto=webp&amp;s=ddc9c55c7477ac3d20d1a5ec41979dd91d76c1dd",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01ed45a4be031327974fcbe48924b7c8d0421993",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8279d4f944e0abce05c11f97b8a014ed4500b3da",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c501e47f0fbce65b45288abd53f58cec0662510d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7417749598717bd5400069706a3c0d563e32ab4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=281861ba0f1aec0cc2a503b3304cf7663f094868",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12d94b438b313810f2f112ab90b5aca70313ccf6",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0g2mk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Elevator5091",
          "discussion_type": null,
          "num_comments": 131,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/",
          "stickied": false,
          "url": "https://analyticsindiamag.com/global-tech/meta-plans-to-abandon-llama-4-behemoth-but-why/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752581342,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, I'm newbie with llama.cpp. I want to run kimi-k2 unsloth Q4 version on a 8xH20 server, but I cannot find any instruction for this. Is it possible? Or I should try other solution?",
          "author_fullname": "t2_t39ypn0v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does llama.cpp support to run kimi-k2 with multi GPUs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m18tr9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752660882,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I&amp;#39;m newbie with llama.cpp. I want to run kimi-k2 unsloth Q4 version on a 8xH20 server, but I cannot find any instruction for this. Is it possible? Or I should try other solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m18tr9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Every_Bathroom_119",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m18tr9/does_llamacpp_support_to_run_kimik2_with_multi/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m18tr9/does_llamacpp_support_to_run_kimik2_with_multi/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752660882,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Apparently, Hermes 4 671B is going to be released sometime this month as well per their Discord. No idea if it is based on the base model or either V3/R1.",
          "author_fullname": "t2_101haj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NousResearch/Hermes-3-Dataset Release",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0t5m9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 71,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 71,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=6f3ab7744ccc8be4dfdca9c8e445dae1d4b17e68",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752612029,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apparently, Hermes 4 671B is going to be released sometime this month as well per their Discord. No idea if it is based on the base model or either V3/R1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?auto=webp&amp;s=8e9789b8acc274334d24a5171a91637a7c51c637",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=15ccfcf70f47df85a2ea95812c42dfc6db35533e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=93fb8f61462bdff5e79e866af15ddffdc8754d43",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c5834756eec60605aabc9bc797cb23fb36c1170",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=22ab9040ca58534a8200b40ff6f5423544d7a18d",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6852e54f5ae16251e78fb080f9cbb58c64727769",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c945e2bd7dc7e5768a8a985dfaaf0b3d7ae0ab79",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0t5m9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheRealMasonMac",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0t5m9/nousresearchhermes3dataset_release/",
          "stickied": false,
          "url": "https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset",
          "subreddit_subscribers": 499772,
          "created_utc": 1752612029,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi gang, in the use case 1 user total, local chat inference, assume model fits in vram, which engine is faster for tokens/sec for any given prompt?",
          "author_fullname": "t2_1l3z4stvkq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vllm vs. llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m1au28",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752667606,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi gang, in the use case 1 user total, local chat inference, assume model fits in vram, which engine is faster for tokens/sec for any given prompt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m1au28",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Agreeable-Prompt-666",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752667606,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For you big rig runners who are fan's of ik_llama.cpp I just released a unique recipe of Kimi-K2-Instruct suitable for running on \"only\" ~368GB RAM - or less if you got any of that $weet $weet VRAM!\n\nThe perplexity clocks in at `3.2741 +/- 0.01689` which is not much higher (worse) than the full massive 1TB `Q8_0` baseline score of `2.9507 +/- 0.01468` despite being 34% of the full size!\n\nThe new `IQ2_KL` quant type just came out this week and I couldn't wait to give it a go. It is runs fast on both CUDA and CPU backend and packs in a ton of quality at only 2.69 bpw!\n\nWendell over at level1techs just hooked me up with a new remote rig with enough RAM and kioxia flash drives to actually maneuver this barge of a model, so big thanks as usual!\n\nI'll be releasing some more sizes soon so feel free to open a discussion on hf if there is a target break point size you'd like to see.\n\n*Remember* this quant only runs on ik_llama.cpp, instructions are on the github to download build and run any quants you already have as well as my quants.\n\nCheers!",
          "author_fullname": "t2_n321yfw5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "IQ2_KL 345.687 GiB (2.892 BPW) Kimi-K2-Instruct GGUF ik exclusive!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0uoqo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=9409131e1802d5805748fc5e36865d9827100222",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752615641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For you big rig runners who are fan&amp;#39;s of ik_llama.cpp I just released a unique recipe of Kimi-K2-Instruct suitable for running on &amp;quot;only&amp;quot; ~368GB RAM - or less if you got any of that $weet $weet VRAM!&lt;/p&gt;\n\n&lt;p&gt;The perplexity clocks in at &lt;code&gt;3.2741 +/- 0.01689&lt;/code&gt; which is not much higher (worse) than the full massive 1TB &lt;code&gt;Q8_0&lt;/code&gt; baseline score of &lt;code&gt;2.9507 +/- 0.01468&lt;/code&gt; despite being 34% of the full size!&lt;/p&gt;\n\n&lt;p&gt;The new &lt;code&gt;IQ2_KL&lt;/code&gt; quant type just came out this week and I couldn&amp;#39;t wait to give it a go. It is runs fast on both CUDA and CPU backend and packs in a ton of quality at only 2.69 bpw!&lt;/p&gt;\n\n&lt;p&gt;Wendell over at level1techs just hooked me up with a new remote rig with enough RAM and kioxia flash drives to actually maneuver this barge of a model, so big thanks as usual!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be releasing some more sizes soon so feel free to open a discussion on hf if there is a target break point size you&amp;#39;d like to see.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Remember&lt;/em&gt; this quant only runs on ik_llama.cpp, instructions are on the github to download build and run any quants you already have as well as my quants.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ubergarm/Kimi-K2-Instruct-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?auto=webp&amp;s=c9cbc5d289fcb88e7f2a478e46fb59540e618b43",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b66a7e69efb7531c88f64412897d24ba07bb4949",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1503aaf6c1d3c756fc94832b2e5690303d632991",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a089811ac299b56e1a0108bf66794197b994c231",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe89633f63f298fcdd1962f72916307a0c4801ec",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d4ca87e21be46a2fba515643708e937b12a1e622",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=065fcaed86fb7a4c21613ab7baf201d7f06089f5",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m0uoqo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VoidAlchemy",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/",
          "stickied": false,
          "url": "https://huggingface.co/ubergarm/Kimi-K2-Instruct-GGUF",
          "subreddit_subscribers": 499772,
          "created_utc": 1752615641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! Just wanted to share some thoughts on my experience with the new Kimi K2 model.\n\nEver since Unsloth released their quantized version of Kimi K2 yesterday, I’ve been giving it a real workout. I’ve mostly been pairing it with Roo Code, and honestly… I’m blown away.\n\nBack in March, I built myself a server mainly for coding experiments and to mess around with all sorts of models and setups (definitely not to save money—let’s be real, using the Claude API probably would have been cheaper). But this became a hobby, and I wanted to really get into it.\n\nUp until now, I’ve tried DeepSeek V3, R1, R1 0528—you name it. Nothing comes close to what I’m seeing with Kimi K2 today. Usually, my server was just for quick bug fixes that didn’t need much context. For anything big or complex, I’d have to use Claude.\n\nBut now that’s changed. Kimi K2 is handling everything I throw at it, even big, complicated tasks. For example, it’s making changes to a C++ firmware project—*deep* into a 90,000-token context—and it’s nailing the search and replace stuff in Roo Code without getting lost or mixing things up.\n\nJust wanted to share my excitement! Huge thanks to the folks at Moonshot AI for releasing this, and big shoutout to Unsloth and Ik\\_llama. Seriously, none of this would be possible without you all. You’re the real MVPs.\n\nIf you’re curious about my setup: I’m running this on a dual EPYC 7532 server, 512GB of DDR4 RAM (overclocked a bit), and three RTX 3090s.",
          "author_fullname": "t2_1c88dc0z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi has impressive coding performance! Even deep into context usage.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0lyjn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 134,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 134,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752595918,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! Just wanted to share some thoughts on my experience with the new Kimi K2 model.&lt;/p&gt;\n\n&lt;p&gt;Ever since Unsloth released their quantized version of Kimi K2 yesterday, I’ve been giving it a real workout. I’ve mostly been pairing it with Roo Code, and honestly… I’m blown away.&lt;/p&gt;\n\n&lt;p&gt;Back in March, I built myself a server mainly for coding experiments and to mess around with all sorts of models and setups (definitely not to save money—let’s be real, using the Claude API probably would have been cheaper). But this became a hobby, and I wanted to really get into it.&lt;/p&gt;\n\n&lt;p&gt;Up until now, I’ve tried DeepSeek V3, R1, R1 0528—you name it. Nothing comes close to what I’m seeing with Kimi K2 today. Usually, my server was just for quick bug fixes that didn’t need much context. For anything big or complex, I’d have to use Claude.&lt;/p&gt;\n\n&lt;p&gt;But now that’s changed. Kimi K2 is handling everything I throw at it, even big, complicated tasks. For example, it’s making changes to a C++ firmware project—&lt;em&gt;deep&lt;/em&gt; into a 90,000-token context—and it’s nailing the search and replace stuff in Roo Code without getting lost or mixing things up.&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share my excitement! Huge thanks to the folks at Moonshot AI for releasing this, and big shoutout to Unsloth and Ik_llama. Seriously, none of this would be possible without you all. You’re the real MVPs.&lt;/p&gt;\n\n&lt;p&gt;If you’re curious about my setup: I’m running this on a dual EPYC 7532 server, 512GB of DDR4 RAM (overclocked a bit), and three RTX 3090s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0lyjn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mattescala",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752595918,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Love LocalLLM and have been hosting smaller models on my 4090 for a long time.  Local LLM seems to be viable now so I got 2x 5090s.  I'm trying to run Devstral small 8Q.  It uses about 85-90% of the dual 5090 memory with full context.    \n  \nThe issue I'm having is they don't hit 100% utilization.   Both GPUs sit at about 40-50% utilization.\n\nThreadripper 7960x  \n256gb ddr5 6000mt/s\n\nTYIA",
          "author_fullname": "t2_53nkx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPUs low utilization?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m16h0b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.65,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/dW52RcEDaQiJjX4tw9tmdCuS78YJESEaHqh0m4LDmfc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752651627,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Love LocalLLM and have been hosting smaller models on my 4090 for a long time.  Local LLM seems to be viable now so I got 2x 5090s.  I&amp;#39;m trying to run Devstral small 8Q.  It uses about 85-90% of the dual 5090 memory with full context.    &lt;/p&gt;\n\n&lt;p&gt;The issue I&amp;#39;m having is they don&amp;#39;t hit 100% utilization.   Both GPUs sit at about 40-50% utilization.&lt;/p&gt;\n\n&lt;p&gt;Threadripper 7960x&lt;br/&gt;\n256gb ddr5 6000mt/s&lt;/p&gt;\n\n&lt;p&gt;TYIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/kdwq8atcw6df1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/kdwq8atcw6df1.png?auto=webp&amp;s=cdf9107daf62d89291b5ae0aa6cc67d4a46c4c6b",
                  "width": 353,
                  "height": 201
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/kdwq8atcw6df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a600e6922e30f4a88be9074b33181ff02b90084",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/kdwq8atcw6df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62af8e712a102326b711b75c6eabe23286c2a8ad",
                    "width": 216,
                    "height": 122
                  },
                  {
                    "url": "https://preview.redd.it/kdwq8atcw6df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17eaed0b5acc766f0c26616616de7556cfa64e4b",
                    "width": 320,
                    "height": 182
                  }
                ],
                "variants": {},
                "id": "ms7iiYVGCNzRK0IIW2Y-Wq-je6rK1N0HVMw9Y8hAeWg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m16h0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rymn",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/",
          "stickied": false,
          "url": "https://i.redd.it/kdwq8atcw6df1.png",
          "subreddit_subscribers": 499772,
          "created_utc": 1752651627,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Comparison of the output from Kimi K2, Claude 4.0 and OpenAI (o3-pro; 4.1):\n\n* [Kimi K2 vs. Claude vs. OpenAI | Cursor Real-World Research Task](https://macro.com/app/md/3f71ab3b-1b25-48b2-83cf-ea771c033f64/md/44f05c78-a96b-46ac-b0c0-c1917216334d)\n\nI personally think Claude 4.0 Sonnet remains the top LLM for performing research tasks and agentic reasoning, followed by o3-pro\n\nHowever, Kimi K2 is quite impressive, and a step in the right direction for open-source models reaching parity with closed-source models in real-life, not benchmarks\n\n* Sonnet followed instructions accurately with no excess verbiage, and was straight to the point—responded with well-researched points (and counterpoints)\n* K2 was very comprehensive and generated some practical insights, similar to o3-pro, but there was a substantial amount of \"fluff\"—the model is, evidently, one of the top reasoning models without question; however, seems to \"overthink\" and hedge each insight too much\n* o3-pro was comprehensive but sort of trailed from the prompt—seemed instructional, rather than research-oriented\n* 4.1 was too vague and the output touched on the right concepts, yet did not \"peel the onion\" enough—comparable to Gemini 2.5 Pro\n\nCouple Points:\n\n* Same Prompt Word-for-Word\n* Reasoning Mode\n* One-Shot Output\n* API Usage (Including Kimi-Researcher)\n* Memory Wiped\n* No Personalization\n* No Custom Instructions (Default)\n\nMy rankings: (1) Claude Sonnet 4.0, (2) Kimi K2, (3) o3 pro, and (4) GPT 4.1\n\nLet me know your thoughts!",
          "author_fullname": "t2_1j3y97g682",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 vs. Claude vs. OpenAI | Cursor Real-World Research Task",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0yqq2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752630462,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752626249,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Comparison of the output from Kimi K2, Claude 4.0 and OpenAI (o3-pro; 4.1):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://macro.com/app/md/3f71ab3b-1b25-48b2-83cf-ea771c033f64/md/44f05c78-a96b-46ac-b0c0-c1917216334d\"&gt;Kimi K2 vs. Claude vs. OpenAI | Cursor Real-World Research Task&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I personally think Claude 4.0 Sonnet remains the top LLM for performing research tasks and agentic reasoning, followed by o3-pro&lt;/p&gt;\n\n&lt;p&gt;However, Kimi K2 is quite impressive, and a step in the right direction for open-source models reaching parity with closed-source models in real-life, not benchmarks&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Sonnet followed instructions accurately with no excess verbiage, and was straight to the point—responded with well-researched points (and counterpoints)&lt;/li&gt;\n&lt;li&gt;K2 was very comprehensive and generated some practical insights, similar to o3-pro, but there was a substantial amount of &amp;quot;fluff&amp;quot;—the model is, evidently, one of the top reasoning models without question; however, seems to &amp;quot;overthink&amp;quot; and hedge each insight too much&lt;/li&gt;\n&lt;li&gt;o3-pro was comprehensive but sort of trailed from the prompt—seemed instructional, rather than research-oriented&lt;/li&gt;\n&lt;li&gt;4.1 was too vague and the output touched on the right concepts, yet did not &amp;quot;peel the onion&amp;quot; enough—comparable to Gemini 2.5 Pro&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Couple Points:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Same Prompt Word-for-Word&lt;/li&gt;\n&lt;li&gt;Reasoning Mode&lt;/li&gt;\n&lt;li&gt;One-Shot Output&lt;/li&gt;\n&lt;li&gt;API Usage (Including Kimi-Researcher)&lt;/li&gt;\n&lt;li&gt;Memory Wiped&lt;/li&gt;\n&lt;li&gt;No Personalization&lt;/li&gt;\n&lt;li&gt;No Custom Instructions (Default)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My rankings: (1) Claude Sonnet 4.0, (2) Kimi K2, (3) o3 pro, and (4) GPT 4.1&lt;/p&gt;\n\n&lt;p&gt;Let me know your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m0yqq2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeveredRecap",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0yqq2/kimi_k2_vs_claude_vs_openai_cursor_realworld/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0yqq2/kimi_k2_vs_claude_vs_openai_cursor_realworld/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752626249,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_xucqa0ilr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Obsidian note summarizer using local LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m10jln",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=8a5aec56d6e76e26a425f41e6d18fd8069210964",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752631457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/rosmur/obsidian-summairize/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?auto=webp&amp;s=f511735dbe37379f21f25298d50dd12f72c7ea32",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4cca3180c9163dcef1b9a25168dd8b685f6abe34",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c53855af209055cbef507b723be871c335de1c31",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c29152bdbcdcebee1ae2c956a04a5a6b00bfd587",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd05d281daa15cb64e1f4bbc7f9338c798cce286",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ab017be2ae7d984202bce270cd863128fb51244",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da398d83483af39540319280c4153a41794040e2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m10jln",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rm-rf-rm",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m10jln/obsidian_note_summarizer_using_local_llms/",
          "stickied": false,
          "url": "https://github.com/rosmur/obsidian-summairize/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752631457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am very new to this AI race, and I haven't figured out much yet, but I want to build something very interesting. 💭\n\nI have seen some schools have very poor education facilities, and teachers don't have proper knowledge either.\n\nI want to build a small app that can help students learn speaking English, Maths, and Science.\n\nPrimarily voice-based inputs, not text — it will be hard for them to use text.\n\nI want the outputs to be animation-based, like Manim or JS animations — if possible, interactive.\n\nIf you want to volunteer for this idea with me, we can do something cool, or at least try.\n\nIf you are experienced and know how much budget this project might take, it would be great insight. (I don't have much.)\n\nThis is not a commercial project — just a tech charity for underprivileged students.",
          "author_fullname": "t2_hmolzecg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help me figure out how ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m19kfw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752663569,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am very new to this AI race, and I haven&amp;#39;t figured out much yet, but I want to build something very interesting. 💭&lt;/p&gt;\n\n&lt;p&gt;I have seen some schools have very poor education facilities, and teachers don&amp;#39;t have proper knowledge either.&lt;/p&gt;\n\n&lt;p&gt;I want to build a small app that can help students learn speaking English, Maths, and Science.&lt;/p&gt;\n\n&lt;p&gt;Primarily voice-based inputs, not text — it will be hard for them to use text.&lt;/p&gt;\n\n&lt;p&gt;I want the outputs to be animation-based, like Manim or JS animations — if possible, interactive.&lt;/p&gt;\n\n&lt;p&gt;If you want to volunteer for this idea with me, we can do something cool, or at least try.&lt;/p&gt;\n\n&lt;p&gt;If you are experienced and know how much budget this project might take, it would be great insight. (I don&amp;#39;t have much.)&lt;/p&gt;\n\n&lt;p&gt;This is not a commercial project — just a tech charity for underprivileged students.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m19kfw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Unlikely-Chicken3286",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m19kfw/help_me_figure_out_how/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m19kfw/help_me_figure_out_how/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752663569,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone!\n\nI’d like to get some advice and maybe inspiration from you all.  \nI’m thinking about building a local RAG setup, paired with a local LLM, that would act as a **narrative Game Master for RPGs**.\n\nHere’s the idea:  \n🎲 I upload a knowledge base (e.g., vector DB or something else) with PDFs and/or markdown files containing RPG rules (like Traveller, Stars Without Number, Ironsworn, etc.).  \n🎲 The local LLM answers as the Game Master: it builds the story, describes scenes, presents meaningful choices, and guides the player through the game according to the rules from the documents.  \n🎲 The model shouldn’t hallucinate the rules but instead use the provided knowledge base, while still narratively tying the game together.  \n🎲 Ideally, the stack would also support the **MCP** or something similar so that the model can read and write the campaign state seamlessly (e.g., in a text-based client).\n\nMy hardware:  \n🖥️ RTX 5090  \n🖥️ AMD R9 9950X3D  \n🖥️ 96 GB RAM\n\nSo far I’ve been playing around with ready-made solutions like **AnythingLLM**, **OpenWebUI**, and **Msty**, but it feels like the models didn’t really use the knowledge effectively - they often ignored or misapplied the rules. The models I tried: qwen3:32b, gemma3:27b, deepseek-r1:32b. Maybe this stack is good enough, and I just need to work on prompts?\n\nI’m not afraid of writing some code to glue things together if needed.\n\nDoes this setup make sense? Is anyone here running something similar?  \nWhat stack would you recommend? (e.g., LangChain? LlamaIndex? Something else?)  \nAny tips on making the model reliably follow the rules while still being engaging as a storyteller? And bonus points if it can work with MCP or similar protocols to persist and manage game state.\n\nThanks in advance - looking forward to your thoughts!",
          "author_fullname": "t2_ed81b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local RAG + LLM as a Narrative RPG Game Master — Does This Make Sense and How to Build It?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m15yss",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752649657,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I’d like to get some advice and maybe inspiration from you all.&lt;br/&gt;\nI’m thinking about building a local RAG setup, paired with a local LLM, that would act as a &lt;strong&gt;narrative Game Master for RPGs&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here’s the idea:&lt;br/&gt;\n🎲 I upload a knowledge base (e.g., vector DB or something else) with PDFs and/or markdown files containing RPG rules (like Traveller, Stars Without Number, Ironsworn, etc.).&lt;br/&gt;\n🎲 The local LLM answers as the Game Master: it builds the story, describes scenes, presents meaningful choices, and guides the player through the game according to the rules from the documents.&lt;br/&gt;\n🎲 The model shouldn’t hallucinate the rules but instead use the provided knowledge base, while still narratively tying the game together.&lt;br/&gt;\n🎲 Ideally, the stack would also support the &lt;strong&gt;MCP&lt;/strong&gt; or something similar so that the model can read and write the campaign state seamlessly (e.g., in a text-based client).&lt;/p&gt;\n\n&lt;p&gt;My hardware:&lt;br/&gt;\n🖥️ RTX 5090&lt;br/&gt;\n🖥️ AMD R9 9950X3D&lt;br/&gt;\n🖥️ 96 GB RAM&lt;/p&gt;\n\n&lt;p&gt;So far I’ve been playing around with ready-made solutions like &lt;strong&gt;AnythingLLM&lt;/strong&gt;, &lt;strong&gt;OpenWebUI&lt;/strong&gt;, and &lt;strong&gt;Msty&lt;/strong&gt;, but it feels like the models didn’t really use the knowledge effectively - they often ignored or misapplied the rules. The models I tried: qwen3:32b, gemma3:27b, deepseek-r1:32b. Maybe this stack is good enough, and I just need to work on prompts?&lt;/p&gt;\n\n&lt;p&gt;I’m not afraid of writing some code to glue things together if needed.&lt;/p&gt;\n\n&lt;p&gt;Does this setup make sense? Is anyone here running something similar?&lt;br/&gt;\nWhat stack would you recommend? (e.g., LangChain? LlamaIndex? Something else?)&lt;br/&gt;\nAny tips on making the model reliably follow the rules while still being engaging as a storyteller? And bonus points if it can work with MCP or similar protocols to persist and manage game state.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance - looking forward to your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m15yss",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "goompas",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m15yss/local_rag_llm_as_a_narrative_rpg_game_master_does/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m15yss/local_rag_llm_as_a_narrative_rpg_game_master_does/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752649657,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It also works on Groq's free plan",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 at ~200 tps on Groq",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0mnjk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=0db98e4b427e392e8163926f9b01f66331c70f1d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752597457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "console.groq.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It also works on Groq&amp;#39;s free plan&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?auto=webp&amp;s=3cb24b39501ef1155b2f601324c5c8555e9b8499",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ecfe3495920ae008fc89ea53bb2c6f6c5031e6d",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa96b2dff17b1ff78d473c712fd58a826538495d",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08adebe4cbdf1dc5e97925955b0409a0b8931b38",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da5aa7094d7d761b72304995d549ba6ca5c343e2",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a4cd6db2f07b108d646c226afb74ebbfea93de5e",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=33f7f023b62e29ed66a2b26764497d5b8fa34cf5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0mnjk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0mnjk/kimi_k2_at_200_tps_on_groq/",
          "stickied": false,
          "url": "https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct",
          "subreddit_subscribers": 499772,
          "created_utc": 1752597457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am in the EU, and I decided to explore options on Google Vertex; I didn't even know they had a model-as-a-service option. The pricing seems high, but they have a wide array of models, including Llama 3 and 4. Now I've spent the last 2 hours trying to get quoata from them, my account is a business one, but I still can't call it via the rest API. Furthermore the only supported region is us-central1, which will cause lag in my flows.I saw that the also have Mistral MaaS, but I couldn't manage to figure out the request format, everything is so complicated. The have this shity SDK, which uses protobuf, but building requests in that is a nightmare. Compared to other APIs I've used this is by far the worst one.\n\nHas anyone else had experience with Vertex? Should I keep pushing for quotas? Is anyone else using GCP for MaaS?\n\n",
          "author_fullname": "t2_632pze90",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using Llama MaaS in Google's Vertex AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m1as5s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752667449,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the EU, and I decided to explore options on Google Vertex; I didn&amp;#39;t even know they had a model-as-a-service option. The pricing seems high, but they have a wide array of models, including Llama 3 and 4. Now I&amp;#39;ve spent the last 2 hours trying to get quoata from them, my account is a business one, but I still can&amp;#39;t call it via the rest API. Furthermore the only supported region is us-central1, which will cause lag in my flows.I saw that the also have Mistral MaaS, but I couldn&amp;#39;t manage to figure out the request format, everything is so complicated. The have this shity SDK, which uses protobuf, but building requests in that is a nightmare. Compared to other APIs I&amp;#39;ve used this is by far the worst one.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else had experience with Vertex? Should I keep pushing for quotas? Is anyone else using GCP for MaaS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m1as5s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Specialist_Bee_9726",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m1as5s/using_llama_maas_in_googles_vertex_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m1as5s/using_llama_maas_in_googles_vertex_ai/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752667449,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know its probably asked a lot , but I cant find the answer . I know you can add the document , much like the hybrid approach to the chat, but I was looking at something like Anything LLM workspace , or OpenWeb UI knowledge ….\n\nSince I'm already using LM Studio to host the Embedding model ,,, how do I use a similar function ? is it got something to do with the RAG MCP  ? ",
          "author_fullname": "t2_14166b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you RAG multiple docs in LM STUDIO",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m1aps5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752667263,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know its probably asked a lot , but I cant find the answer . I know you can add the document , much like the hybrid approach to the chat, but I was looking at something like Anything LLM workspace , or OpenWeb UI knowledge ….&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m already using LM Studio to host the Embedding model ,,, how do I use a similar function ? is it got something to do with the RAG MCP  ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m1aps5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "uber-linny",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m1aps5/how_do_you_rag_multiple_docs_in_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m1aps5/how_do_you_rag_multiple_docs_in_lm_studio/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752667263,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Connect your browser to AI models. No browser switching needed—works seamlessly with any Chromium browser including Chrome &amp; Arc.",
          "author_fullname": "t2_3gc1jq5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open alternative to Dia / Comet AI Browsers - Can run w/ Local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m187yw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=daa7b59aff813b7c848a6a03e340ceacc2f582e4",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752658576,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Connect your browser to AI models. No browser switching needed—works seamlessly with any Chromium browser including Chrome &amp;amp; Arc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/aaronjmars/opendia",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs.png?auto=webp&amp;s=0f6b19d894c57cc6e353e16aee5a79aa07df5d13",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e257c3fa569900a2715753e88e205feb02d6999d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0546caeb8ce839e0f1ea656a357b86a5bfa3d066",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1b78e0a3e8749888779553e3fb81161571a5132d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf8ac7b025bf8acf187599145ec6ba378489282b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=866adb21f194f04616c41304f999536723cce9f8",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2dee73cd7a95d7542de6b89e870cfaa5a0ab6bef",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "o4Q6qMVvAneXpKmIjzKlyB1elh2tsIdhZF7cffXQdWs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m187yw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "feekaj",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m187yw/open_alternative_to_dia_comet_ai_browsers_can_run/",
          "stickied": false,
          "url": "https://github.com/aaronjmars/opendia",
          "subreddit_subscribers": 499772,
          "created_utc": 1752658576,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone found any issues with Exaone 4.0 1.2b yet? the bf16 version i've tried does 11tok/s on my amd 5600G using cpu only inference and it doesnt seemed to repeat itself (the kind that goes on and on and on). It does repeat itself but it will end and that's occasional. I'm very impressed with it.\n\nWhat are your thoughts about this? It's kind of usable to me for filtering spam or vulgar words etc.\n\n[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B)",
          "author_fullname": "t2_5wbwvkoiw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just tried out the Exaone 4.0 1.2b bf16 and i'm extremely suprised at how good a 1.2b can be!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0pxot",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752606357,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752604773,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone found any issues with Exaone 4.0 1.2b yet? the bf16 version i&amp;#39;ve tried does 11tok/s on my amd 5600G using cpu only inference and it doesnt seemed to repeat itself (the kind that goes on and on and on). It does repeat itself but it will end and that&amp;#39;s occasional. I&amp;#39;m very impressed with it.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts about this? It&amp;#39;s kind of usable to me for filtering spam or vulgar words etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?auto=webp&amp;s=fb8de62d0f61fbc6b07c1d3377c15fedd3b626a8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ea586f84848afa9cf43c38a00d3481e13ba17ab",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dca90bd760b8a9b92ad1a938ad4f7af91f33c789",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7495543d8d70892821308abc910a06cf54457fc7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6663014ebf8319cb8cf6f0178a8859c3f05ae157",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcf9aaca4e2279c0fc00fd47b47cb4f4eae94d30",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c3126bd581fb98f2fb0d3a7f83446bf5b2f69f8",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0pxot",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cloudxaas",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752604773,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an AMD pc AMD Ryzen™ AI 7 350 w/ Radeon™ 860M, and it doesn't seem to be compatible with rocm, has anyone managed to use rocm with this machine?",
          "author_fullname": "t2_ba6tvxt1c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ryzen AI 7 350 and rocm",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m17wf2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752657292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an AMD pc AMD Ryzen™ AI 7 350 w/ Radeon™ 860M, and it doesn&amp;#39;t seem to be compatible with rocm, has anyone managed to use rocm with this machine?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m17wf2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dear_Impression8189",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m17wf2/ryzen_ai_7_350_and_rocm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m17wf2/ryzen_ai_7_350_and_rocm/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752657292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In late summer 2025, a publicly developed large language model (LLM) will be released — co-created by researchers at EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS).\n\nThis LLM will be fully open: This openness is designed to support broad adoption and foster innovation across science, society, and industry. \n\nA defining feature of the model is its multilingual fluency in over 1,000 languages.\n\nhttps://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "author_fullname": "t2_vjqrd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Swiss Open LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0j7w4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 93,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 93,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752589655,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In late summer 2025, a publicly developed large language model (LLM) will be released — co-created by researchers at EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS).&lt;/p&gt;\n\n&lt;p&gt;This LLM will be fully open: This openness is designed to support broad adoption and foster innovation across science, society, and industry. &lt;/p&gt;\n\n&lt;p&gt;A defining feature of the model is its multilingual fluency in over 1,000 languages.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html\"&gt;https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?auto=webp&amp;s=c444e4312b925e4518f67c13cd1d0d45a148e086",
                  "width": 1565,
                  "height": 782
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=00f03455ad8e9fc0d7ab20142af7d9f6c62b3273",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6c969005aff0c942b7f56ff899031ce58c131be",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f469ddb87f611d5472215734a3217eb0ca1795e4",
                    "width": 320,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3",
                    "width": 640,
                    "height": 319
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=869859917c64b3c7518aec8cee6c925a903fa611",
                    "width": 960,
                    "height": 479
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bd6e7e7d67e1ccb5d54f14559b874a510f09c70",
                    "width": 1080,
                    "height": 539
                  }
                ],
                "variants": {},
                "id": "TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0j7w4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bleeckerj",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0j7w4/swiss_open_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0j7w4/swiss_open_llm/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752589655,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Seems to run at a descent speed :  \n[https://x.com/awnihannun/status/1943723599971443134](https://x.com/awnihannun/status/1943723599971443134)",
          "author_fullname": "t2_w3uuzkpbi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "2 M3 Ultra’s 512GB running Kimi K2 quant 4 with mlx-lm and mlx.distributed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0r95k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752607722,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems to run at a descent speed :&lt;br/&gt;\n&lt;a href=\"https://x.com/awnihannun/status/1943723599971443134\"&gt;https://x.com/awnihannun/status/1943723599971443134&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gaPf6XmvofqGqY052AGZW_s9fd-PDAd0Uv8JGsePw4Q.jpg?auto=webp&amp;s=10cb13a456a1472d0b2cb7441ff8ef82b213aaa1",
                  "width": 1498,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gaPf6XmvofqGqY052AGZW_s9fd-PDAd0Uv8JGsePw4Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dca17efd157e2ee319081022d7f580396a8ac5bc",
                    "width": 108,
                    "height": 51
                  },
                  {
                    "url": "https://external-preview.redd.it/gaPf6XmvofqGqY052AGZW_s9fd-PDAd0Uv8JGsePw4Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1f83e5b07e411d8f11cfe3dba38b5834051628f",
                    "width": 216,
                    "height": 103
                  },
                  {
                    "url": "https://external-preview.redd.it/gaPf6XmvofqGqY052AGZW_s9fd-PDAd0Uv8JGsePw4Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f23fbbb06599d418dfa188ca8537e8f16f25a9d4",
                    "width": 320,
                    "height": 153
                  },
                  {
                    "url": "https://external-preview.redd.it/gaPf6XmvofqGqY052AGZW_s9fd-PDAd0Uv8JGsePw4Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ee7221ebd38adc2994b4142d73ab7e068f0a360",
                    "width": 640,
                    "height": 307
                  },
                  {
                    "url": "https://external-preview.redd.it/gaPf6XmvofqGqY052AGZW_s9fd-PDAd0Uv8JGsePw4Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1874721419604928ebf3a1c0ea34ec23cf885f3b",
                    "width": 960,
                    "height": 461
                  },
                  {
                    "url": "https://external-preview.redd.it/gaPf6XmvofqGqY052AGZW_s9fd-PDAd0Uv8JGsePw4Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f623d98814bb8f38547ec6a92a7d0713412cafc7",
                    "width": 1080,
                    "height": 519
                  }
                ],
                "variants": {},
                "id": "9dIWPNsKKikSm4HDBxvazZebwPX5icUhEnWo9SSGgpY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0r95k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Careless_Garlic1438",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0r95k/2_m3_ultras_512gb_running_kimi_k2_quant_4_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0r95k/2_m3_ultras_512gb_running_kimi_k2_quant_4_with/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752607722,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey this is Benny from Fireworks. There is a lot of interest around Kimi over the last few days, and I wanted to share some visualization I built to help myself understand MuonClip. Check out https://muon-clip-app-644257448872.us-central1.run.app/ and let me know if you have thoughts or feedback! https://x.com/the_bunny_chen/status/1945281669247955053?t=s3-xCJFEmFKI3U4VrpkPJA&amp;s=09 ",
          "author_fullname": "t2_8wis1im1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Visualization for MuonClip",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0z80y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752627651,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey this is Benny from Fireworks. There is a lot of interest around Kimi over the last few days, and I wanted to share some visualization I built to help myself understand MuonClip. Check out &lt;a href=\"https://muon-clip-app-644257448872.us-central1.run.app/\"&gt;https://muon-clip-app-644257448872.us-central1.run.app/&lt;/a&gt; and let me know if you have thoughts or feedback! &lt;a href=\"https://x.com/the_bunny_chen/status/1945281669247955053?t=s3-xCJFEmFKI3U4VrpkPJA&amp;amp;s=09\"&gt;https://x.com/the_bunny_chen/status/1945281669247955053?t=s3-xCJFEmFKI3U4VrpkPJA&amp;amp;s=09&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0z80y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Civ6forthewin",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0z80y/visualization_for_muonclip/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0z80y/visualization_for_muonclip/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752627651,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from \"praise be machine god\" to \"worse than my toddler\". So here're the findings!\n\nIf there're any questions you think we should explore from a data perspective, feel free to drop them in and we'll get to it!",
          "author_fullname": "t2_vl05s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "0doh5qm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e89395e82c91218ab3c27d0275ac7cf2d39688e"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da2da98e6c09757bddbe69c0188b10b08f970a17"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=91a6e5c5b93017d696fa56858ec4a5562ae8e197"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=198139ae4614f7e8ca04f15f534cae9a893d24cf"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f71a8864a609b569bfa66f8cb19932038d4b5c5"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=039b13f39c9f09f0ab5a0bf9cc493d2d3fd186a1"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=14f43e6010bab3171ee5981ea58f92af7093be72"
              },
              "id": "0doh5qm190df1"
            },
            "4rw1iqm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=583333ac095bc5fd2898347235b781483b78ebe2"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e6990f51af643e82478ba02ddac3b0432c461fc"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae252e6a0daf7351bdb09d7a4f319f30c1b16e37"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d64689c695468c123efec37630673c8c431a2e5"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2606ab8111ba1293184b533c59ffdceecc4a9391"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31fc9a93767303df13aa0efafded75620016dca2"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=7d8670401527804313229c82eeb47ee6a6dcc78e"
              },
              "id": "4rw1iqm190df1"
            },
            "zdh0gmo190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b1ba7e84687b2e715add702a42dd87908af1cc6"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb66a22aab1691bad599f4ed128427af5a636ea1"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f0bb9a895624306531dea55691e2e94adca1ee7"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=331c867add274b7e6f8fc2a73ee629d3873b8e9d"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bb585b6c8cde44d10a289f2d2e3a1caecd19553"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=591c167bd6197c510181b6984149a8febaa143e9"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=226982c0197e3b952e454b3578cfd19c7a573d62"
              },
              "id": "zdh0gmo190df1"
            },
            "mji4zrm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97b615304a4fcbc492af831d5b30b3677ce83654"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=50c1dc1166d11531a88a685eedf1a12bc65291db"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=885156f4aa23e9f28c58a0f33fe317aaa510a628"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fd758c677e0469e9ce2acc40af38d204fec0c20"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e2f8768eb5dc6d4e61ef099400a45456b627aa39"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa21064a7b1fa64209650cc1773d0b311ed1e235"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=b5a95ab4bcaa1120682f371bbad2478463ec6c75"
              },
              "id": "mji4zrm190df1"
            }
          },
          "name": "t3_1m0d0vz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 190,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4rw1iqm190df1",
                "id": 706335576
              },
              {
                "media_id": "zdh0gmo190df1",
                "id": 706335577
              },
              {
                "media_id": "0doh5qm190df1",
                "id": 706335578
              },
              {
                "media_id": "mji4zrm190df1",
                "id": 706335579
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 190,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Qky5LMYmgq28yvhGu7XZfILzJYn7CxOqgZAo-mu3Knk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752570907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from &amp;quot;praise be machine god&amp;quot; to &amp;quot;worse than my toddler&amp;quot;. So here&amp;#39;re the findings!&lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;re any questions you think we should explore from a data perspective, feel free to drop them in and we&amp;#39;ll get to it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m0d0vz",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0d0vz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yingyn",
          "discussion_type": null,
          "num_comments": 69,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m0d0vz",
          "subreddit_subscribers": 499772,
          "created_utc": 1752570907,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Don't get me wrong I am very thankfull for both, but I feel that there would be much to be gained if the projects re-merged. There are very usefull things in both, but the user has to choose: \"Do I want the better quants or do I want the better infrastructure?\" I really do think that the mutually missing parts are becoming more and more evident with each passing day. The work on the quants in ik is great, but with all the work which has gone into cpp in all other directions, cpp is really the better product. E.g. take gemma3 vision, that is currently non-functioning in ik, or even if it was functioning, the flag \"--no-mmproj-offload\" would still be missing.\n\nI don't know what the history of the split was, but really I don't care. I need to assume we're all grown ups here, and looking from outside the two projects fit together perfectly with ik taking care of the technicalities and cpp of the infrastructure.",
          "author_fullname": "t2_6z7m9i7r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I feel that the duality of llama.cpp and ik-llama is worrysome",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0wji2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752620356,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t get me wrong I am very thankfull for both, but I feel that there would be much to be gained if the projects re-merged. There are very usefull things in both, but the user has to choose: &amp;quot;Do I want the better quants or do I want the better infrastructure?&amp;quot; I really do think that the mutually missing parts are becoming more and more evident with each passing day. The work on the quants in ik is great, but with all the work which has gone into cpp in all other directions, cpp is really the better product. E.g. take gemma3 vision, that is currently non-functioning in ik, or even if it was functioning, the flag &amp;quot;--no-mmproj-offload&amp;quot; would still be missing.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what the history of the split was, but really I don&amp;#39;t care. I need to assume we&amp;#39;re all grown ups here, and looking from outside the two projects fit together perfectly with ik taking care of the technicalities and cpp of the infrastructure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0wji2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "erazortt",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752620356,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Coders spent more time prompting and reviewing AI generations than they saved on coding. \nhttps://arstechnica.com/ai/2025/07/study-finds-ai-tools-made-open-source-software-developers-19-percent-slower/",
          "author_fullname": "t2_1gnii9bkc9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Study finds AI tools made open source software developers 19 percent slower",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0gvhm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752583704,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Coders spent more time prompting and reviewing AI generations than they saved on coding. \n&lt;a href=\"https://arstechnica.com/ai/2025/07/study-finds-ai-tools-made-open-source-software-developers-19-percent-slower/\"&gt;https://arstechnica.com/ai/2025/07/study-finds-ai-tools-made-open-source-software-developers-19-percent-slower/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UgkbkW0twEaK4EKI8U2lgX9oKRStfue7y7z4KJ4tSH8.jpeg?auto=webp&amp;s=733f9ee430dc49d3685c46b5811170f182dcf6b1",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UgkbkW0twEaK4EKI8U2lgX9oKRStfue7y7z4KJ4tSH8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ac3de8817471081eb634a1e188f3c511c1ce43e",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/UgkbkW0twEaK4EKI8U2lgX9oKRStfue7y7z4KJ4tSH8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=097112ef8aff1081fa6daea4e5fc5d2b3d274385",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/UgkbkW0twEaK4EKI8U2lgX9oKRStfue7y7z4KJ4tSH8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4742bacfea5686186964bb1010bf7db990ef42a",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/UgkbkW0twEaK4EKI8U2lgX9oKRStfue7y7z4KJ4tSH8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4f2ec0324715340fa2484016d5b64bd8f904e6d",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/UgkbkW0twEaK4EKI8U2lgX9oKRStfue7y7z4KJ4tSH8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=152606949c5c3fe8a16f92007b7334c02ecf787b",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/UgkbkW0twEaK4EKI8U2lgX9oKRStfue7y7z4KJ4tSH8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1ad6e9e91cd78e741ad759cb92ef2380e2d66c7c",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "UgkbkW0twEaK4EKI8U2lgX9oKRStfue7y7z4KJ4tSH8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0gvhm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational_Sun_8813",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0gvhm/study_finds_ai_tools_made_open_source_software/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0gvhm/study_finds_ai_tools_made_open_source_software/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752583704,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you can't run [kimi-k2](https://huggingface.co/moonshotai/Kimi-K2-Instruct) locally, there are now more providers offering API access. DeepInfra is now the cheapest provider, while Groq is (by far) the fastest at around \\~250 tokens per second:\n\n* [https://deepinfra.com/moonshotai/Kimi-K2-Instruct](https://deepinfra.com/moonshotai/Kimi-K2-Instruct) ($0.55/$2.20 in/out Mtoken)\n* [https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct](https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct) ($1/$3 in/out Mtoken, but very fast)\n\nThat makes it cheaper than Claude Haiku 3.5, GPT-4.1 and Gemini 2.5 Pro. Not bad for the best non-thinking model currently publicly available!\n\nIt also shows the power of an open weights model with an permissive license: Even if you can't run it yourself, there's a lot more options in API access.\n\nSee all providers on OpenRouter: [https://openrouter.ai/moonshotai/kimi-k2](https://openrouter.ai/moonshotai/kimi-k2)\n\n**Edit:** There's also a free variant, but I don't know the details: [https://openrouter.ai/moonshotai/kimi-k2:free](https://openrouter.ai/moonshotai/kimi-k2:free)",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2: cheap and fast API access for those who can't run locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cgnl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 173,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 173,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=8f57197bc62e32d57dfc4ad8906e73c48c44542c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568667,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you can&amp;#39;t run &lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;kimi-k2&lt;/a&gt; locally, there are now more providers offering API access. DeepInfra is now the cheapest provider, while Groq is (by far) the fastest at around ~250 tokens per second:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://deepinfra.com/moonshotai/Kimi-K2-Instruct\"&gt;https://deepinfra.com/moonshotai/Kimi-K2-Instruct&lt;/a&gt; ($0.55/$2.20 in/out Mtoken)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct\"&gt;https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct&lt;/a&gt; ($1/$3 in/out Mtoken, but very fast)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That makes it cheaper than Claude Haiku 3.5, GPT-4.1 and Gemini 2.5 Pro. Not bad for the best non-thinking model currently publicly available!&lt;/p&gt;\n\n&lt;p&gt;It also shows the power of an open weights model with an permissive license: Even if you can&amp;#39;t run it yourself, there&amp;#39;s a lot more options in API access.&lt;/p&gt;\n\n&lt;p&gt;See all providers on OpenRouter: &lt;a href=\"https://openrouter.ai/moonshotai/kimi-k2\"&gt;https://openrouter.ai/moonshotai/kimi-k2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; There&amp;#39;s also a free variant, but I don&amp;#39;t know the details: &lt;a href=\"https://openrouter.ai/moonshotai/kimi-k2:free\"&gt;https://openrouter.ai/moonshotai/kimi-k2:free&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/moonshotai/kimi-k2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?auto=webp&amp;s=cdfbadb27e015c40ecfbfec23378ce0b09fc93d6",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13686945e0fc9e9c1db5cae73fc412b5a2cb6b98",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b7e29e79b28e20dec6cdeacc1e5fb8d7b6b3167",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c0b17ebbed757f60a5fa984a7a88e4132fd8967",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8de1b4b36c00b224fb29471c6864b8730dd4f7f2",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd4fb6296529dfc3732737ad6bd08f71d85ed9b0",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=517824341f4cec5e040a2d0d73434db4ebd33dc2",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0cgnl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/",
          "stickied": false,
          "url": "https://openrouter.ai/moonshotai/kimi-k2",
          "subreddit_subscribers": 499772,
          "created_utc": 1752568667,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Running in VRAM is not affordable, I'm guessing a hybrid setup with a x090 GPU on a server with lots of DRAM makes sense.\n\nBut what options are there for decently good RAM servers that are not too expensive?",
          "author_fullname": "t2_8jqx3m14",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OK, now we're at 1T parameter models, what's the 3090 equivalent way to run them locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0mo2d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752597489,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Running in VRAM is not affordable, I&amp;#39;m guessing a hybrid setup with a x090 GPU on a server with lots of DRAM makes sense.&lt;/p&gt;\n\n&lt;p&gt;But what options are there for decently good RAM servers that are not too expensive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0mo2d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeltaSqueezer",
          "discussion_type": null,
          "num_comments": 48,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0mo2d/ok_now_were_at_1t_parameter_models_whats_the_3090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0mo2d/ok_now_were_at_1t_parameter_models_whats_the_3090/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752597489,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Help me find some realistic voice tts models which support realtime streaming of audio.\nI have tried sesame/CSM but it doesn't support streaming which causes delay in generation.",
          "author_fullname": "t2_u85y8pn8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTS models for realtime streaming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m1701z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752653696,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Help me find some realistic voice tts models which support realtime streaming of audio.\nI have tried sesame/CSM but it doesn&amp;#39;t support streaming which causes delay in generation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m1701z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hustler0217",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m1701z/tts_models_for_realtime_streaming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m1701z/tts_models_for_realtime_streaming/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752653696,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I built this shortcut for hands-free, privacy-focused chatting with local AI characters. No cloud services needed, runs on your machine with voice input/output. Here's how it works and how to set it up.\n\nThis shortcut as currently configured has a few prerequisites:\n\n* Install LM Studio (from lmstudio.ai) and download a model like google/gemma-3-27b or your preferred one.\n* Start the local LLM server in LM Studio (defaults to http://localhost:1234).\n* Download and install Docker Desktop for simplicity of starting and stopping the TTS contianer.\n* Pull and run the Kokoro TTS Docker container: docker run -d -p 8880:8000 remsky/kokoro-fastapi\n* Ensure Docker is installed and running.\n\nI have included screenshots with various parameter options to personalise your characters.\n\n\n\nhttps://preview.redd.it/eupwo06736df1.png?width=1188&amp;format=png&amp;auto=webp&amp;s=79af9c550520783b7d27bdeef95625fd701fe79d\n\nHere you can set the system prompt to give your chat bot some personality  \n\n\nhttps://preview.redd.it/0xpn6vt836df1.png?width=1188&amp;format=png&amp;auto=webp&amp;s=7499cd8bd8bd9288f6f2f6e8ecb30afcce1d1748\n\nHere are the various exit commands that will end the shortcut and terminate the conversation. Add remove or change them as you please to personalise which commands you want to end your conversation  \n\n\nhttps://preview.redd.it/5w8prp7b36df1.png?width=1136&amp;format=png&amp;auto=webp&amp;s=9cef2ecbd97a70571e9e635e8864a08307bc9818\n\nThis block includes options for setting your model choice and preffered temperature\n\nhttps://preview.redd.it/2wx2jdjf36df1.png?width=1136&amp;format=png&amp;auto=webp&amp;s=a9ce52bc1ae499dd2003827eaa753d990f4bbea8\n\nFinally, this is the block to call the TTS API, here you can adjust the speed of the generated voice e.g. 0.5, 1, 1.5, 2. You can select the voices available from the kokoro api aswell try mixing voices with values such as af\\_heart(1)+af\\_nicole(2). The numbers in the brackets influence the weight of each selected voice in the final output.\n\n\n\nThis shortcut can be gotten up and running very quickly on a Mac by installing the dependencies mentioned above on your machine.\n\nCould also be used in iOS but would need to point to the server you are hosting LM Studio and Kokoro-FastAPI with instead of Local Host.  \n\n\n\n\nThe shortcut can be added from this icloud link and customised to your needs: [https://www.icloud.com/shortcuts/6c293fac022b44e08786e9e2006fa1e9](https://www.icloud.com/shortcuts/6c293fac022b44e08786e9e2006fa1e9)",
          "author_fullname": "t2_y79fd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DIY Voice Chat with Local LLMs on iOS/Mac: Apple Shortcut Using LM Studio + Kokoro-FastAPI (Free &amp; Private)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 42,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "2wx2jdjf36df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/2wx2jdjf36df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c4b11da695e5088e4afff426b7a574a311dc1f6"
                },
                {
                  "y": 173,
                  "x": 216,
                  "u": "https://preview.redd.it/2wx2jdjf36df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd467794f28fdf00d0399d07edff12fad89996c3"
                },
                {
                  "y": 256,
                  "x": 320,
                  "u": "https://preview.redd.it/2wx2jdjf36df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5288679d9f0e470150b8b8743b88b73b43f4862e"
                },
                {
                  "y": 513,
                  "x": 640,
                  "u": "https://preview.redd.it/2wx2jdjf36df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=315da238990799a6a341d7e08c8d20826efd1f48"
                },
                {
                  "y": 770,
                  "x": 960,
                  "u": "https://preview.redd.it/2wx2jdjf36df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d6c8fbe069a9af2d73cb7b5a290dbdde07712e5"
                },
                {
                  "y": 867,
                  "x": 1080,
                  "u": "https://preview.redd.it/2wx2jdjf36df1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09e1a5bddf0aa30bbd7ea3e458a87e16f7958bf3"
                }
              ],
              "s": {
                "y": 912,
                "x": 1136,
                "u": "https://preview.redd.it/2wx2jdjf36df1.png?width=1136&amp;format=png&amp;auto=webp&amp;s=a9ce52bc1ae499dd2003827eaa753d990f4bbea8"
              },
              "id": "2wx2jdjf36df1"
            },
            "0xpn6vt836df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 32,
                  "x": 108,
                  "u": "https://preview.redd.it/0xpn6vt836df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=296101060a2079334c09643e4d71d5dc753410b3"
                },
                {
                  "y": 65,
                  "x": 216,
                  "u": "https://preview.redd.it/0xpn6vt836df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f5fa6100a87a88ee982b0c474b3d422cde2aa03"
                },
                {
                  "y": 97,
                  "x": 320,
                  "u": "https://preview.redd.it/0xpn6vt836df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=67d882bf38b77812466dd2ab372a0ba687a1a049"
                },
                {
                  "y": 195,
                  "x": 640,
                  "u": "https://preview.redd.it/0xpn6vt836df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7021baf7c9d3436f41faaaacf011d13eb81b285c"
                },
                {
                  "y": 292,
                  "x": 960,
                  "u": "https://preview.redd.it/0xpn6vt836df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b6164c3d06ded395c25eb68eaff4c760e0d7c2e"
                },
                {
                  "y": 329,
                  "x": 1080,
                  "u": "https://preview.redd.it/0xpn6vt836df1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3c738a3fb8dda4b8c31d94bb423663d04eb8c231"
                }
              ],
              "s": {
                "y": 362,
                "x": 1188,
                "u": "https://preview.redd.it/0xpn6vt836df1.png?width=1188&amp;format=png&amp;auto=webp&amp;s=7499cd8bd8bd9288f6f2f6e8ecb30afcce1d1748"
              },
              "id": "0xpn6vt836df1"
            },
            "eupwo06736df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 32,
                  "x": 108,
                  "u": "https://preview.redd.it/eupwo06736df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eea8d5afd991e1119f40a38e65465be73f8a5567"
                },
                {
                  "y": 65,
                  "x": 216,
                  "u": "https://preview.redd.it/eupwo06736df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0748ba48d9866960af07f912582a7a5c46df90ac"
                },
                {
                  "y": 97,
                  "x": 320,
                  "u": "https://preview.redd.it/eupwo06736df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5abe0c12947e26d2acbf1b8bd36bda1efb70743"
                },
                {
                  "y": 195,
                  "x": 640,
                  "u": "https://preview.redd.it/eupwo06736df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5903e956f04ac3d62b8bf8d768eab3fb38f9efe3"
                },
                {
                  "y": 292,
                  "x": 960,
                  "u": "https://preview.redd.it/eupwo06736df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c12c23c6afd8158381bb5b78f3cf50e283eb063a"
                },
                {
                  "y": 329,
                  "x": 1080,
                  "u": "https://preview.redd.it/eupwo06736df1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba44e30a1cda18d4547b9c5f0b03cec92db76b23"
                }
              ],
              "s": {
                "y": 362,
                "x": 1188,
                "u": "https://preview.redd.it/eupwo06736df1.png?width=1188&amp;format=png&amp;auto=webp&amp;s=79af9c550520783b7d27bdeef95625fd701fe79d"
              },
              "id": "eupwo06736df1"
            },
            "5w8prp7b36df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/5w8prp7b36df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a70bcb1d3fe8d5a758504721bec308286415aa6"
                },
                {
                  "y": 165,
                  "x": 216,
                  "u": "https://preview.redd.it/5w8prp7b36df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe78fe6063a66b9ec94317d1bcffb04ded343ae9"
                },
                {
                  "y": 245,
                  "x": 320,
                  "u": "https://preview.redd.it/5w8prp7b36df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6488601f56ed52cc21f036e3c240bc32f05dc9af"
                },
                {
                  "y": 490,
                  "x": 640,
                  "u": "https://preview.redd.it/5w8prp7b36df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfbec22b39eddc97e329994172315d8e008f1345"
                },
                {
                  "y": 735,
                  "x": 960,
                  "u": "https://preview.redd.it/5w8prp7b36df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f5b1eb9a4f648fa4afbe9542ef3cbfdacac4c3a2"
                },
                {
                  "y": 827,
                  "x": 1080,
                  "u": "https://preview.redd.it/5w8prp7b36df1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c482dad6c5a2b867034dd69001966e1d8e3b107d"
                }
              ],
              "s": {
                "y": 870,
                "x": 1136,
                "u": "https://preview.redd.it/5w8prp7b36df1.png?width=1136&amp;format=png&amp;auto=webp&amp;s=9cef2ecbd97a70571e9e635e8864a08307bc9818"
              },
              "id": "5w8prp7b36df1"
            }
          },
          "name": "t3_1m13t7g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/6OQc9gD90Kc3ItVRpFo4yaccLxqzwE3zehDfpATUTds.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752641682,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built this shortcut for hands-free, privacy-focused chatting with local AI characters. No cloud services needed, runs on your machine with voice input/output. Here&amp;#39;s how it works and how to set it up.&lt;/p&gt;\n\n&lt;p&gt;This shortcut as currently configured has a few prerequisites:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Install LM Studio (from lmstudio.ai) and download a model like google/gemma-3-27b or your preferred one.&lt;/li&gt;\n&lt;li&gt;Start the local LLM server in LM Studio (defaults to http://localhost:1234).&lt;/li&gt;\n&lt;li&gt;Download and install Docker Desktop for simplicity of starting and stopping the TTS contianer.&lt;/li&gt;\n&lt;li&gt;Pull and run the Kokoro TTS Docker container: docker run -d -p 8880:8000 remsky/kokoro-fastapi&lt;/li&gt;\n&lt;li&gt;Ensure Docker is installed and running.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have included screenshots with various parameter options to personalise your characters.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/eupwo06736df1.png?width=1188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79af9c550520783b7d27bdeef95625fd701fe79d\"&gt;https://preview.redd.it/eupwo06736df1.png?width=1188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79af9c550520783b7d27bdeef95625fd701fe79d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here you can set the system prompt to give your chat bot some personality  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0xpn6vt836df1.png?width=1188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7499cd8bd8bd9288f6f2f6e8ecb30afcce1d1748\"&gt;https://preview.redd.it/0xpn6vt836df1.png?width=1188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7499cd8bd8bd9288f6f2f6e8ecb30afcce1d1748&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are the various exit commands that will end the shortcut and terminate the conversation. Add remove or change them as you please to personalise which commands you want to end your conversation  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5w8prp7b36df1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9cef2ecbd97a70571e9e635e8864a08307bc9818\"&gt;https://preview.redd.it/5w8prp7b36df1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9cef2ecbd97a70571e9e635e8864a08307bc9818&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This block includes options for setting your model choice and preffered temperature&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2wx2jdjf36df1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9ce52bc1ae499dd2003827eaa753d990f4bbea8\"&gt;https://preview.redd.it/2wx2jdjf36df1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9ce52bc1ae499dd2003827eaa753d990f4bbea8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Finally, this is the block to call the TTS API, here you can adjust the speed of the generated voice e.g. 0.5, 1, 1.5, 2. You can select the voices available from the kokoro api aswell try mixing voices with values such as af_heart(1)+af_nicole(2). The numbers in the brackets influence the weight of each selected voice in the final output.&lt;/p&gt;\n\n&lt;p&gt;This shortcut can be gotten up and running very quickly on a Mac by installing the dependencies mentioned above on your machine.&lt;/p&gt;\n\n&lt;p&gt;Could also be used in iOS but would need to point to the server you are hosting LM Studio and Kokoro-FastAPI with instead of Local Host.  &lt;/p&gt;\n\n&lt;p&gt;The shortcut can be added from this icloud link and customised to your needs: &lt;a href=\"https://www.icloud.com/shortcuts/6c293fac022b44e08786e9e2006fa1e9\"&gt;https://www.icloud.com/shortcuts/6c293fac022b44e08786e9e2006fa1e9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m13t7g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "local-foreigner",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m13t7g/diy_voice_chat_with_local_llms_on_iosmac_apple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m13t7g/diy_voice_chat_with_local_llms_on_iosmac_apple/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752641682,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I'm looking for general purpose chatbot models for a server. This is for testing the waters, nothing too serious yet.\n\nThe server has a Xeon 4314 and 128 GB DDR4-3200 ECC. \n\nThanks in advance!",
          "author_fullname": "t2_t42id",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model recommendations for GPU-less server",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m157wo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752646801,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m looking for general purpose chatbot models for a server. This is for testing the waters, nothing too serious yet.&lt;/p&gt;\n\n&lt;p&gt;The server has a Xeon 4314 and 128 GB DDR4-3200 ECC. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m157wo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Takia_Gecko",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m157wo/model_recommendations_for_gpuless_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m157wo/model_recommendations_for_gpuless_server/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752646801,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m not talking about models that are uncommon like qwen 3 0.6b or glm4 32b since those are still benchmark toppers I’m talking like really underrated ill start first with phi4(non reasoning) and exaone deep:7.8b(which gave me the idea for this post)",
          "author_fullname": "t2_aiof8j1c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the most underrated model in your opinion?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0z8sn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752627712,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m not talking about models that are uncommon like qwen 3 0.6b or glm4 32b since those are still benchmark toppers I’m talking like really underrated ill start first with phi4(non reasoning) and exaone deep:7.8b(which gave me the idea for this post)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0z8sn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yeet5566",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0z8sn/what_is_the_most_underrated_model_in_your_opinion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0z8sn/what_is_the_most_underrated_model_in_your_opinion/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752627712,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Latest update: 15/07/2025)\n\nI've just extracted the FULL Cursor system prompt and internal tools. Over 500 lines (Around 7k tokens).\n\nYou can check it out [here](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/blob/main/Cursor%20Prompts/Agent%20Prompt%20v1.2.txt).",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FULL Cursor System Prompt and Tools [UPDATED, v1.2]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0thc5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752612778,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Latest update: 15/07/2025)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just extracted the FULL Cursor system prompt and internal tools. Over 500 lines (Around 7k tokens).&lt;/p&gt;\n\n&lt;p&gt;You can check it out &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/blob/main/Cursor%20Prompts/Agent%20Prompt%20v1.2.txt\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rAalIMiFK8rl1X8578wKSf4R-0Qm7Mk0Q9CxHFUQlC0.png?auto=webp&amp;s=777accae201b686f4dc56ecba0bfd3952b23de3c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rAalIMiFK8rl1X8578wKSf4R-0Qm7Mk0Q9CxHFUQlC0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f890f158e043748062d3389b8ac6b9f34dcdc55f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rAalIMiFK8rl1X8578wKSf4R-0Qm7Mk0Q9CxHFUQlC0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2f75491cafd415a4e67341bb6b7b2c69441a8e4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rAalIMiFK8rl1X8578wKSf4R-0Qm7Mk0Q9CxHFUQlC0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=265d1e72803d580ada099414bb1c5c80f4c39bad",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rAalIMiFK8rl1X8578wKSf4R-0Qm7Mk0Q9CxHFUQlC0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8221e6f5baad515b820fb2bb9931cf2c7fc8191a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rAalIMiFK8rl1X8578wKSf4R-0Qm7Mk0Q9CxHFUQlC0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=54803f6735837ba66acb7c17c9c0ea4231700b36",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rAalIMiFK8rl1X8578wKSf4R-0Qm7Mk0Q9CxHFUQlC0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56bdf4a05ca43a5d6106438fb2a09e2621074e55",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rAalIMiFK8rl1X8578wKSf4R-0Qm7Mk0Q9CxHFUQlC0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0thc5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0thc5/full_cursor_system_prompt_and_tools_updated_v12/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0thc5/full_cursor_system_prompt_and_tools_updated_v12/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752612778,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1fc9cbovwe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EXAONE 4.0 32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m04a20",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 288,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 288,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8a97b0e14968bf021760fe24b232086843a6916d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752541575,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?auto=webp&amp;s=21346c43501458b33bb875a62eb15906b79b28b2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01c7ab98318dec8e4dfb9ad444e48cb42d1afee0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3ab4d919ac27b9f67137eb710ebbcd8ffae7191",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e17ff014e394f7eaa73049e5608695028dc583e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=18202842c69b787ccdb604277c8c0ce21247e4d3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3d99f4430cca13dd301692b895103c848c110e72",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92fd7f637d351ce6963b08dd9b62b92904ecbc6d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m04a20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "minpeter2",
          "discussion_type": null,
          "num_comments": 104,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/",
          "stickied": false,
          "url": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B",
          "subreddit_subscribers": 499772,
          "created_utc": 1752541575,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yesterday we had a [big discussion](https://www.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/) about Universal Tool Calling Protocol (UTCP), a potential alternative for MCP:\n\n&gt;The **Universal Tool Calling Protocol (UTCP)** is an open standard, as an alternative to the MCP, that describes *how* to call existing tools rather than *proxying* those calls through a new server. After discovery, the agent speaks directly to the tool’s native endpoint (HTTP, gRPC, WebSocket, CLI, …), eliminating the “wrapper tax,” reducing latency, and letting you keep your existing auth, billing and security in place.\n\n* 📚 Read the [Documentation](https://utcp.io/) for tutorials, examples and best practices\n* 💻 Start building with our SDKs:\n   * [TypeScript SDK](https://github.com/universal-tool-calling-protocol/typescript-utcp)\n   * [Python SDK](https://github.com/universal-tool-calling-protocol/python-utcp)\n\nThey now added an about page: [https://www.utcp.io/about](https://www.utcp.io/about). It's a small group of developers, some of them related to [https://www.bevel.software/](https://www.bevel.software/).\n\nIt looks like they're also open to [discussing their structure](https://github.com/universal-tool-calling-protocol/utcp-specification/issues/3).\n\nFor now, I'm mainly curious, is the idea behind UTCP sound in your view, and the concept worth pursuing and standardizing? Is it an improvement or worthwhile addition to MCP?",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone dived into Universal Tool Calling Protocol (UTCP), a potential MCP alternative, yet? Is it worth standardizing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0h6qt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/utGmW0gH0yHk0tI86NCp09MwYJaWuJH4solGKRYcJNU.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=04a270d8f350e120f3e6c8e1a35260d6258516a2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752584583,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday we had a &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/\"&gt;big discussion&lt;/a&gt; about Universal Tool Calling Protocol (UTCP), a potential alternative for MCP:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The &lt;strong&gt;Universal Tool Calling Protocol (UTCP)&lt;/strong&gt; is an open standard, as an alternative to the MCP, that describes &lt;em&gt;how&lt;/em&gt; to call existing tools rather than &lt;em&gt;proxying&lt;/em&gt; those calls through a new server. After discovery, the agent speaks directly to the tool’s native endpoint (HTTP, gRPC, WebSocket, CLI, …), eliminating the “wrapper tax,” reducing latency, and letting you keep your existing auth, billing and security in place.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;📚 Read the &lt;a href=\"https://utcp.io/\"&gt;Documentation&lt;/a&gt; for tutorials, examples and best practices&lt;/li&gt;\n&lt;li&gt;💻 Start building with our SDKs:\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/universal-tool-calling-protocol/typescript-utcp\"&gt;TypeScript SDK&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/universal-tool-calling-protocol/python-utcp\"&gt;Python SDK&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;They now added an about page: &lt;a href=\"https://www.utcp.io/about\"&gt;https://www.utcp.io/about&lt;/a&gt;. It&amp;#39;s a small group of developers, some of them related to &lt;a href=\"https://www.bevel.software/\"&gt;https://www.bevel.software/&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;It looks like they&amp;#39;re also open to &lt;a href=\"https://github.com/universal-tool-calling-protocol/utcp-specification/issues/3\"&gt;discussing their structure&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;For now, I&amp;#39;m mainly curious, is the idea behind UTCP sound in your view, and the concept worth pursuing and standardizing? Is it an improvement or worthwhile addition to MCP?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/universal-tool-calling-protocol",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/utGmW0gH0yHk0tI86NCp09MwYJaWuJH4solGKRYcJNU.png?auto=webp&amp;s=7303aa720cb98f89911611d6ca547b09f87fde5d",
                  "width": 280,
                  "height": 280
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/utGmW0gH0yHk0tI86NCp09MwYJaWuJH4solGKRYcJNU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=24407be52fdf155d771e06f8bc413cc200ae7a28",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/utGmW0gH0yHk0tI86NCp09MwYJaWuJH4solGKRYcJNU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=454d3914aacab41f4a82f46401453d3411459123",
                    "width": 216,
                    "height": 216
                  }
                ],
                "variants": {},
                "id": "utGmW0gH0yHk0tI86NCp09MwYJaWuJH4solGKRYcJNU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0h6qt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0h6qt/has_anyone_dived_into_universal_tool_calling/",
          "stickied": false,
          "url": "https://github.com/universal-tool-calling-protocol",
          "subreddit_subscribers": 499772,
          "created_utc": 1752584583,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys,\n\nI conducted my own personal benchmark of several leading LLMs using problems from the Indian Olympiad Qualifier in Mathematics (IOQM 2024). I wanted to see how they would perform on these challenging math problems (similar to AIME).\n\n|model|score|\n|:-|:-|\n|gemini-2.5-pro|100%|\n|grok-3-mini-high|95%|\n|o3-2025-04-16|95%|\n|grok-4-0706|95%|\n|kimi-k2-0711-preview|90%|\n|o4-mini-2025-04-16|87%|\n|o3-mini|87%|\n|claude-3-7-sonnet-20250219-thinking-32k|81%|\n|gpt-4.1-2025-04-14|67%|\n|claude-opus-4-20250514|60%|\n|claude-sonnet-4-20250514|54%|\n|qwen-235b-a22b-no-thinking|54%|\n|ernie-4.5-300b-r47b|36%|\n|llama-4-scout-17b-16e-instruct|34%|\n|llama-4-maverick-17b-128e-instruct|30%|\n|claude-3-5-haiku-20241022|17%|\n|llama-3.3-70b-instruct|10%|\n|llama-3.1-8b-instruct|7.5%|\n\nWhat do you all think of these results? A single 5 mark problem sets apart grok-4 and o3 from gemini-2.5-pro and a perfect score. Kimi K2 performs extremely well for a non-reasoning model...",
          "author_fullname": "t2_f1jcyjar",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A personal mathematics benchmark (IOQM 2024)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0o6am",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752600822,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I conducted my own personal benchmark of several leading LLMs using problems from the Indian Olympiad Qualifier in Mathematics (IOQM 2024). I wanted to see how they would perform on these challenging math problems (similar to AIME).&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;model&lt;/th&gt;\n&lt;th align=\"left\"&gt;score&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;gemini-2.5-pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;100%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;grok-3-mini-high&lt;/td&gt;\n&lt;td align=\"left\"&gt;95%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;o3-2025-04-16&lt;/td&gt;\n&lt;td align=\"left\"&gt;95%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;grok-4-0706&lt;/td&gt;\n&lt;td align=\"left\"&gt;95%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kimi-k2-0711-preview&lt;/td&gt;\n&lt;td align=\"left\"&gt;90%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;o4-mini-2025-04-16&lt;/td&gt;\n&lt;td align=\"left\"&gt;87%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;o3-mini&lt;/td&gt;\n&lt;td align=\"left\"&gt;87%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;claude-3-7-sonnet-20250219-thinking-32k&lt;/td&gt;\n&lt;td align=\"left\"&gt;81%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;gpt-4.1-2025-04-14&lt;/td&gt;\n&lt;td align=\"left\"&gt;67%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;claude-opus-4-20250514&lt;/td&gt;\n&lt;td align=\"left\"&gt;60%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;claude-sonnet-4-20250514&lt;/td&gt;\n&lt;td align=\"left\"&gt;54%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;qwen-235b-a22b-no-thinking&lt;/td&gt;\n&lt;td align=\"left\"&gt;54%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;ernie-4.5-300b-r47b&lt;/td&gt;\n&lt;td align=\"left\"&gt;36%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;llama-4-scout-17b-16e-instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;34%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;llama-4-maverick-17b-128e-instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;30%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;claude-3-5-haiku-20241022&lt;/td&gt;\n&lt;td align=\"left\"&gt;17%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;llama-3.3-70b-instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;10%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;llama-3.1-8b-instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.5%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;What do you all think of these results? A single 5 mark problem sets apart grok-4 and o3 from gemini-2.5-pro and a perfect score. Kimi K2 performs extremely well for a non-reasoning model...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0o6am",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Informal_Ad_4172",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0o6am/a_personal_mathematics_benchmark_ioqm_2024/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0o6am/a_personal_mathematics_benchmark_ioqm_2024/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752600822,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't know how models work or how to use them what's a simple explanation of how to do so? Could I just double click and the thing just runs? How would I get one to be mainly for chatting with?",
          "author_fullname": "t2_8vsmgw2r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Could I be put in the right direction for the best model/s ive been using an app for chatting with bots but can't use it anymore due to circumstances and I'm totally new to this stuff",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m145cw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752642904,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know how models work or how to use them what&amp;#39;s a simple explanation of how to do so? Could I just double click and the thing just runs? How would I get one to be mainly for chatting with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m145cw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Daglen",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m145cw/could_i_be_put_in_the_right_direction_for_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m145cw/could_i_be_put_in_the_right_direction_for_the/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752642904,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m021nx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 238,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 238,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-v9_FrsEiCPVytIaPFWwPVUJAlmMBmLn8IIAFMlNQyw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752535545,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nl35mhaybxcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?auto=webp&amp;s=8e83e9de13e964005d0f5b777a1fb221aa69c590",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36b347c1b0477d5c0c50ee12646d88d4534cf13b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=476209c3d4f31e1dd5a6915c23da1b9c681dd240",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c156ec7ae4a7d874263af3bf27bdf6511a1f1353",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=427166a43aad977ff4e628d5d89073bd9fd90280",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef0326b347cdcbbb0a7908eec96440b3414e96e9",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6e79c6c9a2199b96314ff79eab544c770d2dfda",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "YueW4SD4xsNrXkAftR3D5z4lByoNaBI-7VPxyeUYmW8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m021nx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/",
          "stickied": false,
          "url": "https://i.redd.it/nl35mhaybxcf1.jpeg",
          "subreddit_subscribers": 499772,
          "created_utc": 1752535545,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am a college student and can't really find articles on running MCPs using local Modles, The hugging face MCP course is a little hard to follow. It would be helpful if you guys can provide me some documentations or articles.",
          "author_fullname": "t2_a2k4th3o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can someone nudge me into the right direction for creating MCPs using Local models. Tutorials or articles or something.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m142vi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752642668,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a college student and can&amp;#39;t really find articles on running MCPs using local Modles, The hugging face MCP course is a little hard to follow. It would be helpful if you guys can provide me some documentations or articles.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m142vi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Resident_Acadia_4798",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m142vi/can_someone_nudge_me_into_the_right_direction_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m142vi/can_someone_nudge_me_into_the_right_direction_for/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752642668,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After my [LangGraph problem analysis](https://vitaliihonchar.com/insights/how-to-build-react-agent) gained significant traction, I kept digging into why AI agent development feels so unnecessarily complex.\n\n**The fundamental issue:** LangGraph treats programming language control flow as a problem to solve, when it's actually the solution.\n\n**What LangGraph does:**\n\n* Vertices = business logic\n* Edges = control flow\n* Runtime graph compilation and validation\n\n**What any programming language already provides:**\n\n* Functions = business logic\n* if/else = control flow\n* Compile-time validation\n\n**My realization:** An AI agent is just this pattern:\n\n    for {\n        response := callLLM(context)\n        if response.ToolCalls {\n            context = executeTools(response.ToolCalls)\n        }\n        if response.Finished {\n            return\n        }\n    }\n\n**So I built** [**go-agent**](https://github.com/vitalii-honchar/go-agent) \\- no graphs, no abstractions, just native Go:\n\n* **Type safety:** Catch errors at compile time, not runtime\n* **Performance:** True parallelism, no Python GIL\n* **Simplicity:** Standard control flow, no graph DSL to learn\n* **Production-ready:** Built for infrastructure workloads\n\nThe developer experience focuses on what matters:\n\n* Define tools with type safety\n* Write behavior prompts\n* Let the library handle ReAct implementation\n\n**Current status:** Active development, MIT licensed, API stabilizing before v1.0.0\n\nFull technical analysis: [Why LangGraph Overcomplicates AI Agents](https://vitaliihonchar.com/insights/go-ai-agent-library)\n\nThoughts? Especially interested in feedback from folks who've hit similar walls with Python-based agent frameworks.",
          "author_fullname": "t2_astexpp3e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why LangGraph overcomplicates AI agents (and my Go alternative)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0hgtt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752585298,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After my &lt;a href=\"https://vitaliihonchar.com/insights/how-to-build-react-agent\"&gt;LangGraph problem analysis&lt;/a&gt; gained significant traction, I kept digging into why AI agent development feels so unnecessarily complex.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The fundamental issue:&lt;/strong&gt; LangGraph treats programming language control flow as a problem to solve, when it&amp;#39;s actually the solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What LangGraph does:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Vertices = business logic&lt;/li&gt;\n&lt;li&gt;Edges = control flow&lt;/li&gt;\n&lt;li&gt;Runtime graph compilation and validation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What any programming language already provides:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Functions = business logic&lt;/li&gt;\n&lt;li&gt;if/else = control flow&lt;/li&gt;\n&lt;li&gt;Compile-time validation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;My realization:&lt;/strong&gt; An AI agent is just this pattern:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;for {\n    response := callLLM(context)\n    if response.ToolCalls {\n        context = executeTools(response.ToolCalls)\n    }\n    if response.Finished {\n        return\n    }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;So I built&lt;/strong&gt; &lt;a href=\"https://github.com/vitalii-honchar/go-agent\"&gt;&lt;strong&gt;go-agent&lt;/strong&gt;&lt;/a&gt; - no graphs, no abstractions, just native Go:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Type safety:&lt;/strong&gt; Catch errors at compile time, not runtime&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; True parallelism, no Python GIL&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Simplicity:&lt;/strong&gt; Standard control flow, no graph DSL to learn&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Production-ready:&lt;/strong&gt; Built for infrastructure workloads&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The developer experience focuses on what matters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Define tools with type safety&lt;/li&gt;\n&lt;li&gt;Write behavior prompts&lt;/li&gt;\n&lt;li&gt;Let the library handle ReAct implementation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Current status:&lt;/strong&gt; Active development, MIT licensed, API stabilizing before v1.0.0&lt;/p&gt;\n\n&lt;p&gt;Full technical analysis: &lt;a href=\"https://vitaliihonchar.com/insights/go-ai-agent-library\"&gt;Why LangGraph Overcomplicates AI Agents&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Especially interested in feedback from folks who&amp;#39;ve hit similar walls with Python-based agent frameworks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NBv2zHHvQuYJ-yRmPxr5hidPXOMcJSHiuh8ab5sKfUw.jpeg?auto=webp&amp;s=6555e079be6b415c9251658acd4e8c1a58052278",
                  "width": 512,
                  "height": 512
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NBv2zHHvQuYJ-yRmPxr5hidPXOMcJSHiuh8ab5sKfUw.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dffc28d383b9c68c558114cee3f0ffbdf8a1dbc5",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NBv2zHHvQuYJ-yRmPxr5hidPXOMcJSHiuh8ab5sKfUw.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3aba70eec8f58e3861ad7a81ec3d74d2f059e08",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/NBv2zHHvQuYJ-yRmPxr5hidPXOMcJSHiuh8ab5sKfUw.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6da7e4d908d8c431ef4875061f328fa68313cea",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "NBv2zHHvQuYJ-yRmPxr5hidPXOMcJSHiuh8ab5sKfUw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m0hgtt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Historical_Wing_9573",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0hgtt/why_langgraph_overcomplicates_ai_agents_and_my_go/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0hgtt/why_langgraph_overcomplicates_ai_agents_and_my_go/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752585298,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I use F5 TTS and OpenAudio. I prefer OpenAudio as it has more settings and runs faster with and ends up with better multi support even for invented languaged, but it can't copy more than 80% of the sample. While F5 TTS doesn't have settings and outputs audio that feels was being heard from a police walkie tokie most of the times.\n\nUnless of course you guys know how I can improve generated voice. I can't find the supported emotions list of OpenAudio..",
          "author_fullname": "t2_eljq22kg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the best offline TTS models at the moment?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0lykb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752595919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use F5 TTS and OpenAudio. I prefer OpenAudio as it has more settings and runs faster with and ends up with better multi support even for invented languaged, but it can&amp;#39;t copy more than 80% of the sample. While F5 TTS doesn&amp;#39;t have settings and outputs audio that feels was being heard from a police walkie tokie most of the times.&lt;/p&gt;\n\n&lt;p&gt;Unless of course you guys know how I can improve generated voice. I can&amp;#39;t find the supported emotions list of OpenAudio..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0lykb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WEREWOLF_BX13",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0lykb/whats_the_best_offline_tts_models_at_the_moment/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0lykb/whats_the_best_offline_tts_models_at_the_moment/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752595919,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 tops creative writing benchmark",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzywie",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 316,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 316,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VH2yrgq-wMtmRSPtQ8VK4WklgU_7GjLW0L_EEyrgxYc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752527951,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q48f55vcpwcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?auto=webp&amp;s=72733329d330a907558da68160618b15b6172b27",
                  "width": 1500,
                  "height": 1000
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b78c6da6f3a69e12a60113ac6638feb8001f4a9",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9698b7792fd84563a11e9d5fcb8304ca9051d91c",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad25ba68ea4d8c123c8ab2e64de62e7e8343e00",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83d8a4d11cd481b0f3d6a15556baa79acf5df855",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c1e5b3512c9a41667a474d9d1a6d3f5fae4ccfa",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3f46623f3ecd81bc3ac63095add6cd4132e7792",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "XgXXR5EePWFyWI8XqcL69hThsTAzGmSFTef0gmKfLjQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzywie",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 64,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/",
          "stickied": false,
          "url": "https://i.redd.it/q48f55vcpwcf1.jpeg",
          "subreddit_subscribers": 499772,
          "created_utc": 1752527951,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The announcement comes just days after Google hired away Windsurf’s CEO Varun Mohan, co-founder Douglas Chen, and research leaders in a $2.4 billion reverse-acquihire that left much of the startup’s 250-person team behind. Google’s deal occurred just hours after OpenAI’s $3 billion offer to acquire Windsurf expired, clearing the way for the AI coding startup to explore other options.",
          "author_fullname": "t2_17n3nqtj56",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cognition, maker of the AI coding agent Devin, acquires Windsurf",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 71,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cgmc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=140&amp;height=71&amp;crop=140:71,smart&amp;auto=webp&amp;s=0cd597e97c42d4a4434b6fa6518393d509f6de44",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "techcrunch.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The announcement comes just days after Google hired away Windsurf’s CEO Varun Mohan, co-founder Douglas Chen, and research leaders in a $2.4 billion reverse-acquihire that left much of the startup’s 250-person team behind. Google’s deal occurred just hours after OpenAI’s $3 billion offer to acquire Windsurf expired, clearing the way for the AI coding startup to explore other options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://techcrunch.com/2025/07/14/cognition-maker-of-the-ai-coding-agent-devin-acquires-windsurf/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?auto=webp&amp;s=3042630aa11746c4e455fed1df9ef33fbaa6e7c8",
                  "width": 1200,
                  "height": 616
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=839b7d352a5d2ddd17832307ab32a76dea5d52eb",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08e4cb9e9357907ae39ab8f3d034a124a1305f89",
                    "width": 216,
                    "height": 110
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81d5b827214f4052c9d3342e42102209265d5f3a",
                    "width": 320,
                    "height": 164
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=045e7a9473212511b88ba864670fee8b5d269a71",
                    "width": 640,
                    "height": 328
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=02c991f48cce1207e0ea4a87ee1f6a8373db5c64",
                    "width": 960,
                    "height": 492
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39bbb22a96486aa76c6a942d036657919c2805d9",
                    "width": 1080,
                    "height": 554
                  }
                ],
                "variants": {},
                "id": "xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0cgmc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FullstackSensei",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/",
          "stickied": false,
          "url": "https://techcrunch.com/2025/07/14/cognition-maker-of-the-ai-coding-agent-devin-acquires-windsurf/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752568663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nIs there a place where I can get notified when a new interesting local LLM drops ?\n\nPreferably oriented for people who only have a desktop computer with a gaming-grade GPU ?\n\nThanks",
          "author_fullname": "t2_cbs8s4j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "News feed for new interesting local LLMs ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0p3bh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752602863,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Is there a place where I can get notified when a new interesting local LLM drops ?&lt;/p&gt;\n\n&lt;p&gt;Preferably oriented for people who only have a desktop computer with a gaming-grade GPU ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0p3bh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaKi_87",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0p3bh/news_feed_for_new_interesting_local_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0p3bh/news_feed_for_new_interesting_local_llms/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752602863,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi folks — hoping someone can help me finally crack this.\n\nI’m trying to run Open WebUI (ghcr.io/open-webui/open-webui:main) via Docker on my Windows machine, connected to a locally running Ollama server, but the WebUI refuses to show up in the browser.\n\n\n---\n\n🛠️ Setup Details\n\nOS: Windows 11 using Docker Desktop (WSL2 backend)\n\nDocker version: 28.3.0\n\nGPU: NVIDIA RTX 5070 (12GB VRAM)\n\nOllama version: v0.9.6 (running fine locally)\n\n\nContainer creation:\n\ndocker run -d ^\n  --name open-webui ^\n  -p 3000:3000 ^\n  -e OLLAMA_API_BASE_URL=http://&lt;my-local-ip&gt;:11434 ^\n  -v open-webui-data:/app/backend/data ^\n  ghcr.io/open-webui/open-webui:main\n\n(I've replaced &lt;my-local-ip&gt; with the correct IPv4 address under vEthernet (WSL) adapter.)\n\n\n---\n\n✅ What’s Working\n\nOllama is running fine on 127.0.0.1:11434\n\nDocker container starts with status healthy\n\ndocker logs shows:\n\nFetching 30 files: 100%|██████████| ...\nINFO: Started server process [1]\nINFO: Waiting for application startup.\n\nNo networking conflicts — port 3000 is clean\n\ndocker exec works fine — shell is responsive\n\nUsing either GUI or CLI to spin up containers results in same behavior\n\n\n\n---\n\n❌ What’s Not Working\n\nOpen WebUI never finishes startup\nIt just hangs at Waiting for application startup forever.\n\nNothing loads in the browser — localhost:3000 and 127.0.0.1:3000 are dead\n\ncurl inside the container returns:\n\ncurl: (7) Failed to connect to host.docker.internal port 11434\n\nConfirmed no outbound firewall issues\n\nNo fatal container errors or restarts — just stalls\n\n\n\n---\n\n🧪 What I’ve Tried\n\nRunning ollama serve before container spin-up ✅\n\nUsing host.docker.internal vs direct IP ✅\n\nRebuilt container from scratch (images, volumes reset) ✅\n\nDocker Desktop GUI and CLI methods ✅\n\nChecked for GPU resource bottlenecks — nothing out of ordinary\n\nSearched GitHub issues &amp; Discord — found similar stuck states but no resolution yet\n\n\n\n---\n\n❓My Ask\n\nWhat’s the cause of this startup stall?\nIf the container is healthy, ports are exposed, and Ollama is live, why won’t Open WebUI move past initialization or respond at localhost:3000?\n\n----\n\nI’ll happily provide logs, configs, or compose files if needed — thanks in advance!",
          "author_fullname": "t2_s5bwwi1o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚨 Docker container stuck on “Waiting for application startup” — Open WebUI won’t load in browser",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m12ij7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752637310,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks — hoping someone can help me finally crack this.&lt;/p&gt;\n\n&lt;p&gt;I’m trying to run Open WebUI (ghcr.io/open-webui/open-webui:main) via Docker on my Windows machine, connected to a locally running Ollama server, but the WebUI refuses to show up in the browser.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;🛠️ Setup Details&lt;/p&gt;\n\n&lt;p&gt;OS: Windows 11 using Docker Desktop (WSL2 backend)&lt;/p&gt;\n\n&lt;p&gt;Docker version: 28.3.0&lt;/p&gt;\n\n&lt;p&gt;GPU: NVIDIA RTX 5070 (12GB VRAM)&lt;/p&gt;\n\n&lt;p&gt;Ollama version: v0.9.6 (running fine locally)&lt;/p&gt;\n\n&lt;p&gt;Container creation:&lt;/p&gt;\n\n&lt;p&gt;docker run -d ^\n  --name open-webui ^\n  -p 3000:3000 ^\n  -e OLLAMA_API_BASE_URL=http://&amp;lt;my-local-ip&amp;gt;:11434 ^\n  -v open-webui-data:/app/backend/data ^\n  ghcr.io/open-webui/open-webui:main&lt;/p&gt;\n\n&lt;p&gt;(I&amp;#39;ve replaced &amp;lt;my-local-ip&amp;gt; with the correct IPv4 address under vEthernet (WSL) adapter.)&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;✅ What’s Working&lt;/p&gt;\n\n&lt;p&gt;Ollama is running fine on 127.0.0.1:11434&lt;/p&gt;\n\n&lt;p&gt;Docker container starts with status healthy&lt;/p&gt;\n\n&lt;p&gt;docker logs shows:&lt;/p&gt;\n\n&lt;p&gt;Fetching 30 files: 100%|██████████| ...\nINFO: Started server process [1]\nINFO: Waiting for application startup.&lt;/p&gt;\n\n&lt;p&gt;No networking conflicts — port 3000 is clean&lt;/p&gt;\n\n&lt;p&gt;docker exec works fine — shell is responsive&lt;/p&gt;\n\n&lt;p&gt;Using either GUI or CLI to spin up containers results in same behavior&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;❌ What’s Not Working&lt;/p&gt;\n\n&lt;p&gt;Open WebUI never finishes startup\nIt just hangs at Waiting for application startup forever.&lt;/p&gt;\n\n&lt;p&gt;Nothing loads in the browser — localhost:3000 and 127.0.0.1:3000 are dead&lt;/p&gt;\n\n&lt;p&gt;curl inside the container returns:&lt;/p&gt;\n\n&lt;p&gt;curl: (7) Failed to connect to host.docker.internal port 11434&lt;/p&gt;\n\n&lt;p&gt;Confirmed no outbound firewall issues&lt;/p&gt;\n\n&lt;p&gt;No fatal container errors or restarts — just stalls&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;🧪 What I’ve Tried&lt;/p&gt;\n\n&lt;p&gt;Running ollama serve before container spin-up ✅&lt;/p&gt;\n\n&lt;p&gt;Using host.docker.internal vs direct IP ✅&lt;/p&gt;\n\n&lt;p&gt;Rebuilt container from scratch (images, volumes reset) ✅&lt;/p&gt;\n\n&lt;p&gt;Docker Desktop GUI and CLI methods ✅&lt;/p&gt;\n\n&lt;p&gt;Checked for GPU resource bottlenecks — nothing out of ordinary&lt;/p&gt;\n\n&lt;p&gt;Searched GitHub issues &amp;amp; Discord — found similar stuck states but no resolution yet&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;❓My Ask&lt;/p&gt;\n\n&lt;p&gt;What’s the cause of this startup stall?\nIf the container is healthy, ports are exposed, and Ollama is live, why won’t Open WebUI move past initialization or respond at localhost:3000?&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I’ll happily provide logs, configs, or compose files if needed — thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m12ij7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "0nlyAxeman",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752637310,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What kind of performance can I expect when using 4× RTX 5090s with vLLM in high-batch scenarios, serving many concurrent users?\n\nI’ve tried looking for benchmarks, but most of them use `batch_size = 1`, which doesn’t reflect my use case.  \nI read that throughput can scale up to 20× when using batching (&gt;128) - assuming there are no VRAM limitations - but I’m not sure how reliable that estimate is.\n\nAnyone have real-world numbers or experience to share?\n\n",
          "author_fullname": "t2_pv1nb9469",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 5090 performance with vLLM and batching?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0pn5c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752604111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What kind of performance can I expect when using 4× RTX 5090s with vLLM in high-batch scenarios, serving many concurrent users?&lt;/p&gt;\n\n&lt;p&gt;I’ve tried looking for benchmarks, but most of them use &lt;code&gt;batch_size = 1&lt;/code&gt;, which doesn’t reflect my use case.&lt;br/&gt;\nI read that throughput can scale up to 20× when using batching (&amp;gt;128) - assuming there are no VRAM limitations - but I’m not sure how reliable that estimate is.&lt;/p&gt;\n\n&lt;p&gt;Anyone have real-world numbers or experience to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0pn5c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GabryIta",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752604111,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_99mff",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta on track to be first lab with a 1GW supercluster",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0115d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 185,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 185,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/0msAxqRJrwqMDcMC7jZ5bCUaL-sRDisyC8M9WV3X4eQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752533016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/584vdadc4xcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/584vdadc4xcf1.png?auto=webp&amp;s=7e7393e2ff25a0fc5887422d47318ea835f0c1b5",
                  "width": 1010,
                  "height": 459
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aec9db4a133d2a998b21becc7885a200bea8a2bc",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55ba757e3b8229c7c3b2eb3aa62befc54fad32c5",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8220fef2199d3b0b707059754b31f54ea20f5be",
                    "width": 320,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e603dc0a062f5e964b5a1e007efdb4a66dc293f",
                    "width": 640,
                    "height": 290
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1b43c580e6c7de0aa1f0815714aca472b23c3a5",
                    "width": 960,
                    "height": 436
                  }
                ],
                "variants": {},
                "id": "-MjK6OyxWPNhUMp76GWSxVzUaVyreb5xWXtn3a_Nrzc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0115d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jd_3d",
          "discussion_type": null,
          "num_comments": 85,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/",
          "stickied": false,
          "url": "https://i.redd.it/584vdadc4xcf1.png",
          "subreddit_subscribers": 499772,
          "created_utc": 1752533016,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/e5g7kixan1df1.png?width=1065&amp;format=png&amp;auto=webp&amp;s=123bc80d7253e034373b95e73d8b271a898beecc\n\n\n\nWe haven't really gotten much details yet, it could be game code, but we have had a bunch of our testers run it without issue.  \n\nJust curious if anyone here has tried, or successfully deployed to Steam with Local llm and some ggufs?\n\n",
          "author_fullname": "t2_71knjqyi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anybody put a game on steam that included Localllm?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 71,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "e5g7kixan1df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 54,
                  "x": 108,
                  "u": "https://preview.redd.it/e5g7kixan1df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3cfa8bc2808553cc36b77111689571e58e27b57"
                },
                {
                  "y": 109,
                  "x": 216,
                  "u": "https://preview.redd.it/e5g7kixan1df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=929a1bb0e737b86b71ec0a744b93f4b34977e6ca"
                },
                {
                  "y": 162,
                  "x": 320,
                  "u": "https://preview.redd.it/e5g7kixan1df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2c9c5ab64eeb8fb6f7d6cd17a12bc505fd29629"
                },
                {
                  "y": 325,
                  "x": 640,
                  "u": "https://preview.redd.it/e5g7kixan1df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92ffa70c6f54e8e821e24fe40180db4052fdd390"
                },
                {
                  "y": 487,
                  "x": 960,
                  "u": "https://preview.redd.it/e5g7kixan1df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e52bc1ca4bfb0a6822e604c75b3435d28ab5681"
                }
              ],
              "s": {
                "y": 541,
                "x": 1065,
                "u": "https://preview.redd.it/e5g7kixan1df1.png?width=1065&amp;format=png&amp;auto=webp&amp;s=123bc80d7253e034373b95e73d8b271a898beecc"
              },
              "id": "e5g7kixan1df1"
            }
          },
          "name": "t3_1m0ihkh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/GGAtVGun9GYUrwGXdCY4AK-l2uDHQ7pjKu_W2R-0CBY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752587896,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/e5g7kixan1df1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123bc80d7253e034373b95e73d8b271a898beecc\"&gt;https://preview.redd.it/e5g7kixan1df1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123bc80d7253e034373b95e73d8b271a898beecc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We haven&amp;#39;t really gotten much details yet, it could be game code, but we have had a bunch of our testers run it without issue.  &lt;/p&gt;\n\n&lt;p&gt;Just curious if anyone here has tried, or successfully deployed to Steam with Local llm and some ggufs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0ihkh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChrisZavadil",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0ihkh/anybody_put_a_game_on_steam_that_included_localllm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0ihkh/anybody_put_a_game_on_steam_that_included_localllm/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752587896,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🌋 Introducing my first (open-source) NPM package: Whisper Node Addon.  \nIt allows to transcribe audio with Whisper.cpp straight in your Node.js environment after just installing it, no manual configuration or compilation needed. Not only that, it comes with scripts if you wish to build your binaries manually.‍\n\n🔥 And the biggest part? It supports GPU acceleration through Vulkan API (or Metal on Apple systems), effectively making real-time transcriptions possible with a decent hardware. If you don't have a GPU or you mind using it (while gaming, for example, to save resources), you can always fall back to CPU usage with a single option.\n\n⚙️ To make all of this possible, I have forked previous works by others and improved upon the addon source in C++, typing (TypeScript), CI/CD (Github Actions) and many other aspects.\n\nGet prebuilt binaries at:  \n[https://www.npmjs.com/package/@kutalia/whisper-node-addon](https://www.npmjs.com/package/@kutalia/whisper-node-addon)  \nSource code:  \n[https://github.com/Kutalia/whisper-node-addon](https://github.com/Kutalia/whisper-node-addon)",
          "author_fullname": "t2_h6o0chk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Whisper.cpp Node.js Addon with Vulkan Support",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0eq11",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752581164,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752577126,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🌋 Introducing my first (open-source) NPM package: Whisper Node Addon.&lt;br/&gt;\nIt allows to transcribe audio with Whisper.cpp straight in your Node.js environment after just installing it, no manual configuration or compilation needed. Not only that, it comes with scripts if you wish to build your binaries manually.‍&lt;/p&gt;\n\n&lt;p&gt;🔥 And the biggest part? It supports GPU acceleration through Vulkan API (or Metal on Apple systems), effectively making real-time transcriptions possible with a decent hardware. If you don&amp;#39;t have a GPU or you mind using it (while gaming, for example, to save resources), you can always fall back to CPU usage with a single option.&lt;/p&gt;\n\n&lt;p&gt;⚙️ To make all of this possible, I have forked previous works by others and improved upon the addon source in C++, typing (TypeScript), CI/CD (Github Actions) and many other aspects.&lt;/p&gt;\n\n&lt;p&gt;Get prebuilt binaries at:&lt;br/&gt;\n&lt;a href=\"https://www.npmjs.com/package/@kutalia/whisper-node-addon\"&gt;https://www.npmjs.com/package/@kutalia/whisper-node-addon&lt;/a&gt;&lt;br/&gt;\nSource code:&lt;br/&gt;\n&lt;a href=\"https://github.com/Kutalia/whisper-node-addon\"&gt;https://github.com/Kutalia/whisper-node-addon&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?auto=webp&amp;s=2b80b48069e6e2900296598568d8700477d6c523",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a058a293cabb63c88c9b65bc5197d6dfecc1cca",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5298bb0bf3f3e0e44cf949c71bec6aeb58ca173",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f87ac1cdb3372440ab71f2629e322dc7dc3c4d5b",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=549bf11b83e2f21f7e2a435f135db8605b5715a8",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a4b94a37a7ef533b4b3bb7e54547a4574ff5a55",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47f187ecab66351badd40ee9ebc2caa50361dfff",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0eq11",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kutalia",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752577126,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I made a one-click solution to let anyone run local models on their mac at home and enjoy them from anywhere on their iPhones. \n\nI find myself telling people to run local models instead of using ChatGPT, but the reality is that the whole thing is too complicated for 99.9% of them.  \nSo I made these two companion apps (one for iOS and one for Mac). You just install them and they work.   \n  \nThe Mac app has a selection of Qwen models that run directly on the Mac app with llama.cpp (advanced users can simply ignore those and turn on their Ollama or LMStudio).  \nThe iOS app is a chatbot app like ChatGPT with voice input, attachments with OCR, web search, thinking mode toggle…  \nThe UI is super intuitive for anyone who has ever used a chatbot. \n\nThey don't need setting up tailscale or any VPN/tunnel. They work by sending back and forward an iCloud record containing the conversation. Your conversations never leave your private Apple environment.  \n  \nThe only thing that is remotely technical is inserting a Serper API Key in the Mac app to allow web search.\n\nThe iOS app is called LLM Pigeon and this is the link:  \n[https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB](https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB)\n\nThe MacOS app is called LLM Pigeon Server and this is the link:  \n[https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;mt=12](https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;mt=12)\n\n",
          "author_fullname": "t2_n1rqaeut",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source and free iOS app to chat with your LLMs when you are away from home.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0dqgh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752573621,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a one-click solution to let anyone run local models on their mac at home and enjoy them from anywhere on their iPhones. &lt;/p&gt;\n\n&lt;p&gt;I find myself telling people to run local models instead of using ChatGPT, but the reality is that the whole thing is too complicated for 99.9% of them.&lt;br/&gt;\nSo I made these two companion apps (one for iOS and one for Mac). You just install them and they work.   &lt;/p&gt;\n\n&lt;p&gt;The Mac app has a selection of Qwen models that run directly on the Mac app with llama.cpp (advanced users can simply ignore those and turn on their Ollama or LMStudio).&lt;br/&gt;\nThe iOS app is a chatbot app like ChatGPT with voice input, attachments with OCR, web search, thinking mode toggle…&lt;br/&gt;\nThe UI is super intuitive for anyone who has ever used a chatbot. &lt;/p&gt;\n\n&lt;p&gt;They don&amp;#39;t need setting up tailscale or any VPN/tunnel. They work by sending back and forward an iCloud record containing the conversation. Your conversations never leave your private Apple environment.  &lt;/p&gt;\n\n&lt;p&gt;The only thing that is remotely technical is inserting a Serper API Key in the Mac app to allow web search.&lt;/p&gt;\n\n&lt;p&gt;The iOS app is called LLM Pigeon and this is the link:&lt;br/&gt;\n&lt;a href=\"https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB\"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The MacOS app is called LLM Pigeon Server and this is the link:&lt;br/&gt;\n&lt;a href=\"https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12\"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?auto=webp&amp;s=448fc3fab8106e6c3c077dbd0f889b318287ce38",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a63933662b194fe098bc5c69f8c72651d00a2dea",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=04dc892def58f636f6674e40c7824308da6d9ebf",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=84d80775a7ae89fc1db23d3d571c1175e85b4b57",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd5e5d251b0a078511eed0316cb89330a7a4a80d",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2caca59a751aa468417298ef108c26be1c134d",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3b71a736f68261d672045f9ff47efb31de4b3c4",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m0dqgh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Valuable-Run2129",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752573621,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you're building with AI agents, RAG, or just tinkering with LLMs...  \nYou're gonna love this... \n\nhttps://preview.redd.it/36glutgx18df1.png?width=800&amp;format=png&amp;auto=webp&amp;s=08c0fb68b7b622b64a6aa96292aeab5a7e8ada36\n\nMicrosoft released MarkItDown a lightweight Python tool that converts LITERALLY any file into Markdown. \n\nPDF, Word, Excel, images, audio, even PowerPoint decks.\n\nCheck this out ! link in the comments :D",
          "author_fullname": "t2_6j7o4ubzm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ok this tool is actually insane!! I just found a tool that turns ANY document into LLM-ready data!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "36glutgx18df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 145,
                  "x": 108,
                  "u": "https://preview.redd.it/36glutgx18df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a66cd33866e8a93166cdac47ae1652aa3314d5c2"
                },
                {
                  "y": 291,
                  "x": 216,
                  "u": "https://preview.redd.it/36glutgx18df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d430e3e40677c96b2dd0af523a7204e2bf031fa"
                },
                {
                  "y": 432,
                  "x": 320,
                  "u": "https://preview.redd.it/36glutgx18df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0a66709163736b1fd2d915fbbdcd01861453c30"
                },
                {
                  "y": 864,
                  "x": 640,
                  "u": "https://preview.redd.it/36glutgx18df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86a4411e8cec91a3c2652b08fa816eb1bfc86ef5"
                }
              ],
              "s": {
                "y": 1080,
                "x": 800,
                "u": "https://preview.redd.it/36glutgx18df1.png?width=800&amp;format=png&amp;auto=webp&amp;s=08c0fb68b7b622b64a6aa96292aeab5a7e8ada36"
              },
              "id": "36glutgx18df1"
            }
          },
          "name": "t3_1m1a4z7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.39,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/OKXgTNHUYl07WjLDBtC5IiWOUFTvhPGWVC7tykxQ9k4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752665432,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re building with AI agents, RAG, or just tinkering with LLMs...&lt;br/&gt;\nYou&amp;#39;re gonna love this... &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/36glutgx18df1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08c0fb68b7b622b64a6aa96292aeab5a7e8ada36\"&gt;https://preview.redd.it/36glutgx18df1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08c0fb68b7b622b64a6aa96292aeab5a7e8ada36&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Microsoft released MarkItDown a lightweight Python tool that converts LITERALLY any file into Markdown. &lt;/p&gt;\n\n&lt;p&gt;PDF, Word, Excel, images, audio, even PowerPoint decks.&lt;/p&gt;\n\n&lt;p&gt;Check this out ! link in the comments :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m1a4z7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdVirtual2648",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m1a4z7/ok_this_tool_is_actually_insane_i_just_found_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m1a4z7/ok_this_tool_is_actually_insane_i_just_found_a/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752665432,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nHey folks, my first post here! \n\nI recently recorded a video on YouTube about my learning related to building an AI agent. \n\nIt got a ton of views… and  prompted a number of security questions, so I made this follow-up explaining the concepts simply (no jargon, just analogies). \n\nhttps://youtu.be/IesP_dkykY0\n\nWould love feedback and would love to know how folks here are thinking about Agents and Agentic Security. \n",
          "author_fullname": "t2_byqvqo3zh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Made a beginner-friendly guide to AI agent security.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0wigu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752620276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, my first post here! &lt;/p&gt;\n\n&lt;p&gt;I recently recorded a video on YouTube about my learning related to building an AI agent. &lt;/p&gt;\n\n&lt;p&gt;It got a ton of views… and  prompted a number of security questions, so I made this follow-up explaining the concepts simply (no jargon, just analogies). &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/IesP_dkykY0\"&gt;https://youtu.be/IesP_dkykY0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback and would love to know how folks here are thinking about Agents and Agentic Security. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dPx-w8ljj551pNNR59jWXWmTBXsfMk63dOTCoOe9e2k.jpeg?auto=webp&amp;s=75a9cf958490f62b480d38d209ce5386fe827979",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dPx-w8ljj551pNNR59jWXWmTBXsfMk63dOTCoOe9e2k.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=34da239c365de1e7a765baa3ebb084d4ddcff762",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/dPx-w8ljj551pNNR59jWXWmTBXsfMk63dOTCoOe9e2k.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77b5fc6481ae3cd35a09f3519e7d5bdc722f2c53",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/dPx-w8ljj551pNNR59jWXWmTBXsfMk63dOTCoOe9e2k.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ff87005411b69ef2fe6d59291b5e599c2d5d0e2",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "dPx-w8ljj551pNNR59jWXWmTBXsfMk63dOTCoOe9e2k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0wigu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun_Concentrate_6163",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0wigu/made_a_beginnerfriendly_guide_to_ai_agent_security/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0wigu/made_a_beginnerfriendly_guide_to_ai_agent_security/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752620276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We just added explainability to our RAG pipeline — the AI now shows **pinpointed citations** down to the **exact paragraph, table row, or cell** it used to generate its answer.\n\nIt doesn’t just name the source file but also **highlights the exact text** and lets you **jump directly to that part of the document**. This works across formats: PDFs, Excel, CSV, Word, PowerPoint, Markdown, and more.\n\nIt makes AI answers easy to **trust and verify**, especially in messy or lengthy enterprise files. You also get insight into the **reasoning** behind the answer.\n\nIt’s fully open-source: [https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)  \nWould love to hear your thoughts or feedback!\n\n📹 Demo: [https://youtu.be/1MPsp71pkVk](https://youtu.be/1MPsp71pkVk)",
          "author_fullname": "t2_vk5ut1sk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We built Explainable AI with pinpointed citations &amp; reasoning — works across PDFs, Excel, CSV, Docs &amp; more",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0gyhy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752583961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just added explainability to our RAG pipeline — the AI now shows &lt;strong&gt;pinpointed citations&lt;/strong&gt; down to the &lt;strong&gt;exact paragraph, table row, or cell&lt;/strong&gt; it used to generate its answer.&lt;/p&gt;\n\n&lt;p&gt;It doesn’t just name the source file but also &lt;strong&gt;highlights the exact text&lt;/strong&gt; and lets you &lt;strong&gt;jump directly to that part of the document&lt;/strong&gt;. This works across formats: PDFs, Excel, CSV, Word, PowerPoint, Markdown, and more.&lt;/p&gt;\n\n&lt;p&gt;It makes AI answers easy to &lt;strong&gt;trust and verify&lt;/strong&gt;, especially in messy or lengthy enterprise files. You also get insight into the &lt;strong&gt;reasoning&lt;/strong&gt; behind the answer.&lt;/p&gt;\n\n&lt;p&gt;It’s fully open-source: &lt;a href=\"https://github.com/pipeshub-ai/pipeshub-ai\"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;br/&gt;\nWould love to hear your thoughts or feedback!&lt;/p&gt;\n\n&lt;p&gt;📹 Demo: &lt;a href=\"https://youtu.be/1MPsp71pkVk\"&gt;https://youtu.be/1MPsp71pkVk&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zMg5NY1VPG4KwPnkHPPEenZYLtiO8WahCmZ-u65Xofw.png?auto=webp&amp;s=a3e36499115ddb4df90887868dde90f246a9c736",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zMg5NY1VPG4KwPnkHPPEenZYLtiO8WahCmZ-u65Xofw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e71e0fc87137dda3b33dfeefbacba2d27983f021",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/zMg5NY1VPG4KwPnkHPPEenZYLtiO8WahCmZ-u65Xofw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3b745a8c44231778c3f5515a82017e4805e749",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/zMg5NY1VPG4KwPnkHPPEenZYLtiO8WahCmZ-u65Xofw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=21ef838d61087a737b7c77e64a217b9339cbd8de",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/zMg5NY1VPG4KwPnkHPPEenZYLtiO8WahCmZ-u65Xofw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3b16a31d74e93f12be1a5557d0262660d020601",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/zMg5NY1VPG4KwPnkHPPEenZYLtiO8WahCmZ-u65Xofw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=155c79ea92e9dca18ebfff2f470d17befd1fb895",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/zMg5NY1VPG4KwPnkHPPEenZYLtiO8WahCmZ-u65Xofw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7e3625ab6ff9b45b767116b5a5fd491a03747a2",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "zMg5NY1VPG4KwPnkHPPEenZYLtiO8WahCmZ-u65Xofw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m0gyhy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Effective-Ad2060",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0gyhy/we_built_explainable_ai_with_pinpointed_citations/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0gyhy/we_built_explainable_ai_with_pinpointed_citations/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752583961,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Not affiliated with the project, this is my unbiased opinion.\n\nI wanted to learn more about LLM function calling, so I prototyped an RPG agent which keeps track of the game state. For example, when new character is introduced, agent calls add\\_character tool, which fleshes out the character by filling out a character model. Why post this here? Naturally, I want to see how far one can get with local models for this sort of thing.\n\nI tested other libraries before (LangChain, LlamaIndex, Haystack, ...), which are bloated, require a lot of boilerplate code and/or use hidden global state, are poorly designed, and poorly documented. Not so PydanticAI, which uses a lot of clever ideas to avoid the boilerplate, and the documentation is superb.\n\nMaking an agent that can keep track of characters in the story is as simple as this:\n\n```py\n    class Character(BaseModel):\n        \"\"\"Character model with stats and description.\"\"\"\n    \n        name: str\n        appearance: str = Field(description=\"Physical appearance and decorative clothing\")\n        personality: str = Field(description=\"Personality traits and behavior\")\n        money: int = Field(ge=0, description=\"Amount of money the character carries\")\n    \n        # skipping other attributes...\n    \n    agent = Agent(...)\n    \n    # dictionary of all characters in the story\n    npcs = {}\n    \n    # This automatically generates a tool signature that the LLM understands\n    u/agent.tool_plain \n    def add_character(\n        character: Character\n    ) -&gt; str:\n        \"\"\"\n        Add a new character to the story.\n    \n        Use this tool for every new named character in the story.\n        \"\"\"\n        if character.name in state_manager.state.npcs:\n            return f\"Character {character.name!r} already exists in the story.\"\n    \n        npcs[character.name] = character\n    \n        return f\"Added character {character.name!r} to the story.\"\n\nNote how you don't have to repeat all the Character attributes in the function call, which makes this super flexible. Need a new character attribute? Just add to the Character model in a single place.\n\nPydanticAI is the first of these libraries that is actually enjoyable to use.\n\nI use Mistral Small 3.2 in my tests and it doesn't work consistently - which is probably an issue with the model and not with PydanticAI -, but when it works, it feels like magic.",
          "author_fullname": "t2_16rvbe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PydanticAI is GOAT for building agents in Python",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cdle",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=36fab7412f33958aa709272a3b83c78d2d4379e3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568312,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ai.pydantic.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not affiliated with the project, this is my unbiased opinion.&lt;/p&gt;\n\n&lt;p&gt;I wanted to learn more about LLM function calling, so I prototyped an RPG agent which keeps track of the game state. For example, when new character is introduced, agent calls add_character tool, which fleshes out the character by filling out a character model. Why post this here? Naturally, I want to see how far one can get with local models for this sort of thing.&lt;/p&gt;\n\n&lt;p&gt;I tested other libraries before (LangChain, LlamaIndex, Haystack, ...), which are bloated, require a lot of boilerplate code and/or use hidden global state, are poorly designed, and poorly documented. Not so PydanticAI, which uses a lot of clever ideas to avoid the boilerplate, and the documentation is superb.&lt;/p&gt;\n\n&lt;p&gt;Making an agent that can keep track of characters in the story is as simple as this:&lt;/p&gt;\n\n&lt;p&gt;```py\n    class Character(BaseModel):\n        &amp;quot;&amp;quot;&amp;quot;Character model with stats and description.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    name: str\n    appearance: str = Field(description=&amp;quot;Physical appearance and decorative clothing&amp;quot;)\n    personality: str = Field(description=&amp;quot;Personality traits and behavior&amp;quot;)\n    money: int = Field(ge=0, description=&amp;quot;Amount of money the character carries&amp;quot;)\n\n    # skipping other attributes...\n\nagent = Agent(...)\n\n# dictionary of all characters in the story\nnpcs = {}\n\n# This automatically generates a tool signature that the LLM understands\nu/agent.tool_plain \ndef add_character(\n    character: Character\n) -&amp;gt; str:\n    &amp;quot;&amp;quot;&amp;quot;\n    Add a new character to the story.\n\n    Use this tool for every new named character in the story.\n    &amp;quot;&amp;quot;&amp;quot;\n    if character.name in state_manager.state.npcs:\n        return f&amp;quot;Character {character.name!r} already exists in the story.&amp;quot;\n\n    npcs[character.name] = character\n\n    return f&amp;quot;Added character {character.name!r} to the story.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note how you don&amp;#39;t have to repeat all the Character attributes in the function call, which makes this super flexible. Need a new character attribute? Just add to the Character model in a single place.&lt;/p&gt;\n\n&lt;p&gt;PydanticAI is the first of these libraries that is actually enjoyable to use.&lt;/p&gt;\n\n&lt;p&gt;I use Mistral Small 3.2 in my tests and it doesn&amp;#39;t work consistently - which is probably an issue with the model and not with PydanticAI -, but when it works, it feels like magic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ai.pydantic.dev/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?auto=webp&amp;s=a643afbbcfdc774370799a0172b9534a973b608e",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88392b566033d09c2d33a1b015aa01b6b7de2b82",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2da0f9c45a0606bf95916ba2df6913a80fce6b10",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbac92d7f2a166324514d0d0add5543123d02b1c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e990c8a8d39910f731443529ec7ac5bc68d1a0d6",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=50c4a7490f3b24511d70273c65851066ef37280b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd230b4257ab61964bfe50e2814dafc7f3dce868",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0cdle",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-lq_pl-",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cdle/pydanticai_is_goat_for_building_agents_in_python/",
          "stickied": false,
          "url": "https://ai.pydantic.dev/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752568312,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nIs there a leaderboard for open source LLMs?  I know [this](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) one for VLMs and there used to be one from HuggingFace, but I think that [one](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?columns=rank%2Cmodel.type_icon%2Cid%2Cmodel.average_score%2Cevaluations.ifeval.normalized_score%2Cevaluations.bbh.normalized_score%2Cevaluations.math.normalized_score%2Cevaluations.gpqa.normalized_score%2Cevaluations.musr.normalized_score%2Cevaluations.mmlu_pro.normalized_score%2Cmetadata.params_billions&amp;official=true) is no longer maintained.",
          "author_fullname": "t2_838sm24m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source LLMs leaderboard",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0c7am",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752567551,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Is there a leaderboard for open source LLMs?  I know &lt;a href=\"https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\"&gt;this&lt;/a&gt; one for VLMs and there used to be one from HuggingFace, but I think that &lt;a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?columns=rank%2Cmodel.type_icon%2Cid%2Cmodel.average_score%2Cevaluations.ifeval.normalized_score%2Cevaluations.bbh.normalized_score%2Cevaluations.math.normalized_score%2Cevaluations.gpqa.normalized_score%2Cevaluations.musr.normalized_score%2Cevaluations.mmlu_pro.normalized_score%2Cmetadata.params_billions&amp;amp;official=true\"&gt;one&lt;/a&gt; is no longer maintained.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?auto=webp&amp;s=7e1b921f2aadc5a0f6eb3d7bd413a05df185fd20",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7996a9b4d61beea62fd32063e03712705ab26f8c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b420d4d2cf1c09672c30f9673ea6f1ac400fd6fb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=499b326baf2a9ad8a46034202c54054ee71fbf03",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f77a5813c7d65ef0d6f8e4c821b62f9d5e939dda",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0267bd806e21c11bcb30fdcd9ddf61fa3420d68d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e532be686a9e8ae2db46f566177856dfda08ede6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0c7am",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oh_my_right_leg",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752567551,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_13hqmc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GitHub - restyler/awesome-sandbox: Awesome Code Sandboxing for AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0muph",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=1846439beae4ce2c745287fc9487cef9c22a7bc2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752597913,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/restyler/awesome-sandbox",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?auto=webp&amp;s=c53efa3d8171149af3aec62675723a3728364a9e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0885a158887f11ee3460610276329b35e06cc364",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba3e1cb1fcca6475cb21c2edd96870a688525b4e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f22fc09f465f24ace4d9253ae21a54932cf0d53",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1bdeb375d1742cba6be4bec953e2d804fb502e55",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d31df855e03a4c3cf920c630ca7f640fda0ce02",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2db697ecb97de6e6e4636f6ac67c1d6147c38427",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "3orai7Y7GblddxnamUhPbryXPrepMmnkXMhTLyT4mF0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0muph",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "superjet1",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0muph/github_restylerawesomesandbox_awesome_code/",
          "stickied": false,
          "url": "https://github.com/restyler/awesome-sandbox",
          "subreddit_subscribers": 499772,
          "created_utc": 1752597913,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4hfmiefj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "‘Waiting… ‘, 2025, whatthehellisa.jpg",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0ukji",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tkK-rL_KuUlIaPcRaf1UiYB4m2zEMEhxxvFSCWn9COI.jpeg?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=37e25015726386dd64e5acbcd83a0496b2009389",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752615358,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "imgflip.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://imgflip.com/i/a0d8c0",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tkK-rL_KuUlIaPcRaf1UiYB4m2zEMEhxxvFSCWn9COI.jpeg?auto=webp&amp;s=f45c2f1b4996df5c449619c4ca399563d8db1955",
                  "width": 740,
                  "height": 788
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tkK-rL_KuUlIaPcRaf1UiYB4m2zEMEhxxvFSCWn9COI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fda59e931195491dc7e730a849ec594df5b76608",
                    "width": 108,
                    "height": 115
                  },
                  {
                    "url": "https://external-preview.redd.it/tkK-rL_KuUlIaPcRaf1UiYB4m2zEMEhxxvFSCWn9COI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec3ce35b266aea9384f190d85f4454bd0a7810c5",
                    "width": 216,
                    "height": 230
                  },
                  {
                    "url": "https://external-preview.redd.it/tkK-rL_KuUlIaPcRaf1UiYB4m2zEMEhxxvFSCWn9COI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d389c7032fd451ea41341b40548fb783ff57e2d0",
                    "width": 320,
                    "height": 340
                  },
                  {
                    "url": "https://external-preview.redd.it/tkK-rL_KuUlIaPcRaf1UiYB4m2zEMEhxxvFSCWn9COI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b95cae55a125dd10bb3824f4838b896c2dbfd25",
                    "width": 640,
                    "height": 681
                  }
                ],
                "variants": {},
                "id": "tkK-rL_KuUlIaPcRaf1UiYB4m2zEMEhxxvFSCWn9COI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m0ukji",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished_Mode170",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0ukji/waiting_2025_whatthehellisajpg/",
          "stickied": false,
          "url": "https://imgflip.com/i/a0d8c0",
          "subreddit_subscribers": 499772,
          "created_utc": 1752615358,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've built a free Ollama client that might be useful for some of you. It lets you:\n\n* Choose between different small models\n* Upload files for analysis or summaries\n* Do web searches\n* Create and organize custom prompts\n\nRuns on Windows, Mac, and laptops. If you don't have a decent GPU, there's an option to connect to a remote Gemma 12B instance.\n\nEverything stays on your machine - no cloud storage, works offline. Your data never leaves your device, so privacy is actually maintained.\n\nAvailable at [skyllbox.com](http://skyllbox.com) if anyone wants to check it out.\n\nhttps://preview.redd.it/hbvg3ni9c7df1.jpg?width=1599&amp;format=pjpg&amp;auto=webp&amp;s=a33a8da2b8fea15a4945503c095d553c27ad7791\n\nhttps://preview.redd.it/30j7um96c7df1.jpg?width=1575&amp;format=pjpg&amp;auto=webp&amp;s=230783bfc96a39aa1a7ee940385b2e8d67ccfb5f\n\n",
          "author_fullname": "t2_f6kn9hou",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running Ollama locally with a smooth UI and no technical skills",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "30j7um96c7df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/30j7um96c7df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=514eb2cfd3f67dc2d6540d38c1cb23f83122cf79"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/30j7um96c7df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6bc3c9b01841903b7f5dae14d329006df6c74c01"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/30j7um96c7df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b15acdc19fa26dc924a8c19c48d29692d0f2612b"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/30j7um96c7df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fce3e48b8868356fc45067c0765007ebeea8ec68"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/30j7um96c7df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=03f1247b48247bf6723951692e6dc119403aefba"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/30j7um96c7df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d3c627068df1f48eec95b979d8008f9a59cc78b"
                }
              ],
              "s": {
                "y": 818,
                "x": 1575,
                "u": "https://preview.redd.it/30j7um96c7df1.jpg?width=1575&amp;format=pjpg&amp;auto=webp&amp;s=230783bfc96a39aa1a7ee940385b2e8d67ccfb5f"
              },
              "id": "30j7um96c7df1"
            },
            "hbvg3ni9c7df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/hbvg3ni9c7df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7aaa374cc0397372b444309630436f29875d542b"
                },
                {
                  "y": 111,
                  "x": 216,
                  "u": "https://preview.redd.it/hbvg3ni9c7df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d207aee9ed56b296a3fc56dc078089b1afc3df1"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/hbvg3ni9c7df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=23a8d498fb33056b16de7d1c9f0d770326b29ed6"
                },
                {
                  "y": 329,
                  "x": 640,
                  "u": "https://preview.redd.it/hbvg3ni9c7df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fcff89c90bc5e7460e79dd6f698d83d2cdf98e7e"
                },
                {
                  "y": 494,
                  "x": 960,
                  "u": "https://preview.redd.it/hbvg3ni9c7df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8a40b4276d2f44103a25c2905fd22241ce0fce86"
                },
                {
                  "y": 555,
                  "x": 1080,
                  "u": "https://preview.redd.it/hbvg3ni9c7df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9938ab28703b328606f4e708ae19dadc7019258c"
                }
              ],
              "s": {
                "y": 823,
                "x": 1599,
                "u": "https://preview.redd.it/hbvg3ni9c7df1.jpg?width=1599&amp;format=pjpg&amp;auto=webp&amp;s=a33a8da2b8fea15a4945503c095d553c27ad7791"
              },
              "id": "hbvg3ni9c7df1"
            }
          },
          "name": "t3_1m17ify",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/NBoNYmjyobR7XzRF5YaxXRv0ZutmmHJMZYgBNPjWUJ4.jpg",
          "edited": 1752656724,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752655758,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve built a free Ollama client that might be useful for some of you. It lets you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Choose between different small models&lt;/li&gt;\n&lt;li&gt;Upload files for analysis or summaries&lt;/li&gt;\n&lt;li&gt;Do web searches&lt;/li&gt;\n&lt;li&gt;Create and organize custom prompts&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Runs on Windows, Mac, and laptops. If you don&amp;#39;t have a decent GPU, there&amp;#39;s an option to connect to a remote Gemma 12B instance.&lt;/p&gt;\n\n&lt;p&gt;Everything stays on your machine - no cloud storage, works offline. Your data never leaves your device, so privacy is actually maintained.&lt;/p&gt;\n\n&lt;p&gt;Available at &lt;a href=\"http://skyllbox.com\"&gt;skyllbox.com&lt;/a&gt; if anyone wants to check it out.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hbvg3ni9c7df1.jpg?width=1599&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a33a8da2b8fea15a4945503c095d553c27ad7791\"&gt;https://preview.redd.it/hbvg3ni9c7df1.jpg?width=1599&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a33a8da2b8fea15a4945503c095d553c27ad7791&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/30j7um96c7df1.jpg?width=1575&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=230783bfc96a39aa1a7ee940385b2e8d67ccfb5f\"&gt;https://preview.redd.it/30j7um96c7df1.jpg?width=1575&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=230783bfc96a39aa1a7ee940385b2e8d67ccfb5f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m17ify",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Constant-Post-122",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m17ify/running_ollama_locally_with_a_smooth_ui_and_no/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m17ify/running_ollama_locally_with_a_smooth_ui_and_no/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752655758,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, I'm writing my first academic paper and planning to submit it to an NLP conference. My work is about getting user input and applying compression on it (I didn’t train a model for this). I’ve already picked the dataset and everything is pretty much ready.\n\nFor the evaluation part, I need to prompt the text after compression to a model and measure how effective the compression is. I’ve read a bunch of papers but still can’t make a final decision, some used instruct models for evaluation, while others chose base models.\n\nNow I’m kind of stuck on which one makes more sense to use and is more accepted in papers. I also read that most models on Hugging Face are saved in BF16, which is commonly used for fine-tuning and evaluation. On the other hand, converting to FP16 seems to be better for inference.\n\nI have a couple of questions:\n\nWhich model would you suggest for evaluation? Is the llama 3.1 8B base or instruct model more widely accepted?\n\nAnd if base is suggested, should I keep it in BF16 or convert it to FP16 when using it with TensorRT-LLM for inference?\n\nWould really appreciate your thoughts on this.",
          "author_fullname": "t2_tct783z28",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Choosing the Right Model for academic Evaluation: Llama 3.1 Base vs Instruct?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0txlx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752613841,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m writing my first academic paper and planning to submit it to an NLP conference. My work is about getting user input and applying compression on it (I didn’t train a model for this). I’ve already picked the dataset and everything is pretty much ready.&lt;/p&gt;\n\n&lt;p&gt;For the evaluation part, I need to prompt the text after compression to a model and measure how effective the compression is. I’ve read a bunch of papers but still can’t make a final decision, some used instruct models for evaluation, while others chose base models.&lt;/p&gt;\n\n&lt;p&gt;Now I’m kind of stuck on which one makes more sense to use and is more accepted in papers. I also read that most models on Hugging Face are saved in BF16, which is commonly used for fine-tuning and evaluation. On the other hand, converting to FP16 seems to be better for inference.&lt;/p&gt;\n\n&lt;p&gt;I have a couple of questions:&lt;/p&gt;\n\n&lt;p&gt;Which model would you suggest for evaluation? Is the llama 3.1 8B base or instruct model more widely accepted?&lt;/p&gt;\n\n&lt;p&gt;And if base is suggested, should I keep it in BF16 or convert it to FP16 when using it with TensorRT-LLM for inference?&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate your thoughts on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0txlx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LocalComposer666",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0txlx/choosing_the_right_model_for_academic_evaluation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0txlx/choosing_the_right_model_for_academic_evaluation/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752613841,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1h9qrwy0w6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UTCP: A safer, scalable tool-calling alternative to MCP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Post of the day  "
            },
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 122,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzl5zk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "transparent",
          "ups": 796,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Post of the day  :X:",
          "can_mod_post": false,
          "score": 796,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NwQpLihWBkRwry7btAEv1kXjEd2jOrNWPfUJ4oMDWTQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752496381,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wv84vx7h3ucf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wv84vx7h3ucf1.png?auto=webp&amp;s=9fc98e1863475ae1707dcf8e031f0c40856c1282",
                  "width": 1874,
                  "height": 1642
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=af7471a080008d1b2a11663755e2386042538cdb",
                    "width": 108,
                    "height": 94
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cbf92935540699f9a5b793b10b599033bdd378c",
                    "width": 216,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5bc00f40fff44e8d417d1c4e7dcd1866bece5c55",
                    "width": 320,
                    "height": 280
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44e4d83d52673aeb1bf507e10f4ab32bff06db95",
                    "width": 640,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4aa90f35e8734ab6dd4bb03e9d5344881c7be6c2",
                    "width": 960,
                    "height": 841
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cafe58b5e161c4d2b61d8cb324738c3b1b2459e3",
                    "width": 1080,
                    "height": 946
                  }
                ],
                "variants": {},
                "id": "kbRMMR47HDIi7lZVVAy5mGTwVKuCZQBEJufsqMy9_24"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5563f7e6-52bf-11f0-a755-7266d77e32bb",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#58a7a4",
          "id": "1lzl5zk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juanviera23",
          "discussion_type": null,
          "num_comments": 149,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/",
          "stickied": false,
          "url": "https://i.redd.it/wv84vx7h3ucf1.png",
          "subreddit_subscribers": 499772,
          "created_utc": 1752496381,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Did this get mentioned here an I just missed it? Is it somehow not relevant? What am I missing? From the PR it looks like it's early days but still would be HUGE for us apple fanboys :)  \n[https://github.com/ml-explore/mlx/pull/1983](https://github.com/ml-explore/mlx/pull/1983)  \n  \n",
          "author_fullname": "t2_ks69h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does anyone know about CUDA support being added to MLX? This sounds intriguing to me but I haven't heard a peep about it except this hackernews thing I saw yesterday linking to the github PR",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0k38k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752591691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did this get mentioned here an I just missed it? Is it somehow not relevant? What am I missing? From the PR it looks like it&amp;#39;s early days but still would be HUGE for us apple fanboys :)&lt;br/&gt;\n&lt;a href=\"https://github.com/ml-explore/mlx/pull/1983\"&gt;https://github.com/ml-explore/mlx/pull/1983&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HlOZBmPVBfwFcO0skgQQ_tFtM_K-Vr_43e0hAvIXHac.png?auto=webp&amp;s=e80c2d209e89216acdf2a7f2226479e03f2db076",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HlOZBmPVBfwFcO0skgQQ_tFtM_K-Vr_43e0hAvIXHac.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2fc70f9d2ea2d303d900995f8535e8bc07a10f14",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HlOZBmPVBfwFcO0skgQQ_tFtM_K-Vr_43e0hAvIXHac.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c72e8a578cf1a58ac061ddb26b287d84c4badab",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HlOZBmPVBfwFcO0skgQQ_tFtM_K-Vr_43e0hAvIXHac.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0be0958327de176537533306f0410d2d820db785",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HlOZBmPVBfwFcO0skgQQ_tFtM_K-Vr_43e0hAvIXHac.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aabaaa2f4411207809cffe691b8e8fd5db23a844",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HlOZBmPVBfwFcO0skgQQ_tFtM_K-Vr_43e0hAvIXHac.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a9a9f822fe77a0a3b428d2ba8d5d28e82a4d554",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HlOZBmPVBfwFcO0skgQQ_tFtM_K-Vr_43e0hAvIXHac.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bfd06c7cfde4d6a569c4f6ddee03a6438c69dd6d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HlOZBmPVBfwFcO0skgQQ_tFtM_K-Vr_43e0hAvIXHac"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0k38k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "spanielrassler",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0k38k/what_does_anyone_know_about_cuda_support_being/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0k38k/what_does_anyone_know_about_cuda_support_being/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752591691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looks pretty impressive: https://www.youtube.com/shorts/G8bd-uloo48. I tried on their App, all things (text, audio, lip sync, body movement) are generated in real time.\n\nHow do they implement that? Is there any open source work to achieve similar results?",
          "author_fullname": "t2_26de42xa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How is the new Grok AI girlfriend animation implemented?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0yw9z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752628282,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752626700,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks pretty impressive: &lt;a href=\"https://www.youtube.com/shorts/G8bd-uloo48\"&gt;https://www.youtube.com/shorts/G8bd-uloo48&lt;/a&gt;. I tried on their App, all things (text, audio, lip sync, body movement) are generated in real time.&lt;/p&gt;\n\n&lt;p&gt;How do they implement that? Is there any open source work to achieve similar results?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/puy5ocqs3_L4pQOcJ1QkmtC6jnDwYIiT6NKOc_rbCTA.jpeg?auto=webp&amp;s=32ae8f730fe1e5bf9be3e92823835e12b51699c3",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/puy5ocqs3_L4pQOcJ1QkmtC6jnDwYIiT6NKOc_rbCTA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef4ebaaaae3cef35510b84bf647212dee97c32b8",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/puy5ocqs3_L4pQOcJ1QkmtC6jnDwYIiT6NKOc_rbCTA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ff464cb2b4a44a1a9f094ca7f08e4a1b5f9dcaf",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/puy5ocqs3_L4pQOcJ1QkmtC6jnDwYIiT6NKOc_rbCTA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a9edb48870a3283368209ff40c561d682c816bd",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "puy5ocqs3_L4pQOcJ1QkmtC6jnDwYIiT6NKOc_rbCTA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0yw9z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EvilKY45",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0yw9z/how_is_the_new_grok_ai_girlfriend_animation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0yw9z/how_is_the_new_grok_ai_girlfriend_animation/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752626700,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Hi everyone,**\n\nI’m looking to run an Ollama model locally for building my AI assistant, but my laptop isn’t so powerful. Here are my current specs:\n\nDell Latitude 3500\n\n8 GB RAM\n\nIntel Core i3‑8145U (4 cores)\n\nIntel UHD Graphics 620\n\nUbuntu 24.04\n\nI know these specs aren’t ideal, but I’d love your help figuring out which model would strike the best balance between usability and performance.\n\n",
          "author_fullname": "t2_1sx560oicu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Seeking advice: Which Ollama model should I run on my modest laptop?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m13eg0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752640131,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I’m looking to run an Ollama model locally for building my AI assistant, but my laptop isn’t so powerful. Here are my current specs:&lt;/p&gt;\n\n&lt;p&gt;Dell Latitude 3500&lt;/p&gt;\n\n&lt;p&gt;8 GB RAM&lt;/p&gt;\n\n&lt;p&gt;Intel Core i3‑8145U (4 cores)&lt;/p&gt;\n\n&lt;p&gt;Intel UHD Graphics 620&lt;/p&gt;\n\n&lt;p&gt;Ubuntu 24.04&lt;/p&gt;\n\n&lt;p&gt;I know these specs aren’t ideal, but I’d love your help figuring out which model would strike the best balance between usability and performance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m13eg0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AerieExotic342",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752640131,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello !\n\nI'm trying to run devstral using llama server, and it's working fine, i'm using this command to serve the model, as you see I'm using the alias to be able to select it more easily in openhand.\n\nThen in openhand advanced settings, I tried every prefix in front of my model name like openai, lm\\_studio, custom and even without even any prefix, litellm cannot access it\n\nFor the endpoint, I tried [http://127.0.0.1:8080/v1](http://127.0.0.1:8080/v1) and [http://127.0.0.1:8080](http://127.0.0.1:8080)\n\nWhen I try with the openai prefix, it tries to connect to the openai api.\n\nDid someone here managed to make openhands works with llama server ? \n\nThank you in advance and I wish you a good day, take care\n\n    ./llama-server.exe --model \"thisismyfolder\\models\\unsloth\\Devstral-Small-2507-GGUF\\Devstral-Small-2507-UD-Q5_K_XL.gguf\" --threads -1 --ctx-size 131072 --cache-type-k q8_0 --n-gpu-layers 99 --seed 3407 --prio 2 --temp 0.15 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 --host 127.0.0.1 --port 8080 --mlock --no-mmap --alias \"devstral\"",
          "author_fullname": "t2_3t20nkoj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How did you manage to use llama server with openhands ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0ssma",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752611204,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello !&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to run devstral using llama server, and it&amp;#39;s working fine, i&amp;#39;m using this command to serve the model, as you see I&amp;#39;m using the alias to be able to select it more easily in openhand.&lt;/p&gt;\n\n&lt;p&gt;Then in openhand advanced settings, I tried every prefix in front of my model name like openai, lm_studio, custom and even without even any prefix, litellm cannot access it&lt;/p&gt;\n\n&lt;p&gt;For the endpoint, I tried &lt;a href=\"http://127.0.0.1:8080/v1\"&gt;http://127.0.0.1:8080/v1&lt;/a&gt; and &lt;a href=\"http://127.0.0.1:8080\"&gt;http://127.0.0.1:8080&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;When I try with the openai prefix, it tries to connect to the openai api.&lt;/p&gt;\n\n&lt;p&gt;Did someone here managed to make openhands works with llama server ? &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance and I wish you a good day, take care&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server.exe --model &amp;quot;thisismyfolder\\models\\unsloth\\Devstral-Small-2507-GGUF\\Devstral-Small-2507-UD-Q5_K_XL.gguf&amp;quot; --threads -1 --ctx-size 131072 --cache-type-k q8_0 --n-gpu-layers 99 --seed 3407 --prio 2 --temp 0.15 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 --host 127.0.0.1 --port 8080 --mlock --no-mmap --alias &amp;quot;devstral&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0ssma",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wemos_D1",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0ssma/how_did_you_manage_to_use_llama_server_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0ssma/how_did_you_manage_to_use_llama_server_with/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752611204,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone - there are some **245GB quants (80% size reduction)** for Kimi K2 at https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF. The Unsloth dynamic Q2\\_K\\_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.\n\nPlease use `-ot \".ffn_.*_exps.=CPU\"` to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.\n\nYou need to use either [https://github.com/ggml-org/llama.cpp/pull/14654](https://github.com/ggml-org/llama.cpp/pull/14654) or our fork [https://github.com/unslothai/llama.cpp](https://github.com/unslothai/llama.cpp) to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!\n\nThe suggested parameters are:\n\n    temperature = 0.6\n    min_p = 0.01 (set it to a small number)\n\nDocs has more details: [https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally](https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally)",
          "author_fullname": "t2_5wukhd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 1.8bit Unsloth Dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzps3b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 367,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 367,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752507676,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone - there are some &lt;strong&gt;245GB quants (80% size reduction)&lt;/strong&gt; for Kimi K2 at &lt;a href=\"https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF&lt;/a&gt;. The Unsloth dynamic Q2_K_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.&lt;/p&gt;\n\n&lt;p&gt;Please use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.&lt;/p&gt;\n\n&lt;p&gt;You need to use either &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14654\"&gt;https://github.com/ggml-org/llama.cpp/pull/14654&lt;/a&gt; or our fork &lt;a href=\"https://github.com/unslothai/llama.cpp\"&gt;https://github.com/unslothai/llama.cpp&lt;/a&gt; to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!&lt;/p&gt;\n\n&lt;p&gt;The suggested parameters are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;temperature = 0.6\nmin_p = 0.01 (set it to a small number)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Docs has more details: &lt;a href=\"https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally\"&gt;https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?auto=webp&amp;s=bc3027fa5da20b74e927173e28d8aca06d1918f9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=539e7c53ad8fe4d04c6029c11344ff605d38589a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6263e8d5e0d9129138827f26082b6f9517086361",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=247bab7783d0737b150b5f5183ba9db8a0966436",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3e7dfec653d356a4ccb836ae65b4546e9a5ad00",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a4a1e7fdeac330903838e1542ed085ea53ec142",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff85dc8c4e34c5580c7c7bf51d308d118a9e322f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzps3b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danielhanchen",
          "discussion_type": null,
          "num_comments": 113,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752507676,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://youtu.be/vW30o4U9BFE",
          "author_fullname": "t2_7kg5p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A very nice overview on how llama.cpp quantization works",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m03sh9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752540238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://youtu.be/vW30o4U9BFE\"&gt;https://youtu.be/vW30o4U9BFE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?auto=webp&amp;s=b3e9aec855a9a197fe9619de2626df9b16de04d8",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=617b8e7b6a8328658f045f639d60a0618107415a",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c67f23813dd77438f9dd053b5b49625c7db6c5a8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee9cbfbcf35cda08614a8ac4291b096d6010b0b5",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m03sh9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kooshi_Govno",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752540238,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think that happened. Because Elon Musk forgot or canceled that Grok-2 would be open sourced after Grok-3 was stable. And now Grok-4 but Elon Musk did not open source Grok-2 or even Grok-3. I think Elon Musk is following the OpenAI or ANTHROP\\C. Until now Elon Musk still makes announcements that he will open source Grok-2 and Grok-3 and it is unknown whether Elon Musk will cut off the API for these two models. \n\nEdit : \nSam Atlam : Elon Musk Will Promise That I Will Open Source Grok-2 Once Grok-3 Is Stable. But not Elon Musk doesn't Open-source any model (e.g Grok-2 or Grok-3) and now.\n\nMe : xAI promise Open-source grok-2 or Grok-3?\n\nSam Atlam: xAI is lie. OpenAI release Open-source thinking model soon. Say tuned!\n\n```\nxAI has been take down API Grok-2 text generation. And now Grok-2-vision and Grok-3-mini will take down API.\n```",
          "author_fullname": "t2_7ktr17a6i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grok no more model Open-source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m04ic2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752605116,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752542208,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think that happened. Because Elon Musk forgot or canceled that Grok-2 would be open sourced after Grok-3 was stable. And now Grok-4 but Elon Musk did not open source Grok-2 or even Grok-3. I think Elon Musk is following the OpenAI or ANTHROP\\C. Until now Elon Musk still makes announcements that he will open source Grok-2 and Grok-3 and it is unknown whether Elon Musk will cut off the API for these two models. &lt;/p&gt;\n\n&lt;p&gt;Edit : \nSam Atlam : Elon Musk Will Promise That I Will Open Source Grok-2 Once Grok-3 Is Stable. But not Elon Musk doesn&amp;#39;t Open-source any model (e.g Grok-2 or Grok-3) and now.&lt;/p&gt;\n\n&lt;p&gt;Me : xAI promise Open-source grok-2 or Grok-3?&lt;/p&gt;\n\n&lt;p&gt;Sam Atlam: xAI is lie. OpenAI release Open-source thinking model soon. Say tuned!&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nxAI has been take down API Grok-2 text generation. And now Grok-2-vision and Grok-3-mini will take down API.\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m04ic2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Brilliant_Stock_5137",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752542208,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "IQ4\\\\\\_XS works well for text models. but for visual models, if you ask to recognize images, IQ4\\\\\\_XS are hardly to figure out. I am switching to Q5\\\\\\_K\\\\\\_S.  \n\nfor the example pic, IQ4\\\\\\_XS may fault on gender, clothes, pose, sometimes it even picked tail. 🫨\n\nthe model I tested is this: \\[Qwen2.5-VL-7B-NSFW-Caption-V3\\](https://huggingface.co/bartowski/thesby\\_Qwen2.5-VL-7B-NSFW-Caption-V3-GGUF)",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "seems visual models are more sensitive than text models on quantization loss.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0mcbq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/JNVsqZjBuEnkaAbUB__AEmQgPQGHEBMvpqWKBZ8E99c.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752596769,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;IQ4\\_XS works well for text models. but for visual models, if you ask to recognize images, IQ4\\_XS are hardly to figure out. I am switching to Q5\\_K\\_S.  &lt;/p&gt;\n\n&lt;p&gt;for the example pic, IQ4\\_XS may fault on gender, clothes, pose, sometimes it even picked tail. 🫨&lt;/p&gt;\n\n&lt;p&gt;the model I tested is this: [Qwen2.5-VL-7B-NSFW-Caption-V3](&lt;a href=\"https://huggingface.co/bartowski/thesby%5C_Qwen2.5-VL-7B-NSFW-Caption-V3-GGUF\"&gt;https://huggingface.co/bartowski/thesby\\_Qwen2.5-VL-7B-NSFW-Caption-V3-GGUF&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qd85dzoic2df1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qd85dzoic2df1.png?auto=webp&amp;s=be16dcb0e84214752ca9232af2b2a65dde9c0255",
                  "width": 694,
                  "height": 487
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qd85dzoic2df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0531ff23dea662eacdde47695581f4906327961",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/qd85dzoic2df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e998c01f5b75ce4d9075ace5ad0b67815b9e83e9",
                    "width": 216,
                    "height": 151
                  },
                  {
                    "url": "https://preview.redd.it/qd85dzoic2df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9e8a38ed6df433fb8b26bc5c529db3665aa531b",
                    "width": 320,
                    "height": 224
                  },
                  {
                    "url": "https://preview.redd.it/qd85dzoic2df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32cdec51e3df8f1f763e2bcb86e5569d548b5220",
                    "width": 640,
                    "height": 449
                  }
                ],
                "variants": {},
                "id": "WCAhbhfZ_ONc5yADLHkM81F6K7yo16icw0LM1LtU1EE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0mcbq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0mcbq/seems_visual_models_are_more_sensitive_than_text/",
          "stickied": false,
          "url": "https://i.redd.it/qd85dzoic2df1.png",
          "subreddit_subscribers": 499772,
          "created_utc": 1752596769,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thx 🙏🏻",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What version of Deepseek is being served in Deepseek app as the reasoning model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0vci4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752617262,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thx 🙏🏻&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0vci4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0vci4/what_version_of_deepseek_is_being_served_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0vci4/what_version_of_deepseek_is_being_served_in/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752617262,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi , I created an app in Python to use Langchain to ingest documents and create a vector database using Weaviate \n\nIt works well but when I a query using Open WebUI I see in the docker pipeline logs that it is trying to connect to the Ollama  embedding using localhost not host docker.internal\n\nAny thoughts?\n\nMy configuration is:\nWeaviate, open WebUI, and pipelines containers are in a docker network \n\nOllama is standalone using ollama server app ",
          "author_fullname": "t2_lsixf36sr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open WebUI RAG and pipelines",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0vc09",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752617227,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi , I created an app in Python to use Langchain to ingest documents and create a vector database using Weaviate &lt;/p&gt;\n\n&lt;p&gt;It works well but when I a query using Open WebUI I see in the docker pipeline logs that it is trying to connect to the Ollama  embedding using localhost not host docker.internal&lt;/p&gt;\n\n&lt;p&gt;Any thoughts?&lt;/p&gt;\n\n&lt;p&gt;My configuration is:\nWeaviate, open WebUI, and pipelines containers are in a docker network &lt;/p&gt;\n\n&lt;p&gt;Ollama is standalone using ollama server app &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0vc09",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Wolf-2007",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0vc09/open_webui_rag_and_pipelines/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0vc09/open_webui_rag_and_pipelines/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752617227,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_wkg30tqo1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "XSched: Preemptive Scheduling for Diverse XPUs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 64,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cnzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g84ct0fn40df1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 882,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g84ct0fn40df1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g84ct0fn40df1/DASHPlaylist.mpd?a=1755262516%2CODFjNjg3Y2U1Njg1ZWQ5YTc4MmIwNzQ1MjEwYWVkZWY5ZDliYzc2MTEwZDNlNTEwMzliMzZlNWQzY2FmMjkxMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/g84ct0fn40df1/HLSPlaylist.m3u8?a=1755262516%2CZjA3YjBkMWY3ZWRhM2I5MTAwZmY1MmRmNDRlNTNjOGMzZjg1MjBiZjdiZTBiM2Q3ZTU0NTc0OTc5NzViMGY1Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=140&amp;height=64&amp;crop=140:64,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=2b5d0ca0de620b826f48ad0d0c0a34c290e27f78",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752569498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g84ct0fn40df1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?format=pjpg&amp;auto=webp&amp;s=d40bc6e47e567135be91b9dff7fd5198d041470c",
                  "width": 3052,
                  "height": 1402
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e2d1c9fd4919f27d33757da4b55652f6fd667a1c",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=34068c5143fa7cf767f84da0e5e3fb4fdb860131",
                    "width": 216,
                    "height": 99
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a419572068a6f0ac7e2158867f241559feab41a0",
                    "width": 320,
                    "height": 146
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d05834a11688d2966e4c21c3660370f469ce2284",
                    "width": 640,
                    "height": 293
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c6cd094405941cdcde3a49fcccae07b7ce9eb2fc",
                    "width": 960,
                    "height": 440
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=89a68f8eb3c152af102ca8171454c8b34bb14dc0",
                    "width": 1080,
                    "height": 496
                  }
                ],
                "variants": {},
                "id": "YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0cnzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LibraryNo6067",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cnzs/xsched_preemptive_scheduling_for_diverse_xpus/",
          "stickied": false,
          "url": "https://v.redd.it/g84ct0fn40df1",
          "subreddit_subscribers": 499772,
          "created_utc": 1752569498,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g84ct0fn40df1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 882,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g84ct0fn40df1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g84ct0fn40df1/DASHPlaylist.mpd?a=1755262516%2CODFjNjg3Y2U1Njg1ZWQ5YTc4MmIwNzQ1MjEwYWVkZWY5ZDliYzc2MTEwZDNlNTEwMzliMzZlNWQzY2FmMjkxMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/g84ct0fn40df1/HLSPlaylist.m3u8?a=1755262516%2CZjA3YjBkMWY3ZWRhM2I5MTAwZmY1MmRmNDRlNTNjOGMzZjg1MjBiZjdiZTBiM2Q3ZTU0NTc0OTc5NzViMGY1Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_73rg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Moonshot AI’s open source Kimi K2 outperforms GPT-4 in key benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m013ou",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752533189,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "moonshotai.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://moonshotai.github.io/Kimi-K2/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m013ou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yogthos",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m013ou/moonshot_ais_open_source_kimi_k2_outperforms_gpt4/",
          "stickied": false,
          "url": "https://moonshotai.github.io/Kimi-K2/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752533189,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! I'm new to this space but I'm trying to develop an agent interface that does the following:\n\n\\- Reads through my company's Slack workspace daily for product/company updates\n\n\\- Scours the internet for industry trends in external communities, news sources, etc.\n\n\\- Collects PRs in my company's product on GitHub\n\n\\- References work that myself or other people in my company have already done (so not to suggest duplicates)\n\n\\- Scans competitor sites and socials\n\n  \nEssentially, I do technical marketing for a software company. It's a small company, so it's basically up to me to decide what I work on daily. Most of my work includes creating content, making videos, walkthroughs, supporting developers, and promoting our brand amongst technical crowds.\n\nMy ideal result would be some kind of dashboard that I can check every day, where it has scanned all the resources I noted above and suggest and pre-draft a number of tasks, slack responses, content ideas, etc., based on the latest available changes.\n\nAny advice? Thanks in advance!",
          "author_fullname": "t2_1qdz26r56v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG Agent that tells me what to work on",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0k27c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752591628,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I&amp;#39;m new to this space but I&amp;#39;m trying to develop an agent interface that does the following:&lt;/p&gt;\n\n&lt;p&gt;- Reads through my company&amp;#39;s Slack workspace daily for product/company updates&lt;/p&gt;\n\n&lt;p&gt;- Scours the internet for industry trends in external communities, news sources, etc.&lt;/p&gt;\n\n&lt;p&gt;- Collects PRs in my company&amp;#39;s product on GitHub&lt;/p&gt;\n\n&lt;p&gt;- References work that myself or other people in my company have already done (so not to suggest duplicates)&lt;/p&gt;\n\n&lt;p&gt;- Scans competitor sites and socials&lt;/p&gt;\n\n&lt;p&gt;Essentially, I do technical marketing for a software company. It&amp;#39;s a small company, so it&amp;#39;s basically up to me to decide what I work on daily. Most of my work includes creating content, making videos, walkthroughs, supporting developers, and promoting our brand amongst technical crowds.&lt;/p&gt;\n\n&lt;p&gt;My ideal result would be some kind of dashboard that I can check every day, where it has scanned all the resources I noted above and suggest and pre-draft a number of tasks, slack responses, content ideas, etc., based on the latest available changes.&lt;/p&gt;\n\n&lt;p&gt;Any advice? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0k27c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Habit7971",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0k27c/rag_agent_that_tells_me_what_to_work_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0k27c/rag_agent_that_tells_me_what_to_work_on/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752591628,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With Nvlink or something... Sorry if this question has already sounded before",
          "author_fullname": "t2_uitx4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it possible to get a common memory pool of 48 on two 3090?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0tkly",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752612994,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With Nvlink or something... Sorry if this question has already sounded before&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0tkly",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Andre4s11",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752612994,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For those of you who aren't familiar with SurfSense, it aims to be the **open-source alternative to NotebookLM, Perplexity, or Glean.**\n\nIn short, it's a **Highly Customizable AI Research Agent** that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.\n\nI'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere’s a quick look at what SurfSense offers right now:\n\n📊 **Feature**s\n\n* Supports 100+ LLMs\n* Supports local Ollama or vLLM setups\n* 6000+ Embedding Models\n* Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)\n* Hierarchical Indices (2-tiered RAG setup)\n* Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)\n* Offers a RAG-as-a-Service API Backend\n* 50+ File extensions supported\n\n🎙️ **Podcast**s\n\n* Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)\n* Convert chat conversations into engaging audio\n* Multiple TTS providers supported\n\nℹ️ **External Sources Integration**\n\n* Search engines (Tavily, LinkUp)\n* Slack\n* Linear\n* Notion\n* YouTube videos\n* GitHub\n* Discord\n* ...and more on the way\n\n🔖 **Cross-Browser Extensio**n\n\nThe SurfSense extension lets you save any dynamic webpage you want, including authenticated content.\n\n**Interested in contributing?**\n\nSurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
          "author_fullname": "t2_63zmedmg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Alternative to NotebookLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzw6yu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 106,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 106,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752521799,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you who aren&amp;#39;t familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In short, it&amp;#39;s a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for contributors to help shape the future of SurfSense! If you&amp;#39;re interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt;\n\n&lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt;\n\n&lt;p&gt;📊 &lt;strong&gt;Feature&lt;/strong&gt;s&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Supports 100+ LLMs&lt;/li&gt;\n&lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt;\n&lt;li&gt;6000+ Embedding Models&lt;/li&gt;\n&lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt;\n&lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt;\n&lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt;\n&lt;li&gt;Offers a RAG-as-a-Service API Backend&lt;/li&gt;\n&lt;li&gt;50+ File extensions supported&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;🎙️ &lt;strong&gt;Podcast&lt;/strong&gt;s&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt;\n&lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt;\n&lt;li&gt;Multiple TTS providers supported&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;ℹ️ &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt;\n&lt;li&gt;Slack&lt;/li&gt;\n&lt;li&gt;Linear&lt;/li&gt;\n&lt;li&gt;Notion&lt;/li&gt;\n&lt;li&gt;YouTube videos&lt;/li&gt;\n&lt;li&gt;GitHub&lt;/li&gt;\n&lt;li&gt;Discord&lt;/li&gt;\n&lt;li&gt;...and more on the way&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;🔖 &lt;strong&gt;Cross-Browser Extensio&lt;/strong&gt;n&lt;/p&gt;\n\n&lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you&amp;#39;re welcome to join in.&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/MODSetter/SurfSense\"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?auto=webp&amp;s=344bed7e266c934d23a7957319ca13c4100c57cd",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4cc9007548328d59c5d49b07997ca37e0c33349",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e50ed04f44d9e471185d98e483ce202837b37726",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9fdc40a870cb735030c8be40d94ff73f8fae0e5d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b3d2dfc8cba3d61bcd473981e8d0d2e8cf77005",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3aee46322a0a56f4709de62484df290bd68312e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=725ea9d297e60b5f56be92de035c5f6c2bba3d2d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lzw6yu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Uiqueblhats",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752521799,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the author’s insights genuinely thought-provoking. The original Chinese version is [here](https://bigeagle.me/2025/07/kimi-k2/)—feel free to read it in full (and of course you can use Kimi K2 as your translator). Here’s my own distilled summary of the main points:\n\n\n\n• Beyond chatbots: Kimi K2 experiments with an “artifact-first” interaction model that has the AI immediately build interactive front-end deliverables—PPT-like pages, diagrams, even mini-games—rather than simply returning markdown text.\n\n• Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.\n\n• What makes an agentic model: A minimal loop—think, choose tools, observe results, iterate—can be learned from synthetic trajectories. Today’s agent abilities are early-stage; the next pre-training wave still holds plenty of upside.\n\n• Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit “hacky” hidden pipelines, forcing genuinely strong, general models—exactly what an AGI-oriented startup needs.\n\n• Marketing controversies &amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1’s viral rise proved that raw model quality markets itself and validates the “foundation-model-first” path.\n\n• Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.\n\n\n\nFrom the entire blog, this is the paragraph I loved the most:\n\n&gt;A while ago, ‘Agent’ products were all the rage. I kept hearing people say that Kimi shouldn’t compete on large models and should focus on Agents instead. Let me be clear: **the vast majority of Agent products are nothing without Claude behind them.** Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we don’t keep pushing that ceiling higher, I won’t stay here a single extra day.\n\n&gt;Chasing AGI is an extremely narrow, perilous bridge—there’s no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, ‘As an investor, I care about the ROI of AI applications.’ In that moment I knew the company he founded wouldn’t last long.",
          "author_fullname": "t2_sqi8xxun",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "After Kimi K2 Is Released: No Longer Just a ChatBot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzm645",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 330,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 330,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752499086,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the author’s insights genuinely thought-provoking. The original Chinese version is &lt;a href=\"https://bigeagle.me/2025/07/kimi-k2/\"&gt;here&lt;/a&gt;—feel free to read it in full (and of course you can use Kimi K2 as your translator). Here’s my own distilled summary of the main points:&lt;/p&gt;\n\n&lt;p&gt;• Beyond chatbots: Kimi K2 experiments with an “artifact-first” interaction model that has the AI immediately build interactive front-end deliverables—PPT-like pages, diagrams, even mini-games—rather than simply returning markdown text.&lt;/p&gt;\n\n&lt;p&gt;• Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.&lt;/p&gt;\n\n&lt;p&gt;• What makes an agentic model: A minimal loop—think, choose tools, observe results, iterate—can be learned from synthetic trajectories. Today’s agent abilities are early-stage; the next pre-training wave still holds plenty of upside.&lt;/p&gt;\n\n&lt;p&gt;• Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit “hacky” hidden pipelines, forcing genuinely strong, general models—exactly what an AGI-oriented startup needs.&lt;/p&gt;\n\n&lt;p&gt;• Marketing controversies &amp;amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1’s viral rise proved that raw model quality markets itself and validates the “foundation-model-first” path.&lt;/p&gt;\n\n&lt;p&gt;• Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.&lt;/p&gt;\n\n&lt;p&gt;From the entire blog, this is the paragraph I loved the most:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;A while ago, ‘Agent’ products were all the rage. I kept hearing people say that Kimi shouldn’t compete on large models and should focus on Agents instead. Let me be clear: &lt;strong&gt;the vast majority of Agent products are nothing without Claude behind them.&lt;/strong&gt; Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we don’t keep pushing that ceiling higher, I won’t stay here a single extra day.&lt;/p&gt;\n\n&lt;p&gt;Chasing AGI is an extremely narrow, perilous bridge—there’s no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, ‘As an investor, I care about the ROI of AI applications.’ In that moment I knew the company he founded wouldn’t last long.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzm645",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nekofneko",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752499086,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_hdcx5ggfg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzv16g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 106,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 106,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=684dded7ebf0cced3ec460c9dda8f551b9ecbd73",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752519240,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "nytimes.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?auto=webp&amp;s=211ff5c9d8860c633734a0f69515f881de8905e4",
                  "width": 1050,
                  "height": 550
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f431ed3ef795de81f0d9be2452ed2466f4727f88",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c728fd0b47256c06b7e53063606348710b74999",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a1dc415541c74d1ee2dd3620b8e6997e56ad7f2",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a0ebffa84a0071645409fce2ba2a7d33bd6a731",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=361e53f2aa166522931efd9533bd8c76685cfc5a",
                    "width": 960,
                    "height": 502
                  }
                ],
                "variants": {},
                "id": "62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzv16g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "showmeufos",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/",
          "stickied": false,
          "url": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "subreddit_subscribers": 499772,
          "created_utc": 1752519240,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Last week, a small group of top members of the lab, including Alexandr Wang, 28, Meta’s new chief A.I. officer, discussed abandoning the company’s most powerful open source A.I. model, called Behemoth, in favor of developing a closed model, two people with knowledge of the matter said.\n\nMeta had finished feeding in data to improve its Behemoth model, a process known as “training,” but has delayed its release because of poor internal performance, said the people with knowledge of the matter, who were not authorized to discuss private conversations. After the company announced the formation of the superintelligence lab last month, teams working on the Behemoth model — which is known as a “frontier” model — stopped running new tests on it, one of the people said.\n\n\n",
          "author_fullname": "t2_u398xzta",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 127,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m041m4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7b2503awF-4b2icSl8JuZh6vDtrrHCXba5mxqJvk1zA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752540939,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last week, a small group of top members of the lab, including Alexandr Wang, 28, Meta’s new chief A.I. officer, discussed abandoning the company’s most powerful open source A.I. model, called Behemoth, in favor of developing a closed model, two people with knowledge of the matter said.&lt;/p&gt;\n\n&lt;p&gt;Meta had finished feeding in data to improve its Behemoth model, a process known as “training,” but has delayed its release because of poor internal performance, said the people with knowledge of the matter, who were not authorized to discuss private conversations. After the company announced the formation of the superintelligence lab last month, teams working on the Behemoth model — which is known as a “frontier” model — stopped running new tests on it, one of the people said.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3f68h6pzrxcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?auto=webp&amp;s=0896b8b618e97942c4ee1042d76206fda02632e1",
                  "width": 1080,
                  "height": 984
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d2ac4cc26b27343c96b0a03f825276999d5b174",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400750301dac83d5f73809374e9b7b466d85b02e",
                    "width": 216,
                    "height": 196
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=06576965338eaac8ae34bb2407956757d30b744d",
                    "width": 320,
                    "height": 291
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b646755e2c619c761965d3179957a1ab2c65c147",
                    "width": 640,
                    "height": 583
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84a06d1a5914a6525f604045a7fc9a8b7f0954c9",
                    "width": 960,
                    "height": 874
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06d3e9060e92dd36bfb8b5c6dde39bd596967ff8",
                    "width": 1080,
                    "height": 984
                  }
                ],
                "variants": {},
                "id": "yVwjvgqtLycnuN38b4xHyA_D8ioyPBQ4lGHqp1UdmK4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m041m4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sunshinecheung",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m041m4/metas_new_superintelligence_lab_is_discussing/",
          "stickied": false,
          "url": "https://i.redd.it/3f68h6pzrxcf1.jpeg",
          "subreddit_subscribers": 499772,
          "created_utc": 1752540939,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/Rivridis/Assistant-Client](https://github.com/Rivridis/Assistant-Client)\n\nOver the past few years, I have been developing a AI function calling agent, that can perfectly call functions with models as small as 3B or 7B parameters. Most of the frameworks I found while researching this topic just did not work with smaller, and non finetuned models. I tried llama-cpp openai, langchain and ollama but the function call success rate was disappointing for these small models.\n\nThe app can work with any LLM, no specific function calling finetunes needed. I took the suggestions from all the comments, and ported the UI to pyside from gradio. The app now comes in a desktop app format, and supports OpenAI API, so any models can be used. The models can be served from KoboldCPP or similar endpoints.\n\nThe current functions that it supports are search, music as well as weather. I tried to make it as easy to extend as possible, so feel free to add functions on top of it for your own use cases.\n\nIt also has a basic PDF query mode, as well as a code editor mode. \n\nThanks for all the support! If anyone has further ideas or improvements, please let me know. If anyone wants a tutorial or a guide, I shall provide that too.",
          "author_fullname": "t2_gs1v5r62",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Assistant Agent with function calling - Update 2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0drwa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752573750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/Rivridis/Assistant-Client\"&gt;https://github.com/Rivridis/Assistant-Client&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Over the past few years, I have been developing a AI function calling agent, that can perfectly call functions with models as small as 3B or 7B parameters. Most of the frameworks I found while researching this topic just did not work with smaller, and non finetuned models. I tried llama-cpp openai, langchain and ollama but the function call success rate was disappointing for these small models.&lt;/p&gt;\n\n&lt;p&gt;The app can work with any LLM, no specific function calling finetunes needed. I took the suggestions from all the comments, and ported the UI to pyside from gradio. The app now comes in a desktop app format, and supports OpenAI API, so any models can be used. The models can be served from KoboldCPP or similar endpoints.&lt;/p&gt;\n\n&lt;p&gt;The current functions that it supports are search, music as well as weather. I tried to make it as easy to extend as possible, so feel free to add functions on top of it for your own use cases.&lt;/p&gt;\n\n&lt;p&gt;It also has a basic PDF query mode, as well as a code editor mode. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the support! If anyone has further ideas or improvements, please let me know. If anyone wants a tutorial or a guide, I shall provide that too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0drwa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rivridis",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0drwa/ai_assistant_agent_with_function_calling_update_2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0drwa/ai_assistant_agent_with_function_calling_update_2/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752573750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys,\nI'm looking to build my \"first PC\" (not my first, but I currently only have a bad notebook), rn I'm stuck on deciding the GPU part. \nI'm a electronic engineer major and would like to have access to AI workload for a few projects (mostly Computer Vision and LLMs for tool control and human/machine interaction). \n\nI'm currently between 2 GPU's:\n\nRTX 5060 ti 16gb - R$3400.00($610.00)\n\nRTX 5070 12gb - R$4000.00($715.00)\n\nYes, GPUs are quite expensive in my country...\n\nSo considering I will use the PC for both gaming/game dev and AI workload, what would be the recommendation for GPU. Is it better to go with the 16gb version GPU or with Quantization the 40% improved performance on 5070 processing power is better?\n\nEdit: Text structure Formatting ",
          "author_fullname": "t2_4h864s27",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU for local LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0fp0r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752580244,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,\nI&amp;#39;m looking to build my &amp;quot;first PC&amp;quot; (not my first, but I currently only have a bad notebook), rn I&amp;#39;m stuck on deciding the GPU part. \nI&amp;#39;m a electronic engineer major and would like to have access to AI workload for a few projects (mostly Computer Vision and LLMs for tool control and human/machine interaction). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently between 2 GPU&amp;#39;s:&lt;/p&gt;\n\n&lt;p&gt;RTX 5060 ti 16gb - R$3400.00($610.00)&lt;/p&gt;\n\n&lt;p&gt;RTX 5070 12gb - R$4000.00($715.00)&lt;/p&gt;\n\n&lt;p&gt;Yes, GPUs are quite expensive in my country...&lt;/p&gt;\n\n&lt;p&gt;So considering I will use the PC for both gaming/game dev and AI workload, what would be the recommendation for GPU. Is it better to go with the 16gb version GPU or with Quantization the 40% improved performance on 5070 processing power is better?&lt;/p&gt;\n\n&lt;p&gt;Edit: Text structure Formatting &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0fp0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GabePs",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0fp0r/gpu_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0fp0r/gpu_for_local_llm/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752580244,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've noticed that if you begin a chat with a reasoning model like Qwen 3 and then in subsequent messages switch to a different non-reasoning model (such as Gemma 3 12b or Devstral 2507) the non-reasoning model will sometimes also generate reasoning tokens and respond with a final answer afterwards like it was trained to perform reasoning. This is also without any system prompt.",
          "author_fullname": "t2_i305y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Non-reasoning models adopting reasoning behavior from previous messages",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m06nhe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752548299,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed that if you begin a chat with a reasoning model like Qwen 3 and then in subsequent messages switch to a different non-reasoning model (such as Gemma 3 12b or Devstral 2507) the non-reasoning model will sometimes also generate reasoning tokens and respond with a final answer afterwards like it was trained to perform reasoning. This is also without any system prompt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m06nhe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thedudely1",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752548299,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Don't our ideas and \"novel\" methodologies (the way we build on top of existing methods) get used for training the next set of llms?\n\nMore to the point, Anthropic's Claude, which is meant to be one of the safest close-models to use, has these certifications: SOC 2 Type I&amp;II, ISO 27001:2022, ISO/IEC 42001:2023. With SOC 2's \"**Confidentiality\"** criterion addressing how organisations protect sensitive information that is restricted to \"certain parties\", I find that to be the only relation to protecting our IP which does not sound robust. I hope someone answers with more knowledge than me and comforts that miserable dread of us just working for big brother.",
          "author_fullname": "t2_h7sh5c8pt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "As a developer vibe coding with intellectual property...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0pjk9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752603886,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t our ideas and &amp;quot;novel&amp;quot; methodologies (the way we build on top of existing methods) get used for training the next set of llms?&lt;/p&gt;\n\n&lt;p&gt;More to the point, Anthropic&amp;#39;s Claude, which is meant to be one of the safest close-models to use, has these certifications: SOC 2 Type I&amp;amp;II, ISO 27001:2022, ISO/IEC 42001:2023. With SOC 2&amp;#39;s &amp;quot;&lt;strong&gt;Confidentiality&amp;quot;&lt;/strong&gt; criterion addressing how organisations protect sensitive information that is restricted to &amp;quot;certain parties&amp;quot;, I find that to be the only relation to protecting our IP which does not sound robust. I hope someone answers with more knowledge than me and comforts that miserable dread of us just working for big brother.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0pjk9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Short-Cobbler-901",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0pjk9/as_a_developer_vibe_coding_with_intellectual/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0pjk9/as_a_developer_vibe_coding_with_intellectual/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752603886,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a 7900xt and 32gb of ddr5, I am planning on adding an mi50 32gb to my system, do I need to upgrade my ram for this?\n\nWeird situation but my knowledge of pc building is mostly centred around gaming hardware, and this scenario basically never happens in that context.\n\nWill I need to upgrade my ram in order for llms to load properly? I’ve heard that the model is loaded into system ram then into vram, if I don’t have enough system ram, does it just not work?\n\n",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you have more vram than system ram?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0jeyu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752590118,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 7900xt and 32gb of ddr5, I am planning on adding an mi50 32gb to my system, do I need to upgrade my ram for this?&lt;/p&gt;\n\n&lt;p&gt;Weird situation but my knowledge of pc building is mostly centred around gaming hardware, and this scenario basically never happens in that context.&lt;/p&gt;\n\n&lt;p&gt;Will I need to upgrade my ram in order for llms to load properly? I’ve heard that the model is loaded into system ram then into vram, if I don’t have enough system ram, does it just not work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0jeyu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/",
          "subreddit_subscribers": 499772,
          "created_utc": 1752590118,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}