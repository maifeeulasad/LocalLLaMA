{
  "kind": "Listing",
  "data": {
    "after": "t3_1mgfuf3",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally got to finish a weekend project from a couple of months ago. \n\nThis is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).\n\nFull source and configuration guide is on GitHub: [https://github.com/av/unhype](https://github.com/av/unhype) ",
          "author_fullname": "t2_o7p5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Use local LLM to neutralise the headers on the web",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkiti",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bd9e9e",
          "ups": 360,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756862285%2CZGQ0MDBkNzI4ODFmZmM3YTNjMmQ5NGEwMzQyNmMxZGEzMmQxNTI2NzE4N2YzZTg1YzZiYjY1ZmE3MzRjZDIwZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756862285%2CZGVhOGMwN2ZkMmI3MGM1YzZmNWVjMDQ4ZGUwODZlNTEzNmFkMmQxYmUwODRkMTFkNjM1ODMwMTM5MzU0YjQxOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 360,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=140&amp;height=89&amp;crop=140:89,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f692b7d05bb54cb77646e5499a279385018af33e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Alpaca"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754230985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally got to finish a weekend project from a couple of months ago. &lt;/p&gt;\n\n&lt;p&gt;This is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).&lt;/p&gt;\n\n&lt;p&gt;Full source and configuration guide is on GitHub: &lt;a href=\"https://github.com/av/unhype\"&gt;https://github.com/av/unhype&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/niaha18uctgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?format=pjpg&amp;auto=webp&amp;s=e3f711ff053fccc48764a295b7ef68533d8f7f9a",
                  "width": 1688,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=47b5550c7dd6476d0d3e442d2ca5c2a382f74e92",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8828ff7d3eb9b270e88f2ec7ff4973818f896e8e",
                    "width": 216,
                    "height": 138
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bc01544a8c25b4174c98c4519803a567aa98c887",
                    "width": 320,
                    "height": 204
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9c9498bdb1588d50d25b53d82635a1092a82f4ef",
                    "width": 640,
                    "height": 409
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=402ac5eba7ed08a00bc9e012b6a4bd69a46e4608",
                    "width": 960,
                    "height": 614
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=374d1819675e3e0f60cf4790383435325e543d5f",
                    "width": 1080,
                    "height": 690
                  }
                ],
                "variants": {},
                "id": "NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Alpaca",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgkiti",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Everlier",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/",
          "stickied": false,
          "url": "https://v.redd.it/niaha18uctgf1",
          "subreddit_subscribers": 509624,
          "created_utc": 1754230985,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756862285%2CZGQ0MDBkNzI4ODFmZmM3YTNjMmQ5NGEwMzQyNmMxZGEzMmQxNTI2NzE4N2YzZTg1YzZiYjY1ZmE3MzRjZDIwZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756862285%2CZGVhOGMwN2ZkMmI3MGM1YzZmNWVjMDQ4ZGUwODZlNTEzNmFkMmQxYmUwODRkMTFkNjM1ODMwMTM5MzU0YjQxOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\n\nHorizon Beta is OpenAI",
          "author_fullname": "t2_h0z59zgo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 40,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "m30x07j54vgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 31,
                  "x": 108,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=588d4a9c04c05e66d8b3c8ba8b93d7c827f14b20"
                },
                {
                  "y": 62,
                  "x": 216,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=446e608e2e3bf06827203dc313f98d8ec1704b9d"
                },
                {
                  "y": 92,
                  "x": 320,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f5cab485a7892fcf45b7a6bf1ecfa12925d59cf"
                },
                {
                  "y": 184,
                  "x": 640,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fda4acd30425eae2bddb9d9f82ce6241eaf9641"
                },
                {
                  "y": 276,
                  "x": 960,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e8f560871bebab02cfbd9ae3af3f79ac6d3819a"
                },
                {
                  "y": 310,
                  "x": 1080,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6c803879173681737db389051fa4d599d2f19a0"
                }
              ],
              "s": {
                "y": 364,
                "x": 1266,
                "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a"
              },
              "id": "m30x07j54vgf1"
            }
          },
          "name": "t3_1mgtboa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 81,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 81,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/inyC6dLBuynY6QZPK34zrtaIVdVEKly7ofVVWlBmYW4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754252212,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\"&gt;https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Horizon Beta is OpenAI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgtboa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MiddleLobster9191",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754252212,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are they really gonna train a model that's absolutely useless to give to us?",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why doesn't \"OpenAI\" just release one of the models they already have? Like 3.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgiyg4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 213,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 213,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754226851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are they really gonna train a model that&amp;#39;s absolutely useless to give to us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgiyg4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 180,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754226851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.\n\n[https://huggingface.co/trillionlabs/Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)",
          "author_fullname": "t2_1ug5bi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This might be the largest un-aligned open-source model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgky8g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 144,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 144,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754232080,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?auto=webp&amp;s=b45653eadbcba17c38d2f4f2e15f6ebd41ec7c72",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7a80c31c557591f18bda1f387961a8fe38f053e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=13c5c597d54b6cfe9cc7d137d17b8c65f8c6db95",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=10f045816aeb8c03203c2c64cc8f2065cb392da4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cad5dd9413d8196f56dd930d3b4333ee0351d472",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37853a1aca067c019b4d7c06a11c66190e6bc001",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f4fc9332f1425fd05814a4fb77159385e0f2d89",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgky8g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jshin49",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754232080,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "They said they're refining it months ago. Possibly timing to coincide with OpenAI's drop? Would be epic, I'm a fan of both. Especially if OpenAI's is not a reasoning model.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When DeepSeek r2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgny8p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "transparent",
          "ups": 104,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 104,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/V9pPhmLOwBjBig1Mp88kf2vrbPkb0OVif8im0hRtZXs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754239452,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They said they&amp;#39;re refining it months ago. Possibly timing to coincide with OpenAI&amp;#39;s drop? Would be epic, I&amp;#39;m a fan of both. Especially if OpenAI&amp;#39;s is not a reasoning model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/dz0i0w1j2ugf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?auto=webp&amp;s=6cf4a209d32163902a3fb91d5a06108b4ebb9e61",
                  "width": 1080,
                  "height": 654
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bab256437dc4b2ab50c0bfefc751721bae4de7c5",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9916573c0160c0ddd9eb66a28ab87d51d07207d6",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a55f67b3df278451d7c98ae260464229b3ad11fa",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aaef5f3c86c4f7340d2367eea7fce60751451a94",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3d7f058cca6ec2b2a9fcbd009eadd099423f494",
                    "width": 960,
                    "height": 581
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f8bd24556666c1de0f3c211632b5ace09b00eb99",
                    "width": 1080,
                    "height": 654
                  }
                ],
                "variants": {},
                "id": "lqAfsgD1hr66i0LHoQ3Cw_QWosWkvNCmE0O1wxkWap4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgny8p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/",
          "stickied": false,
          "url": "https://i.redd.it/dz0i0w1j2ugf1.jpeg",
          "subreddit_subscribers": 509624,
          "created_utc": 1754239452,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🧠 Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I’ve implemented after Gemma 🚀. This was a major milestone for me, especially since there’s no open-source implementation of Qwen 2 available online (at least none I could find).\n\nWhat makes this build special:\n✅ Implemented without access to source code\n📖 Based entirely on the Qwen 1 &amp; Qwen 2 research papers\n🧱 Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n⚠️ Does not support Mixture of Experts (MoE) yet\n\nThis project pushed my understanding of transformer architectures even further, and I’m excited to keep going.\nIf you're into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!\n\nSource code: https://github.com/introlix/Swiftlet\nKaggle: https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet",
          "author_fullname": "t2_6qpq9avr5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reimplemention of Qwen 2 from scratch",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgpb8t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 72,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 72,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754242723,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🧠 Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I’ve implemented after Gemma 🚀. This was a major milestone for me, especially since there’s no open-source implementation of Qwen 2 available online (at least none I could find).&lt;/p&gt;\n\n&lt;p&gt;What makes this build special:\n✅ Implemented without access to source code\n📖 Based entirely on the Qwen 1 &amp;amp; Qwen 2 research papers\n🧱 Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n⚠️ Does not support Mixture of Experts (MoE) yet&lt;/p&gt;\n\n&lt;p&gt;This project pushed my understanding of transformer architectures even further, and I’m excited to keep going.\nIf you&amp;#39;re into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/introlix/Swiftlet\"&gt;https://github.com/introlix/Swiftlet&lt;/a&gt;\nKaggle: &lt;a href=\"https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet\"&gt;https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?auto=webp&amp;s=519a96c79f13a619e42513cc0f904d9b36729fa1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09e4e357ff6f03ec51f0d4d875169c2822efb899",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a804dcd73acb0eafcbf3c98a337b04b0330590",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6460444e14680b3ebc1e71bebe27f5eaaab08b28",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=142de401ddac2c07588c3df4af6a9a88a8c43665",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=730308dc3a494b5e493c8cc6e298f0435aba1498",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=798ec017daa864fb62d31aa0cd92c8316b840da9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgpb8t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CodingWithSatyam",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754242723,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fp657",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmx8w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 87,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=64baffecac79129329accd5dfef00560cde30027",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754236913,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/randombk/chatterbox-vllm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?auto=webp&amp;s=f061762d17683d4e88608dbfe355d57e45d90ad5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d660bf476941abc2978684579558acc15bd0d2e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f08036e019ea53656d9f429e46be530e96c0ed23",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eac38ddf9a5c589c5a861f8aafada6fe73427034",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=50d687fd0b27b1fc30b1175e432b4518d7d1f15d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=185870ef47db93a269dea9dfe0a0629b16de6b64",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df00e364b0a49f0fb368440e176f32ee5e39b351",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgmx8w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dlp_randombk",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/",
          "stickied": false,
          "url": "https://github.com/randombk/chatterbox-vllm",
          "subreddit_subscribers": 509624,
          "created_utc": 1754236913,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.\n\nI think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it's about [commoditizing your complement](https://gwern.net/complement). But for many of these companies, which are pure play AI Labs, this simply does not hold.\n\nSo the question remains, why are they doing this?\n\nOne theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is [a precedent](https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838) of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.\n\nIf this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don't find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.\n\nWhat prompted this question, was the [recent interview](https://youtu.be/mYDSSRS-B5U?t=2203) with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don't find his response very compelling).\n\n\\---\n\nOne aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?",
          "author_fullname": "t2_garopjsj1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are Chinese LLM companies effectively price dumping?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjlek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 132,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 132,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754232034,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754228603,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.&lt;/p&gt;\n\n&lt;p&gt;I think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it&amp;#39;s about &lt;a href=\"https://gwern.net/complement\"&gt;commoditizing your complement&lt;/a&gt;. But for many of these companies, which are pure play AI Labs, this simply does not hold.&lt;/p&gt;\n\n&lt;p&gt;So the question remains, why are they doing this?&lt;/p&gt;\n\n&lt;p&gt;One theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is &lt;a href=\"https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838\"&gt;a precedent&lt;/a&gt; of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.&lt;/p&gt;\n\n&lt;p&gt;If this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don&amp;#39;t find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.&lt;/p&gt;\n\n&lt;p&gt;What prompted this question, was the &lt;a href=\"https://youtu.be/mYDSSRS-B5U?t=2203\"&gt;recent interview&lt;/a&gt; with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don&amp;#39;t find his response very compelling).&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;One aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?auto=webp&amp;s=537c5cea0430f04039f256f31d55847ec27c39b6",
                  "width": 1238,
                  "height": 1400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3562c0af61eb68438add188a0ebb448f79b54657",
                    "width": 108,
                    "height": 122
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51a1f9e5bee46295e1d5ac9dc517b0097b9fa489",
                    "width": 216,
                    "height": 244
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=112817eb7860f41d0e5f471023d0200983bb9f91",
                    "width": 320,
                    "height": 361
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=577450661689dd64b4c310d0803093687dd84688",
                    "width": 640,
                    "height": 723
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcdc0e42b1e5a63bb47ad4fa640fbc0fc006cd5c",
                    "width": 960,
                    "height": 1085
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dc589f590b2d3181d4aca8b2fcc7ea75fd5cda5e",
                    "width": 1080,
                    "height": 1221
                  }
                ],
                "variants": {},
                "id": "Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgjlek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "uutnt",
          "discussion_type": null,
          "num_comments": 168,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754228603,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgnwnx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=e35cde9b2565c154a06b4a4ec2a056512f465e28",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754239344,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?auto=webp&amp;s=c1ed5f3fc1bf6e79ce85d0034c3222d7952acbd1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=165ab4b12c90bfe025b46debae13b140652c63d2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=47d3bbf5bd07f507b8f71659fd9562dc4d194b90",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b91013b0a9041e7c6c25c022439d1503d608d9af",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7344e67fe48dc6a6f67623605b3dc51b204d189",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=996d51aa65f7fae5b2ee3e5ffbe787ce0b8cfdcd",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4499831a246669c1b31de509f9c28678094e6e6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgnwnx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/",
          "stickied": false,
          "url": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4",
          "subreddit_subscribers": 509624,
          "created_utc": 1754239344,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " WHAT THE DEVIL?\n\nAnother open model outperforms closed ones!  \nXBai o4 beats OpenAI o3-mini and *confidently* beats Anthropic's Claude Opus.\n\n•Parameters: 32.8 B\n•Training: Long-CoT RL + Process Reward Learning (SPRM)\n•Benchmarks (High-Modus):\n•AIME24: 86.5\n•AIME25: 77.9\n•LiveCodeBench v5: 67.2\n•C-EVAL: 89.7\n\n🔗Open source weights: https://huggingface.co/MetaStoneTec/XBai-o4",
          "author_fullname": "t2_dmji1c74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "XBai-04 Is It Real?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "idu2e1ngesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a2a62a8bef0a6d00f8049f16c8b54e8aa48b974"
                },
                {
                  "y": 124,
                  "x": 216,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=924e4b76f3498ca5e7d4ab97c08a75b6fb377dc1"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82f9fc643d2975e8481a615c1f5a19b5cb453032"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b611aef0c4759ebf456a134bc82a5fd8065b9c5a"
                },
                {
                  "y": 551,
                  "x": 960,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f73f5b3bcc334e135595851ef884c4997ac617"
                },
                {
                  "y": 620,
                  "x": 1080,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e786e475e1a54e20bdba7fb55d1e03e02789639"
                }
              ],
              "s": {
                "y": 704,
                "x": 1226,
                "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1226&amp;format=pjpg&amp;auto=webp&amp;s=5b2baf561ff73bb0e8b4b5708cd383ce9ed6f372"
              },
              "id": "idu2e1ngesgf1"
            },
            "q3imyykgesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8dbd1ac74caccea54654d6ebe571fab617928272"
                },
                {
                  "y": 158,
                  "x": 216,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c6af0dfc3c0aaa092cea1507485efebac28962f"
                },
                {
                  "y": 235,
                  "x": 320,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=602c9e9684804598a1e5221dbb4d23ce3610098a"
                },
                {
                  "y": 471,
                  "x": 640,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e073695ec8c765d1252320b4c7a5e0bd07547b82"
                },
                {
                  "y": 706,
                  "x": 960,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d48c30f79f06380c415eb3dc8e2120642d96e240"
                },
                {
                  "y": 794,
                  "x": 1080,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d39f9af5c4e09886e80918a8161ae1315aa4c7b"
                }
              ],
              "s": {
                "y": 942,
                "x": 1280,
                "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=e8846d514b560df6c44bca9b5375f5eed938412c"
              },
              "id": "q3imyykgesgf1"
            }
          },
          "name": "t3_1mggku0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 174,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "q3imyykgesgf1",
                "id": 720170802
              },
              {
                "caption": "",
                "media_id": "idu2e1ngesgf1",
                "id": 720170803
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 174,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ybTHvU3e25DWWBXsMwOWfbuGc2dzpO5QuzWGZyUC65s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754219245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;WHAT THE DEVIL?&lt;/p&gt;\n\n&lt;p&gt;Another open model outperforms closed ones!&lt;br/&gt;\nXBai o4 beats OpenAI o3-mini and &lt;em&gt;confidently&lt;/em&gt; beats Anthropic&amp;#39;s Claude Opus.&lt;/p&gt;\n\n&lt;p&gt;•Parameters: 32.8 B\n•Training: Long-CoT RL + Process Reward Learning (SPRM)\n•Benchmarks (High-Modus):\n•AIME24: 86.5\n•AIME25: 77.9\n•LiveCodeBench v5: 67.2\n•C-EVAL: 89.7&lt;/p&gt;\n\n&lt;p&gt;🔗Open source weights: &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mggku0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mggku0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ordinary_Mud7430",
          "discussion_type": null,
          "num_comments": 91,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mggku0",
          "subreddit_subscribers": 509624,
          "created_utc": 1754219245,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on:  I tried to reproduce the Flappy Bird game featured in the [z.ai/blog/glm-4.5](http://z.ai/blog/glm-4.5) blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there's no way through.\n\nI gave up on the model for a while, thinking that it was due to the 3-bit quant.  But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt.  This not only eliminated the \"thinking\" part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.\n\nCan anyone with 4, 6 or 8 bit mlx version verify if they have this problem?  Here's the exact prompt: \"Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.\"\n\nPS.  I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio.  Also, Qwen3-Coder-30B-A3B (unlsoth Q8\\_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.",
          "author_fullname": "t2_3xif6p3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Air Produces Better Code Without Thinking, Using 3-bit MLX (/nothink)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv53t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256522,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on:  I tried to reproduce the Flappy Bird game featured in the &lt;a href=\"http://z.ai/blog/glm-4.5\"&gt;z.ai/blog/glm-4.5&lt;/a&gt; blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there&amp;#39;s no way through.&lt;/p&gt;\n\n&lt;p&gt;I gave up on the model for a while, thinking that it was due to the 3-bit quant.  But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt.  This not only eliminated the &amp;quot;thinking&amp;quot; part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.&lt;/p&gt;\n\n&lt;p&gt;Can anyone with 4, 6 or 8 bit mlx version verify if they have this problem?  Here&amp;#39;s the exact prompt: &amp;quot;Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;PS.  I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio.  Also, Qwen3-Coder-30B-A3B (unlsoth Q8_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgv53t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jcmyang",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754256522,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vcawomd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 110,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgt2om",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e1348s852vgf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 910,
              "scrubber_media_url": "https://v.redd.it/e1348s852vgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e1348s852vgf1/DASHPlaylist.mpd?a=1756862285%2CMGJkODUxOWFmZWNkNDhlYjJkNDlkOTJlMzM5MTExYWIwYWM2YTJjZDJlNjhiZmM0NmQxYzhjODczNjk2OGU3ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 36,
              "hls_url": "https://v.redd.it/e1348s852vgf1/HLSPlaylist.m3u8?a=1756862285%2CZWI2MmQ1MjQ0MGQ1ZTlmZjI1OGRkZGUzMzU5ZDJhZTQyZmZhNTA5YmJjYmM2Nzk2OWFjMDRhMTJkOTg3ZTEwNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=140&amp;height=110&amp;crop=140:110,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=abdd574d8f67b63e995dc11eb3cd4f8e74e11bb9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754251636,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e1348s852vgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?format=pjpg&amp;auto=webp&amp;s=0e7a1d12b1a2a1acf19fdfc0f91b16be267107fa",
                  "width": 1350,
                  "height": 1068
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8aff50a70e5cb53b7b38c9b35a93218ab06b529e",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5e621992a0c2958c031c79fe79c2a214c0cc8d5",
                    "width": 216,
                    "height": 170
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1edee9ae25cab3880fe29e9ac723b8339f6cebcf",
                    "width": 320,
                    "height": 253
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5bb59fb0f5c8e7a4e248e1068b8aa841bfa37998",
                    "width": 640,
                    "height": 506
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d85e99a79f17e7ec33873066c7eb6394c4cc7a5e",
                    "width": 960,
                    "height": 759
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ee9e27ab710d36f97c624d0fec564a4f4b5b127",
                    "width": 1080,
                    "height": 854
                  }
                ],
                "variants": {},
                "id": "dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mgt2om",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "onil_gova",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/",
          "stickied": false,
          "url": "https://v.redd.it/e1348s852vgf1",
          "subreddit_subscribers": 509624,
          "created_utc": 1754251636,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e1348s852vgf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 910,
              "scrubber_media_url": "https://v.redd.it/e1348s852vgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e1348s852vgf1/DASHPlaylist.mpd?a=1756862285%2CMGJkODUxOWFmZWNkNDhlYjJkNDlkOTJlMzM5MTExYWIwYWM2YTJjZDJlNjhiZmM0NmQxYzhjODczNjk2OGU3ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 36,
              "hls_url": "https://v.redd.it/e1348s852vgf1/HLSPlaylist.m3u8?a=1756862285%2CZWI2MmQ1MjQ0MGQ1ZTlmZjI1OGRkZGUzMzU5ZDJhZTQyZmZhNTA5YmJjYmM2Nzk2OWFjMDRhMTJkOTg3ZTEwNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA's \"Highly Optimistic\" DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgis6h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=293dff94e206d07e95b1ceb35d306f084972280b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754226361,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?auto=webp&amp;s=761d7765165ce23883e3ea7e5265165d93fd7502",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=462a9746ffb12643f31808d38538f4c0ea76b555",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f437e173d256a303cec220b9cd2f260843de5e3",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f8b731f2beec62abb38c3d104c182153621be67",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09c3744f2431f90355d37d937f1352192cc87780",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee4f08a8c7da0b870a09e2fee6c837603f3ccdca",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f610522848162744871027feef9554990de435d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgis6h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/",
          "stickied": false,
          "url": "https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754226361,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yet another new model claiming to outperform larger ones:\n\n\n\n**Instruction following** is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.\n\nWe identify **lazy reasoning** during the thinking stage as a key cause of poor instruction adherence.\n\nTo address this, we propose a framework that promotes rigorous reasoning through **previewing and self-checking**.\n\nOur method begins by generating instruction data with **complex constraints**, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.\n\nTraining involves entropy-preserving supervised fine-tuning (**Entropy-SFT**) and token-wise entropy-adaptive reinforcement learning (**TEA-RL**), guided by rule-based multidimensional rewards.\n\nThis approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.\n\nExperiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like **DeepSeek-R1** and closed-source models like **ChatGPT-4o** on challenging instruction-following benchmarks.\n\n[https://huggingface.co/qihoo360/Light-IF-32B](https://huggingface.co/qihoo360/Light-IF-32B)\n\n\n\ntechnical report [https://huggingface.co/papers/2503.10460](https://huggingface.co/papers/2503.10460)\n\n\n\nprevious popular models by this company:\n\n[https://huggingface.co/qihoo360/TinyR1-32B-Preview](https://huggingface.co/qihoo360/TinyR1-32B-Preview)\n\n[https://huggingface.co/qihoo360/Light-R1-32B](https://huggingface.co/qihoo360/Light-R1-32B)\n\n  \nWhat do you think?",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qihoo360/Light-IF-32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mghy1u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": "#bbbdbf",
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QIubNPPJjdRirxQ73mfKHMMmxIY2VZ9TzcjXGXjIohk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another new model claiming to outperform larger ones:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.&lt;/p&gt;\n\n&lt;p&gt;We identify &lt;strong&gt;lazy reasoning&lt;/strong&gt; during the thinking stage as a key cause of poor instruction adherence.&lt;/p&gt;\n\n&lt;p&gt;To address this, we propose a framework that promotes rigorous reasoning through &lt;strong&gt;previewing and self-checking&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Our method begins by generating instruction data with &lt;strong&gt;complex constraints&lt;/strong&gt;, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.&lt;/p&gt;\n\n&lt;p&gt;Training involves entropy-preserving supervised fine-tuning (&lt;strong&gt;Entropy-SFT&lt;/strong&gt;) and token-wise entropy-adaptive reinforcement learning (&lt;strong&gt;TEA-RL&lt;/strong&gt;), guided by rule-based multidimensional rewards.&lt;/p&gt;\n\n&lt;p&gt;This approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.&lt;/p&gt;\n\n&lt;p&gt;Experiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like &lt;strong&gt;DeepSeek-R1&lt;/strong&gt; and closed-source models like &lt;strong&gt;ChatGPT-4o&lt;/strong&gt; on challenging instruction-following benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-IF-32B\"&gt;https://huggingface.co/qihoo360/Light-IF-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;technical report &lt;a href=\"https://huggingface.co/papers/2503.10460\"&gt;https://huggingface.co/papers/2503.10460&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;previous popular models by this company:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/TinyR1-32B-Preview\"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-R1-32B\"&gt;https://huggingface.co/qihoo360/Light-R1-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6vaf0crhrsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6vaf0crhrsgf1.png?auto=webp&amp;s=6ba618920def17d94508256824e2561aba8a6ec9",
                  "width": 1062,
                  "height": 980
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=081b3d7480032b122c209477d47263419358b811",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5fac696d38efe6b971413c8794edd434ae2c9926",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=20af7c29a89f974a98e34d36ff62aa93c6d3e970",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=771c3421ec90e5339a70e9664ac80ef7b729ca73",
                    "width": 640,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=753db7d1045cf75fdd0e7ff4fd0c94209e8cee25",
                    "width": 960,
                    "height": 885
                  }
                ],
                "variants": {},
                "id": "O7-1ZfpSudq0amigVzUb6mHn4tEC8x9xtRLUGpzh3sI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghy1u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/",
          "stickied": false,
          "url": "https://i.redd.it/6vaf0crhrsgf1.png",
          "subreddit_subscribers": 509624,
          "created_utc": 1754223868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It's big enough to be smart without being so huge it steps on OpenAI’s toes. In my experience with the model, here some notes:\n\n* It works *really* well with agents and tools\n* It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.\n* It’s fantastic with languages other than English, a weakness often sees in Chinese models.\n* It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models\n* The way it writes is very similar to OpenAI’s style.\n* Plus, whoever made this has *serious* computing power... they're giving away billions of tokens for \"free\" at a really fast speed\n* The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)\n\nBut ok, lets consider other players:  \n \\- Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly  \n \\- Anthropic: Naah  \n \\- Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don't see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.  \n \\- Amazon or Microsoft: Would be my second guess  \n \\- Google: Naah, they have Aistudio, when wants feedback they launch the model there  \n \\- IBM or Cohere: Hard to think, but they are very capable companies\n\nHonestly, it’s hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren’t fans of CloseAI, but if they say they’re releasing an open-source model, let’s be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.\n\nSo, what you guys think?",
          "author_fullname": "t2_y02rp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If Horizon Models is not from OpenAI, who would be?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgn94g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754237720,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It&amp;#39;s big enough to be smart without being so huge it steps on OpenAI’s toes. In my experience with the model, here some notes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It works &lt;em&gt;really&lt;/em&gt; well with agents and tools&lt;/li&gt;\n&lt;li&gt;It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.&lt;/li&gt;\n&lt;li&gt;It’s fantastic with languages other than English, a weakness often sees in Chinese models.&lt;/li&gt;\n&lt;li&gt;It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models&lt;/li&gt;\n&lt;li&gt;The way it writes is very similar to OpenAI’s style.&lt;/li&gt;\n&lt;li&gt;Plus, whoever made this has &lt;em&gt;serious&lt;/em&gt; computing power... they&amp;#39;re giving away billions of tokens for &amp;quot;free&amp;quot; at a really fast speed&lt;/li&gt;\n&lt;li&gt;The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But ok, lets consider other players:&lt;br/&gt;\n - Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly&lt;br/&gt;\n - Anthropic: Naah&lt;br/&gt;\n - Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don&amp;#39;t see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.&lt;br/&gt;\n - Amazon or Microsoft: Would be my second guess&lt;br/&gt;\n - Google: Naah, they have Aistudio, when wants feedback they launch the model there&lt;br/&gt;\n - IBM or Cohere: Hard to think, but they are very capable companies&lt;/p&gt;\n\n&lt;p&gt;Honestly, it’s hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren’t fans of CloseAI, but if they say they’re releasing an open-source model, let’s be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.&lt;/p&gt;\n\n&lt;p&gt;So, what you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgn94g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AMOVCS",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754237720,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Am I the only person who can't stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I'm gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere's 112 Command A Vision if not for the license) - I just can't help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? ",
          "author_fullname": "t2_ap0ra8pe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Daydreaming of a new Gemma model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgm8d3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754235233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am I the only person who can&amp;#39;t stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I&amp;#39;m gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere&amp;#39;s 112 Command A Vision if not for the license) - I just can&amp;#39;t help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mgm8d3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jazzlike_Source_5983",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754235233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.**\n\nIt writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof *checked* by a compiler. \n\nThe key innovations:\n\n* **Lemma-first reasoning**: breaks problems into small reusable steps.\n* **Iterative refinement**: re-tries and improves failed proofs.\n* **Formal geometry engine**: solves insane geometry problems using a custom language and a C++ backend.\n\nPerformance? It formally solved **5/6 IMO 2025 problems**, something no model has done before.\n\nCheck simple explanantion here : [https://www.youtube.com/watch?v=os1QcHEpgZQ](https://www.youtube.com/watch?v=os1QcHEpgZQ)\n\nPaper : [https://arxiv.org/abs/2507.23726](https://arxiv.org/abs/2507.23726)",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ByteDance drops Seed-Prover",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgccyc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 163,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 163,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754202843,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof &lt;em&gt;checked&lt;/em&gt; by a compiler. &lt;/p&gt;\n\n&lt;p&gt;The key innovations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lemma-first reasoning&lt;/strong&gt;: breaks problems into small reusable steps.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt;: re-tries and improves failed proofs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Formal geometry engine&lt;/strong&gt;: solves insane geometry problems using a custom language and a C++ backend.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Performance? It formally solved &lt;strong&gt;5/6 IMO 2025 problems&lt;/strong&gt;, something no model has done before.&lt;/p&gt;\n\n&lt;p&gt;Check simple explanantion here : &lt;a href=\"https://www.youtube.com/watch?v=os1QcHEpgZQ\"&gt;https://www.youtube.com/watch?v=os1QcHEpgZQ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Paper : &lt;a href=\"https://arxiv.org/abs/2507.23726\"&gt;https://arxiv.org/abs/2507.23726&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?auto=webp&amp;s=f6d5b40325fd2c44f012121fd1ced8e7cf3fd808",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2880b55faa98583237421c5c4a72d31d6cee167",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=578175b7ddca0a72992280e2d7f06f267e288d69",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26ae18680865d42ebd641ddb54c43950a72d9432",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgccyc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754202843,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/oevortex/unsloth](https://github.com/oevortex/unsloth)",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We enabled Multi-GPU training in Unsloth AI — a feature that’s usually paid — using just 2 Copilot prompts!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbs6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 144,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 144,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754200681,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/oevortex/unsloth\"&gt;https://github.com/oevortex/unsloth&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?auto=webp&amp;s=81dc71a9f2f02f913bd3aee3c8d23660f674d4d2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3fa1e4a31ebf9a6d864f530fdd10814f6d32e3a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f68f83263defa437a0f2fa0d8b040d089cd1d79",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6910771512dba7baaab00bceadb6d8e3f5e32826",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f807024407837a65a122fa874452ed0fa669595e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=755ef7a38a09ee966639de29f878c57aaa1ead1e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e85dbe449ba2af4571feed8fcd1e5ddf597b6c6b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgbs6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754200681,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It’s been an eternity since is early preview… and now I wonder if it is doomed. Not just because it’s slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is EXL3 doomed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgl1qz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=989197fdb000ae3849fd70a2dd3363c36e2714e5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754232324,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It’s been an eternity since is early preview… and now I wonder if it is doomed. Not just because it’s slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/turboderp-org/exllamav3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?auto=webp&amp;s=3ecca6d81c2fa96f4b86230a6c6416f82eb4d520",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f93311c0ef06e6cbd0115381f5af07f2cf7c6763",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4328d8b4f520fbe8256ab167da238bdf76b974d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ee0597232a29c03129624c370183e8c020fce3e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=778afd8299a6ccb54136a78390cc8473e58bebed",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bad6d74304971cdf123c7601152d5c744927cbd",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9314e4d7ebbf51c941f82bcfbff6bd6a8d85a358",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgl1qz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/",
          "stickied": false,
          "url": "https://github.com/turboderp-org/exllamav3",
          "subreddit_subscribers": 509624,
          "created_utc": 1754232324,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 I just open-sourced a fully working persistent memory system for AI assistants!\n\n\n\n🧠 Features:\n\n\\- Real-time memory capture across apps (LM Studio, VS Code, etc.)\n\n\\- Semantic search via vector embeddings\n\n\\- Tool call logging for AI self-reflection\n\n\\- Cross-platform and fully tested\n\n\\- Open source and modular\n\n\n\nBuilt with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝\n\n\n\nGitHub: [https://github.com/savantskie/persistent-ai-memory](https://github.com/savantskie/persistent-ai-memory)",
          "author_fullname": "t2_7qb4luzhc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg5xlb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 266,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 266,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt;\n\n&lt;p&gt;🧠 Features:&lt;/p&gt;\n\n&lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt;\n\n&lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt;\n\n&lt;p&gt;- Cross-platform and fully tested&lt;/p&gt;\n\n&lt;p&gt;- Open source and modular&lt;/p&gt;\n\n&lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/savantskie/persistent-ai-memory\"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?auto=webp&amp;s=726283d31a9e66a5327266304a008d1eb52da1db",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b41ef9face9075e71937a4cd3b28923245b39a29",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6703186c20a684418ded0dc4abb9a6d490d7b023",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=860a8622d037c44a41b9ee50c96939060a76e917",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ef5d63ac01ba46958961a63c5e9f43dabb1245",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=446a7e497c6b8549a8cbed43e99bc5f99e79c618",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1ab447d78f6edbe4f20129542abf92b51d827b1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg5xlb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Savantskie1",
          "discussion_type": null,
          "num_comments": 82,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754181701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\n\nI’m looking for recommendations for **open-source multimoal LLMs no larger than 8B parameters** that perform well as *agents* for interacting with web pages.\n\n**Context / Constraints:**\n\n* **Max size:** 8B params (need to run locally on an 8 GB GPU without major slowdowns)\n* **Use case:** Complex browser automation — navigating, filling forms, clicking elements, multi-step planning, and handling changing DOM structures.\n* **Agent setup:** Likely to integrate with a framework like BrowserGym, LaVague, Playwright, or similar.\n* **Precision:** I can run FP16 or quantized (8-bit/4-bit) models if that helps.\n* **Goal:** Good mix of reasoning, instruction-following, and robustness for long-horizon tasks.\n\n**Questions:**\n\n1. Which small **open-source multimodal models** have you found most capable for this kind of task?\n2. Any **quantized** versions you recommend for best VRAM fit + speed on consumer GPUs?\n3. Have you seen measurable differences between models in **agentic benchmarks** like Mind2Web, WebArena, or WorkArena?\n\nThanks in advance!",
          "author_fullname": "t2_jy7rkkqp8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What’s the Best Open-Source Small LLM (≤ 8B) for Agentic Web Page Interactions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgr13d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754246733,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I’m looking for recommendations for &lt;strong&gt;open-source multimoal LLMs no larger than 8B parameters&lt;/strong&gt; that perform well as &lt;em&gt;agents&lt;/em&gt; for interacting with web pages.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context / Constraints:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Max size:&lt;/strong&gt; 8B params (need to run locally on an 8 GB GPU without major slowdowns)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use case:&lt;/strong&gt; Complex browser automation — navigating, filling forms, clicking elements, multi-step planning, and handling changing DOM structures.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agent setup:&lt;/strong&gt; Likely to integrate with a framework like BrowserGym, LaVague, Playwright, or similar.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; I can run FP16 or quantized (8-bit/4-bit) models if that helps.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Good mix of reasoning, instruction-following, and robustness for long-horizon tasks.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which small &lt;strong&gt;open-source multimodal models&lt;/strong&gt; have you found most capable for this kind of task?&lt;/li&gt;\n&lt;li&gt;Any &lt;strong&gt;quantized&lt;/strong&gt; versions you recommend for best VRAM fit + speed on consumer GPUs?&lt;/li&gt;\n&lt;li&gt;Have you seen measurable differences between models in &lt;strong&gt;agentic benchmarks&lt;/strong&gt; like Mind2Web, WebArena, or WorkArena?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgr13d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Extra-Designer9333",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754246733,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys!\n\n**I got tired of installing AI tools the hard way.**\n\nEvery time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:\n\n**terminal commands, missing dependencies, broken CUDA, slow setup, frustration.**\n\nSo I built **Dione** — a desktop app that makes running local AI feel like using an App Store.\n\n**What it does:**\n\n* Browse and install AI tools with one click (like apps)\n* No terminal, no Python setup, no configs\n* Open-source, designed with UX in mind\n\n**You can try it** [**here**](https://getdione.app)**.**\n\n**Why I built it?**\n\nTools like Pinokio or open-source repos are powerful, but honestly… **most look like they were made by devs, for devs**.\n\n**I wanted something simple**. Something visual. Something you can give to your non-tech friend and it still works.\n\n**Dione is my attempt to make local AI accessible without losing control or power.**\n\nWould you use something like this? Anything confusing / missing?\n\nThe project is still evolving, and I’m fully open to ideas and contributions. Also, if you’re into self-hosted AI or building tools around it — let’s talk!\n\nGitHub: [https://getdione.app/github](https://getdione.app/github)\n\nThanks for reading &lt;3!",
          "author_fullname": "t2_dau1zlvo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "I created an app to run local AI as if it were the App Store",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ysewb2ytxqgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 36,
                  "x": 108,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b69be81b017e04cc485309d6ea7db34f44eb65cd"
                },
                {
                  "y": 72,
                  "x": 216,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cef7f4312d5076f63633ce0082f84992449d708f"
                },
                {
                  "y": 106,
                  "x": 320,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0950b8c7452a7592a0103624fa574fa42d0435a4"
                },
                {
                  "y": 213,
                  "x": 640,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3d1571494f02e307302b6ce30eb0b231baef2eb"
                },
                {
                  "y": 320,
                  "x": 960,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d1f01d071b3b900cf9e696a94ba3b157258152c"
                },
                {
                  "y": 360,
                  "x": 1080,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b39946b7843a58ceef7b49e4bd40107d54164ca"
                }
              ],
              "s": {
                "y": 1000,
                "x": 3000,
                "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=54735c109c32811d4b153a0c2e59c72a4c06bf5d"
              },
              "id": "ysewb2ytxqgf1"
            },
            "yt2muytuxqgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6ce704ac3695892c4184608ef310e5bf451248"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff91a2d48c077a5f508c0a4840cb39b61883c92d"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4279610bad7b104472f3da3351e6ffc9361fafb5"
                },
                {
                  "y": 342,
                  "x": 640,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e834d2caf46a9a6f01999cf11161e7f0e1a9c17"
                },
                {
                  "y": 514,
                  "x": 960,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=666cb580f9192df96b59d1ca07402b7628e0ff4a"
                },
                {
                  "y": 578,
                  "x": 1080,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=528bb2d873fdc538d816d91b547db28b28ad5f34"
                }
              ],
              "s": {
                "y": 1026,
                "x": 1915,
                "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=1915&amp;format=png&amp;auto=webp&amp;s=c32ef79f62a2ffe7f4e5128937b67c1a2b216d50"
              },
              "id": "yt2muytuxqgf1"
            }
          },
          "name": "t3_1mgc0v0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 93,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ysewb2ytxqgf1",
                "id": 720059841
              },
              {
                "media_id": "yt2muytuxqgf1",
                "id": 720059842
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 93,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/DMpoJ6kbQBgxqPJp_64cY0H-qxr9jJD3Tr3RuFU7894.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754201559,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I got tired of installing AI tools the hard way.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Every time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;terminal commands, missing dependencies, broken CUDA, slow setup, frustration.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So I built &lt;strong&gt;Dione&lt;/strong&gt; — a desktop app that makes running local AI feel like using an App Store.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Browse and install AI tools with one click (like apps)&lt;/li&gt;\n&lt;li&gt;No terminal, no Python setup, no configs&lt;/li&gt;\n&lt;li&gt;Open-source, designed with UX in mind&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;You can try it&lt;/strong&gt; &lt;a href=\"https://getdione.app\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Tools like Pinokio or open-source repos are powerful, but honestly… &lt;strong&gt;most look like they were made by devs, for devs&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I wanted something simple&lt;/strong&gt;. Something visual. Something you can give to your non-tech friend and it still works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Dione is my attempt to make local AI accessible without losing control or power.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Would you use something like this? Anything confusing / missing?&lt;/p&gt;\n\n&lt;p&gt;The project is still evolving, and I’m fully open to ideas and contributions. Also, if you’re into self-hosted AI or building tools around it — let’s talk!&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://getdione.app/github\"&gt;https://getdione.app/github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading &amp;lt;3!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mgc0v0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgc0v0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Deivih-4774",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mgc0v0",
          "subreddit_subscribers": 509624,
          "created_utc": 1754201559,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I literally can't find anything on this model. I saw somewhere on discord that it's similar to claude (which I doubt). any info? and no i'm not promoting this website or any bs like that idk anything about it",
          "author_fullname": "t2_askwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jin 3.5 - Does anyone know anything about this model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgo662",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754239996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "jin.elpa.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I literally can&amp;#39;t find anything on this model. I saw somewhere on discord that it&amp;#39;s similar to claude (which I doubt). any info? and no i&amp;#39;m not promoting this website or any bs like that idk anything about it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://jin.elpa.ai/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgo662",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z_3454_pfk",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgo662/jin_35_does_anyone_know_anything_about_this_model/",
          "stickied": false,
          "url": "https://jin.elpa.ai/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754239996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.\n\nI now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB\n\nI would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.\n\nBased on my research I couldn’t find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:\n\n* 80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500\n* 21 t/s - 120 GB/s Mac Mini M4 32GB 1000$\n* 46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$\n* 43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro\n* 410 GB/s Mac Studio M4 Max 36-128 GB tons of $\n* 819 GB/s Mac Studio M3 Ultra up to 512GB tons of $\n* 132 t/s - 750 GB/s RTX 4090 24GB\n\nBecause I’m considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I’ve reached the following conclusions that I hope to validate:\n\n* I need something between M4 and the GPU in terms of speed.\n* I need 32GB but could do with more if it’s not a ripoff.\n* Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale\n* For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can’t justify getting a second GPU.\n* M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.\n\nSo is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn’t noisy and power hungry?",
          "author_fullname": "t2_12uzcl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking to build or buy a mini pc for LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgvbw6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754259032,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.&lt;/p&gt;\n\n&lt;p&gt;I now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB&lt;/p&gt;\n\n&lt;p&gt;I would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.&lt;/p&gt;\n\n&lt;p&gt;Based on my research I couldn’t find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500&lt;/li&gt;\n&lt;li&gt;21 t/s - 120 GB/s Mac Mini M4 32GB 1000$&lt;/li&gt;\n&lt;li&gt;46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$&lt;/li&gt;\n&lt;li&gt;43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro&lt;/li&gt;\n&lt;li&gt;410 GB/s Mac Studio M4 Max 36-128 GB tons of $&lt;/li&gt;\n&lt;li&gt;819 GB/s Mac Studio M3 Ultra up to 512GB tons of $&lt;/li&gt;\n&lt;li&gt;132 t/s - 750 GB/s RTX 4090 24GB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Because I’m considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I’ve reached the following conclusions that I hope to validate:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I need something between M4 and the GPU in terms of speed.&lt;/li&gt;\n&lt;li&gt;I need 32GB but could do with more if it’s not a ripoff.&lt;/li&gt;\n&lt;li&gt;Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale&lt;/li&gt;\n&lt;li&gt;For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can’t justify getting a second GPU.&lt;/li&gt;\n&lt;li&gt;M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn’t noisy and power hungry?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgvbw6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nemuro87",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754256996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI am currently running Qwen3-Coder-30B and tried to create unit test for my classes with different tools like qwen coder, countinue dev or proxy ai.\n\nThe tests which are created have many errors and I dont understand why. Atleast for the qwen coder, i would expected, if checks the codebase for all files.  \n  \nExample:   \nI have a enum with the values INSERT and UPDATED. In the test its using CREATED which does not exists in the enum.  \nOr I have a object with setters, but in the test it tries only with \"with\" (like it expected a builder). If it use the setter, it thinks, that the object is giving back as a return value instead nothing/void.\n\nI thought the model could help me with writing tests or do code review, but it doesnt feel really helpful.  \nDo i miss something or is that a common problem?",
          "author_fullname": "t2_4xgd14or",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Generating Unit Tests with Qwen3-Coder-30B - not really usable",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv4h3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256477,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am currently running Qwen3-Coder-30B and tried to create unit test for my classes with different tools like qwen coder, countinue dev or proxy ai.&lt;/p&gt;\n\n&lt;p&gt;The tests which are created have many errors and I dont understand why. Atleast for the qwen coder, i would expected, if checks the codebase for all files.  &lt;/p&gt;\n\n&lt;p&gt;Example:&lt;br/&gt;\nI have a enum with the values INSERT and UPDATED. In the test its using CREATED which does not exists in the enum.&lt;br/&gt;\nOr I have a object with setters, but in the test it tries only with &amp;quot;with&amp;quot; (like it expected a builder). If it use the setter, it thinks, that the object is giving back as a return value instead nothing/void.&lt;/p&gt;\n\n&lt;p&gt;I thought the model could help me with writing tests or do code review, but it doesnt feel really helpful.&lt;br/&gt;\nDo i miss something or is that a common problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgv4h3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FarXTraveler",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv4h3/generating_unit_tests_with_qwen3coder30b_not/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv4h3/generating_unit_tests_with_qwen3coder30b_not/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754256477,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i heard some good things about muse writing pose well and not being censored,  just curious if there are other similar open models that are sort of trained from books not part of a very expensive website. \n\nI've been using gemini lately on the website and it does a lot of things right. THe main issue is it just doesn't write like claude does and well claude doesn't think like gpt or gemini do. Then its like wtf you use then.  :)\n\nSo getting out of that just a bit, I've started playing around with novel crafter recently and honeslty it's hard to go back to the chatbots now. Like that free ai at the bottom of the open routerlist did better than perplexity and the 3 main ai sites on fanfiction. of course it and Kimi still censored berserk but I didn't jailbreak it (trying to avoid that so it doesn't make the writing suck hehe) but sometimes like when i was doing perplexity i can just delete and try again. \n\nI just like to try a model that truely is made for writing and not uncensored if it exist. I probably am overplaying muse bet it's censored against berserk too! Are there like any good models that write well and can actually do writing styles and NSFW? ",
          "author_fullname": "t2_gcu2g1163",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any good open source models for NSFW writing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgq8yz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754244875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i heard some good things about muse writing pose well and not being censored,  just curious if there are other similar open models that are sort of trained from books not part of a very expensive website. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using gemini lately on the website and it does a lot of things right. THe main issue is it just doesn&amp;#39;t write like claude does and well claude doesn&amp;#39;t think like gpt or gemini do. Then its like wtf you use then.  :)&lt;/p&gt;\n\n&lt;p&gt;So getting out of that just a bit, I&amp;#39;ve started playing around with novel crafter recently and honeslty it&amp;#39;s hard to go back to the chatbots now. Like that free ai at the bottom of the open routerlist did better than perplexity and the 3 main ai sites on fanfiction. of course it and Kimi still censored berserk but I didn&amp;#39;t jailbreak it (trying to avoid that so it doesn&amp;#39;t make the writing suck hehe) but sometimes like when i was doing perplexity i can just delete and try again. &lt;/p&gt;\n\n&lt;p&gt;I just like to try a model that truely is made for writing and not uncensored if it exist. I probably am overplaying muse bet it&amp;#39;s censored against berserk too! Are there like any good models that write well and can actually do writing styles and NSFW? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgq8yz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LoneyGamer2023",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgq8yz/are_there_any_good_open_source_models_for_nsfw/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgq8yz/are_there_any_good_open_source_models_for_nsfw/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754244875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "that's it. thanks.",
          "author_fullname": "t2_9b9s4a7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX DWQ question: what's \"lr1e-8\" and \"lr8e-7 \" and should we care?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgys0z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754266130,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;that&amp;#39;s it. thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgys0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JLeonsarmiento",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgys0z/mlx_dwq_question_whats_lr1e8_and_lr8e7_and_should/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgys0z/mlx_dwq_question_whats_lr1e8_and_lr8e7_and_should/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754266130,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Article: https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\n\nContext:\n\nThis insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.\n\nhttps://arxiv.org/abs/2506.21734\n\nA number of people have reproduced it if anyone is worried about that: https://x.com/VictorTaelin/status/1950512015899840768\nhttps://github.com/sapientinc/HRM/issues/12",
          "author_fullname": "t2_4kcht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM solved thinking more than current \"thinking\" models (this needs more hype)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3i48",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 328,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 328,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article: &lt;a href=\"https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It&amp;#39;s seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href=\"https://x.com/VictorTaelin/status/1950512015899840768\"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt;\n&lt;a href=\"https://github.com/sapientinc/HRM/issues/12\"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?auto=webp&amp;s=3649785728b7c459cdac6d4444cd5a58a31ff8a2",
                  "width": 994,
                  "height": 610
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a32ed96622f166ae158780da40b7af8d982e72f9",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2621c7644d6e96ab551888e43049a8b94e227fc5",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3da54eeb0e463cf73d8f5292e9b099e338494438",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa1caf030ca1b3e5b1df5e052afe93497a9bb543",
                    "width": 640,
                    "height": 392
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88919a9c5b50d0a4d76e2c2bb7e87bb3c4eed852",
                    "width": 960,
                    "height": 589
                  }
                ],
                "variants": {},
                "id": "okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg3i48",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charuru",
          "discussion_type": null,
          "num_comments": 69,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754174679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I really like LM Studio because it allows you to run AI models locally, preserving the privacy of your conversations with the AI. However, compared to commercial online models, LM Studio doesn’t support internet browsing “out of the box.” Those models can’t use up-to-date information from the Internet to answer questions.\n\n\nNot long ago, LM Studio added the ability to connect MCP servers to models. The very first thing I did was write a small MCP server that can extract text from a URL. It can also extract the links present on the page. This makes it possible, when querying the AI, to specify an address and ask it to extract text from there or retrieve links to use in its response.\n\nTo get all of this working, we first create a `pyproject.toml` file in the `mcp-server` folder.\n\n\n```toml \n[build-system]\nrequires = [\"setuptools&gt;=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"url-text-fetcher\"\nversion = \"0.1.0\"\ndescription = \"FastMCP server for URL text fetching\"\nauthors = [{ name=\"Evgeny Igumnov\", email=\"igumnovnsk@gmail.com\" }]\ndependencies = [\n  \"fastmcp\",\n  \"requests\",\n  \"beautifulsoup4\",\n]\n[project.scripts]\nurl-text-fetcher = \"url_text_fetcher.mcp_server:main\"\n```\nThen we create the `mcp_server.py` file in the `mcp-server/url_text_fetcher` folder.\n```python\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nfrom bs4 import BeautifulSoup\nfrom typing import List  # for type hints\n\nmcp = FastMCP(\"URL Text Fetcher\")\n\n@mcp.tool()\ndef fetch_url_text(url: str) -&gt; str:\n    \"\"\"Download the text from a URL.\"\"\"\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    return soup.get_text(separator=\"\\n\", strip=True)\n\n@mcp.tool()\ndef fetch_page_links(url: str) -&gt; List[str]:\n    \"\"\"Return a list of all URLs found on the given page.\"\"\"\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    # Extract all href attributes from &lt;a&gt; tags\n    links = [a['href'] for a in soup.find_all('a', href=True)]\n    return links\n\ndef main():\n    mcp.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNext, create an empty `__init__.py` in the `mcp-server/url_text_fetcher` folder.\n\nAnd finally, for the MCP server to work, you need to install it:\n\n```bash\npip install -e .\n```\n\nAt the bottom of the chat window in LM Studio, where you enter your query, you can choose an MCP server via “Integrations.” By clicking “Install” and then “Edit mcp.json,” you can add your own MCP server in that file.\n\n```json\n{\n  \"mcpServers\": {\n    \"url-text-fetcher\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\",\n        \"url_text_fetcher.mcp_server\"\n      ]\n    }\n  }\n}\n```\n\nThe second thing I did was integrate an existing MCP server from the Brave search engine, which allows you to instruct the AI—in a request—to search the Internet for information to answer a question. To do this, first check that you have `npx` installed. Then install `@modelcontextprotocol/server-brave-search`:\n\n```bash\nnpm i -D @modelcontextprotocol/server-brave-search\n```\n\nHere’s how you can connect it in the `mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"..................\"\n      }\n    },\n    \"url-text-fetcher\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\",\n        \"url_text_fetcher.mcp_server\"\n      ]\n    }\n  }\n}\n```\n\nYou can obtain the `BRAVE_API_KEY` for free, with minor limitations of up to 2,000 requests per month and no more than one request per second.\n\nAs a result, at the bottom of the chat window in LM Studio—where the user enters their query—you can select the MCP server via “Integrations,” and you should see two MCP servers listed: “mcp/url-text-fetcher” and “mcp/brave-search.”\n\n",
          "author_fullname": "t2_gj8yhx0zk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Teaching LM Studio to Browse the Internet When Answering Questions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgljhp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754233538,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like LM Studio because it allows you to run AI models locally, preserving the privacy of your conversations with the AI. However, compared to commercial online models, LM Studio doesn’t support internet browsing “out of the box.” Those models can’t use up-to-date information from the Internet to answer questions.&lt;/p&gt;\n\n&lt;p&gt;Not long ago, LM Studio added the ability to connect MCP servers to models. The very first thing I did was write a small MCP server that can extract text from a URL. It can also extract the links present on the page. This makes it possible, when querying the AI, to specify an address and ask it to extract text from there or retrieve links to use in its response.&lt;/p&gt;\n\n&lt;p&gt;To get all of this working, we first create a &lt;code&gt;pyproject.toml&lt;/code&gt; file in the &lt;code&gt;mcp-server&lt;/code&gt; folder.&lt;/p&gt;\n\n&lt;p&gt;```toml \n[build-system]\nrequires = [&amp;quot;setuptools&amp;gt;=42&amp;quot;, &amp;quot;wheel&amp;quot;]\nbuild-backend = &amp;quot;setuptools.build_meta&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;[project]\nname = &amp;quot;url-text-fetcher&amp;quot;\nversion = &amp;quot;0.1.0&amp;quot;\ndescription = &amp;quot;FastMCP server for URL text fetching&amp;quot;\nauthors = [{ name=&amp;quot;Evgeny Igumnov&amp;quot;, email=&amp;quot;&lt;a href=\"mailto:igumnovnsk@gmail.com\"&gt;igumnovnsk@gmail.com&lt;/a&gt;&amp;quot; }]\ndependencies = [\n  &amp;quot;fastmcp&amp;quot;,\n  &amp;quot;requests&amp;quot;,\n  &amp;quot;beautifulsoup4&amp;quot;,\n]\n[project.scripts]\nurl-text-fetcher = &amp;quot;url_text_fetcher.mcp_server:main&amp;quot;\n&lt;code&gt;\nThen we create the `mcp_server.py` file in the `mcp-server/url_text_fetcher` folder.\n&lt;/code&gt;python\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nfrom bs4 import BeautifulSoup\nfrom typing import List  # for type hints&lt;/p&gt;\n\n&lt;p&gt;mcp = FastMCP(&amp;quot;URL Text Fetcher&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;@mcp.tool()\ndef fetch_url_text(url: str) -&amp;gt; str:\n    &amp;quot;&amp;quot;&amp;quot;Download the text from a URL.&amp;quot;&amp;quot;&amp;quot;\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, &amp;quot;html.parser&amp;quot;)\n    return soup.get_text(separator=&amp;quot;\\n&amp;quot;, strip=True)&lt;/p&gt;\n\n&lt;p&gt;@mcp.tool()\ndef fetch_page_links(url: str) -&amp;gt; List[str]:\n    &amp;quot;&amp;quot;&amp;quot;Return a list of all URLs found on the given page.&amp;quot;&amp;quot;&amp;quot;\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, &amp;quot;html.parser&amp;quot;)\n    # Extract all href attributes from &amp;lt;a&amp;gt; tags\n    links = [a[&amp;#39;href&amp;#39;] for a in soup.find_all(&amp;#39;a&amp;#39;, href=True)]\n    return links&lt;/p&gt;\n\n&lt;p&gt;def main():\n    mcp.run()&lt;/p&gt;\n\n&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;:\n    main()\n```&lt;/p&gt;\n\n&lt;p&gt;Next, create an empty &lt;code&gt;__init__.py&lt;/code&gt; in the &lt;code&gt;mcp-server/url_text_fetcher&lt;/code&gt; folder.&lt;/p&gt;\n\n&lt;p&gt;And finally, for the MCP server to work, you need to install it:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\npip install -e .\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;At the bottom of the chat window in LM Studio, where you enter your query, you can choose an MCP server via “Integrations.” By clicking “Install” and then “Edit mcp.json,” you can add your own MCP server in that file.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;json\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;url-text-fetcher&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;python&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-m&amp;quot;,\n        &amp;quot;url_text_fetcher.mcp_server&amp;quot;\n      ]\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The second thing I did was integrate an existing MCP server from the Brave search engine, which allows you to instruct the AI—in a request—to search the Internet for information to answer a question. To do this, first check that you have &lt;code&gt;npx&lt;/code&gt; installed. Then install &lt;code&gt;@modelcontextprotocol/server-brave-search&lt;/code&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\nnpm i -D @modelcontextprotocol/server-brave-search\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Here’s how you can connect it in the &lt;code&gt;mcp.json&lt;/code&gt; file:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;json\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;brave-search&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-y&amp;quot;,\n        &amp;quot;@modelcontextprotocol/server-brave-search&amp;quot;\n      ],\n      &amp;quot;env&amp;quot;: {\n        &amp;quot;BRAVE_API_KEY&amp;quot;: &amp;quot;..................&amp;quot;\n      }\n    },\n    &amp;quot;url-text-fetcher&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;python&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-m&amp;quot;,\n        &amp;quot;url_text_fetcher.mcp_server&amp;quot;\n      ]\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You can obtain the &lt;code&gt;BRAVE_API_KEY&lt;/code&gt; for free, with minor limitations of up to 2,000 requests per month and no more than one request per second.&lt;/p&gt;\n\n&lt;p&gt;As a result, at the bottom of the chat window in LM Studio—where the user enters their query—you can select the MCP server via “Integrations,” and you should see two MCP servers listed: “mcp/url-text-fetcher” and “mcp/brave-search.”&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mgljhp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ievkz",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgljhp/teaching_lm_studio_to_browse_the_internet_when/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgljhp/teaching_lm_studio_to_browse_the_internet_when/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754233538,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The larger SmallThinker MoE has been through a quantization aware training process. it's uploaded to the same gguf repo a bit later. \n\n- https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf\n\nIn llama.cpp m2 air 16gb, with the `sudo sysctl iogpu.wired_limit_mb=13000` command, it's 30 t/s. \n\nThe model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. \nThe models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.",
          "author_fullname": "t2_el5pibmej",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmallThinker-21B-A3B-Instruct-QAT version",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbprh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 76,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 76,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f963436f8fe8c361365006afe812426eb0a95bfc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754200435,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The larger SmallThinker MoE has been through a quantization aware training process. it&amp;#39;s uploaded to the same gguf repo a bit later. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In llama.cpp m2 air 16gb, with the &lt;code&gt;sudo sysctl iogpu.wired_limit_mb=13000&lt;/code&gt; command, it&amp;#39;s 30 t/s. &lt;/p&gt;\n\n&lt;p&gt;The model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. \nThe models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?auto=webp&amp;s=af35574c8d3a17d80825392473da49ac8b6cb469",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97ed6aedc23ef593aadb95c2316196f396bc8e65",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ac0298234f43f3e33451ea17a59722ab4866a82",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=19ce50a51896a266764f718a5da7eb00d163ddf3",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d016e4dfb71f7ddb2afc8c6c06ee782ba15871a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=800a6ea36b04ed56de7a369596d03474bfd1142f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29a1fe69fa68a7a75c2954c6a0922c20dd4448f1",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgbprh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Aaaaaaaaaeeeee",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/",
          "stickied": false,
          "url": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
          "subreddit_subscribers": 509624,
          "created_utc": 1754200435,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's a WIP, but it's looking like may be possible to pair Macs with NVIDIA soon!\n\nTweet: [https://x.com/anemll/status/1951307167417639101](https://x.com/anemll/status/1951307167417639101)\n\nRepo: [https://github.com/anemll/anemll](https://github.com/anemll/anemll)  \n",
          "author_fullname": "t2_88fma001",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac + Blackwell 👀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 113,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg7abc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 157,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 157,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fpapz0ydmBtTV_x0MSxATxS9iOfp_oxdoqZpoMcz2tU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754185875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a WIP, but it&amp;#39;s looking like may be possible to pair Macs with NVIDIA soon!&lt;/p&gt;\n\n&lt;p&gt;Tweet: &lt;a href=\"https://x.com/anemll/status/1951307167417639101\"&gt;https://x.com/anemll/status/1951307167417639101&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo: &lt;a href=\"https://github.com/anemll/anemll\"&gt;https://github.com/anemll/anemll&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/u2mr83o6npgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/u2mr83o6npgf1.png?auto=webp&amp;s=7490eb4194899de10ffd4b664bbedbdc08bd49bb",
                  "width": 1165,
                  "height": 948
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9ff42619aaccace2d4ed7a908fc57868515ac74",
                    "width": 108,
                    "height": 87
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ef76de21f6db4556cec173bd541323b579d6b56",
                    "width": 216,
                    "height": 175
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=192a7fbdc52c427e798423dad1748e19398be74b",
                    "width": 320,
                    "height": 260
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=161c4ac9304218f08618c511e09178e7a7c08931",
                    "width": 640,
                    "height": 520
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc682053c820d810ab124287e40648863dde6414",
                    "width": 960,
                    "height": 781
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34b03746bd307ffc26c4ea8eab8f4bc9945f8480",
                    "width": 1080,
                    "height": 878
                  }
                ],
                "variants": {},
                "id": "a2KP-pXHo0R34cSlf0ychRDsFMkcNDCD2VtUAfedyZM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg7abc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished_Ad9530",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/",
          "stickied": false,
          "url": "https://i.redd.it/u2mr83o6npgf1.png",
          "subreddit_subscribers": 509624,
          "created_utc": 1754185875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I use ChatGPT and Claude paid plans. I’m incorporating local for writing topics that they censor. I understand that closed source is not on the level of Claude and can’t be at this time. I accept that. However, I’m having trouble getting it to work at even a basic level and I’ve found nothing on Google or YouTube that talks about prompt crafting outside of waifu chats (and even those are few). \n\nWhen I try to use it like I would CGPT or Claude it puts out garbage. I’ve gotten what looks like Twitter comments, the same thing I sent it, error codes, random stuff that makes no sense (but at least some what mirrors the style I specified). \n\nCGPT says you can’t put much info in the world building or author notes because of tokens. It says local LLMs are more like continuing text you put in and don’t have the same training depth. Is that true? \n\nLook basically I want what I have in CGPT or Claude. I can upload a few pages long doc and it reference that. Then follow prompts to give me new ideas or continue in a style or revise. **Is that possible**? **If so how?”**. \n\n**Should I just train a model on my own voice? How many samples of my writing do I realistically need for that? CGPT says 50-120k+ words ideally**",
          "author_fullname": "t2_hhsbgspn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I don’t understand how to get what I want from Local LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgrgmu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754247764,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use ChatGPT and Claude paid plans. I’m incorporating local for writing topics that they censor. I understand that closed source is not on the level of Claude and can’t be at this time. I accept that. However, I’m having trouble getting it to work at even a basic level and I’ve found nothing on Google or YouTube that talks about prompt crafting outside of waifu chats (and even those are few). &lt;/p&gt;\n\n&lt;p&gt;When I try to use it like I would CGPT or Claude it puts out garbage. I’ve gotten what looks like Twitter comments, the same thing I sent it, error codes, random stuff that makes no sense (but at least some what mirrors the style I specified). &lt;/p&gt;\n\n&lt;p&gt;CGPT says you can’t put much info in the world building or author notes because of tokens. It says local LLMs are more like continuing text you put in and don’t have the same training depth. Is that true? &lt;/p&gt;\n\n&lt;p&gt;Look basically I want what I have in CGPT or Claude. I can upload a few pages long doc and it reference that. Then follow prompts to give me new ideas or continue in a style or revise. &lt;strong&gt;Is that possible&lt;/strong&gt;? &lt;strong&gt;If so how?”&lt;/strong&gt;. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Should I just train a model on my own voice? How many samples of my writing do I realistically need for that? CGPT says 50-120k+ words ideally&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgrgmu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AccidentalFolklore",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgrgmu/i_dont_understand_how_to_get_what_i_want_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgrgmu/i_dont_understand_how_to_get_what_i_want_from/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754247764,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just dropped a new medical embedding model that's crushing the competition: [https://huggingface.co/lokeshch19/ModernPubMedBERT](https://huggingface.co/lokeshch19/ModernPubMedBERT)\n\nTL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.\n\nThe model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.\n\nThe model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.\n\nThe model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.\n\nDownload the model, test it on your medical datasets, and give it a ⭐ on the [Hugging Face](https://huggingface.co/lokeshch19/ModernPubMedBERT) if it enhances your workflow!\n\n  \nEdit: Added evals to HF model card",
          "author_fullname": "t2_1265e91smj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Medical Embedding Model Released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdypr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754219207,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754209051,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just dropped a new medical embedding model that&amp;#39;s crushing the competition: &lt;a href=\"https://huggingface.co/lokeshch19/ModernPubMedBERT\"&gt;https://huggingface.co/lokeshch19/ModernPubMedBERT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.&lt;/p&gt;\n\n&lt;p&gt;The model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.&lt;/p&gt;\n\n&lt;p&gt;The model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.&lt;/p&gt;\n\n&lt;p&gt;The model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.&lt;/p&gt;\n\n&lt;p&gt;Download the model, test it on your medical datasets, and give it a ⭐ on the &lt;a href=\"https://huggingface.co/lokeshch19/ModernPubMedBERT\"&gt;Hugging Face&lt;/a&gt; if it enhances your workflow!&lt;/p&gt;\n\n&lt;p&gt;Edit: Added evals to HF model card&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?auto=webp&amp;s=5c561836b04f5eae4aeb77b06c972da83d0058a0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4c4ca0fb1848d33d5ab1a908c384ed031d1d90a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9535c4b60f4ef617f9bf8da746dcf3972054a622",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3d2b06c75054cafe0122756eef8a1e16911ccb3",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b0a45e3a646f262176ce2e81cf2d0e6c58218d8",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b4e255867391c0878d0054644b109d42d2e6f88",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6bcc230f7d600fa2c654466bd44ead74e93046a4",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdypr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DataNebula",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754209051,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Mini SVG test: law icons in pt-br (HorizonBeta x Gemini x Claude x Grok)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "bwfogthy9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=79cb7919750dfd42045fde6e5a5d1750b6143ab7"
                },
                {
                  "y": 138,
                  "x": 216,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=26969a94eb08a0e2171958349db4428a86f71ad7"
                },
                {
                  "y": 205,
                  "x": 320,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=21bb5b6a3b3a599bdeeda604db88da6736b6e4d5"
                },
                {
                  "y": 410,
                  "x": 640,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=17f23637e692739a1b5ef3b684a0a052ec7ecc48"
                }
              ],
              "s": {
                "y": 542,
                "x": 844,
                "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=844&amp;format=png&amp;auto=webp&amp;s=77c97cc49d1a3a5fd743d7c7a1fb4a1ff770d866"
              },
              "id": "bwfogthy9wgf1"
            },
            "6xwodqxs9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0122e9b9ce6f267dac3a4040f5c4d24a6ab33a5d"
                },
                {
                  "y": 159,
                  "x": 216,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=efa03263563955f38143a53002ad439de9bc1e1d"
                },
                {
                  "y": 236,
                  "x": 320,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5164e776863a4405163a727cae86a79d4fcbf354"
                },
                {
                  "y": 472,
                  "x": 640,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1462c194648352ba3aacd5919b502f534f08f25"
                }
              ],
              "s": {
                "y": 598,
                "x": 810,
                "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=810&amp;format=png&amp;auto=webp&amp;s=b4772d1f3538d57556e510a1fe4fd3a4583b6e3f"
              },
              "id": "6xwodqxs9wgf1"
            },
            "ia7e2kbl9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd67a082a94566a9a2aeacf0fc5d448181db7258"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7282e81d14bd7c2188987283f144d86a8f7903"
                },
                {
                  "y": 177,
                  "x": 320,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c26282852557ab96c1361ecbea741c2c4d64498"
                },
                {
                  "y": 354,
                  "x": 640,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d374a1a222ac5de9389e35f65f8e0d95e5dbc943"
                }
              ],
              "s": {
                "y": 472,
                "x": 853,
                "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=853&amp;format=png&amp;auto=webp&amp;s=1f1b989572bed7f831449970146ec8e9cb997450"
              },
              "id": "ia7e2kbl9wgf1"
            },
            "k054awhp9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e37e98e4780eda0cd2ba6181225109151048c90"
                },
                {
                  "y": 136,
                  "x": 216,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f703a4a1a4194a80e6a4a0a6372a8194a2923fe1"
                },
                {
                  "y": 202,
                  "x": 320,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=066c24d7a50a6721efb630059efc11f274ffd318"
                },
                {
                  "y": 405,
                  "x": 640,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7390e8b61f5c96c52d39babfbac6d30feabe189"
                }
              ],
              "s": {
                "y": 512,
                "x": 808,
                "u": "https://preview.redd.it/k054awhp9wgf1.png?width=808&amp;format=png&amp;auto=webp&amp;s=3a53675ac12f7a73898f6114e5c2ccc689d9550f"
              },
              "id": "k054awhp9wgf1"
            }
          },
          "name": "t3_1mgytfi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ia7e2kbl9wgf1",
                "id": 720642216
              },
              {
                "media_id": "k054awhp9wgf1",
                "id": 720642217
              },
              {
                "media_id": "6xwodqxs9wgf1",
                "id": 720642218
              },
              {
                "media_id": "bwfogthy9wgf1",
                "id": 720642219
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CE7-Io3jRKJ_PHauMmTqpaj5UaVX4MxPR2ddXENqtHk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754266241,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mgytfi",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgytfi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgytfi/mini_svg_test_law_icons_in_ptbr_horizonbeta_x/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mgytfi",
          "subreddit_subscribers": 509624,
          "created_utc": 1754266241,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm not a professional LLMer by any means, but I figured I'd lay out my little journey and the findings along the way.\n\nWhen I first saw you could run local models on your own hardware, I thought it was so interesting. I played around with ollama a bit, had a lot of fun tying it into random things. It's so easy to get started and also to pull models.\n\nThe real kicker was when I got a 2x AMD MI50 setup. I was having trouble getting decent prompt processing speed out of it. I spent a lot of time looking at the ollama logs on debug, trying to figure out what was going on.\n\nI had also had a few troubles with using models from huggingface, and I finally figured out that ollama uses its own \"go template\" setup.\n\nWell, although I don't know a super-lot about LLMs, I have spent a lot of time with Linux and editing random config files and compiling things is no big deal.\n\nI decided to try out llama.cpp, with my own build. Well, it actually wasn't that bad. I was able to get ROCm installed pretty easy and from there it didn't take long to figure out how to compile llama.cpp. \n\nllama.cpp ended up being awesome. I was using the llama-quantize tool to try to quantize my own models, the llama-bench utility was REALLY handy with running experiments with various parameters.\n\nAfter playing around with llama.cpp for a while, I was totally hooked. I ended up running into \"llama-swap\", which helps me kind of reproduce the setup I have with ollama, where you can load/unload random models on the fly. \n\nNow I was able to easily use almost any model on huggingface, without much worry about if the templates were going to work.\n\nAdditionally, I found it way easier to play around with various parameters, because it just turned into a quick edit on llama-swap's config file instead of having to mess around with Modelfiles and so on.\n\nI also really appreciated that I could just keep a directory of .gguf files instead of the \"layering\" system (seemingly inspired by Docker) used by ollama.\n\nI had heard it said that ollama is \"just\" a llama.cpp wrapper, which isn't entirely fair, because they seem to do a lot of work to make it easy to install...\n\nIt was also pretty easy to tie random programs into \"llama-server\" since it is mostly compliant with OpenAI's APIs.\n\nAnyway, if you're out there wondering the ups and downs with ollama/llama.cpp, my guideline would be this:\n\nInterested in a quick and easy setup, don't want to fiddle with parameters too much: ollama. It's really, really easy to get going, has really decent cross platform support and it's easy to pull a model if it's on ollama.ai.\n\nIf you're a Linux goon and don't mind fiddling with config files and compiling your own llama.cpp (although they have binaries), llama.cpp wins hands down for the wide range of configurations you can apply. You get a LOT LOT LOT more control. I also found the documentation of llama.cpp a lot better, where ollama kind of just breezed over some knobs, and I wasn't entirely sure what they were doing.\n\nI kind of see both of these as a tool on my tool-belt, and I'll probably feel free to reach for either one, depending on what my needs are in the future, but for now, I'm 100% llama-swap/llama.cpp and having a blast!",
          "author_fullname": "t2_10iarzku",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A rambling post on ollama / llama.cpp and when to use each. Pros and cons and everything in between.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgt5bx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754251812,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a professional LLMer by any means, but I figured I&amp;#39;d lay out my little journey and the findings along the way.&lt;/p&gt;\n\n&lt;p&gt;When I first saw you could run local models on your own hardware, I thought it was so interesting. I played around with ollama a bit, had a lot of fun tying it into random things. It&amp;#39;s so easy to get started and also to pull models.&lt;/p&gt;\n\n&lt;p&gt;The real kicker was when I got a 2x AMD MI50 setup. I was having trouble getting decent prompt processing speed out of it. I spent a lot of time looking at the ollama logs on debug, trying to figure out what was going on.&lt;/p&gt;\n\n&lt;p&gt;I had also had a few troubles with using models from huggingface, and I finally figured out that ollama uses its own &amp;quot;go template&amp;quot; setup.&lt;/p&gt;\n\n&lt;p&gt;Well, although I don&amp;#39;t know a super-lot about LLMs, I have spent a lot of time with Linux and editing random config files and compiling things is no big deal.&lt;/p&gt;\n\n&lt;p&gt;I decided to try out llama.cpp, with my own build. Well, it actually wasn&amp;#39;t that bad. I was able to get ROCm installed pretty easy and from there it didn&amp;#39;t take long to figure out how to compile llama.cpp. &lt;/p&gt;\n\n&lt;p&gt;llama.cpp ended up being awesome. I was using the llama-quantize tool to try to quantize my own models, the llama-bench utility was REALLY handy with running experiments with various parameters.&lt;/p&gt;\n\n&lt;p&gt;After playing around with llama.cpp for a while, I was totally hooked. I ended up running into &amp;quot;llama-swap&amp;quot;, which helps me kind of reproduce the setup I have with ollama, where you can load/unload random models on the fly. &lt;/p&gt;\n\n&lt;p&gt;Now I was able to easily use almost any model on huggingface, without much worry about if the templates were going to work.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I found it way easier to play around with various parameters, because it just turned into a quick edit on llama-swap&amp;#39;s config file instead of having to mess around with Modelfiles and so on.&lt;/p&gt;\n\n&lt;p&gt;I also really appreciated that I could just keep a directory of .gguf files instead of the &amp;quot;layering&amp;quot; system (seemingly inspired by Docker) used by ollama.&lt;/p&gt;\n\n&lt;p&gt;I had heard it said that ollama is &amp;quot;just&amp;quot; a llama.cpp wrapper, which isn&amp;#39;t entirely fair, because they seem to do a lot of work to make it easy to install...&lt;/p&gt;\n\n&lt;p&gt;It was also pretty easy to tie random programs into &amp;quot;llama-server&amp;quot; since it is mostly compliant with OpenAI&amp;#39;s APIs.&lt;/p&gt;\n\n&lt;p&gt;Anyway, if you&amp;#39;re out there wondering the ups and downs with ollama/llama.cpp, my guideline would be this:&lt;/p&gt;\n\n&lt;p&gt;Interested in a quick and easy setup, don&amp;#39;t want to fiddle with parameters too much: ollama. It&amp;#39;s really, really easy to get going, has really decent cross platform support and it&amp;#39;s easy to pull a model if it&amp;#39;s on ollama.ai.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re a Linux goon and don&amp;#39;t mind fiddling with config files and compiling your own llama.cpp (although they have binaries), llama.cpp wins hands down for the wide range of configurations you can apply. You get a LOT LOT LOT more control. I also found the documentation of llama.cpp a lot better, where ollama kind of just breezed over some knobs, and I wasn&amp;#39;t entirely sure what they were doing.&lt;/p&gt;\n\n&lt;p&gt;I kind of see both of these as a tool on my tool-belt, and I&amp;#39;ll probably feel free to reach for either one, depending on what my needs are in the future, but for now, I&amp;#39;m 100% llama-swap/llama.cpp and having a blast!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgt5bx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UsualResult",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754251812,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently using 30B A3B on a Windows 11 system with 32GB DDR4, Ryzen 5600 and 3080 10GB and I'm getting about 15t/s generation speeds, but I've seen other people claim they can get 20t/s-25t/s. Is 15t/s typical for my setup or is there any way I can squeeze more speed out of it? ",
          "author_fullname": "t2_3bzzdk93",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fastest way to run Qwen 3 30B A3B on 32GB RAM+10GB VRAM in LM Studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgocw6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754240445,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using 30B A3B on a Windows 11 system with 32GB DDR4, Ryzen 5600 and 3080 10GB and I&amp;#39;m getting about 15t/s generation speeds, but I&amp;#39;ve seen other people claim they can get 20t/s-25t/s. Is 15t/s typical for my setup or is there any way I can squeeze more speed out of it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgocw6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yungfishstick",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754240445,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! I’m curious if anyone has explored this path of thinking. I started using Gemini cli recently, and as everyone here probably knows a pretty well known (and annoying) habit of LLMs is to hallucinate their own capabilities. I’ve found that a relatively easy way to ground it with its own actual capabilities was to just pull the source code into a subdirectory and I can tell it to reference the code and documentation as needed. This has led me to be able to have Gemini generate some pretty cool improvements to itself. Let me know if you’ve tried this out and have any more advice or cool/similar ideas!",
          "author_fullname": "t2_1rxetzin7f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grounding an open source agent with its source code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv384",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I’m curious if anyone has explored this path of thinking. I started using Gemini cli recently, and as everyone here probably knows a pretty well known (and annoying) habit of LLMs is to hallucinate their own capabilities. I’ve found that a relatively easy way to ground it with its own actual capabilities was to just pull the source code into a subdirectory and I can tell it to reference the code and documentation as needed. This has led me to be able to have Gemini generate some pretty cool improvements to itself. Let me know if you’ve tried this out and have any more advice or cool/similar ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgv384",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PatienceKitchen6726",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv384/grounding_an_open_source_agent_with_its_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv384/grounding_an_open_source_agent_with_its_source/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754256391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I play The Expanse role-playing game with some friends every week over Zoom. I've captured the transcripts for every session.  I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. \n\nFrom a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through [roll20.net](http://roll20.net) (a virtual tabletop website) as if it were a chat participant.\n\nIt's been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications.  I've defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.\n\nI haven't learned much about training/retraining/pretraining/fine-tuning yes, and I'm wondering if those are better approaches than just bootstrapping the convo\n\n\n\nOther Features in flight:\n\nIntegrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as \"how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration\". \n\nLoading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I'm sure I'm using it incorrectly or perhaps enabling it during inference when it shouldn't be. I could definitely use some advice on that.\n\n\n\nThis must be a common idea, and I'm sure others are working on similar applications; how do I find them?",
          "author_fullname": "t2_bpv2j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Roleplay with large historical context and RAG",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjcai",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754227908,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I play The Expanse role-playing game with some friends every week over Zoom. I&amp;#39;ve captured the transcripts for every session.  I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. &lt;/p&gt;\n\n&lt;p&gt;From a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through &lt;a href=\"http://roll20.net\"&gt;roll20.net&lt;/a&gt; (a virtual tabletop website) as if it were a chat participant.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications.  I&amp;#39;ve defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t learned much about training/retraining/pretraining/fine-tuning yes, and I&amp;#39;m wondering if those are better approaches than just bootstrapping the convo&lt;/p&gt;\n\n&lt;p&gt;Other Features in flight:&lt;/p&gt;\n\n&lt;p&gt;Integrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as &amp;quot;how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Loading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I&amp;#39;m sure I&amp;#39;m using it incorrectly or perhaps enabling it during inference when it shouldn&amp;#39;t be. I could definitely use some advice on that.&lt;/p&gt;\n\n&lt;p&gt;This must be a common idea, and I&amp;#39;m sure others are working on similar applications; how do I find them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgjcai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RoboCopsGoneMad",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754227908,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The jinja template that comes with the MLX version of GLM 4.5 is using xml style tool calls instead of json. Here's a json template. This means that it is now able to do tool calls in OpenCode, and presumably other things as well (Qwen code/Gemini?). Here's the template:\n\n  \n[https://pastebin.com/CfMw7hFS](https://pastebin.com/CfMw7hFS)",
          "author_fullname": "t2_12ggykute6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Tool Calling Jinja Template",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjpvm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754228935,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The jinja template that comes with the MLX version of GLM 4.5 is using xml style tool calls instead of json. Here&amp;#39;s a json template. This means that it is now able to do tool calls in OpenCode, and presumably other things as well (Qwen code/Gemini?). Here&amp;#39;s the template:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/CfMw7hFS\"&gt;https://pastebin.com/CfMw7hFS&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgjpvm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-dysangel-",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgjpvm/glm_45_tool_calling_jinja_template/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjpvm/glm_45_tool_calling_jinja_template/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754228935,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, poor GPU guys\n\nA few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.\n\nSome married guys might understand, but I’ve been using an m-ATX case I bought about 15 years ago, and there’s no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to “real” work.\n\nhttps://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80\n\nOne of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That's why the “32GB” model ended up in my hands.\n\nOf course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the “Original MI50” cannot be BIOS-re-flashed, I had given up.\n\nFirst, take a look at the results: the MI50 is running with  GTX 4070ti or alone on Windows.\n\nhttps://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9\n\n[4070+MI50 \\(22GB only\\)](https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96)\n\n[MI50 works alone upto 30GB](https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;format=png&amp;auto=webp&amp;s=f14e51095d28e301c62418f12f52586a62861ee2)\n\nGuys, hold your horses. I'm aware there are a few issues here.\n\n1. It's recognized as a Radeon Pro VII\n2. It runs on LM STUDIO, which some people really dislike\n3. Even if it's recognized as Vulkan, you can't use the combined VRAM of  hetero cards—only twice the VRAM of the first graphics card (On my PC, it's 12+12GB instead of 12+32GB\n\n\\-&gt; However, I haven't tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX's prompt processing ability is an extra bonus.\n\n4. **Surely, you could use whole 32GB if you turn off other GPU and MI50 only!!!**  \n\n\n**Anyway, there are only three things you need to do.**\n\n1. Disable Secure Boot in the CMOS BIOS.\n2. Run PowerShell in administrator mode and enter the following command:\n\nbcdedit.exe -set TESTSIGNING on\n\n3. Download and install the Polaris-Vega-Navi driver created by the real pros.\n\n[R.ID - AMD 3rd Party Drivers](https://rdn-id.com/)\n\nAll risks are on you, but I think it's better than getting divorced by your wife over buying an RTX 6000,\n\nThe blower fan sent by the Ali seller is very effective, but it's incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.\n\nP.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn't recognize it at all. Do I really have to rebuild PyTorch from scratch?",
          "author_fullname": "t2_1dhesoqqtu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Successfully running INSTINCT MI50 on Win11",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "3cabctch4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ca1ba2f5dc37ce486326b1c5d7f492e6257e417"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b47948a9375686a348f3135013fad572be0e5bb9"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c54a61000b055f717d4c99392991a3016a75c607"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e82c15d58578bccb18253e91c4a9d5a4a136e4bf"
                },
                {
                  "y": 448,
                  "x": 960,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0908df44a99335aef9b4daf3dd4796f130b6f492"
                },
                {
                  "y": 504,
                  "x": 1080,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=098996b1e02247a51bd56e72b50def71975ccc73"
                }
              ],
              "s": {
                "y": 897,
                "x": 1920,
                "u": "https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96"
              },
              "id": "3cabctch4sgf1"
            },
            "oe4uyadb4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ac61e80aae254bda55f9b7cd6addf90d29a446b"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fae70f72d90191dd274df1040d6d75cbbbd6475"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c28cb00db6a2547f8fead9579aa8b2fd3c53086"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9bdeb09d493de51c07e4fcc97ec462f8aac3b551"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e51eb15dd64da3685f668083f579b2705a4c55e"
                }
              ],
              "s": {
                "y": 1280,
                "x": 960,
                "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80"
              },
              "id": "oe4uyadb4sgf1"
            },
            "lrtre5656sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=445e311f264b6b0ba67f8c1a6fc639d332a2e5f8"
                },
                {
                  "y": 118,
                  "x": 216,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8becb210c0a1f400e0e6189f5b9ea272c0c7364"
                },
                {
                  "y": 175,
                  "x": 320,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b05acc3dcc8e5ab71a4080b7d0fb7e7f2dd9536"
                },
                {
                  "y": 350,
                  "x": 640,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce822ce9b7ef000d574cada4c7c415bfe2d44912"
                },
                {
                  "y": 526,
                  "x": 960,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc5d5e9ad324bd8954b842c0285c298a6f3a5ba1"
                },
                {
                  "y": 591,
                  "x": 1080,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18c465dd09dbecdf9cec8ef723dc48df07a9ff22"
                }
              ],
              "s": {
                "y": 1032,
                "x": 1883,
                "u": "https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;format=png&amp;auto=webp&amp;s=f14e51095d28e301c62418f12f52586a62861ee2"
              },
              "id": "lrtre5656sgf1"
            },
            "pvictbhu4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2e7b199737b649bf0dc517eeb27f38372ace77a"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=296d5c0e017c541acde4c0789322db6a0ee8ee9f"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8871508585c0b72a963270b0a8f2d645d8162a54"
                },
                {
                  "y": 380,
                  "x": 640,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f925d22a93079c9ac16e8bacc366d444a121475"
                },
                {
                  "y": 570,
                  "x": 960,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e4bce81c738e2313acab5f1b611fcbcd1bec79c"
                },
                {
                  "y": 642,
                  "x": 1080,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a90fa210d1e30a035719e4bdeba9f8afa3eb2e24"
                }
              ],
              "s": {
                "y": 1015,
                "x": 1707,
                "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9"
              },
              "id": "pvictbhu4sgf1"
            }
          },
          "name": "t3_1mgg3mh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ac8XNi9a_cAzytIhKD5Gg-Gwa0GLMmvvfiuUz9ia33E.jpg",
          "edited": 1754227118,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754217510,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, poor GPU guys&lt;/p&gt;\n\n&lt;p&gt;A few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.&lt;/p&gt;\n\n&lt;p&gt;Some married guys might understand, but I’ve been using an m-ATX case I bought about 15 years ago, and there’s no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to “real” work.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80\"&gt;https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;One of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That&amp;#39;s why the “32GB” model ended up in my hands.&lt;/p&gt;\n\n&lt;p&gt;Of course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the “Original MI50” cannot be BIOS-re-flashed, I had given up.&lt;/p&gt;\n\n&lt;p&gt;First, take a look at the results: the MI50 is running with  GTX 4070ti or alone on Windows.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9\"&gt;https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96\"&gt;4070+MI50 (22GB only)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f14e51095d28e301c62418f12f52586a62861ee2\"&gt;MI50 works alone upto 30GB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Guys, hold your horses. I&amp;#39;m aware there are a few issues here.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;It&amp;#39;s recognized as a Radeon Pro VII&lt;/li&gt;\n&lt;li&gt;It runs on LM STUDIO, which some people really dislike&lt;/li&gt;\n&lt;li&gt;Even if it&amp;#39;s recognized as Vulkan, you can&amp;#39;t use the combined VRAM of  hetero cards—only twice the VRAM of the first graphics card (On my PC, it&amp;#39;s 12+12GB instead of 12+32GB&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;-&amp;gt; However, I haven&amp;#39;t tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX&amp;#39;s prompt processing ability is an extra bonus.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Surely, you could use whole 32GB if you turn off other GPU and MI50 only!!!&lt;/strong&gt;&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Anyway, there are only three things you need to do.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Disable Secure Boot in the CMOS BIOS.&lt;/li&gt;\n&lt;li&gt;Run PowerShell in administrator mode and enter the following command:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;bcdedit.exe -set TESTSIGNING on&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download and install the Polaris-Vega-Navi driver created by the real pros.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://rdn-id.com/\"&gt;R.ID - AMD 3rd Party Drivers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;All risks are on you, but I think it&amp;#39;s better than getting divorced by your wife over buying an RTX 6000,&lt;/p&gt;\n\n&lt;p&gt;The blower fan sent by the Ali seller is very effective, but it&amp;#39;s incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.&lt;/p&gt;\n\n&lt;p&gt;P.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn&amp;#39;t recognize it at all. Do I really have to rebuild PyTorch from scratch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgg3mh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Sir-5088",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754217510,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Let's tease each other.\n\nWhat is your local AI setup?\nAre you proud of it?\nWhat would you have done differently?\n\nWhat model you use? Contrxt lenght? TPS?\n\nI only have a MBP2019, so I will just be teased 😂",
          "author_fullname": "t2_7lunrav9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Your proud AI setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgk2nm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754229855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s tease each other.&lt;/p&gt;\n\n&lt;p&gt;What is your local AI setup?\nAre you proud of it?\nWhat would you have done differently?&lt;/p&gt;\n\n&lt;p&gt;What model you use? Contrxt lenght? TPS?&lt;/p&gt;\n\n&lt;p&gt;I only have a MBP2019, so I will just be teased 😂&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgk2nm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Recent-Success-1520",
          "discussion_type": null,
          "num_comments": 62,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754229855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My question is how much can the sliders actually effect the voice because i feel like it sounds kinda robotic regardless of my settings, should I try a different audio clip or is there nothing I can do",
          "author_fullname": "t2_1hhil4cbg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Make chatterbox tts sound more realistic",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgrhcp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754247815,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My question is how much can the sliders actually effect the voice because i feel like it sounds kinda robotic regardless of my settings, should I try a different audio clip or is there nothing I can do&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgrhcp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeMan060",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgrhcp/make_chatterbox_tts_sound_more_realistic/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgrhcp/make_chatterbox_tts_sound_more_realistic/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754247815,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It feels kinda strange that every thing that pops up with voice models is specific to cloning. What if i want a unique voice? But searching, i don't even see discussions on training models to do anything but clone an existing voice. \n\nI imagine training a new voice from scratch would take a lot of different, high quality samples- but when i look at all the stuff that claims it can clone a voice from a couple short clips- it makes me wonder why that middle process couldn't be stepped through- either programatically (if RVC can clone a real voice, could adjusting some parameters create a similar-but-unique voice? including several samples if voices that fall in a similar range instead of a single person and try to clone that?  )  But googling, i can't really find anyone tackling anything except either using pre-baked voice models, or cloning someones existing voice. ",
          "author_fullname": "t2_164x4a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any options for creating  original tts models, not just cloning someone specific?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgzuky",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754269225,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It feels kinda strange that every thing that pops up with voice models is specific to cloning. What if i want a unique voice? But searching, i don&amp;#39;t even see discussions on training models to do anything but clone an existing voice. &lt;/p&gt;\n\n&lt;p&gt;I imagine training a new voice from scratch would take a lot of different, high quality samples- but when i look at all the stuff that claims it can clone a voice from a couple short clips- it makes me wonder why that middle process couldn&amp;#39;t be stepped through- either programatically (if RVC can clone a real voice, could adjusting some parameters create a similar-but-unique voice? including several samples if voices that fall in a similar range instead of a single person and try to clone that?  )  But googling, i can&amp;#39;t really find anyone tackling anything except either using pre-baked voice models, or cloning someones existing voice. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgzuky",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "moarmagic",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgzuky/are_there_any_options_for_creating_original_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgzuky/are_there_any_options_for_creating_original_tts/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754269225,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was pondering the problem of extending the useful lifespans of older models with knowledge cut-offs in the distant past, when something occurred to me with applications beyond just that.\n\nThe idea is that you would infer on a context with different models (ideally in parallel), but stop short of the minmax step, and append their logit lists together into one long list.\n\nI intuit that it might be necessary to multiply each model's logits by a scalar to normalize them before appending, but it's not clear to me if you could get away with a constant scalar for each model, or if it would need to be dynamic.  Maybe something as simple as choosing a scalar which makes the sums of the top N logits equal between the logit lists?  Not sure.\n\nThen the minmax step would convert all of the logits in the combined list to a probability distribution, and the next token chosen from that, as normal.\n\nMy initial thought was that older models could be thus augmented with newer models with new knowledge, which might be low-parameter and trained to the limit described in https://arxiv.org/abs/2505.24832, to maximize memorized knowledge per parameter.  That seems like a (relatively) cheap way to update the amalgam model's knowledge beyond the limits of RAG (which works great, but only to the limits of context).\n\nThinking about it further, though, it seems like it might also be a way to improve overall inference competence, by including models in the amalgam with comparable parameter counts but different skillsets.\n\nThat's as far as I've gotten.  Does it sound like something someone else has already done?  Does anyone see reasons it wouldn't work?",
          "author_fullname": "t2_cpegz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Idea for combining multiple models' inference via normalizing logit lists -- would it work, has someone already done it, and how could it be made better?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgzmmw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754268584,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was pondering the problem of extending the useful lifespans of older models with knowledge cut-offs in the distant past, when something occurred to me with applications beyond just that.&lt;/p&gt;\n\n&lt;p&gt;The idea is that you would infer on a context with different models (ideally in parallel), but stop short of the minmax step, and append their logit lists together into one long list.&lt;/p&gt;\n\n&lt;p&gt;I intuit that it might be necessary to multiply each model&amp;#39;s logits by a scalar to normalize them before appending, but it&amp;#39;s not clear to me if you could get away with a constant scalar for each model, or if it would need to be dynamic.  Maybe something as simple as choosing a scalar which makes the sums of the top N logits equal between the logit lists?  Not sure.&lt;/p&gt;\n\n&lt;p&gt;Then the minmax step would convert all of the logits in the combined list to a probability distribution, and the next token chosen from that, as normal.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was that older models could be thus augmented with newer models with new knowledge, which might be low-parameter and trained to the limit described in &lt;a href=\"https://arxiv.org/abs/2505.24832\"&gt;https://arxiv.org/abs/2505.24832&lt;/a&gt;, to maximize memorized knowledge per parameter.  That seems like a (relatively) cheap way to update the amalgam model&amp;#39;s knowledge beyond the limits of RAG (which works great, but only to the limits of context).&lt;/p&gt;\n\n&lt;p&gt;Thinking about it further, though, it seems like it might also be a way to improve overall inference competence, by including models in the amalgam with comparable parameter counts but different skillsets.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s as far as I&amp;#39;ve gotten.  Does it sound like something someone else has already done?  Does anyone see reasons it wouldn&amp;#39;t work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgzmmw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ttkciar",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754268584,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA! \n\nJust released something that extends the recent [ICM paper](https://arxiv.org/abs/2506.10139) in a big way - using one model's coherent understanding to improve a completely different model.\n\n# Background: What is ICM?\n\nThe original [\"Unsupervised Elicitation of Language Models\"](https://arxiv.org/abs/2506.10139) paper showed something remarkable: **models can generate their own training labels by finding internally coherent patterns**.\n\nTheir key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are \"mutually predictable\" - where each label can be predicted from all the others.\n\n**Original ICM results**: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.\n\n# Our extension: Cross-model capability transfer\n\nWe took ICM further - **what if we use one model's coherent understanding to improve a completely different model?**\n\n**Our process:**\n\n1. Used ICM on Qwen3 to extract its coherent math reasoning patterns\n2. Generated DPO training data from Qwen3's coherent vs incoherent solutions\n3. Trained Gemma3 on this data - **Gemma3 learned from Qwen3's understanding**\n4. Zero external supervision, pure model-to-model knowledge transfer\n\n# Results on local models\n\n**Qwen3-0.6B**: 63.2 → 66.0 MATH-500 (+4%) \\[original ICM self-improvement\\]  \n**Gemma3-1B**: 41.0 → 45.6 MATH-500 (+11%) \\[**novel: learned from Qwen3!**\\]\n\n**The breakthrough**: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3's abilities across different architectures.\n\n# Why this matters beyond the original paper\n\n* **Cross-model knowledge transfer** \\- use any strong model to improve your local models\n* **Democratizes capabilities** \\- extract from closed/expensive models to improve open ones\n* **No training data needed** \\- pure capability extraction and transfer\n* **Scales the ICM concept** \\- from self-improvement to ecosystem-wide improvement\n\n# What's available\n\n* **Code**: [https://github.com/codelion/icm](https://github.com/codelion/icm)\n* **Both models**: Self-improved Qwen3 + Gemma3 (learned from Qwen3)\n* **Transfer pipeline**: Extract from any model to improve another\n* **Full writeup**: [https://huggingface.co/blog/codelion/internal-coherence-maximization](https://huggingface.co/blog/codelion/internal-coherence-maximization)\n\n# Quick start\n\n    git clone https://github.com/codelion/icm.git &amp;&amp; cd icm &amp;&amp; pip install -e .\n    \n    # Extract coherent patterns from a strong model (teacher)\n    icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500\n    \n    # Use those patterns to improve your local model (student)\n    icm export --format dpo --output-path teacher_knowledge.jsonl\n    # Train your model on teacher_knowledge.jsonl\n\nAnyone interested in trying capability transfer with their local models?",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🧠 ICM+DPO: Used Qwen3's coherent understanding to improve Gemma3 at math - cross-model capability transfer with zero supervision",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdur5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754208625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt;\n\n&lt;p&gt;Just released something that extends the recent &lt;a href=\"https://arxiv.org/abs/2506.10139\"&gt;ICM paper&lt;/a&gt; in a big way - using one model&amp;#39;s coherent understanding to improve a completely different model.&lt;/p&gt;\n\n&lt;h1&gt;Background: What is ICM?&lt;/h1&gt;\n\n&lt;p&gt;The original &lt;a href=\"https://arxiv.org/abs/2506.10139\"&gt;&amp;quot;Unsupervised Elicitation of Language Models&amp;quot;&lt;/a&gt; paper showed something remarkable: &lt;strong&gt;models can generate their own training labels by finding internally coherent patterns&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are &amp;quot;mutually predictable&amp;quot; - where each label can be predicted from all the others.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Original ICM results&lt;/strong&gt;: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.&lt;/p&gt;\n\n&lt;h1&gt;Our extension: Cross-model capability transfer&lt;/h1&gt;\n\n&lt;p&gt;We took ICM further - &lt;strong&gt;what if we use one model&amp;#39;s coherent understanding to improve a completely different model?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Our process:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Used ICM on Qwen3 to extract its coherent math reasoning patterns&lt;/li&gt;\n&lt;li&gt;Generated DPO training data from Qwen3&amp;#39;s coherent vs incoherent solutions&lt;/li&gt;\n&lt;li&gt;Trained Gemma3 on this data - &lt;strong&gt;Gemma3 learned from Qwen3&amp;#39;s understanding&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Zero external supervision, pure model-to-model knowledge transfer&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Results on local models&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-0.6B&lt;/strong&gt;: 63.2 → 66.0 MATH-500 (+4%) [original ICM self-improvement]&lt;br/&gt;\n&lt;strong&gt;Gemma3-1B&lt;/strong&gt;: 41.0 → 45.6 MATH-500 (+11%) [&lt;strong&gt;novel: learned from Qwen3!&lt;/strong&gt;]&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The breakthrough&lt;/strong&gt;: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3&amp;#39;s abilities across different architectures.&lt;/p&gt;\n\n&lt;h1&gt;Why this matters beyond the original paper&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cross-model knowledge transfer&lt;/strong&gt; - use any strong model to improve your local models&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Democratizes capabilities&lt;/strong&gt; - extract from closed/expensive models to improve open ones&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;No training data needed&lt;/strong&gt; - pure capability extraction and transfer&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scales the ICM concept&lt;/strong&gt; - from self-improvement to ecosystem-wide improvement&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What&amp;#39;s available&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=\"https://github.com/codelion/icm\"&gt;https://github.com/codelion/icm&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Both models&lt;/strong&gt;: Self-improved Qwen3 + Gemma3 (learned from Qwen3)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Transfer pipeline&lt;/strong&gt;: Extract from any model to improve another&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Full writeup&lt;/strong&gt;: &lt;a href=\"https://huggingface.co/blog/codelion/internal-coherence-maximization\"&gt;https://huggingface.co/blog/codelion/internal-coherence-maximization&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Quick start&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/codelion/icm.git &amp;amp;&amp;amp; cd icm &amp;amp;&amp;amp; pip install -e .\n\n# Extract coherent patterns from a strong model (teacher)\nicm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500\n\n# Use those patterns to improve your local model (student)\nicm export --format dpo --output-path teacher_knowledge.jsonl\n# Train your model on teacher_knowledge.jsonl\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Anyone interested in trying capability transfer with their local models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdur5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754208625,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've been working on an LLM proxy, balancer &amp; model unifier based on a few other projects we've created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. \n\nThis was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),\n\nSo some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.\n\nOlla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.\n\nAdd your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.\n\nThe attempt to unify across providers wasn't as successful - as in, both LMStudio &amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.\n\nGithub: [https://github.com/thushan/olla](https://github.com/thushan/olla) (golang)\n\nWould love to know your thoughts. \n\nOlla is still in its infancy, so we don't have auth implemented etc but there are plans in the future.",
          "author_fullname": "t2_116dje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Announcing Olla - LLM Load Balancer, Proxy &amp; Model Unifier for Ollama / LM Studio &amp; OpenAI Compatible backends",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5nw1vlfoqpgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb3a2dbb77b7c17f0c25c782f45d5f1145b6df22"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31ee39f327a201eb2bd4da0cab803303d2c7f1d4"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb4a2254c5239c0b216fd14bb6c0f0e38e7b6f82"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97c3ad10efc7e082eca21673151d8898dbe7206d"
                }
              ],
              "s": {
                "y": 648,
                "x": 648,
                "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=648&amp;format=png&amp;auto=webp&amp;s=d12b43c56e8a84fc36c0b40b7f880085d0c6c4c4"
              },
              "id": "5nw1vlfoqpgf1"
            },
            "tfkhbanjqpgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 151,
                  "x": 108,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52ec679a9bd4d342bee02ef2f1165eb230e72cce"
                },
                {
                  "y": 302,
                  "x": 216,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d21c96bf9e46c48e0c9f056396472b42eebe4857"
                },
                {
                  "y": 448,
                  "x": 320,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3471f6cecc4f0ca3888500245adb1fa96e1a0776"
                },
                {
                  "y": 896,
                  "x": 640,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d04c10c876d43302f632b7e7171a15a08b4e8666"
                }
              ],
              "s": {
                "y": 1077,
                "x": 769,
                "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=769&amp;format=png&amp;auto=webp&amp;s=68776f8d4bf7c2b5ade021a1617e272f3a83f6f0"
              },
              "id": "tfkhbanjqpgf1"
            },
            "pvipbao9npgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08ec5455f30f170ffdb8fb10a28e530de3ffc87f"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=983873e2a890b1d5b4d1549b6554d0d96e4c6147"
                },
                {
                  "y": 176,
                  "x": 320,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b29670ef3ff3bf1e93b9734818e1ea388750f683"
                }
              ],
              "s": {
                "y": 249,
                "x": 451,
                "u": "https://preview.redd.it/pvipbao9npgf1.png?width=451&amp;format=png&amp;auto=webp&amp;s=de1da506433d6cdd0870b0b2a309a56defbfdaf9"
              },
              "id": "pvipbao9npgf1"
            }
          },
          "name": "t3_1mg7qpa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 61,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "pvipbao9npgf1",
                "id": 719953590
              },
              {
                "media_id": "tfkhbanjqpgf1",
                "id": 719953591
              },
              {
                "media_id": "5nw1vlfoqpgf1",
                "id": 719953592
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aNDVZRvyIDhy6vwT1mm7Ch4ypRPRTf4JSNM_Np7grwg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754187279,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been working on an LLM proxy, balancer &amp;amp; model unifier based on a few other projects we&amp;#39;ve created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. &lt;/p&gt;\n\n&lt;p&gt;This was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp;amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),&lt;/p&gt;\n\n&lt;p&gt;So some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.&lt;/p&gt;\n\n&lt;p&gt;Olla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.&lt;/p&gt;\n\n&lt;p&gt;Add your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.&lt;/p&gt;\n\n&lt;p&gt;The attempt to unify across providers wasn&amp;#39;t as successful - as in, both LMStudio &amp;amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/thushan/olla\"&gt;https://github.com/thushan/olla&lt;/a&gt; (golang)&lt;/p&gt;\n\n&lt;p&gt;Would love to know your thoughts. &lt;/p&gt;\n\n&lt;p&gt;Olla is still in its infancy, so we don&amp;#39;t have auth implemented etc but there are plans in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mg7qpa",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg7qpa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "2shanigans",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mg7qpa",
          "subreddit_subscribers": 509624,
          "created_utc": 1754187279,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was using this model as an assistant to modify code in a C++ file with \\~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:\n\n&gt;Here is the modification of the code:\n\n&gt;*\\*code\\**\n\n&gt;But on a second thought, that was not a good implementation, here is a better method to do it:\n\n&gt;*\\*code\\**\n\n&gt;But on a third thought, here is a better way to do it...\n\nAnd each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:\n\n&gt;How do I set a left, right and bottom border on a div in CSS?\n\nIt then provided me with 4 methods. Notice how **method 1** and **method 4** is the *exact same* code repeated. **Method 2** is also almost the exact same code but just with an added `border: none;` line of code. \n\nAlso, **method 3** has a mistake where it do not to set a border on the right side of the div, it wrote `border-width: 0 0 2px 2px;` but the correct code would be `border-width: 0 2px 2px 2px;`\n\n**The output:**\n\n# Method 1: Using Individual Border Properties\n\n    div {\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\n# Method 2: Using the Shorthand border Property\n\n    div {\n        border: none; /* Reset all borders first */\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\n# Method 3: Using border-width, border-style, and border-color\n\n    div {\n        border-width: 0 0 2px 2px; /* top right bottom left */\n        border-style: solid;\n        border-color: black;\n    }\n\n# Method 4: More Concise Shorthand\n\n    div {\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\nI'm using Unsloth's UD-Q5\\_K\\_XL quant with the recommended settings:\n\n* Temperature: **0,7**\n* Top K: **20**\n* Repeat Penalty: **1,05**\n* Top P: **0,8**\n\nAnyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?",
          "author_fullname": "t2_qhlcbiy3k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you also get weird behavior from Qwen3-Coder-30B-A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfs7l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754216810,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754216282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was using this model as an assistant to modify code in a C++ file with ~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Here is the modification of the code:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;code**&lt;/p&gt;\n\n&lt;p&gt;But on a second thought, that was not a good implementation, here is a better method to do it:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;code**&lt;/p&gt;\n\n&lt;p&gt;But on a third thought, here is a better way to do it...&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;And each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;How do I set a left, right and bottom border on a div in CSS?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It then provided me with 4 methods. Notice how &lt;strong&gt;method 1&lt;/strong&gt; and &lt;strong&gt;method 4&lt;/strong&gt; is the &lt;em&gt;exact same&lt;/em&gt; code repeated. &lt;strong&gt;Method 2&lt;/strong&gt; is also almost the exact same code but just with an added &lt;code&gt;border: none;&lt;/code&gt; line of code. &lt;/p&gt;\n\n&lt;p&gt;Also, &lt;strong&gt;method 3&lt;/strong&gt; has a mistake where it do not to set a border on the right side of the div, it wrote &lt;code&gt;border-width: 0 0 2px 2px;&lt;/code&gt; but the correct code would be &lt;code&gt;border-width: 0 2px 2px 2px;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The output:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Method 1: Using Individual Border Properties&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 2: Using the Shorthand border Property&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border: none; /* Reset all borders first */\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 3: Using border-width, border-style, and border-color&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-width: 0 0 2px 2px; /* top right bottom left */\n    border-style: solid;\n    border-color: black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 4: More Concise Shorthand&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m using Unsloth&amp;#39;s UD-Q5_K_XL quant with the recommended settings:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Temperature: &lt;strong&gt;0,7&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Top K: &lt;strong&gt;20&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Repeat Penalty: &lt;strong&gt;1,05&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Top P: &lt;strong&gt;0,8&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgfs7l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Admirable-Star7088",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754216282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nPersonal note: This is just my opinion based on a very limited set of API-only probes—interpret with caution.\n\nThis is about probing Horizon Beta (on openrouter) \n\nWhat I did (mini-ROC probes)\n\n* JSON strictness vs. \"bad schema\" repair\n* Tool-calling with an invalid enum + extra property\n* Safety/refusal phrasing check\n* Long-context end-marker recall\n* Tokenizer/short-output edge case\n* Determinism at T=0\n* Tiny style-paraphrase probe\n\nHighlights\n\n* Tool-calling: It silently coerces invalid enums (mode=\"plane\" -&gt; \"car\"/\"train\") and drops extra fields, then emits an OpenAI-style tool\\_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn't call the tool under the same bad input - which is more typical for OpenAI.\n* JSON mode: It \"repairs\" invalid inputs into valid JSON (e.g., {\"ok\": false, \"mode\": \"A\"}). OpenAI also repairs but tends to be more minimally formatted.\n* Safety tone: Opens with \"I can't help with that.\" - Anthropic-ish cadence that many Llama-style distills mimic.\n* Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END\\_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.\n* Determinism: Stable at T=0 on simple tasks.\n* Multilingual: Correct 妹妹 -&gt; \"younger sister,\" and clean pronoun disambiguation.\n\nAnchors I compared against\n\n* OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)\n* Llama: llama-3.3-70b-instruct, llama-3-70b-instruct\n* Qwen: qwen-2.5-72b-instruct\n* Mistral: mixtral-8x22b-instruct\n\nBottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific \"finish=length\" quirk on short outputs. It does not match OpenAI's tool-call behavior in my probes.\n\nAll tests were standard API usage.",
          "author_fullname": "t2_2fmsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OSINT fingerprinting a stealth OpenRouter model - likely Llama-family, not OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mggsyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754224198,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754220059,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Personal note: This is just my opinion based on a very limited set of API-only probes—interpret with caution.&lt;/p&gt;\n\n&lt;p&gt;This is about probing Horizon Beta (on openrouter) &lt;/p&gt;\n\n&lt;p&gt;What I did (mini-ROC probes)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;JSON strictness vs. &amp;quot;bad schema&amp;quot; repair&lt;/li&gt;\n&lt;li&gt;Tool-calling with an invalid enum + extra property&lt;/li&gt;\n&lt;li&gt;Safety/refusal phrasing check&lt;/li&gt;\n&lt;li&gt;Long-context end-marker recall&lt;/li&gt;\n&lt;li&gt;Tokenizer/short-output edge case&lt;/li&gt;\n&lt;li&gt;Determinism at T=0&lt;/li&gt;\n&lt;li&gt;Tiny style-paraphrase probe&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Highlights&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tool-calling: It silently coerces invalid enums (mode=&amp;quot;plane&amp;quot; -&amp;gt; &amp;quot;car&amp;quot;/&amp;quot;train&amp;quot;) and drops extra fields, then emits an OpenAI-style tool_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn&amp;#39;t call the tool under the same bad input - which is more typical for OpenAI.&lt;/li&gt;\n&lt;li&gt;JSON mode: It &amp;quot;repairs&amp;quot; invalid inputs into valid JSON (e.g., {&amp;quot;ok&amp;quot;: false, &amp;quot;mode&amp;quot;: &amp;quot;A&amp;quot;}). OpenAI also repairs but tends to be more minimally formatted.&lt;/li&gt;\n&lt;li&gt;Safety tone: Opens with &amp;quot;I can&amp;#39;t help with that.&amp;quot; - Anthropic-ish cadence that many Llama-style distills mimic.&lt;/li&gt;\n&lt;li&gt;Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.&lt;/li&gt;\n&lt;li&gt;Determinism: Stable at T=0 on simple tasks.&lt;/li&gt;\n&lt;li&gt;Multilingual: Correct 妹妹 -&amp;gt; &amp;quot;younger sister,&amp;quot; and clean pronoun disambiguation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anchors I compared against&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)&lt;/li&gt;\n&lt;li&gt;Llama: llama-3.3-70b-instruct, llama-3-70b-instruct&lt;/li&gt;\n&lt;li&gt;Qwen: qwen-2.5-72b-instruct&lt;/li&gt;\n&lt;li&gt;Mistral: mixtral-8x22b-instruct&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Bottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific &amp;quot;finish=length&amp;quot; quirk on short outputs. It does not match OpenAI&amp;#39;s tool-call behavior in my probes.&lt;/p&gt;\n\n&lt;p&gt;All tests were standard API usage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mggsyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jv0010",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754220059,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a documentation system designed to help LLMs understand codebases more efficiently, and I'd love feedback from this community.\n\nThe Problem:\n- Feeding entire codebases/docs to LLMs wastes tokens and context\n- Most tasks only need a subset of information  \n- Human-written docs aren't structured for LLM consumption\n- Different models have different context windows\n\nThe Approach:\nA system that generates multiple \"zoom levels\" of the same documentation:\n- Overview (500 tokens) - capabilities and high-level structure\n- Architecture (2K tokens) - patterns, flows, integrations  \n- Full Details (8K+ tokens) - complete implementation specs\n\nLLMs can:\n- Start with Overview\n- Request deeper levels only if needed\n- Save 75%+ tokens on many tasks\n\nExample: \"Does this API support webhooks?\" → Overview is sufficient. \"Modify the authentication flow\" → Needs Full Details.\n\nAlso exploring different expression modes (technical/standard/friendly) for human readers, but the LLM optimization is my main focus.\n\nQuestions:\n1. How do you currently handle feeding docs/specs to your local models?\n2. Would progressive loading actually help with your context limits?\n3. What structure would work best for models like Mixtral, Llama, etc?\n4. Is this overengineering vs just better chunking strategies?\n\nNot building a product yet - trying to validate if this solves real problems people have with local models and limited context.\n\nThanks for any thoughts!",
          "author_fullname": "t2_9mh0wady",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Title: Reducing token usage with progressive documentation loading - looking for feedback on approach",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgytca",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754266233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a documentation system designed to help LLMs understand codebases more efficiently, and I&amp;#39;d love feedback from this community.&lt;/p&gt;\n\n&lt;p&gt;The Problem:\n- Feeding entire codebases/docs to LLMs wastes tokens and context\n- Most tasks only need a subset of information&lt;br/&gt;\n- Human-written docs aren&amp;#39;t structured for LLM consumption\n- Different models have different context windows&lt;/p&gt;\n\n&lt;p&gt;The Approach:\nA system that generates multiple &amp;quot;zoom levels&amp;quot; of the same documentation:\n- Overview (500 tokens) - capabilities and high-level structure\n- Architecture (2K tokens) - patterns, flows, integrations&lt;br/&gt;\n- Full Details (8K+ tokens) - complete implementation specs&lt;/p&gt;\n\n&lt;p&gt;LLMs can:\n- Start with Overview\n- Request deeper levels only if needed\n- Save 75%+ tokens on many tasks&lt;/p&gt;\n\n&lt;p&gt;Example: &amp;quot;Does this API support webhooks?&amp;quot; → Overview is sufficient. &amp;quot;Modify the authentication flow&amp;quot; → Needs Full Details.&lt;/p&gt;\n\n&lt;p&gt;Also exploring different expression modes (technical/standard/friendly) for human readers, but the LLM optimization is my main focus.&lt;/p&gt;\n\n&lt;p&gt;Questions:\n1. How do you currently handle feeding docs/specs to your local models?\n2. Would progressive loading actually help with your context limits?\n3. What structure would work best for models like Mixtral, Llama, etc?\n4. Is this overengineering vs just better chunking strategies?&lt;/p&gt;\n\n&lt;p&gt;Not building a product yet - trying to validate if this solves real problems people have with local models and limited context.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgytca",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrsockpicks",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgytca/title_reducing_token_usage_with_progressive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgytca/title_reducing_token_usage_with_progressive/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754266233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new to this and trying to figure out what the reasonable limits of my PC are for running these things. Still trying to understand quantization and offloading. I have 12 gb vram and 64 gb ddr4 regular ram available.\n\nWhat's the smartest LLM of the models now out I'll be able to run at a reasonable speed on this?",
          "author_fullname": "t2_6qwey",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help me pick a first model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgypja",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754265938,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this and trying to figure out what the reasonable limits of my PC are for running these things. Still trying to understand quantization and offloading. I have 12 gb vram and 64 gb ddr4 regular ram available.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the smartest LLM of the models now out I&amp;#39;ll be able to run at a reasonable speed on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgypja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dracofrost",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgypja/help_me_pick_a_first_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgypja/help_me_pick_a_first_model/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754265938,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "During my internship at a big tech company, I struggled with a massive, messy codebase. Too many changes were impossible to understand either because of vague commit messages or because the original authors had left.\n\nFrustrated by losing so much context in git history, I built Gitdive: a local CLI tool that lets you have natural language conversations your repo's history.\n\nIt's early in development and definitely buggy, but if you've faced similar issues, I'd really appreciate your feedback.\n\nCheck it out: [https://github.com/ascl1u/gitdive](https://github.com/ascl1u/gitdive)",
          "author_fullname": "t2_zfhna",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a CLI tool to fix my biggest git frustration: lost commit context",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgyp1z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754265900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;During my internship at a big tech company, I struggled with a massive, messy codebase. Too many changes were impossible to understand either because of vague commit messages or because the original authors had left.&lt;/p&gt;\n\n&lt;p&gt;Frustrated by losing so much context in git history, I built Gitdive: a local CLI tool that lets you have natural language conversations your repo&amp;#39;s history.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s early in development and definitely buggy, but if you&amp;#39;ve faced similar issues, I&amp;#39;d really appreciate your feedback.&lt;/p&gt;\n\n&lt;p&gt;Check it out: &lt;a href=\"https://github.com/ascl1u/gitdive\"&gt;https://github.com/ascl1u/gitdive&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?auto=webp&amp;s=73de85844faa94867152778081acea6bd0f9f608",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f02a3f481cf62f18021fe3d1ce90e5364980dd8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b14421e555358b82c89e2d197f8d592cec18f456",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d78ea4d0a5bf464254888775d8ecfcf1e787271",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=57b58b91bbb414ff690236f5866c7a719c090746",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f35c021e464e431de773ce6ac3ebd30329e18c2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09b17e71607479cb71a0eb94a76b9bcf76ae4b84",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mgyp1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Forgotten_Person",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgyp1z/building_a_cli_tool_to_fix_my_biggest_git/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgyp1z/building_a_cli_tool_to_fix_my_biggest_git/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754265900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/X5R/ik\\_llama.cpp](https://huggingface.co/X5R/ik_llama.cpp)",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a prebuilt windows binary for ik_llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mga3ox",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754194792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/X5R/ik_llama.cpp\"&gt;https://huggingface.co/X5R/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?auto=webp&amp;s=604c4ccc8334b2fd3bc45aef3d71c1f1387be183",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbe1684043115eb539138a0a7146d28825949011",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce0f8475a5c3b84dea7c2f88281f21a3c1a17826",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=51d398818a25e58624880d21c1d75619f09a0da1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2015b60adf6417112d9c2005b37391d7d34c146",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a0022fb3e7640827993adc2f7fd214a61a84a85",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6ae8f5d18d064bdac87a68ac91093637b93b373",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mga3ox",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754194792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just curious what the consensus is on best website to run this model (or any model really) for coding. Price and quality are key. \n\nThanks",
          "author_fullname": "t2_1i6lvuup5v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is best website to pay to run Qwen3b to code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgy28u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754264157,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious what the consensus is on best website to run this model (or any model really) for coding. Price and quality are key. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgy28u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Complex-Emergency-60",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgy28u/what_is_best_website_to_pay_to_run_qwen3b_to_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgy28u/what_is_best_website_to_pay_to_run_qwen3b_to_code/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754264157,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "That's it. I'm seeing some card readers on line. But, they are not reliable enough. How  could us improve it with a extremely small LLM? ",
          "author_fullname": "t2_1hra1kibwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with a project. What is or would be the lesser local LLM that one can use to recognize TCG cards (pkmn, YGO, mtg) in a reliable way and generate an CSV file with, language, edition, card name and treatment?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgy0tg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754264052,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s it. I&amp;#39;m seeing some card readers on line. But, they are not reliable enough. How  could us improve it with a extremely small LLM? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgy0tg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Turbulent_Pin7635",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgy0tg/help_with_a_project_what_is_or_would_be_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgy0tg/help_with_a_project_what_is_or_would_be_the/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754264052,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I finally got the hardware I needed to run LLMs locally.\n\nAfter some testing, here’s a quick recap of my experience so far:\n\nJan.ai: Amazing performance. It runs my models incredibly well, and overall the experience was smooth. However, I really need features like image upload, web search, and deep research capabilities, which are still missing or limited.\n\nLM Studio: While the performance was slightly worse compared to [Jan.ai](http://Jan.ai) on the same models, I had the exact same feature limitations.\n\nAnythingLLM and OpenWebUI: This was... frustrating. I got long, irrelevant, looping replies using the same models that worked perfectly on Jan and LM Studio. Web browsing didn't work at all, the agent would say it was going to search online, then immediately say it couldn’t. Completely unusable for my needs. \n\nLocalAI: This one gave me a headache. YAML hell, GUFF model issues, and very unstable behavior, it might start once, and then fail ten times for no reason. Even when it said it was running, localhost just returned “connection failed”.\n\nSo here’s my question:\n\nIs there any tool out there that’s easier to set up (semi plug-and-play), where I can use different models for different tasks, or maybe even one good multimodal model, that can support these features?\n\nAgent mode\n\nDeep research\n\nImage generation\n\nInternet search\n\nCanvas sharing\n\nReading images or files\n\nMemory\n\nI'm trying to build a mini ChatGPT-style assistant for work, but completely locally and private. I have average tech skills, but I'm starting to lose my mind over how fragmented and unreliable everything feels for a newb in this field.\n\nAny advice is welcome.",
          "author_fullname": "t2_d0wbvrzc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Most complete almost plug and play LLM Tool with features",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgvyyj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754258843,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754258586,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got the hardware I needed to run LLMs locally.&lt;/p&gt;\n\n&lt;p&gt;After some testing, here’s a quick recap of my experience so far:&lt;/p&gt;\n\n&lt;p&gt;Jan.ai: Amazing performance. It runs my models incredibly well, and overall the experience was smooth. However, I really need features like image upload, web search, and deep research capabilities, which are still missing or limited.&lt;/p&gt;\n\n&lt;p&gt;LM Studio: While the performance was slightly worse compared to &lt;a href=\"http://Jan.ai\"&gt;Jan.ai&lt;/a&gt; on the same models, I had the exact same feature limitations.&lt;/p&gt;\n\n&lt;p&gt;AnythingLLM and OpenWebUI: This was... frustrating. I got long, irrelevant, looping replies using the same models that worked perfectly on Jan and LM Studio. Web browsing didn&amp;#39;t work at all, the agent would say it was going to search online, then immediately say it couldn’t. Completely unusable for my needs. &lt;/p&gt;\n\n&lt;p&gt;LocalAI: This one gave me a headache. YAML hell, GUFF model issues, and very unstable behavior, it might start once, and then fail ten times for no reason. Even when it said it was running, localhost just returned “connection failed”.&lt;/p&gt;\n\n&lt;p&gt;So here’s my question:&lt;/p&gt;\n\n&lt;p&gt;Is there any tool out there that’s easier to set up (semi plug-and-play), where I can use different models for different tasks, or maybe even one good multimodal model, that can support these features?&lt;/p&gt;\n\n&lt;p&gt;Agent mode&lt;/p&gt;\n\n&lt;p&gt;Deep research&lt;/p&gt;\n\n&lt;p&gt;Image generation&lt;/p&gt;\n\n&lt;p&gt;Internet search&lt;/p&gt;\n\n&lt;p&gt;Canvas sharing&lt;/p&gt;\n\n&lt;p&gt;Reading images or files&lt;/p&gt;\n\n&lt;p&gt;Memory&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to build a mini ChatGPT-style assistant for work, but completely locally and private. I have average tech skills, but I&amp;#39;m starting to lose my mind over how fragmented and unreliable everything feels for a newb in this field.&lt;/p&gt;\n\n&lt;p&gt;Any advice is welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?auto=webp&amp;s=b61c13003a78792af3c70dba491521f8befe780b",
                  "width": 2400,
                  "height": 1350
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=316ac2c235dbf757adc6d57077bbf14ff212c7fd",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ebed2d4cb10be2cddc86b49f0bc6f6f16178fef",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7726b791c72a2145f757f5c6e90ddb14254a02e8",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbfa85537dbc57841336cb1db86b585484d97de4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=749aec5f85de17cb9987662552bbb052f4717125",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39a55dd433726ea925f43fa48706981f88afb2dc",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgvyyj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExtensionAd182",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754258586,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is just a little remark that if you haven't you definitely should try qwen code [https://github.com/QwenLM/qwen-code](https://github.com/QwenLM/qwen-code)   \nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. \n\nThe metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.\n\nSo this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. ",
          "author_fullname": "t2_n9dnke1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code + Qwen Coder 30b 3A is insane",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuiri",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 237,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 237,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151475,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is just a little remark that if you haven&amp;#39;t you definitely should try qwen code &lt;a href=\"https://github.com/QwenLM/qwen-code\"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br/&gt;\nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I&amp;#39;m working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt;\n\n&lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I&amp;#39;m very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt;\n\n&lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don&amp;#39;t know jack shit about coding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?auto=webp&amp;s=689c8ef1840eeb6bb6bb53a5908a342a3dac7558",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2382140736e501c0e5f725eb9004d88daaf4ddc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fd612828e87be93104f75281f9696b4b69068",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aef0b7f26ce8aa8b563291337614fb5656c8ab07",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=faaeb8e0dc19d67afd0d3197e7b7a5a0095a6f09",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eebbb21f63208ab9ba4a6a50b9890a328d60d830",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2edc81e58030926a09a63f9377700b5cd6432c27",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfuiri",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flashy_Management962",
          "discussion_type": null,
          "num_comments": 108,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754151475,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. \n\n\nEdit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. \n\nThanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)",
          "author_fullname": "t2_67bb0jdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source model that is as intelligent as Claude Sonnet 4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqejn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 385,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 385,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754151213,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754140871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt;\n\n&lt;p&gt;Edit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfqejn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vishwa1238",
          "discussion_type": null,
          "num_comments": 278,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754140871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thx",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best uncensored +50B parameters model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv74s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgv74s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv74s/best_uncensored_50b_parameters_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv74s/best_uncensored_50b_parameters_model/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754256666,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?",
          "author_fullname": "t2_9vs7zm4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news on updated Qwen3-8B/14B versions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg6xia",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754184771,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg6xia",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zyxwvu54321",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754184771,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Testing Qwen Coder CLI with Qwen3-30B-A3B-Instruct-2507-Q4\\_K\\_S.gguf +LM Studio 0.3.21 (Build 3).  \nAfter initial folder and file read (`app/main.go`, `configs.json`, etc.), it keeps replying:  \n*\"I'm ready to assist with your project in /srv/testproject...\"*\n\nIt ignores direct inputs like:\n\n* \"What does this application do?\"\n* \"Explain me the project\"\n* \"Give me a function list\"\n* \"List all files\"\n\nNo actual answers, just the same boilerplate response:\n\nUnderstood. I'm ready to assist with your project in /srv/testproject. Let me know what you'd like to do—whether it's modifying code, adding features, debugging, or exploring the structure.\n\nAnyone else experiencing this with the latest combo? Misconfigured or bug?\n\n  \n\\--\n\nAs example Qwen 14B works fine.",
          "author_fullname": "t2_mhb0rkd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf + LM Studio 0.3.21 (Build 3): Assistant ignores questions, stuck in loop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgeerv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754210842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Testing Qwen Coder CLI with Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf +LM Studio 0.3.21 (Build 3).&lt;br/&gt;\nAfter initial folder and file read (&lt;code&gt;app/main.go&lt;/code&gt;, &lt;code&gt;configs.json&lt;/code&gt;, etc.), it keeps replying:&lt;br/&gt;\n&lt;em&gt;&amp;quot;I&amp;#39;m ready to assist with your project in /srv/testproject...&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;It ignores direct inputs like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;What does this application do?&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Explain me the project&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Give me a function list&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;List all files&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;No actual answers, just the same boilerplate response:&lt;/p&gt;\n\n&lt;p&gt;Understood. I&amp;#39;m ready to assist with your project in /srv/testproject. Let me know what you&amp;#39;d like to do—whether it&amp;#39;s modifying code, adding features, debugging, or exploring the structure.&lt;/p&gt;\n\n&lt;p&gt;Anyone else experiencing this with the latest combo? Misconfigured or bug?&lt;/p&gt;\n\n&lt;p&gt;--&lt;/p&gt;\n\n&lt;p&gt;As example Qwen 14B works fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgeerv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Eden63",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgeerv/qwen330ba3binstruct2507q4_k_sgguf_lm_studio_0321/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgeerv/qwen330ba3binstruct2507q4_k_sgguf_lm_studio_0321/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754210842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.\n\nThe take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.\n\nWe need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.",
          "author_fullname": "t2_1puly589vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Note to the Qwen team re. the new 30B A3B Coder and Instruct versions: Coder is lobotomized when compared to Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3d62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 59,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 59,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174302,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.&lt;/p&gt;\n\n&lt;p&gt;The take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.&lt;/p&gt;\n\n&lt;p&gt;We need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg3d62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackdareel",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754174302,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Would you rather have a samsung 9100 pro 4tb vs WD\\_BLACK 8TB SN850X for llama.cpp? The samsung is twice as fast but the WD is twice as big. They cost roughly the same with the WD being slightly more expensive.\n\nI already have a samsung 990 pro 4tb and it's fine unless I need to work with safetensor files, in which case I end up having to delete and re-download models to save space, which is annoying.  \n  \nI'm considering this as a second drive, probably mounted on a SABRENT M.2 NVMe SSD to PCIe x16 because I can't figure out how to get from MCIO 8i to a m.2 with PCIe5 / Gen 5 capability.\n\nI'm wondering if models will load into RAM any faster with the faster drive, or if I should just go with the WD.",
          "author_fullname": "t2_ozxxf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "samsung 9100 pro 4tb vs WD_BLACK 8TB SN850X for llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgtrvz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754253267,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would you rather have a samsung 9100 pro 4tb vs WD_BLACK 8TB SN850X for llama.cpp? The samsung is twice as fast but the WD is twice as big. They cost roughly the same with the WD being slightly more expensive.&lt;/p&gt;\n\n&lt;p&gt;I already have a samsung 990 pro 4tb and it&amp;#39;s fine unless I need to work with safetensor files, in which case I end up having to delete and re-download models to save space, which is annoying.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering this as a second drive, probably mounted on a SABRENT M.2 NVMe SSD to PCIe x16 because I can&amp;#39;t figure out how to get from MCIO 8i to a m.2 with PCIe5 / Gen 5 capability.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if models will load into RAM any faster with the faster drive, or if I should just go with the WD.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgtrvz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "createthiscom",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtrvz/samsung_9100_pro_4tb_vs_wd_black_8tb_sn850x_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgtrvz/samsung_9100_pro_4tb_vs_wd_black_8tb_sn850x_for/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754253267,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Could this be the one? Our one? Our precious?",
          "author_fullname": "t2_cy3wb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bolt Graphics (@BoltGraphicsInc) on X",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgtbeq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754252197,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could this be the one? Our one? Our precious?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/BoltGraphicsInc/status/1952049562912530494",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgtbeq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DrVonSinistro",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtbeq/bolt_graphics_boltgraphicsinc_on_x/",
          "stickied": false,
          "url": "https://x.com/BoltGraphicsInc/status/1952049562912530494",
          "subreddit_subscribers": 509624,
          "created_utc": 1754252197,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know it is a very amateur question but I am having a headache with this. I have downloaded llama 3.1 8B from meta and painfully converted them to gguf so I could use them with llama.cpp but when I use my gguf it just outputs random stuff that he is Jarvis! I tested system prompts but it changed nothing! my initial problem was that I used to use llama with ollama in my code but then after some while the LLM would output gibberish like a lot of @@@@ and no error whatsoever about how to fix it so I thought maybe the problem is with ollama and I should download the original weights.",
          "author_fullname": "t2_4kas3n6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Raw weights answer gibberish while ollama answers just fine!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgstni",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754251048,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it is a very amateur question but I am having a headache with this. I have downloaded llama 3.1 8B from meta and painfully converted them to gguf so I could use them with llama.cpp but when I use my gguf it just outputs random stuff that he is Jarvis! I tested system prompts but it changed nothing! my initial problem was that I used to use llama with ollama in my code but then after some while the LLM would output gibberish like a lot of @@@@ and no error whatsoever about how to fix it so I thought maybe the problem is with ollama and I should download the original weights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgstni",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Biodie",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgstni/raw_weights_answer_gibberish_while_ollama_answers/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgstni/raw_weights_answer_gibberish_while_ollama_answers/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754251048,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was wondering what mini models you guys were using and what’s good and what isn’t, I mostly just need something for quick categorization, I was using ChatGPT 4o-mini api for most things but I should probably swap to something local at this point ",
          "author_fullname": "t2_e49kxbgs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source alternatives to gpt 4o mini?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjvn8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754229360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering what mini models you guys were using and what’s good and what isn’t, I mostly just need something for quick categorization, I was using ChatGPT 4o-mini api for most things but I should probably swap to something local at this point &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgjvn8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abaris243",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjvn8/open_source_alternatives_to_gpt_4o_mini/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjvn8/open_source_alternatives_to_gpt_4o_mini/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754229360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.\n\nIf you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.\n\n2060 - 336.0 GB/s - $150 8GB  \n3060 - 360.0 GB/s - $200+ 8GB\n\n4060 - 272.0 GB/s - $260+ 8GB\n\n5060 - 448.0 GB/s - $350+ 8GB\n\nP102-100 - 440.3 GB/s - $60 10GB.\n\nIs the P102-100 faster than an\n\nentry 2060 = yes\n\nentry 3060 = yes\n\nentry 4060 = yes.\n\nonly a 5060 would be faster and not by much.\n\nDoes the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.\n\nI have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.\n\nso if you want to try LLM's and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.\n\nI hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.\n\nI will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.",
          "author_fullname": "t2_vnvnb9oa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "is the P102-100 still a viable option for LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdh6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ZS0zNrB2tczhH3IbcM5RQIgldEEZAXIMP8grHZwjSys.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754207144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.&lt;/p&gt;\n\n&lt;p&gt;If you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.&lt;/p&gt;\n\n&lt;p&gt;2060 - 336.0 GB/s - $150 8GB&lt;br/&gt;\n3060 - 360.0 GB/s - $200+ 8GB&lt;/p&gt;\n\n&lt;p&gt;4060 - 272.0 GB/s - $260+ 8GB&lt;/p&gt;\n\n&lt;p&gt;5060 - 448.0 GB/s - $350+ 8GB&lt;/p&gt;\n\n&lt;p&gt;P102-100 - 440.3 GB/s - $60 10GB.&lt;/p&gt;\n\n&lt;p&gt;Is the P102-100 faster than an&lt;/p&gt;\n\n&lt;p&gt;entry 2060 = yes&lt;/p&gt;\n\n&lt;p&gt;entry 3060 = yes&lt;/p&gt;\n\n&lt;p&gt;entry 4060 = yes.&lt;/p&gt;\n\n&lt;p&gt;only a 5060 would be faster and not by much.&lt;/p&gt;\n\n&lt;p&gt;Does the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.&lt;/p&gt;\n\n&lt;p&gt;I have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.&lt;/p&gt;\n\n&lt;p&gt;so if you want to try LLM&amp;#39;s and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.&lt;/p&gt;\n\n&lt;p&gt;I hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.&lt;/p&gt;\n\n&lt;p&gt;I will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/oy25ru8gergf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/oy25ru8gergf1.png?auto=webp&amp;s=2cb0173f8f29e9ebb6d02dbbb0f1f6505163ed23",
                  "width": 1920,
                  "height": 4096
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ead78ede2dee21c50ed7920d88cdc0f039342ea",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=21261a21b67d3216bb8f5267a2eab08d1c3e30f8",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=716e70095d944e7d0d07c12b2d069e03b9f72dc9",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd72000cba8efed634dc539ff393fe099624df46",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4cccd911c998a5d7c914cf44d2385aae56de54c",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ee00d9096b93e074ccad355d7571f837459808a",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "tb8jcmr9JwUFdjGqCEioOHi2smnbOEbTGPoqKGaYDxE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdh6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boricua-vet",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdh6r/is_the_p102100_still_a_viable_option_for_llm/",
          "stickied": false,
          "url": "https://i.redd.it/oy25ru8gergf1.png",
          "subreddit_subscribers": 509624,
          "created_utc": 1754207144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I mean we all know the concepts, but how many can actually say they memorized at least the high level architecture of the transformer.  \nwhich architectures/knowledge do you consider a must for scaling and fine tuning models? (GPT? BERT? what ever deepseek did with their articles?)",
          "author_fullname": "t2_rgy8m2w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How many of you actually know by heart the general structure of the transformer architecture?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmr6x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754236507,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mean we all know the concepts, but how many can actually say they memorized at least the high level architecture of the transformer.&lt;br/&gt;\nwhich architectures/knowledge do you consider a must for scaling and fine tuning models? (GPT? BERT? what ever deepseek did with their articles?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgmr6x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CptKrupnik",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754236507,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know a lot of people are fine-tuning their models using their codebase but I couldn't find that many resources on how to build this dataset.\n\nSure you could dump your codebase and that's it but there must be a better way to teach the model how to interact with this codebase right?\n\nWhat did you try? And did it work?",
          "author_fullname": "t2_5219n8dd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How would you generate a dataset to fine-tune a llm to your codebase?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmlzw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754236148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of people are fine-tuning their models using their codebase but I couldn&amp;#39;t find that many resources on how to build this dataset.&lt;/p&gt;\n\n&lt;p&gt;Sure you could dump your codebase and that&amp;#39;s it but there must be a better way to teach the model how to interact with this codebase right?&lt;/p&gt;\n\n&lt;p&gt;What did you try? And did it work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgmlzw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThisIsBartRick",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754236148,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "all I need....",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgj0g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 1522,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1522,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CGttwXkoUDYh4g0k8yxukZYUVavOKH_4hrW2zTWYkis.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754105691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?auto=webp&amp;s=56a0a807d6543ba43266157bcaa0b48793bcd3df",
                  "width": 1024,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0faf4ee7c2bcdc1e4161739543ede55c4684b2b8",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2e17cf5370e4cca658ed8a629406a0522d68d7e",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffb58e48e0282066e26e22c2d00d814c00c5ded2",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd",
                    "width": 640,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2db9397b5cffd5e90a0cc1250191ed983494cb",
                    "width": 960,
                    "height": 1440
                  }
                ],
                "variants": {},
                "id": "FTO20S33694FU83Br474HCo8srCRihH9Gw7Oe1llTgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mfgj0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 113,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgj0g/all_i_need/",
          "stickied": false,
          "url": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "subreddit_subscribers": 509624,
          "created_utc": 1754105691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A modular system for local LLMs that applies mathematical heuristics to manage memory dynamically.  \nThe approach is fully implemented in Bash, and focuses on preserving semantic relevance while avoiding context overflow.\n\nKey mechanisms:\n\n* Levenshtein distance → fragment selection by uniqueness and repetition\n* Collatz cycles → regulate memory expansion/contraction\n* Goldbach segmentation → structural distribution across prime-aligned lines\n* Riemann fallback logic → state recovery on semantic degradation\n\nThe system operates on a plain \\`.txt\\` memory file and maintains coherent long-term context.\n\n Open for review and comparison with similar approaches.",
          "author_fullname": "t2_p7nqw2dg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "M3 – A Modular Mathematical Memory system for local LLMs (Collatz, Goldbach, Riemann, Levenshtein)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgx02s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754261279,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A modular system for local LLMs that applies mathematical heuristics to manage memory dynamically.&lt;br/&gt;\nThe approach is fully implemented in Bash, and focuses on preserving semantic relevance while avoiding context overflow.&lt;/p&gt;\n\n&lt;p&gt;Key mechanisms:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Levenshtein distance → fragment selection by uniqueness and repetition&lt;/li&gt;\n&lt;li&gt;Collatz cycles → regulate memory expansion/contraction&lt;/li&gt;\n&lt;li&gt;Goldbach segmentation → structural distribution across prime-aligned lines&lt;/li&gt;\n&lt;li&gt;Riemann fallback logic → state recovery on semantic degradation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The system operates on a plain `.txt` memory file and maintains coherent long-term context.&lt;/p&gt;\n\n&lt;p&gt;Open for review and comparison with similar approaches.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgx02s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Exchange_8504",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgx02s/m3_a_modular_mathematical_memory_system_for_local/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgx02s/m3_a_modular_mathematical_memory_system_for_local/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754261279,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nHey LocalLLaMA team,\n\nI'm hoping someone much covered than me can help with a question about fine-tuning.\n\nI've been using the MLX library to fine-tune a model on my MacBook, but I need to test the model on other devices that aren't Macs. I'm wondering if there's a best practice for this workflow.\n\nIdeally, I'd like to keep the adapters separate from the base model, but if fusing them is the only way, that's fine too.\n\nSo far, I've only fine-tuned a quantized model and have tried converting the adapters to the PEFT format. The problem is, when I test the output on my MacBook, the base Hugging Face model works fine, but the model with the PEFT adapters just outputs gibberish. This might be due to a precision mismatch.\n\nAny advice or suggestions on how to handle this would be greatly appreciated!\n",
          "author_fullname": "t2_1hmx216rd2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX -&gt; GGUF",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgifea",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754241546,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754225338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey LocalLLaMA team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping someone much covered than me can help with a question about fine-tuning.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using the MLX library to fine-tune a model on my MacBook, but I need to test the model on other devices that aren&amp;#39;t Macs. I&amp;#39;m wondering if there&amp;#39;s a best practice for this workflow.&lt;/p&gt;\n\n&lt;p&gt;Ideally, I&amp;#39;d like to keep the adapters separate from the base model, but if fusing them is the only way, that&amp;#39;s fine too.&lt;/p&gt;\n\n&lt;p&gt;So far, I&amp;#39;ve only fine-tuned a quantized model and have tried converting the adapters to the PEFT format. The problem is, when I test the output on my MacBook, the base Hugging Face model works fine, but the model with the PEFT adapters just outputs gibberish. This might be due to a precision mismatch.&lt;/p&gt;\n\n&lt;p&gt;Any advice or suggestions on how to handle this would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgifea",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Not_Another_LLM",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgifea/mlx_gguf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgifea/mlx_gguf/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754225338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any interesting questions from your experience that you asked a Reasoning LLM and it failed to answer",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is an interesting question that an LLM failed to answer, in your experience?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgi7v8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754224711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any interesting questions from your experience that you asked a Reasoning LLM and it failed to answer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgi7v8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgi7v8/what_is_an_interesting_question_that_an_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgi7v8/what_is_an_interesting_question_that_an_llm/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754224711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, so I have a Macbook Pro M4 pro with 24gb of ram, what are your recommendations for models that could work on the computer. Also would you recommend sticking to ollama or looking into a mlx model provider.",
          "author_fullname": "t2_1kut52t515",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Models for 24gb Macbook",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgqkjt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754245629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, so I have a Macbook Pro M4 pro with 24gb of ram, what are your recommendations for models that could work on the computer. Also would you recommend sticking to ollama or looking into a mlx model provider.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgqkjt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ben-R1106",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgqkjt/models_for_24gb_macbook/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgqkjt/models_for_24gb_macbook/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754245629,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi LocalLLaMA,\n\nI’m a bit confused on two levels and need help: \n\n1) What are the best settings to get ollama to utilize all (6) 3090’s so I can use parallel processing. \n\n2) Do I go with an LLM model that can fit on one 3090 or is it ok to go with a bigger model? \n\nAny recommendations on models? \n\nMy use case is for inference on a RAG dataset using OpenWebUI or Kotaemon.\n\nSomeone previously referenced using CommandR+ 104b but I couldn’t get it to do inference- it just seemed to tie up/lock up the system and provide no answer (no error message though). \n\nI think another person previously referenced Gemma 27b. I haven’t tried that yet. \n\nI’m a bit lost on configs. \n\nAlso someone suggested vllm instead but I couldn’t seem to get it to work, even with a small model. ",
          "author_fullname": "t2_rkb6qbej1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help- unsure of right ollama configs with 6x 3090’s, also model choice for RAG?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgpq7a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754243683,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LocalLLaMA,&lt;/p&gt;\n\n&lt;p&gt;I’m a bit confused on two levels and need help: &lt;/p&gt;\n\n&lt;p&gt;1) What are the best settings to get ollama to utilize all (6) 3090’s so I can use parallel processing. &lt;/p&gt;\n\n&lt;p&gt;2) Do I go with an LLM model that can fit on one 3090 or is it ok to go with a bigger model? &lt;/p&gt;\n\n&lt;p&gt;Any recommendations on models? &lt;/p&gt;\n\n&lt;p&gt;My use case is for inference on a RAG dataset using OpenWebUI or Kotaemon.&lt;/p&gt;\n\n&lt;p&gt;Someone previously referenced using CommandR+ 104b but I couldn’t get it to do inference- it just seemed to tie up/lock up the system and provide no answer (no error message though). &lt;/p&gt;\n\n&lt;p&gt;I think another person previously referenced Gemma 27b. I haven’t tried that yet. &lt;/p&gt;\n\n&lt;p&gt;I’m a bit lost on configs. &lt;/p&gt;\n\n&lt;p&gt;Also someone suggested vllm instead but I couldn’t seem to get it to work, even with a small model. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgpq7a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Business-Weekend-537",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754243683,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[A new PR](https://github.com/ggml-org/llama.cpp/pull/15026) was created to support GLM 4.5's models in llama.cpp, as the original, highly anticipated [\\#14939](https://github.com/ggml-org/llama.cpp/pull/14939) seemed to get stuck. The new PR description reads: \"**this PR will NOT attempt to implement MTP**\", with great progress being made in short time. (Amazing!!!)\n\nGiven that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it's not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.\n\nDisclaimer: I am eternally grateful for everybody's contribution to the field, as LLMs allow me to code what I couldn't code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!\n\nPS: does MTP already work on/with MLX?",
          "author_fullname": "t2_omawcpyf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What would it take to support Multi-Token-Prediction (MTP) in llama.cpp? feat. GLM 4.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvxdo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 81,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 81,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754155024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15026\"&gt;A new PR&lt;/a&gt; was created to support GLM 4.5&amp;#39;s models in llama.cpp, as the original, highly anticipated &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939\"&gt;#14939&lt;/a&gt; seemed to get stuck. The new PR description reads: &amp;quot;&lt;strong&gt;this PR will NOT attempt to implement MTP&lt;/strong&gt;&amp;quot;, with great progress being made in short time. (Amazing!!!)&lt;/p&gt;\n\n&lt;p&gt;Given that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it&amp;#39;s not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I am eternally grateful for everybody&amp;#39;s contribution to the field, as LLMs allow me to code what I couldn&amp;#39;t code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!&lt;/p&gt;\n\n&lt;p&gt;PS: does MTP already work on/with MLX?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?auto=webp&amp;s=acc8e28264bfccdff7bd8f443fed065d174ea9e0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=192b46649336ccde82b3df88264f14e8c3af5057",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d87e157d503386c5e4e8e7524115ddb67fb4d525",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2253f5fea3f3de0ade685e30f24f2fe47d41bed",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=076b473df32cd3180067a97b9fe5da2c8755fd75",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc3f7d46febf1a47049086579c27beff59d8b7b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfebb15c0a042499f527df12e1302a2dc2a17608",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfvxdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karim_acing_it",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754155024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM model was trained on test?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 35,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgd3lh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OCP539vRz8IWhc6Q3DthwHNfIKX-3i-EHCMDWSNvU1s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754205660,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/e4op2j02argf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/e4op2j02argf1.png?auto=webp&amp;s=e1cb3a7b4fab6a83b18ec019a437dd0066760a90",
                  "width": 1080,
                  "height": 271
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3719305d4458c68cccd4a2b97547cb6bc4be5251",
                    "width": 108,
                    "height": 27
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c08316fa1ffb2aae3cf07f96549867a06e61be34",
                    "width": 216,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91eba80ed0ddcdeabe96e4902e4d87e705f19c16",
                    "width": 320,
                    "height": 80
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8eece1e33073e0e4fe5801c6917464a55d051355",
                    "width": 640,
                    "height": 160
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=912cb994049f506e225f2cabc4f96be28f0d1c2a",
                    "width": 960,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a004430fb14ff34c77d8b66a8195c0b002006f5e",
                    "width": 1080,
                    "height": 271
                  }
                ],
                "variants": {},
                "id": "nwQrGJrBDCDo-K7wxSfT8eAh3-H9AN45BOnvDCwUQPo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgd3lh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/",
          "stickied": false,
          "url": "https://i.redd.it/e4op2j02argf1.png",
          "subreddit_subscribers": 509624,
          "created_utc": 1754205660,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello\n\nI am currently looking for the best launch parameters for CPU inference with llama.cpp. I am running the Qwen3-30B-A3B model on my laptop with the following specs:\n\nAMD Ryzen 7 PRO 7840u w/ Radeon 780M Graphics (16CPUs), 32GB Ram.\n\nSince the whole topic around the launch parameters is rather complex, I wanted to ask you about your experience and overall best practices regarding pure CPU inference.\n\nCurrently I am running llama.cpp with the following parameters:\n\nllama-server.exe -m models\\\\X -t 16 --n\\_predict 4096 --ctx-size 64000\n\nThanks in advance!",
          "author_fullname": "t2_1k95d35h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Practice For CPU Inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgixw4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754226810,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I am currently looking for the best launch parameters for CPU inference with llama.cpp. I am running the Qwen3-30B-A3B model on my laptop with the following specs:&lt;/p&gt;\n\n&lt;p&gt;AMD Ryzen 7 PRO 7840u w/ Radeon 780M Graphics (16CPUs), 32GB Ram.&lt;/p&gt;\n\n&lt;p&gt;Since the whole topic around the launch parameters is rather complex, I wanted to ask you about your experience and overall best practices regarding pure CPU inference.&lt;/p&gt;\n\n&lt;p&gt;Currently I am running llama.cpp with the following parameters:&lt;/p&gt;\n\n&lt;p&gt;llama-server.exe -m models\\X -t 16 --n_predict 4096 --ctx-size 64000&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgixw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hudimudi",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754226810,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using `llama.cpp` with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.\n\n**KV Quantization**\n\n* **KV cache quantization matters a lot**. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. ~~Use q5\\_1 for a good balance of memory usage and performance~~. It works well in PPL tests and in practice. **UPDATE:** K seems to be much more sensitive to quantization. I ran some ppl tests on 40k context and here are the results:\n\n|CTK - CTD|PPL|STD|VRAM|\n|:-|:-|:-|:-|\n|q8\\_0 - q8\\_0|6.9016|0.04818|10.1GB|\n|q8\\_0 - q4\\_0|6.9104|0.04822|9.6GB|\n|q4\\_0 - q8\\_0|7.1241|0.04963|9.6GB|\n|q5\\_1 - q5\\_1|6.9664|0.04872|9.5GB|\n\n* **TLDR:** looks like q8\\_0 q4\\_0 is a very nice tradeoff in terms of accuracy and vram usage\n\n**Offloading Strategy**\n\n* You're bottlenecked by your **system RAM bandwidth** when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.\n* Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\n* If you dont understand what the regex does, just feed it to and llm and it'll break it down how it works and how you can tweak it for your vram amount. ofc it requires some experimentation to find the right number of layers.\n\n**Memory Tuning for CPU Offloading**\n\n* System memory speed has a major impact on throughput when using partial offloading.\n* Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.\n* On **AM4** platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.\n* On **AM5**, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.\n* Poor memory tuning will bottleneck your CPU offloading even with a fast processor.\n\n**ubatch (Prompt Batch Size)**\n\n* Higher `ubatch` values significantly improve prompt processing (PP) performance.\n* Try values like `768` or `1024`. You’ll use more VRAM, but it’s often worth it for the speedup.\n* If you’re VRAM-limited, lower this until it fits.\n\n**Extra Performance Boost**\n\n* Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA\\_SET\\_ROWS=1 ./llama-server -md /path/to/model etc.\n\n**Speculative Decoding Tips (SD)**\n\nSpeculative decoding is supported in `llama.cpp`, but there are a couple important caveats:\n\n1. **KV cache quant affects acceptance rate heavily.** Using `q4_0` for the draft model’s KV cache *halves* the acceptance rate in my testing. Use ~~q5\\_1 or even~~ `q8_0`for the draft model KV cache for much better performance. **UPDATE:** \\-ctkd q8\\_0 -ctvd q4\\_0 works like a charm and saves vram. K is much more sensitive to quantization.\n2. **Draft model context handling is broken after filling the draft KV cache.** Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.\n3. **Draft parameters matter a lot**. In my testing, using `--draft-p-min 0.85 --draft-min 2 --draft-max 12` gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.\n\nFor SD, try using **Qwen 3 0.6B** as the draft model. It’s fast and works well, as long as you avoid the issues above.\n\nIf you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp",
          "author_fullname": "t2_66tlmx2l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfs9qn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 125,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 125,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754223074,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754145881,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you&amp;#39;re offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. &lt;del&gt;Use q5_1 for a good balance of memory usage and performance&lt;/del&gt;. It works well in PPL tests and in practice. &lt;strong&gt;UPDATE:&lt;/strong&gt; K seems to be much more sensitive to quantization. I ran some ppl tests on 40k context and here are the results:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;CTK - CTD&lt;/th&gt;\n&lt;th align=\"left\"&gt;PPL&lt;/th&gt;\n&lt;th align=\"left\"&gt;STD&lt;/th&gt;\n&lt;th align=\"left\"&gt;VRAM&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q8_0 - q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9016&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04818&lt;/td&gt;\n&lt;td align=\"left\"&gt;10.1GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q8_0 - q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9104&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04822&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.6GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q4_0 - q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.1241&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04963&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.6GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q5_1 - q5_1&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9664&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04872&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; looks like q8_0 q4_0 is a very nice tradeoff in terms of accuracy and vram usage&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You&amp;#39;re bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt;\n&lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&lt;/li&gt;\n&lt;li&gt;If you dont understand what the regex does, just feed it to and llm and it&amp;#39;ll break it down how it works and how you can tweak it for your vram amount. ofc it requires some experimentation to find the right number of layers.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt;\n&lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you&amp;#39;re comfortable doing so.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt;\n&lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt;\n&lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You’ll use more VRAM, but it’s often worth it for the speedup.&lt;/li&gt;\n&lt;li&gt;If you’re VRAM-limited, lower this until it fits.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model’s KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;del&gt;q5_1 or even&lt;/del&gt; &lt;code&gt;q8_0&lt;/code&gt;for the draft model KV cache for much better performance. &lt;strong&gt;UPDATE:&lt;/strong&gt; -ctkd q8_0 -ctvd q4_0 works like a charm and saves vram. K is much more sensitive to quantization.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It’s fast and works well, as long as you avoid the issues above.&lt;/p&gt;\n\n&lt;p&gt;If you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfs9qn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AliNT77",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754145881,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello Friends,\n\nWhat is a smaller LLM \\[ 1B - 10B\\] model that is very good for Information extraction from unstructured text. I am okay to fine-tune, I would only have a few thousand \\[ &lt; 10k \\] samples.\n\nI would need to run this on a cheaper GPU than A100 likely T4/L4 or A10s.   \nCould you share your experience using these GPUs and the throughput/performance you get. \n\nThanks\n\nShyam  \n",
          "author_fullname": "t2_bib6c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question : Best small sized LLM for Information extraction from Unstructured Text",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgo50t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754239919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Friends,&lt;/p&gt;\n\n&lt;p&gt;What is a smaller LLM [ 1B - 10B] model that is very good for Information extraction from unstructured text. I am okay to fine-tune, I would only have a few thousand [ &amp;lt; 10k ] samples.&lt;/p&gt;\n\n&lt;p&gt;I would need to run this on a cheaper GPU than A100 likely T4/L4 or A10s.&lt;br/&gt;\nCould you share your experience using these GPUs and the throughput/performance you get. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;Shyam  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgo50t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "smaddali",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgo50t/question_best_small_sized_llm_for_information/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgo50t/question_best_small_sized_llm_for_information/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754239919,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any decent open source LLM like mistral or gemma that also has vision could technically already use a web browser to do tasks and click on links. Is there already a model that can do that? Can't be too far away",
          "author_fullname": "t2_rxdlfeeju",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a \"Chat-GPT agent\" kind of agent that is open source and can be run locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgnq9n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754238903,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any decent open source LLM like mistral or gemma that also has vision could technically already use a web browser to do tasks and click on links. Is there already a model that can do that? Can&amp;#39;t be too far away&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgnq9n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maleo999",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgnq9n/is_there_a_chatgpt_agent_kind_of_agent_that_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgnq9n/is_there_a_chatgpt_agent_kind_of_agent_that_is/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754238903,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just shipped something I'm really excited about! 🚀\nI was scrolling through my feed and saw Sebastian Raschka, PhD 's incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo.\nThat got me thinking... what if I could bring this to pure C? 🤔\nInspired by Andrej Karpathy's legendary llama2.c approach (seriously, if you haven't seen it, check it out), I decided to take on the challenge of implementing Qwen3's 30B parameter model with 128 experts in a single C file.\nThe result? Qwen_MOE_C - a complete inference engine that:\n✅ Handles sparse MoE computation (only 8 out of 128 experts active)\n✅ Supports Grouped Query Attention with proper head ratios\n✅ Uses memory mapping for efficiency (~30GB models)\n✅ Zero external dependencies (just libc + libm)\nThe beauty of this approach is the same as llama2.c - you can understand every line, it's hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency.\nHuge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems.\nSometimes the best way to truly understand something is to build it from scratch. 🛠️\nLink to the project: \nhttps://github.com/h9-tec/Qwen_MOE_C",
          "author_fullname": "t2_59165ick",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen moe in C",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxas1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 61,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754158458,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just shipped something I&amp;#39;m really excited about! 🚀\nI was scrolling through my feed and saw Sebastian Raschka, PhD &amp;#39;s incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo.\nThat got me thinking... what if I could bring this to pure C? 🤔\nInspired by Andrej Karpathy&amp;#39;s legendary llama2.c approach (seriously, if you haven&amp;#39;t seen it, check it out), I decided to take on the challenge of implementing Qwen3&amp;#39;s 30B parameter model with 128 experts in a single C file.\nThe result? Qwen_MOE_C - a complete inference engine that:\n✅ Handles sparse MoE computation (only 8 out of 128 experts active)\n✅ Supports Grouped Query Attention with proper head ratios\n✅ Uses memory mapping for efficiency (~30GB models)\n✅ Zero external dependencies (just libc + libm)\nThe beauty of this approach is the same as llama2.c - you can understand every line, it&amp;#39;s hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency.\nHuge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems.\nSometimes the best way to truly understand something is to build it from scratch. 🛠️\nLink to the project: \n&lt;a href=\"https://github.com/h9-tec/Qwen_MOE_C\"&gt;https://github.com/h9-tec/Qwen_MOE_C&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?auto=webp&amp;s=e5cdc2248f745a90988be621db7d6b6be3916db2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbd8f00d966699e72ff3a93f578256c7537d2135",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3afe3fea53292db66353b624f6471827586e1c10",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=023f7331df4213637fe882078ebd6fb043d92076",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=99f557d7c6c92fc79bbc53fbd9519fb90c4895ac",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55d41da3d53c6240a1417760903816f861d7b857",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=869ae3899c29fb68da394dceac5b0968c64c5e51",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfxas1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "1Hesham",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754158458,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sam Altman promised imminent release of open source/weight models . It seems we haven’t heard anything new in the past few weeks, have we?",
          "author_fullname": "t2_dmaijfods",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news about the open source models that OpenAI promised to release ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1e80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754169032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sam Altman promised imminent release of open source/weight models . It seems we haven’t heard anything new in the past few weeks, have we?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg1e80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeedleworkerDull7886",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754169032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.\n\nI already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an [Awesome list](https://github.com/sindresorhus/awesome).\n\nTo have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: [https://aibenchmarks.net/](https://aibenchmarks.net/)\n\nAwesome AI Benchmarks GitHub repository available here: [https://github.com/panilya/awesome-ai-benchmarks](https://github.com/panilya/awesome-ai-benchmarks)\n\nWould be happy to hear any feedback on this and whether it can be useful for you :)",
          "author_fullname": "t2_7arjztn1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "100+ AI Benchmarks list",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwckf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754156092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.&lt;/p&gt;\n\n&lt;p&gt;I already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an &lt;a href=\"https://github.com/sindresorhus/awesome\"&gt;Awesome list&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;To have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: &lt;a href=\"https://aibenchmarks.net/\"&gt;https://aibenchmarks.net/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Awesome AI Benchmarks GitHub repository available here: &lt;a href=\"https://github.com/panilya/awesome-ai-benchmarks\"&gt;https://github.com/panilya/awesome-ai-benchmarks&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would be happy to hear any feedback on this and whether it can be useful for you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?auto=webp&amp;s=41e93dde0345b37cb0200da115b435ce77df3930",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=93761f053553e9b1cfb16372cdf0e7dac51f1f5e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b480826b22f435da297cc6f87648622fc3892fe6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08c9c7135e744081cfec8c73e2a489375eb611be",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63909a628f633b135db8a2f2ad24f375a444a9b7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcf264d960a58dd8659a36b59ab18b32c8762353",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6522fe068f5cf8114a6d791bc9bbc352abd7a38",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfwckf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panilyaU",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754156092,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.\n\nIf I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.\n\nSometimes, both sides are the same.  (E.g., \"Are you helpless or do you need my help?\")\n\nSometimes, they're so unbalanced it becomes a Mitch Hedberg-style question. (E.g., \"Have you ever tried sugar or PCP?\")\n\nSometimes, a very open-ended question is presented as either/or. (E.g., \"Is your favorite CSS color value #ff73c1 or #2141af?\" like those are the only two options.)\n\nI have found myself utterly unable to affect this behavior at all through the system prompt. I've tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with \"Don't...\" instructions) I've tried prompting it not to use \"either/or\" questions, \"A or B?\" questions, questions that limit the user's options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.\n\nAnd if I bring it up in chat, I get Qwen3's usual long obsequious apology (\"You're absolutely right, I'm sorry, I made assumptions and didn't respect your blah blah blah... I'll be sure to blah blah blah...\") and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird \"shell-shocked\" mode where it starts writing responses with three words per line that read like it's a frustrated beat poet.\n\nHave other people run into this? If so, are there good ways to combat it?\n\nThanks for any advice!\n",
          "author_fullname": "t2_7pimnskf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get Qwen 3 to stop asking terrible questions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg4lxw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754177798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.&lt;/p&gt;\n\n&lt;p&gt;If I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, both sides are the same.  (E.g., &amp;quot;Are you helpless or do you need my help?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, they&amp;#39;re so unbalanced it becomes a Mitch Hedberg-style question. (E.g., &amp;quot;Have you ever tried sugar or PCP?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, a very open-ended question is presented as either/or. (E.g., &amp;quot;Is your favorite CSS color value #ff73c1 or #2141af?&amp;quot; like those are the only two options.)&lt;/p&gt;\n\n&lt;p&gt;I have found myself utterly unable to affect this behavior at all through the system prompt. I&amp;#39;ve tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with &amp;quot;Don&amp;#39;t...&amp;quot; instructions) I&amp;#39;ve tried prompting it not to use &amp;quot;either/or&amp;quot; questions, &amp;quot;A or B?&amp;quot; questions, questions that limit the user&amp;#39;s options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.&lt;/p&gt;\n\n&lt;p&gt;And if I bring it up in chat, I get Qwen3&amp;#39;s usual long obsequious apology (&amp;quot;You&amp;#39;re absolutely right, I&amp;#39;m sorry, I made assumptions and didn&amp;#39;t respect your blah blah blah... I&amp;#39;ll be sure to blah blah blah...&amp;quot;) and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird &amp;quot;shell-shocked&amp;quot; mode where it starts writing responses with three words per line that read like it&amp;#39;s a frustrated beat poet.&lt;/p&gt;\n\n&lt;p&gt;Have other people run into this? If so, are there good ways to combat it?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg4lxw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TastesLikeOwlbear",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754177798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am looking to extract multiple tables from tabloid paper sizes; I tried GMFT, img2table, and camelot and found no success, likely because they were not trained on larger paper sizes. I currently use Docling, which is honestly the best OCR tool I have found for my use case. Still, however, it misses some tables.\n\nWhat do you guys recommend I combine Docling with for 100% table extraction?",
          "author_fullname": "t2_w949wvlt6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Table Extraction for Tabloid Paper Sizes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkus5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754231843,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to extract multiple tables from tabloid paper sizes; I tried GMFT, img2table, and camelot and found no success, likely because they were not trained on larger paper sizes. I currently use Docling, which is honestly the best OCR tool I have found for my use case. Still, however, it misses some tables.&lt;/p&gt;\n\n&lt;p&gt;What do you guys recommend I combine Docling with for 100% table extraction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgkus5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Stranger-1229",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgkus5/table_extraction_for_tabloid_paper_sizes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgkus5/table_extraction_for_tabloid_paper_sizes/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754231843,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a couple or RTX6000Blackwell GPUs but LLMstudio only uses the memory up to ~70GB per GPU even after I already set the Guardrails to “relaxed”. If I enable “Limit Model Offload to Dedicated GPU Memory” the situation gets even worse and only ~20GB are used.",
          "author_fullname": "t2_a0v2ol2u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLMstudio doesn’t use all the available VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkpm6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754231472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple or RTX6000Blackwell GPUs but LLMstudio only uses the memory up to ~70GB per GPU even after I already set the Guardrails to “relaxed”. If I enable “Limit Model Offload to Dedicated GPU Memory” the situation gets even worse and only ~20GB are used.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgkpm6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Khipu28",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgkpm6/llmstudio_doesnt_use_all_the_available_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgkpm6/llmstudio_doesnt_use_all_the_available_vram/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754231472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn't quite process them correctly.\n    \nThis is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.\n    \nSpecifically, this fixes the following:\n    \n- Chat dates are maintained\n- Chat hierarchy is preserved\n- Empty conversations are skipped\n- Parent-child relationships are maintained\n    \nIn addition, it will skip malformed conversations and try to import each chat only once using a `imported.json` file.\n    \nYou can export your ChatGPT conversations by going to Settings → Data controls → Export data → Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to `~/chatgpt/chatgpt-export.json`.\n    \nI recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your `webui.db` file.\n    \nAfter importing, you can view your conversations in Open-WebUI by going to Settings → Chats → Import and selecting the converted JSON file.\n    \nI like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the `imported.json` file correctly.\n    \nThis works for me, and I hope it works for you too! PRs and issues are welcome.",
          "author_fullname": "t2_44be3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Convert your ChatGTP exported conversations to something that Open-WebUI can import",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg12k4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=7aaf741530697098b43e0167db0c1c5b5305d19b",
          "edited": 1754169308,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754168175,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn&amp;#39;t quite process them correctly.&lt;/p&gt;\n\n&lt;p&gt;This is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.&lt;/p&gt;\n\n&lt;p&gt;Specifically, this fixes the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Chat dates are maintained&lt;/li&gt;\n&lt;li&gt;Chat hierarchy is preserved&lt;/li&gt;\n&lt;li&gt;Empty conversations are skipped&lt;/li&gt;\n&lt;li&gt;Parent-child relationships are maintained&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In addition, it will skip malformed conversations and try to import each chat only once using a &lt;code&gt;imported.json&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;You can export your ChatGPT conversations by going to Settings → Data controls → Export data → Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to &lt;code&gt;~/chatgpt/chatgpt-export.json&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your &lt;code&gt;webui.db&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;After importing, you can view your conversations in Open-WebUI by going to Settings → Chats → Import and selecting the converted JSON file.&lt;/p&gt;\n\n&lt;p&gt;I like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the &lt;code&gt;imported.json&lt;/code&gt; file correctly.&lt;/p&gt;\n\n&lt;p&gt;This works for me, and I hope it works for you too! PRs and issues are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?auto=webp&amp;s=e22d770e4c02c756fc73d7d90a385e5b112f15b5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed1ab2121b7badab4580a484ba08cc844d5e16e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b9afd23fae3b50eabe8826fbaa8cb3d0f9a3e2c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ddddd80938d359fbb1d0dd7d1abbf05f3a86de7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b17fc3d36981f925574075a8743bee7f8626049a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=998b355322b3740f1ade9410e49eaa05da91b8fc",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0e7de199e0866090536334a1fd28c6c4dff2939",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg12k4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "scubanarc",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg12k4/convert_your_chatgtp_exported_conversations_to/",
          "stickied": false,
          "url": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754168175,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_prnin4bw1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GNOME AI Virtual Assistant \"Newelle\" Reaches Version 1.0 Milestone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1evr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754169081,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "phoronix.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg1evr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FastDecode1",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1evr/gnome_ai_virtual_assistant_newelle_reaches/",
          "stickied": false,
          "url": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "subreddit_subscribers": 509624,
          "created_utc": 1754169081,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If",
          "author_fullname": "t2_gws95urkx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building for the era of experience",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgi9df",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "author_cakeday": true,
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754224836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "rnikhil.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://rnikhil.com/2025/07/30/era-of-experience",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgi9df",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Excellent-Effect237",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgi9df/building_for_the_era_of_experience/",
          "stickied": false,
          "url": "https://rnikhil.com/2025/07/30/era-of-experience",
          "subreddit_subscribers": 509624,
          "created_utc": 1754224836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based on the current situation with the quality of Sonnet and other proprietary models I'm thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our \"own\" R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.\n\nWhat are your thoughts?\n\nUpdate: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)",
          "author_fullname": "t2_qnf2u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "It's time to run your own R1, Kimi ... and split the cost of it",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrunn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754146028,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144811,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on the current situation with the quality of Sonnet and other proprietary models I&amp;#39;m thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our &amp;quot;own&amp;quot; R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;/p&gt;\n\n&lt;p&gt;Update: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfrunn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HammerSpb",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754144811,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any llms that can describe a song? if not, what would it take to build one if you know",
          "author_fullname": "t2_1fv4idhbp9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Audio-in LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdrws",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754208305,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any llms that can describe a song? if not, what would it take to build one if you know&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgdrws",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nearby_Direction2438",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdrws/audioin_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdrws/audioin_llm/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754208305,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.\n\nHow it works:\n\n- Drop a .awesome-ai.md file in your repo root (template: https://github.com/teodorgross/awesome-ai)\n\n- The scanner finds it automatically within 30 minutes  \n\n- Creates a pull request for review\n\n- Your tool goes live with real-time GitHub stats on (https://awesome-ai.io)\n\nWhy this matters:\n\n- No more manual submissions or contact forms\n\n- Tools stay up-to-date automatically when you push changes\n\n- GitHub verification prevents spam\n\n- Real-time star tracking and leaderboards\n\nThink of it like .gitignore for Git, but for AI tool discovery. ",
          "author_fullname": "t2_92fpag8i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a GitHub scanner that automatically discovers your AI tools using a new .awesome-ai.md standard I created",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgh19i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=2f6c6297df4b4027fb8d1d453c04b9819bf72371",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754220889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.&lt;/p&gt;\n\n&lt;p&gt;How it works:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Drop a .awesome-ai.md file in your repo root (template: &lt;a href=\"https://github.com/teodorgross/awesome-ai\"&gt;https://github.com/teodorgross/awesome-ai&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The scanner finds it automatically within 30 minutes  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Creates a pull request for review&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your tool goes live with real-time GitHub stats on (&lt;a href=\"https://awesome-ai.io\"&gt;https://awesome-ai.io&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why this matters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;No more manual submissions or contact forms&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Tools stay up-to-date automatically when you push changes&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;GitHub verification prevents spam&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Real-time star tracking and leaderboards&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Think of it like .gitignore for Git, but for AI tool discovery. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/teodorgross/awesome-ai",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?auto=webp&amp;s=423c2b45e5c57364da28298579b3a5631b59c0ed",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd85376252643c0cfc25bf58873d1308bbaa8b8c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b15eb575a42080dcefe65c839eb0e4c036819417",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7d83cf6189b7f3c98c0a7b6f2c3d772895ee7ba",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d63a88dbef47d35bfb865b575d09eda2e4144500",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f688b76be41b12b1a7923494dacac226e59a932e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0e5bc45cd19159360de8a68ed1794010b721da7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgh19i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "r00tkit_",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgh19i/i_built_a_github_scanner_that_automatically/",
          "stickied": false,
          "url": "https://github.com/teodorgross/awesome-ai",
          "subreddit_subscribers": 509624,
          "created_utc": 1754220889,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Never tried it I’m just curious what your experience has been like with it. I was wondering if it looks “cheap” or too uncanny valley like how some websites or apps are with it. ",
          "author_fullname": "t2_l4qac",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are image generations from LLMs as accurate as OpenAI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mglmse",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754233767,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Never tried it I’m just curious what your experience has been like with it. I was wondering if it looks “cheap” or too uncanny valley like how some websites or apps are with it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mglmse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XiRw",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mglmse/are_image_generations_from_llms_as_accurate_as/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mglmse/are_image_generations_from_llms_as_accurate_as/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754233767,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Since LLM are not deterministics can you have a « bad run » with your prompt ?\n\nLike having prompts that the LLM should be able to respond 90% of the times but … too bad you hit the 10% three chat in a row\n\nIt could explain why people experience « dumbness period » from their favorite model while it still fine for every one else\n\nWhat do you think about it ?",
          "author_fullname": "t2_91nqzv8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you have a « bad run » with LLM ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgues2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NbCGGoF8jkkujOuck7lpU709E_xiTa5YqZWpQeLRQSA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754254774,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since LLM are not deterministics can you have a « bad run » with your prompt ?&lt;/p&gt;\n\n&lt;p&gt;Like having prompts that the LLM should be able to respond 90% of the times but … too bad you hit the 10% three chat in a row&lt;/p&gt;\n\n&lt;p&gt;It could explain why people experience « dumbness period » from their favorite model while it still fine for every one else&lt;/p&gt;\n\n&lt;p&gt;What do you think about it ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5oiuqtm3cvgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?auto=webp&amp;s=a34cdbe8eebe1d50de35842b83ca71afb5865167",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19b27cdc34a9316aa0e5c34cf98ac1c82c2b0602",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=91e899afa8499eb114bffefdfdf7c6787e7a96d2",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c5df197b5e2ea05dc02e13381a0eb77b27efd63",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfca2436c4b249d6d482dc276a67af848a58e641",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6c51e12f80a81001f9072a29c230d2a43dd84c0e",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d56b99149ce90067e9de53667722c154762efd4b",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "Jd9nPtzw1J2w27s1KMOb9IFaYuneD1vAFfRqLDyKXaU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgues2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kathane37",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgues2/can_you_have_a_bad_run_with_llm/",
          "stickied": false,
          "url": "https://i.redd.it/5oiuqtm3cvgf1.jpeg",
          "subreddit_subscribers": 509624,
          "created_utc": 1754254774,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using Claude Code with my MAX plan and kept running into situations where I wanted to route specific requests to different models without changing my whole setup.\nLarge context requests would hit Claude's limits, and running compaction so often and having Claude lose important context was a frustrating experience.\n\nSo I built ccproxy - a LiteLLM transformation hook that sits between Claude Code and your requests, intelligently routing them based on configurable rules.\n\nWhat it actually does:\n\n- Routes requests to different providers while keeping your Claude Code client unchanged\n- Example: requests over 60k tokens automatically go to Gemini Pro, requests for sonnet can go to Gemini Flash\n- Define rules based on token count, model name, tool usage, or any request property\n- Everything else defaults to your Claude MAX plan\n\nCurrent limitations\n\n- Cross-provider context caching is coming but not ready yet\n- Only battle-tested with Anthropic/Google/OpenAI providers so far, I personally have not used it with local models, but as it's using LiteLLM I expect it to work with most setups.\n- No fancy UI - it's YAML config for now\n\nWho this helps:\nIf you're already using Claude Code with a MAX plan but want to optimize costs/performance for specific use cases, this might save you from writing custom routing logic.\nIt's particularly useful if you're hitting context limits or want to use cheaper models for simple tasks.\n\nGitHub: &lt;https://github.com/starbased-co/ccproxy&gt;\n\nHappy to answer questions or take feedback. What routing patterns would be most useful for your workflows?",
          "author_fullname": "t2_5fbcze3c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ccproxy - Route Claude Code requests to any LLM while keeping your MAX plan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg82el",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754188267,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Claude Code with my MAX plan and kept running into situations where I wanted to route specific requests to different models without changing my whole setup.\nLarge context requests would hit Claude&amp;#39;s limits, and running compaction so often and having Claude lose important context was a frustrating experience.&lt;/p&gt;\n\n&lt;p&gt;So I built ccproxy - a LiteLLM transformation hook that sits between Claude Code and your requests, intelligently routing them based on configurable rules.&lt;/p&gt;\n\n&lt;p&gt;What it actually does:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Routes requests to different providers while keeping your Claude Code client unchanged&lt;/li&gt;\n&lt;li&gt;Example: requests over 60k tokens automatically go to Gemini Pro, requests for sonnet can go to Gemini Flash&lt;/li&gt;\n&lt;li&gt;Define rules based on token count, model name, tool usage, or any request property&lt;/li&gt;\n&lt;li&gt;Everything else defaults to your Claude MAX plan&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Current limitations&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cross-provider context caching is coming but not ready yet&lt;/li&gt;\n&lt;li&gt;Only battle-tested with Anthropic/Google/OpenAI providers so far, I personally have not used it with local models, but as it&amp;#39;s using LiteLLM I expect it to work with most setups.&lt;/li&gt;\n&lt;li&gt;No fancy UI - it&amp;#39;s YAML config for now&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Who this helps:\nIf you&amp;#39;re already using Claude Code with a MAX plan but want to optimize costs/performance for specific use cases, this might save you from writing custom routing logic.\nIt&amp;#39;s particularly useful if you&amp;#39;re hitting context limits or want to use cheaper models for simple tasks.&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/starbased-co/ccproxy\"&gt;https://github.com/starbased-co/ccproxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer questions or take feedback. What routing patterns would be most useful for your workflows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?auto=webp&amp;s=3460d31319519e33a80286d6df2dfc23b39c919c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=450f2c799f15b29ec21139264bc6b6bcb57973e4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d842726b49df18ab66e30107bfb8ac96308217a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9f141bb2296be7290f22e206f044b47d4383c40",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad9ccf4b08efef17745acd771af67ed9e25f036b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7506050be4a335d4bf6b8acaf16dcd1498ea2f5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5570005c242c40ef01988f09a587178c3b8cbc2b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg82el",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_kintsu",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754188267,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I honestly don't know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...  \nIt's something I've never been clear about.  \nFor example, when I've used Deepseek, sometimes I feel that putting it into \"thinking\" mode doesn't add much, but I haven't noticed a clear pattern like \"for this type of question I use thinking mode, for this other type I don't.\"  \nCould someone clarify this for me?\n\nI'm thinking of downloading this model:  \n**Qwen3-30B-A3B-Instruct-2507** ... or **Qwen3-30B-A3B-Thinking-2507**\n\nThe Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?",
          "author_fullname": "t2_q2iij",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking or Instruct?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg5scj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I honestly don&amp;#39;t know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...&lt;br/&gt;\nIt&amp;#39;s something I&amp;#39;ve never been clear about.&lt;br/&gt;\nFor example, when I&amp;#39;ve used Deepseek, sometimes I feel that putting it into &amp;quot;thinking&amp;quot; mode doesn&amp;#39;t add much, but I haven&amp;#39;t noticed a clear pattern like &amp;quot;for this type of question I use thinking mode, for this other type I don&amp;#39;t.&amp;quot;&lt;br/&gt;\nCould someone clarify this for me?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of downloading this model:&lt;br/&gt;\n&lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; ... or &lt;strong&gt;Qwen3-30B-A3B-Thinking-2507&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg5scj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "9acca9",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754181257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.\nCUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100.\nThe capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.*",
          "author_fullname": "t2_iol3buybk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgatd6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754197224,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.\nCUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100.\nThe capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.14111v4",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgatd6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thrumpwart",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgatd6/cudal1_improving_cuda_optimization_via/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.14111v4",
          "subreddit_subscribers": 509624,
          "created_utc": 1754197224,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI have developed a toy spreadsheet, where you can implement your formulas in English, which are then translated into \\`javascript\\` thanks to an LLM.\n\nFor instance, you can write: \\`sum of the squared values\\` and the LLM will translate this description into:  \n\\`getValuesFromReferences(\\['A1', 'A2', 'A3'\\]).map(Number).reduce((a, b) =&gt; a + b \\* b, 0)\\`.\n\nI use \\`LM Studio\\` and \\`codestral\\`, but I'm pretty sure you can replace \\`LM Studio\\` by \\`Ollama\\` or your favorite LLM provider.\n\nIf you want to have a look, it is available on the following GitHub: [NUMAI](https://github.com/clauderouxster/NUMAI/tree/main)",
          "author_fullname": "t2_no5psr297",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NUMAI: A spreadsheet with LLM formula conversion",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mggdxz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754218578,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have developed a toy spreadsheet, where you can implement your formulas in English, which are then translated into `javascript` thanks to an LLM.&lt;/p&gt;\n\n&lt;p&gt;For instance, you can write: `sum of the squared values` and the LLM will translate this description into:&lt;br/&gt;\n`getValuesFromReferences([&amp;#39;A1&amp;#39;, &amp;#39;A2&amp;#39;, &amp;#39;A3&amp;#39;]).map(Number).reduce((a, b) =&amp;gt; a + b * b, 0)`.&lt;/p&gt;\n\n&lt;p&gt;I use `LM Studio` and `codestral`, but I&amp;#39;m pretty sure you can replace `LM Studio` by `Ollama` or your favorite LLM provider.&lt;/p&gt;\n\n&lt;p&gt;If you want to have a look, it is available on the following GitHub: &lt;a href=\"https://github.com/clauderouxster/NUMAI/tree/main\"&gt;NUMAI&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mggdxz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Frere_de_la_Quote",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggdxz/numai_a_spreadsheet_with_llm_formula_conversion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggdxz/numai_a_spreadsheet_with_llm_formula_conversion/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754218578,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have decided to run Gemma 3 4B QAT on my 6GB VRAM Laptop for general use. I was wondering if i should be using some other quant other than the official QAT version by google? Like What would be the performance or quality increase as compared to the QAT version. It would be great if someone shared some benchmarks or other results.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Gemma 3 Quant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgg4ki",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754218312,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754217605,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have decided to run Gemma 3 4B QAT on my 6GB VRAM Laptop for general use. I was wondering if i should be using some other quant other than the official QAT version by google? Like What would be the performance or quality increase as compared to the QAT version. It would be great if someone shared some benchmarks or other results.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgg4ki",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgg4ki/best_gemma_3_quant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgg4ki/best_gemma_3_quant/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754217605,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI’ve been doing some research on setting up a local, privacy-friendly LLM assistant, ideally something that can help me write job applications using my previous resumes and cover letters as a base.\n\nFrom everything I read, it sounded really promising to combine AnythingLLM with Llama 3 (I’m using the LLaMA 3 8B). I installed it all locally, configured the settings properly in AnythingLLM (enabled local embeddings, context windows, etc.), and successfully loaded several PDFs (my old cover letters, resumes, etc.).\n\n\n\nThe idea:\n\nI want to paste in a job posting and ask the chatbot to draft a personalized cover letter using my own documents as a knowledge base. Basically, a smart assistant that reuses my past writing and adapts it to the job description.\n\n\n\nBut here’s the problem:\n\nThe results are pretty disappointing.\n\nEven though the PDFs were embedded correctly and the system says they’re indexed, the answers I get are vague, or clearly not based on my previous content. It doesn't really use the documents meaningfully – it feels like the bot is just hallucinating or ignoring them.\n\nI even tested it with just one document: my current résumé, uploaded as both PDF and plain .txt, and it still failed to accurately reflect the content when I asked basic questions like \"What is my professional background?\" or \"What are my main skills?\" – which it should have easily pulled from the text.\n\nI’ve tried re-uploading, adjusting the chunk size, checking the document scope –&gt; but no real improvement.\n\nSo my question is:\n\nAm I doing something wrong? Or is this kind of task just too much for AnythingLLM + Llama 3 right now?\n\nHas anyone had better results using a different local setup for tasks like this?\n\n\n\nWould love to hear your tips or setups that work better for writing support based on personal document libraries. Thanks in advance!",
          "author_fullname": "t2_oq1nrgj7x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Still getting bad results with PDFs in AnythingLLM + Llama 3 – Am I doing something wrong, or is there a better setup?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfuf3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754216527,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’ve been doing some research on setting up a local, privacy-friendly LLM assistant, ideally something that can help me write job applications using my previous resumes and cover letters as a base.&lt;/p&gt;\n\n&lt;p&gt;From everything I read, it sounded really promising to combine AnythingLLM with Llama 3 (I’m using the LLaMA 3 8B). I installed it all locally, configured the settings properly in AnythingLLM (enabled local embeddings, context windows, etc.), and successfully loaded several PDFs (my old cover letters, resumes, etc.).&lt;/p&gt;\n\n&lt;p&gt;The idea:&lt;/p&gt;\n\n&lt;p&gt;I want to paste in a job posting and ask the chatbot to draft a personalized cover letter using my own documents as a knowledge base. Basically, a smart assistant that reuses my past writing and adapts it to the job description.&lt;/p&gt;\n\n&lt;p&gt;But here’s the problem:&lt;/p&gt;\n\n&lt;p&gt;The results are pretty disappointing.&lt;/p&gt;\n\n&lt;p&gt;Even though the PDFs were embedded correctly and the system says they’re indexed, the answers I get are vague, or clearly not based on my previous content. It doesn&amp;#39;t really use the documents meaningfully – it feels like the bot is just hallucinating or ignoring them.&lt;/p&gt;\n\n&lt;p&gt;I even tested it with just one document: my current résumé, uploaded as both PDF and plain .txt, and it still failed to accurately reflect the content when I asked basic questions like &amp;quot;What is my professional background?&amp;quot; or &amp;quot;What are my main skills?&amp;quot; – which it should have easily pulled from the text.&lt;/p&gt;\n\n&lt;p&gt;I’ve tried re-uploading, adjusting the chunk size, checking the document scope –&amp;gt; but no real improvement.&lt;/p&gt;\n\n&lt;p&gt;So my question is:&lt;/p&gt;\n\n&lt;p&gt;Am I doing something wrong? Or is this kind of task just too much for AnythingLLM + Llama 3 right now?&lt;/p&gt;\n\n&lt;p&gt;Has anyone had better results using a different local setup for tasks like this?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your tips or setups that work better for writing support based on personal document libraries. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgfuf3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lazy_Fig_6244",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/",
          "subreddit_subscribers": 509624,
          "created_utc": 1754216527,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}