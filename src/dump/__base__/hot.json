{
  "kind": "Listing",
  "data": {
    "after": "t3_1lz5sm6",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from \"praise be machine god\" to \"worse than my toddler\". So here're the findings!\n\nIf there're any questions you think we should explore from a data perspective, feel free to drop them in and we'll get to it!",
          "author_fullname": "t2_vl05s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "0doh5qm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e89395e82c91218ab3c27d0275ac7cf2d39688e"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da2da98e6c09757bddbe69c0188b10b08f970a17"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=91a6e5c5b93017d696fa56858ec4a5562ae8e197"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=198139ae4614f7e8ca04f15f534cae9a893d24cf"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f71a8864a609b569bfa66f8cb19932038d4b5c5"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=039b13f39c9f09f0ab5a0bf9cc493d2d3fd186a1"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=14f43e6010bab3171ee5981ea58f92af7093be72"
              },
              "id": "0doh5qm190df1"
            },
            "4rw1iqm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=583333ac095bc5fd2898347235b781483b78ebe2"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e6990f51af643e82478ba02ddac3b0432c461fc"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae252e6a0daf7351bdb09d7a4f319f30c1b16e37"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d64689c695468c123efec37630673c8c431a2e5"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2606ab8111ba1293184b533c59ffdceecc4a9391"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31fc9a93767303df13aa0efafded75620016dca2"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=7d8670401527804313229c82eeb47ee6a6dcc78e"
              },
              "id": "4rw1iqm190df1"
            },
            "zdh0gmo190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b1ba7e84687b2e715add702a42dd87908af1cc6"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb66a22aab1691bad599f4ed128427af5a636ea1"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f0bb9a895624306531dea55691e2e94adca1ee7"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=331c867add274b7e6f8fc2a73ee629d3873b8e9d"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bb585b6c8cde44d10a289f2d2e3a1caecd19553"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=591c167bd6197c510181b6984149a8febaa143e9"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=226982c0197e3b952e454b3578cfd19c7a573d62"
              },
              "id": "zdh0gmo190df1"
            },
            "mji4zrm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97b615304a4fcbc492af831d5b30b3677ce83654"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=50c1dc1166d11531a88a685eedf1a12bc65291db"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=885156f4aa23e9f28c58a0f33fe317aaa510a628"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fd758c677e0469e9ce2acc40af38d204fec0c20"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e2f8768eb5dc6d4e61ef099400a45456b627aa39"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa21064a7b1fa64209650cc1773d0b311ed1e235"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=b5a95ab4bcaa1120682f371bbad2478463ec6c75"
              },
              "id": "mji4zrm190df1"
            }
          },
          "name": "t3_1m0d0vz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 99,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4rw1iqm190df1",
                "id": 706335576
              },
              {
                "media_id": "zdh0gmo190df1",
                "id": 706335577
              },
              {
                "media_id": "0doh5qm190df1",
                "id": 706335578
              },
              {
                "media_id": "mji4zrm190df1",
                "id": 706335579
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 99,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Qky5LMYmgq28yvhGu7XZfILzJYn7CxOqgZAo-mu3Knk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752570907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from &amp;quot;praise be machine god&amp;quot; to &amp;quot;worse than my toddler&amp;quot;. So here&amp;#39;re the findings!&lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;re any questions you think we should explore from a data perspective, feel free to drop them in and we&amp;#39;ll get to it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m0d0vz",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0d0vz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yingyn",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m0d0vz",
          "subreddit_subscribers": 499292,
          "created_utc": 1752570907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1fc9cbovwe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EXAONE 4.0 32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m04a20",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 248,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 248,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8a97b0e14968bf021760fe24b232086843a6916d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752541575,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?auto=webp&amp;s=21346c43501458b33bb875a62eb15906b79b28b2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01c7ab98318dec8e4dfb9ad444e48cb42d1afee0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3ab4d919ac27b9f67137eb710ebbcd8ffae7191",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e17ff014e394f7eaa73049e5608695028dc583e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=18202842c69b787ccdb604277c8c0ce21247e4d3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3d99f4430cca13dd301692b895103c848c110e72",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92fd7f637d351ce6963b08dd9b62b92904ecbc6d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m04a20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "minpeter2",
          "discussion_type": null,
          "num_comments": 81,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/",
          "stickied": false,
          "url": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B",
          "subreddit_subscribers": 499292,
          "created_utc": 1752541575,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We're likely getting a closed source model instead ",
          "author_fullname": "t2_5luz9ozsa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Well, if anyone was waiting for Llama 4 Behemoth, it's gone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0g2mk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=860ae22badddb8fc2c343a9ccac78f148022a869",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752581342,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "analyticsindiamag.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re likely getting a closed source model instead &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://analyticsindiamag.com/global-tech/meta-plans-to-abandon-llama-4-behemoth-but-why/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?auto=webp&amp;s=ddc9c55c7477ac3d20d1a5ec41979dd91d76c1dd",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01ed45a4be031327974fcbe48924b7c8d0421993",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8279d4f944e0abce05c11f97b8a014ed4500b3da",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c501e47f0fbce65b45288abd53f58cec0662510d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7417749598717bd5400069706a3c0d563e32ab4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=281861ba0f1aec0cc2a503b3304cf7663f094868",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12d94b438b313810f2f112ab90b5aca70313ccf6",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0g2mk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Elevator5091",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/",
          "stickied": false,
          "url": "https://analyticsindiamag.com/global-tech/meta-plans-to-abandon-llama-4-behemoth-but-why/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752581342,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you can't run [kimi-k2](https://huggingface.co/moonshotai/Kimi-K2-Instruct) locally, there are now more providers offering API access. DeepInfra is now the cheapest provider, while Groq is (by far) the fastest at around \\~250 tokens per second:\n\n* [https://deepinfra.com/moonshotai/Kimi-K2-Instruct](https://deepinfra.com/moonshotai/Kimi-K2-Instruct) ($0.55/$2.20 in/out Mtoken)\n* [https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct](https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct) ($1/$3 in/out Mtoken, but very fast)\n\nThat makes it cheaper than Claude Haiku 3.5, GPT-4.1 and Gemini 2.5 Pro. Not bad for the best non-thinking model currently publicly available!\n\nIt also shows the power of an open weights model with an permissive license: Even if you can't run it yourself, there's a lot more options in API access.\n\nSee all providers on OpenRouter: [https://openrouter.ai/moonshotai/kimi-k2](https://openrouter.ai/moonshotai/kimi-k2)\n\n**Edit:** There's also a free variant, but I don't know the details: [https://openrouter.ai/moonshotai/kimi-k2:free](https://openrouter.ai/moonshotai/kimi-k2:free)",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2: cheap and fast API access for those who can't run locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cgnl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=8f57197bc62e32d57dfc4ad8906e73c48c44542c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568667,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you can&amp;#39;t run &lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;kimi-k2&lt;/a&gt; locally, there are now more providers offering API access. DeepInfra is now the cheapest provider, while Groq is (by far) the fastest at around ~250 tokens per second:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://deepinfra.com/moonshotai/Kimi-K2-Instruct\"&gt;https://deepinfra.com/moonshotai/Kimi-K2-Instruct&lt;/a&gt; ($0.55/$2.20 in/out Mtoken)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct\"&gt;https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct&lt;/a&gt; ($1/$3 in/out Mtoken, but very fast)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That makes it cheaper than Claude Haiku 3.5, GPT-4.1 and Gemini 2.5 Pro. Not bad for the best non-thinking model currently publicly available!&lt;/p&gt;\n\n&lt;p&gt;It also shows the power of an open weights model with an permissive license: Even if you can&amp;#39;t run it yourself, there&amp;#39;s a lot more options in API access.&lt;/p&gt;\n\n&lt;p&gt;See all providers on OpenRouter: &lt;a href=\"https://openrouter.ai/moonshotai/kimi-k2\"&gt;https://openrouter.ai/moonshotai/kimi-k2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; There&amp;#39;s also a free variant, but I don&amp;#39;t know the details: &lt;a href=\"https://openrouter.ai/moonshotai/kimi-k2:free\"&gt;https://openrouter.ai/moonshotai/kimi-k2:free&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/moonshotai/kimi-k2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?auto=webp&amp;s=cdfbadb27e015c40ecfbfec23378ce0b09fc93d6",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13686945e0fc9e9c1db5cae73fc412b5a2cb6b98",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b7e29e79b28e20dec6cdeacc1e5fb8d7b6b3167",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c0b17ebbed757f60a5fa984a7a88e4132fd8967",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8de1b4b36c00b224fb29471c6864b8730dd4f7f2",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd4fb6296529dfc3732737ad6bd08f71d85ed9b0",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=517824341f4cec5e040a2d0d73434db4ebd33dc2",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0cgnl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/",
          "stickied": false,
          "url": "https://openrouter.ai/moonshotai/kimi-k2",
          "subreddit_subscribers": 499292,
          "created_utc": 1752568667,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 tops creative writing benchmark",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzywie",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 245,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 245,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VH2yrgq-wMtmRSPtQ8VK4WklgU_7GjLW0L_EEyrgxYc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752527951,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q48f55vcpwcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?auto=webp&amp;s=72733329d330a907558da68160618b15b6172b27",
                  "width": 1500,
                  "height": 1000
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b78c6da6f3a69e12a60113ac6638feb8001f4a9",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9698b7792fd84563a11e9d5fcb8304ca9051d91c",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad25ba68ea4d8c123c8ab2e64de62e7e8343e00",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83d8a4d11cd481b0f3d6a15556baa79acf5df855",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c1e5b3512c9a41667a474d9d1a6d3f5fae4ccfa",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3f46623f3ecd81bc3ac63095add6cd4132e7792",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "XgXXR5EePWFyWI8XqcL69hThsTAzGmSFTef0gmKfLjQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzywie",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/",
          "stickied": false,
          "url": "https://i.redd.it/q48f55vcpwcf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752527951,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m021nx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 150,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 150,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-v9_FrsEiCPVytIaPFWwPVUJAlmMBmLn8IIAFMlNQyw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752535545,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nl35mhaybxcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?auto=webp&amp;s=8e83e9de13e964005d0f5b777a1fb221aa69c590",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36b347c1b0477d5c0c50ee12646d88d4534cf13b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=476209c3d4f31e1dd5a6915c23da1b9c681dd240",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c156ec7ae4a7d874263af3bf27bdf6511a1f1353",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=427166a43aad977ff4e628d5d89073bd9fd90280",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef0326b347cdcbbb0a7908eec96440b3414e96e9",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6e79c6c9a2199b96314ff79eab544c770d2dfda",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "YueW4SD4xsNrXkAftR3D5z4lByoNaBI-7VPxyeUYmW8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m021nx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/",
          "stickied": false,
          "url": "https://i.redd.it/nl35mhaybxcf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752535545,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_99mff",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta on track to be first lab with a 1GW supercluster",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0115d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 150,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 150,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/0msAxqRJrwqMDcMC7jZ5bCUaL-sRDisyC8M9WV3X4eQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752533016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/584vdadc4xcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/584vdadc4xcf1.png?auto=webp&amp;s=7e7393e2ff25a0fc5887422d47318ea835f0c1b5",
                  "width": 1010,
                  "height": 459
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aec9db4a133d2a998b21becc7885a200bea8a2bc",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55ba757e3b8229c7c3b2eb3aa62befc54fad32c5",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8220fef2199d3b0b707059754b31f54ea20f5be",
                    "width": 320,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e603dc0a062f5e964b5a1e007efdb4a66dc293f",
                    "width": 640,
                    "height": 290
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1b43c580e6c7de0aa1f0815714aca472b23c3a5",
                    "width": 960,
                    "height": 436
                  }
                ],
                "variants": {},
                "id": "-MjK6OyxWPNhUMp76GWSxVzUaVyreb5xWXtn3a_Nrzc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0115d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jd_3d",
          "discussion_type": null,
          "num_comments": 72,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/",
          "stickied": false,
          "url": "https://i.redd.it/584vdadc4xcf1.png",
          "subreddit_subscribers": 499292,
          "created_utc": 1752533016,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1h9qrwy0w6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UTCP: A safer, scalable tool-calling alternative to MCP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Post of the day  "
            },
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 122,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzl5zk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "transparent",
          "ups": 744,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Post of the day  :X:",
          "can_mod_post": false,
          "score": 744,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NwQpLihWBkRwry7btAEv1kXjEd2jOrNWPfUJ4oMDWTQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752496381,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wv84vx7h3ucf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wv84vx7h3ucf1.png?auto=webp&amp;s=9fc98e1863475ae1707dcf8e031f0c40856c1282",
                  "width": 1874,
                  "height": 1642
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=af7471a080008d1b2a11663755e2386042538cdb",
                    "width": 108,
                    "height": 94
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cbf92935540699f9a5b793b10b599033bdd378c",
                    "width": 216,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5bc00f40fff44e8d417d1c4e7dcd1866bece5c55",
                    "width": 320,
                    "height": 280
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44e4d83d52673aeb1bf507e10f4ab32bff06db95",
                    "width": 640,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4aa90f35e8734ab6dd4bb03e9d5344881c7be6c2",
                    "width": 960,
                    "height": 841
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cafe58b5e161c4d2b61d8cb324738c3b1b2459e3",
                    "width": 1080,
                    "height": 946
                  }
                ],
                "variants": {},
                "id": "kbRMMR47HDIi7lZVVAy5mGTwVKuCZQBEJufsqMy9_24"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5563f7e6-52bf-11f0-a755-7266d77e32bb",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#58a7a4",
          "id": "1lzl5zk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juanviera23",
          "discussion_type": null,
          "num_comments": 134,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/",
          "stickied": false,
          "url": "https://i.redd.it/wv84vx7h3ucf1.png",
          "subreddit_subscribers": 499292,
          "created_utc": 1752496381,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The announcement comes just days after Google hired away Windsurfs CEO Varun Mohan, co-founder Douglas Chen, and research leaders in a $2.4 billion reverse-acquihire that left much of the startups 250-person team behind. Googles deal occurred just hours after OpenAIs $3 billion offer to acquire Windsurf expired, clearing the way for the AI coding startup to explore other options.",
          "author_fullname": "t2_17n3nqtj56",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cognition, maker of the AI coding agent Devin, acquires Windsurf",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 71,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cgmc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=140&amp;height=71&amp;crop=140:71,smart&amp;auto=webp&amp;s=0cd597e97c42d4a4434b6fa6518393d509f6de44",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "techcrunch.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The announcement comes just days after Google hired away Windsurfs CEO Varun Mohan, co-founder Douglas Chen, and research leaders in a $2.4 billion reverse-acquihire that left much of the startups 250-person team behind. Googles deal occurred just hours after OpenAIs $3 billion offer to acquire Windsurf expired, clearing the way for the AI coding startup to explore other options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://techcrunch.com/2025/07/14/cognition-maker-of-the-ai-coding-agent-devin-acquires-windsurf/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?auto=webp&amp;s=3042630aa11746c4e455fed1df9ef33fbaa6e7c8",
                  "width": 1200,
                  "height": 616
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=839b7d352a5d2ddd17832307ab32a76dea5d52eb",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08e4cb9e9357907ae39ab8f3d034a124a1305f89",
                    "width": 216,
                    "height": 110
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81d5b827214f4052c9d3342e42102209265d5f3a",
                    "width": 320,
                    "height": 164
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=045e7a9473212511b88ba864670fee8b5d269a71",
                    "width": 640,
                    "height": 328
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=02c991f48cce1207e0ea4a87ee1f6a8373db5c64",
                    "width": 960,
                    "height": 492
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39bbb22a96486aa76c6a942d036657919c2805d9",
                    "width": 1080,
                    "height": 554
                  }
                ],
                "variants": {},
                "id": "xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0cgmc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FullstackSensei",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/",
          "stickied": false,
          "url": "https://techcrunch.com/2025/07/14/cognition-maker-of-the-ai-coding-agent-devin-acquires-windsurf/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752568663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I made a one-click solution to let anyone run local models on their mac at home and enjoy them from anywhere on their iPhones.\n\nI find myself telling people to run local models instead of using ChatGPT, but the reality is that the whole thing is too complicated for 99.9% of them.  \nSo I made these two companion apps (one for iOS and one for Mac). You just install them and they work.   \n  \nThe Mac app has a selection of Qwen models that run directly on the Mac app with llama.cpp (advanced users can simply ignore those and turn on their Ollama or LMStudio).  \nThe iOS app is a chatbot app like ChatGPT with voice input, attachments with OCR, web search, thinking mode toggle  \nThe UI is super intuitive for anyone who has ever used a chatbot.\n\nThey don't need setting up tailscale or any VPN/tunnel. They work by sending back and forward an iCloud record containing the conversation. Your conversations never leave your private Apple environment.  \n  \nThe only thing that is remotely technical is inserting a Serper API Key in the Mac app to allow web search.\n\nThe iOS app is called LLM Pigeon and this is the link:  \n[https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB](https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB)\n\nThe MacOS app is called LLM Pigeon Server and this is the link:  \n[https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;mt=12](https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;mt=12)\n\n",
          "author_fullname": "t2_n1rqaeut",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source and free iOS app to chat with your LLMs when you are away from home.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0dqgh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752573621,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a one-click solution to let anyone run local models on their mac at home and enjoy them from anywhere on their iPhones.&lt;/p&gt;\n\n&lt;p&gt;I find myself telling people to run local models instead of using ChatGPT, but the reality is that the whole thing is too complicated for 99.9% of them.&lt;br/&gt;\nSo I made these two companion apps (one for iOS and one for Mac). You just install them and they work.   &lt;/p&gt;\n\n&lt;p&gt;The Mac app has a selection of Qwen models that run directly on the Mac app with llama.cpp (advanced users can simply ignore those and turn on their Ollama or LMStudio).&lt;br/&gt;\nThe iOS app is a chatbot app like ChatGPT with voice input, attachments with OCR, web search, thinking mode toggle&lt;br/&gt;\nThe UI is super intuitive for anyone who has ever used a chatbot.&lt;/p&gt;\n\n&lt;p&gt;They don&amp;#39;t need setting up tailscale or any VPN/tunnel. They work by sending back and forward an iCloud record containing the conversation. Your conversations never leave your private Apple environment.  &lt;/p&gt;\n\n&lt;p&gt;The only thing that is remotely technical is inserting a Serper API Key in the Mac app to allow web search.&lt;/p&gt;\n\n&lt;p&gt;The iOS app is called LLM Pigeon and this is the link:&lt;br/&gt;\n&lt;a href=\"https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB\"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The MacOS app is called LLM Pigeon Server and this is the link:&lt;br/&gt;\n&lt;a href=\"https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12\"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?auto=webp&amp;s=448fc3fab8106e6c3c077dbd0f889b318287ce38",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a63933662b194fe098bc5c69f8c72651d00a2dea",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=04dc892def58f636f6674e40c7824308da6d9ebf",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=84d80775a7ae89fc1db23d3d571c1175e85b4b57",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd5e5d251b0a078511eed0316cb89330a7a4a80d",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2caca59a751aa468417298ef108c26be1c134d",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3b71a736f68261d672045f9ff47efb31de4b3c4",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m0dqgh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Valuable-Run2129",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752573621,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone - there are some **245GB quants (80% size reduction)** for Kimi K2 at https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF. The Unsloth dynamic Q2\\_K\\_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.\n\nPlease use `-ot \".ffn_.*_exps.=CPU\"` to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.\n\nYou need to use either [https://github.com/ggml-org/llama.cpp/pull/14654](https://github.com/ggml-org/llama.cpp/pull/14654) or our fork [https://github.com/unslothai/llama.cpp](https://github.com/unslothai/llama.cpp) to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!\n\nThe suggested parameters are:\n\n    temperature = 0.6\n    min_p = 0.01 (set it to a small number)\n\nDocs has more details: [https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally](https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally)",
          "author_fullname": "t2_5wukhd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 1.8bit Unsloth Dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzps3b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 347,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 347,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752507676,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone - there are some &lt;strong&gt;245GB quants (80% size reduction)&lt;/strong&gt; for Kimi K2 at &lt;a href=\"https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF&lt;/a&gt;. The Unsloth dynamic Q2_K_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.&lt;/p&gt;\n\n&lt;p&gt;Please use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.&lt;/p&gt;\n\n&lt;p&gt;You need to use either &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14654\"&gt;https://github.com/ggml-org/llama.cpp/pull/14654&lt;/a&gt; or our fork &lt;a href=\"https://github.com/unslothai/llama.cpp\"&gt;https://github.com/unslothai/llama.cpp&lt;/a&gt; to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!&lt;/p&gt;\n\n&lt;p&gt;The suggested parameters are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;temperature = 0.6\nmin_p = 0.01 (set it to a small number)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Docs has more details: &lt;a href=\"https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally\"&gt;https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?auto=webp&amp;s=bc3027fa5da20b74e927173e28d8aca06d1918f9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=539e7c53ad8fe4d04c6029c11344ff605d38589a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6263e8d5e0d9129138827f26082b6f9517086361",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=247bab7783d0737b150b5f5183ba9db8a0966436",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3e7dfec653d356a4ccb836ae65b4546e9a5ad00",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a4a1e7fdeac330903838e1542ed085ea53ec142",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff85dc8c4e34c5580c7c7bf51d308d118a9e322f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzps3b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danielhanchen",
          "discussion_type": null,
          "num_comments": 96,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752507676,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think that happened. Because Elon Musk forgot or canceled that Grok-2 would be open sourced after Grok-3 was stable. And now Grok-4 but Elon Musk did not open source Grok-2 or even Grok-3. I think Elon Musk is following the OpenAI or ANTHROP\\C. Until now Elon Musk still makes announcements that he will open source Grok-2 and Grok-3 and it is unknown whether Elon Musk will cut off the API for these two models. \n\nEdit : \nSam Atlam : Elon Musk Will Promise That I Will Open Source Grok-2 Once Grok-3 Is Stable. But not Elon Musk doesn't Open-source any model (e.g Grok-2 or Grok-3) and now.\n\nMe : xAI promise Open-source grok-2 or Grok-3?\n\nSam Atlam: xAI is lie. OpenAI release Open-source thinking model soon. Say tuned!",
          "author_fullname": "t2_7ktr17a6i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grok no more model Open-source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m04ic2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752574571,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752542208,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think that happened. Because Elon Musk forgot or canceled that Grok-2 would be open sourced after Grok-3 was stable. And now Grok-4 but Elon Musk did not open source Grok-2 or even Grok-3. I think Elon Musk is following the OpenAI or ANTHROP\\C. Until now Elon Musk still makes announcements that he will open source Grok-2 and Grok-3 and it is unknown whether Elon Musk will cut off the API for these two models. &lt;/p&gt;\n\n&lt;p&gt;Edit : \nSam Atlam : Elon Musk Will Promise That I Will Open Source Grok-2 Once Grok-3 Is Stable. But not Elon Musk doesn&amp;#39;t Open-source any model (e.g Grok-2 or Grok-3) and now.&lt;/p&gt;\n\n&lt;p&gt;Me : xAI promise Open-source grok-2 or Grok-3?&lt;/p&gt;\n\n&lt;p&gt;Sam Atlam: xAI is lie. OpenAI release Open-source thinking model soon. Say tuned!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m04ic2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Brilliant_Stock_5137",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752542208,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://youtu.be/vW30o4U9BFE",
          "author_fullname": "t2_7kg5p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A very nice overview on how llama.cpp quantization works",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m03sh9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752540238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://youtu.be/vW30o4U9BFE\"&gt;https://youtu.be/vW30o4U9BFE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?auto=webp&amp;s=b3e9aec855a9a197fe9619de2626df9b16de04d8",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=617b8e7b6a8328658f045f639d60a0618107415a",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c67f23813dd77438f9dd053b5b49625c7db6c5a8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee9cbfbcf35cda08614a8ac4291b096d6010b0b5",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m03sh9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kooshi_Govno",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752540238,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For those of you who aren't familiar with SurfSense, it aims to be the**open-source alternative to NotebookLM, Perplexity, or Glean.**\n\nIn short, it's a**Highly Customizable AI Research Agent**that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.\n\nI'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHeres a quick look at what SurfSense offers right now:\n\n**Feature**s\n\n* Supports 100+ LLMs\n* Supports local Ollama or vLLM setups\n* 6000+ Embedding Models\n* Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)\n* Hierarchical Indices (2-tiered RAG setup)\n* Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)\n* Offers a RAG-as-a-Service API Backend\n* 50+ File extensions supported\n\n**Podcast**s\n\n* Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)\n* Convert chat conversations into engaging audio\n* Multiple TTS providers supported\n\n**External Sources Integration**\n\n* Search engines (Tavily, LinkUp)\n* Slack\n* Linear\n* Notion\n* YouTube videos\n* GitHub\n* Discord\n* ...and more on the way\n\n**Cross-Browser Extensio**n\n\nThe SurfSense extension lets you save any dynamic webpage you want, including authenticated content.\n\n**Interested in contributing?**\n\nSurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.\n\nGitHub:[https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
          "author_fullname": "t2_63zmedmg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Alternative to NotebookLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzw6yu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 95,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 95,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752521799,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you who aren&amp;#39;t familiar with SurfSense, it aims to be the&lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In short, it&amp;#39;s a&lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt;that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for contributors to help shape the future of SurfSense! If you&amp;#39;re interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt;\n\n&lt;p&gt;Heres a quick look at what SurfSense offers right now:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Feature&lt;/strong&gt;s&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Supports 100+ LLMs&lt;/li&gt;\n&lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt;\n&lt;li&gt;6000+ Embedding Models&lt;/li&gt;\n&lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt;\n&lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt;\n&lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt;\n&lt;li&gt;Offers a RAG-as-a-Service API Backend&lt;/li&gt;\n&lt;li&gt;50+ File extensions supported&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Podcast&lt;/strong&gt;s&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt;\n&lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt;\n&lt;li&gt;Multiple TTS providers supported&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt;\n&lt;li&gt;Slack&lt;/li&gt;\n&lt;li&gt;Linear&lt;/li&gt;\n&lt;li&gt;Notion&lt;/li&gt;\n&lt;li&gt;YouTube videos&lt;/li&gt;\n&lt;li&gt;GitHub&lt;/li&gt;\n&lt;li&gt;Discord&lt;/li&gt;\n&lt;li&gt;...and more on the way&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Cross-Browser Extensio&lt;/strong&gt;n&lt;/p&gt;\n\n&lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you&amp;#39;re welcome to join in.&lt;/p&gt;\n\n&lt;p&gt;GitHub:&lt;a href=\"https://github.com/MODSetter/SurfSense\"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?auto=webp&amp;s=344bed7e266c934d23a7957319ca13c4100c57cd",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4cc9007548328d59c5d49b07997ca37e0c33349",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e50ed04f44d9e471185d98e483ce202837b37726",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9fdc40a870cb735030c8be40d94ff73f8fae0e5d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b3d2dfc8cba3d61bcd473981e8d0d2e8cf77005",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3aee46322a0a56f4709de62484df290bd68312e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=725ea9d297e60b5f56be92de035c5f6c2bba3d2d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lzw6yu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Uiqueblhats",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752521799,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the authors insights genuinely thought-provoking. The original Chinese version is [here](https://bigeagle.me/2025/07/kimi-k2/)feel free to read it in full (and of course you can use Kimi K2 as your translator). Heres my own distilled summary of the main points:\n\n\n\n Beyond chatbots: Kimi K2 experiments with an artifact-first interaction model that has the AI immediately build interactive front-end deliverablesPPT-like pages, diagrams, even mini-gamesrather than simply returning markdown text.\n\n Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.\n\n What makes an agentic model: A minimal loopthink, choose tools, observe results, iteratecan be learned from synthetic trajectories. Todays agent abilities are early-stage; the next pre-training wave still holds plenty of upside.\n\n Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit hacky hidden pipelines, forcing genuinely strong, general modelsexactly what an AGI-oriented startup needs.\n\n Marketing controversies &amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1s viral rise proved that raw model quality markets itself and validates the foundation-model-first path.\n\n Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.\n\n\n\nFrom the entire blog, this is the paragraph I loved the most:\n\n&gt;A while ago, Agent products were all the rage. I kept hearing people say that Kimi shouldnt compete on large models and should focus on Agents instead. Let me be clear: **the vast majority of Agent products are nothing without Claude behind them.** Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we dont keep pushing that ceiling higher, I wont stay here a single extra day.\n\n&gt;Chasing AGI is an extremely narrow, perilous bridgetheres no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, As an investor, I care about the ROI of AI applications. In that moment I knew the company he founded wouldnt last long.",
          "author_fullname": "t2_sqi8xxun",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "After Kimi K2 Is Released: No Longer Just a ChatBot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzm645",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 306,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 306,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752499086,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the authors insights genuinely thought-provoking. The original Chinese version is &lt;a href=\"https://bigeagle.me/2025/07/kimi-k2/\"&gt;here&lt;/a&gt;feel free to read it in full (and of course you can use Kimi K2 as your translator). Heres my own distilled summary of the main points:&lt;/p&gt;\n\n&lt;p&gt; Beyond chatbots: Kimi K2 experiments with an artifact-first interaction model that has the AI immediately build interactive front-end deliverablesPPT-like pages, diagrams, even mini-gamesrather than simply returning markdown text.&lt;/p&gt;\n\n&lt;p&gt; Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.&lt;/p&gt;\n\n&lt;p&gt; What makes an agentic model: A minimal loopthink, choose tools, observe results, iteratecan be learned from synthetic trajectories. Todays agent abilities are early-stage; the next pre-training wave still holds plenty of upside.&lt;/p&gt;\n\n&lt;p&gt; Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit hacky hidden pipelines, forcing genuinely strong, general modelsexactly what an AGI-oriented startup needs.&lt;/p&gt;\n\n&lt;p&gt; Marketing controversies &amp;amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1s viral rise proved that raw model quality markets itself and validates the foundation-model-first path.&lt;/p&gt;\n\n&lt;p&gt; Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.&lt;/p&gt;\n\n&lt;p&gt;From the entire blog, this is the paragraph I loved the most:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;A while ago, Agent products were all the rage. I kept hearing people say that Kimi shouldnt compete on large models and should focus on Agents instead. Let me be clear: &lt;strong&gt;the vast majority of Agent products are nothing without Claude behind them.&lt;/strong&gt; Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we dont keep pushing that ceiling higher, I wont stay here a single extra day.&lt;/p&gt;\n\n&lt;p&gt;Chasing AGI is an extremely narrow, perilous bridgetheres no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, As an investor, I care about the ROI of AI applications. In that moment I knew the company he founded wouldnt last long.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzm645",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nekofneko",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752499086,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nIs there a leaderboard for open source LLMs?  I know [this](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) one for VLMs and there used to be one from HuggingFace, but I think that [one](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?columns=rank%2Cmodel.type_icon%2Cid%2Cmodel.average_score%2Cevaluations.ifeval.normalized_score%2Cevaluations.bbh.normalized_score%2Cevaluations.math.normalized_score%2Cevaluations.gpqa.normalized_score%2Cevaluations.musr.normalized_score%2Cevaluations.mmlu_pro.normalized_score%2Cmetadata.params_billions&amp;official=true) is no longer maintained.",
          "author_fullname": "t2_838sm24m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source LLMs leaderboard",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0c7am",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752567551,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Is there a leaderboard for open source LLMs?  I know &lt;a href=\"https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\"&gt;this&lt;/a&gt; one for VLMs and there used to be one from HuggingFace, but I think that &lt;a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?columns=rank%2Cmodel.type_icon%2Cid%2Cmodel.average_score%2Cevaluations.ifeval.normalized_score%2Cevaluations.bbh.normalized_score%2Cevaluations.math.normalized_score%2Cevaluations.gpqa.normalized_score%2Cevaluations.musr.normalized_score%2Cevaluations.mmlu_pro.normalized_score%2Cmetadata.params_billions&amp;amp;official=true\"&gt;one&lt;/a&gt; is no longer maintained.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?auto=webp&amp;s=7e1b921f2aadc5a0f6eb3d7bd413a05df185fd20",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7996a9b4d61beea62fd32063e03712705ab26f8c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b420d4d2cf1c09672c30f9673ea6f1ac400fd6fb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=499b326baf2a9ad8a46034202c54054ee71fbf03",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f77a5813c7d65ef0d6f8e4c821b62f9d5e939dda",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0267bd806e21c11bcb30fdcd9ddf61fa3420d68d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e532be686a9e8ae2db46f566177856dfda08ede6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0c7am",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oh_my_right_leg",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752567551,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_73rg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Moonshot AIs open source Kimi K2 outperforms GPT-4 in key benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m013ou",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 52,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 52,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752533189,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "moonshotai.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://moonshotai.github.io/Kimi-K2/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m013ou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yogthos",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m013ou/moonshot_ais_open_source_kimi_k2_outperforms_gpt4/",
          "stickied": false,
          "url": "https://moonshotai.github.io/Kimi-K2/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752533189,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_hdcx5ggfg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Metas New Superintelligence Lab Is Discussing Major A.I. Strategy Changes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzv16g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 103,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 103,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=684dded7ebf0cced3ec460c9dda8f551b9ecbd73",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752519240,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "nytimes.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?auto=webp&amp;s=211ff5c9d8860c633734a0f69515f881de8905e4",
                  "width": 1050,
                  "height": 550
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f431ed3ef795de81f0d9be2452ed2466f4727f88",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c728fd0b47256c06b7e53063606348710b74999",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a1dc415541c74d1ee2dd3620b8e6997e56ad7f2",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a0ebffa84a0071645409fce2ba2a7d33bd6a731",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=361e53f2aa166522931efd9533bd8c76685cfc5a",
                    "width": 960,
                    "height": 502
                  }
                ],
                "variants": {},
                "id": "62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzv16g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "showmeufos",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/",
          "stickied": false,
          "url": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "subreddit_subscribers": 499292,
          "created_utc": 1752519240,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Last week, a small group of top members of the lab, including Alexandr Wang, 28, Metas new chief A.I. officer, discussed abandoning the companys most powerful open source A.I. model, called Behemoth, in favor of developing a closed model, two people with knowledge of the matter said.\n\nMeta had finished feeding in data to improve its Behemoth model, a process known as training, but has delayed its release because of poor internal performance, said the people with knowledge of the matter, who were not authorized to discuss private conversations. After the company announced the formation of the superintelligence lab last month, teams working on the Behemoth model  which is known as a frontier model  stopped running new tests on it, one of the people said.\n\n\n",
          "author_fullname": "t2_u398xzta",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Metas New Superintelligence Lab Is Discussing Major A.I. Strategy Changes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 127,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m041m4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7b2503awF-4b2icSl8JuZh6vDtrrHCXba5mxqJvk1zA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752540939,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last week, a small group of top members of the lab, including Alexandr Wang, 28, Metas new chief A.I. officer, discussed abandoning the companys most powerful open source A.I. model, called Behemoth, in favor of developing a closed model, two people with knowledge of the matter said.&lt;/p&gt;\n\n&lt;p&gt;Meta had finished feeding in data to improve its Behemoth model, a process known as training, but has delayed its release because of poor internal performance, said the people with knowledge of the matter, who were not authorized to discuss private conversations. After the company announced the formation of the superintelligence lab last month, teams working on the Behemoth model  which is known as a frontier model  stopped running new tests on it, one of the people said.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3f68h6pzrxcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?auto=webp&amp;s=0896b8b618e97942c4ee1042d76206fda02632e1",
                  "width": 1080,
                  "height": 984
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d2ac4cc26b27343c96b0a03f825276999d5b174",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400750301dac83d5f73809374e9b7b466d85b02e",
                    "width": 216,
                    "height": 196
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=06576965338eaac8ae34bb2407956757d30b744d",
                    "width": 320,
                    "height": 291
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b646755e2c619c761965d3179957a1ab2c65c147",
                    "width": 640,
                    "height": 583
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84a06d1a5914a6525f604045a7fc9a8b7f0954c9",
                    "width": 960,
                    "height": 874
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06d3e9060e92dd36bfb8b5c6dde39bd596967ff8",
                    "width": 1080,
                    "height": 984
                  }
                ],
                "variants": {},
                "id": "yVwjvgqtLycnuN38b4xHyA_D8ioyPBQ4lGHqp1UdmK4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m041m4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sunshinecheung",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m041m4/metas_new_superintelligence_lab_is_discussing/",
          "stickied": false,
          "url": "https://i.redd.it/3f68h6pzrxcf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752540939,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm attempting to fine-tune Qwen3-8B for a specific domain.  Since this model produces thinking tokens, I'm a bit unsure how to handle them during training.\n\nI'm attempting to use `DPOConfig` and `DPOTrainer` from `trl`, with `Lora` for lower VRAM usage.\n\nFor training, do I include the `&lt;thinking&gt;` tokens in the `chosen` and `rejected` outputs for the training data?  It's a bit unclear to me how to handle these.",
          "author_fullname": "t2_5imxo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can I fine-tune Qwen3 with DPO? How do I handle &lt;thinking&gt; tokens?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0d6ry",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "author_cakeday": true,
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752571552,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m attempting to fine-tune Qwen3-8B for a specific domain.  Since this model produces thinking tokens, I&amp;#39;m a bit unsure how to handle them during training.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m attempting to use &lt;code&gt;DPOConfig&lt;/code&gt; and &lt;code&gt;DPOTrainer&lt;/code&gt; from &lt;code&gt;trl&lt;/code&gt;, with &lt;code&gt;Lora&lt;/code&gt; for lower VRAM usage.&lt;/p&gt;\n\n&lt;p&gt;For training, do I include the &lt;code&gt;&amp;lt;thinking&amp;gt;&lt;/code&gt; tokens in the &lt;code&gt;chosen&lt;/code&gt; and &lt;code&gt;rejected&lt;/code&gt; outputs for the training data?  It&amp;#39;s a bit unclear to me how to handle these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0d6ry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pragmojo",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0d6ry/can_i_finetune_qwen3_with_dpo_how_do_i_handle/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0d6ry/can_i_finetune_qwen3_with_dpo_how_do_i_handle/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752571552,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've noticed that if you begin a chat with a reasoning model like Qwen 3 and then in subsequent messages switch to a different non-reasoning model (such as Gemma 3 12b or Devstral 2507) the non-reasoning model will sometimes also generate reasoning tokens and respond with a final answer afterwards like it was trained to perform reasoning. This is also without any system prompt.",
          "author_fullname": "t2_i305y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Non-reasoning models adopting reasoning behavior from previous messages",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m06nhe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752548299,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed that if you begin a chat with a reasoning model like Qwen 3 and then in subsequent messages switch to a different non-reasoning model (such as Gemma 3 12b or Devstral 2507) the non-reasoning model will sometimes also generate reasoning tokens and respond with a final answer afterwards like it was trained to perform reasoning. This is also without any system prompt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m06nhe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thedudely1",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752548299,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_wkg30tqo1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "XSched: Preemptive Scheduling for Diverse XPUs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 64,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cnzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g84ct0fn40df1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 882,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g84ct0fn40df1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g84ct0fn40df1/DASHPlaylist.mpd?a=1755175134%2CMjc4ZjFlZmU4NjIzZDNlODc0NzFjMjExYjM4ZjQzNzlkODZiZWQxOGVkYTgxNDIzZTI1MzJlMGEzZGExZjBmYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/g84ct0fn40df1/HLSPlaylist.m3u8?a=1755175134%2CNGQxM2QyNTVjZmNlNTU2NmQ2MTg1MzkxM2ViYzFjODljNDU1YWU5YjdlYTMzMzY1ZjdlZWQyYmYwYzkyZTJlYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=140&amp;height=64&amp;crop=140:64,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=2b5d0ca0de620b826f48ad0d0c0a34c290e27f78",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752569498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g84ct0fn40df1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?format=pjpg&amp;auto=webp&amp;s=d40bc6e47e567135be91b9dff7fd5198d041470c",
                  "width": 3052,
                  "height": 1402
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e2d1c9fd4919f27d33757da4b55652f6fd667a1c",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=34068c5143fa7cf767f84da0e5e3fb4fdb860131",
                    "width": 216,
                    "height": 99
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a419572068a6f0ac7e2158867f241559feab41a0",
                    "width": 320,
                    "height": 146
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d05834a11688d2966e4c21c3660370f469ce2284",
                    "width": 640,
                    "height": 293
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c6cd094405941cdcde3a49fcccae07b7ce9eb2fc",
                    "width": 960,
                    "height": 440
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=89a68f8eb3c152af102ca8171454c8b34bb14dc0",
                    "width": 1080,
                    "height": 496
                  }
                ],
                "variants": {},
                "id": "YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0cnzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LibraryNo6067",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cnzs/xsched_preemptive_scheduling_for_diverse_xpus/",
          "stickied": false,
          "url": "https://v.redd.it/g84ct0fn40df1",
          "subreddit_subscribers": 499292,
          "created_utc": 1752569498,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g84ct0fn40df1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 882,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g84ct0fn40df1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g84ct0fn40df1/DASHPlaylist.mpd?a=1755175134%2CMjc4ZjFlZmU4NjIzZDNlODc0NzFjMjExYjM4ZjQzNzlkODZiZWQxOGVkYTgxNDIzZTI1MzJlMGEzZGExZjBmYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/g84ct0fn40df1/HLSPlaylist.m3u8?a=1755175134%2CNGQxM2QyNTVjZmNlNTU2NmQ2MTg1MzkxM2ViYzFjODljNDU1YWU5YjdlYTMzMzY1ZjdlZWQyYmYwYzkyZTJlYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/Rivridis/Assistant-Client](https://github.com/Rivridis/Assistant-Client)\n\nOver the past few years, I have been developing a AI function calling agent, that can perfectly call functions with models as small as 3B or 7B parameters. Most of the frameworks I found while researching this topic just did not work with smaller, and non finetuned models. I tried llama-cpp openai, langchain and ollama but the function call success rate was disappointing for these small models.\n\nThe app can work with any LLM, no specific function calling finetunes needed. I took the suggestions from all the comments, and ported the UI to pyside from gradio. The app now comes in a desktop app format, and supports OpenAI API, so any models can be used. The models can be served from KoboldCPP or similar endpoints.\n\nThe current functions that it supports are search, music as well as weather. I tried to make it as easy to extend as possible, so feel free to add functions on top of it for your own use cases.\n\nIt also has a basic PDF query mode, as well as a code editor mode. \n\nThanks for all the support! If anyone has further ideas or improvements, please let me know. If anyone wants a tutorial or a guide, I shall provide that too.",
          "author_fullname": "t2_gs1v5r62",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Assistant Agent with function calling - Update 2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0drwa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752573750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/Rivridis/Assistant-Client\"&gt;https://github.com/Rivridis/Assistant-Client&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Over the past few years, I have been developing a AI function calling agent, that can perfectly call functions with models as small as 3B or 7B parameters. Most of the frameworks I found while researching this topic just did not work with smaller, and non finetuned models. I tried llama-cpp openai, langchain and ollama but the function call success rate was disappointing for these small models.&lt;/p&gt;\n\n&lt;p&gt;The app can work with any LLM, no specific function calling finetunes needed. I took the suggestions from all the comments, and ported the UI to pyside from gradio. The app now comes in a desktop app format, and supports OpenAI API, so any models can be used. The models can be served from KoboldCPP or similar endpoints.&lt;/p&gt;\n\n&lt;p&gt;The current functions that it supports are search, music as well as weather. I tried to make it as easy to extend as possible, so feel free to add functions on top of it for your own use cases.&lt;/p&gt;\n\n&lt;p&gt;It also has a basic PDF query mode, as well as a code editor mode. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the support! If anyone has further ideas or improvements, please let me know. If anyone wants a tutorial or a guide, I shall provide that too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0drwa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rivridis",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0drwa/ai_assistant_agent_with_function_calling_update_2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0drwa/ai_assistant_agent_with_function_calling_update_2/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752573750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We published a step by step tutorial for building AI agents that actually do things, not just chat. Each section adds a key capability, with runnable code and examples.\n\nhttps://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;format=png&amp;auto=webp&amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915\n\nTutorial:[https://voltagent.dev/tutorial/introduction/](https://voltagent.dev/tutorial/introduction/)\n\nGitHub Repo:[https://github.com/voltagent/voltagent](https://github.com/voltagent/voltagent)\n\nTutorial Source Code:[https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial](https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial)\n\nWeve been building OSS dev tools for over 7 years. From that experience, weve seen that tutorials which combine key concepts with hands-on code examples are the most effective way to understand the why and how of agent development.\n\nWhat we implemented:\n\n**1  The Chatbot Problem**\n\nWhy most chatbots are limited and what makes AI agents fundamentally different.\n\n**2  Tools: Give Your Agent Superpowers**\n\nLet your agent do real work: call APIs, send emails, query databases, and more.\n\n**3  Memory: Remember Every Conversation**\n\nPersist conversations so your agent builds context over time.\n\n**4  MCP: Connect to Everything**\n\nUsing MCP to integrate GitHub, Slack, databases, etc.\n\n**5  Subagents: Build Agent Teams**\n\nCreate specialized agents that collaborate to handle complex tasks.\n\nIts all built using VoltAgent, our TypeScript-first open-source AI agent framework.(I'm maintainer) It handles routing, memory, observability, and tool execution, so you can focus on logic and behavior.\n\nAlthough the tutorial uses VoltAgent, the core ideas tools, memory, coordination are framework-agnostic. So even if youre using another framework or building from scratch, the steps should still be useful.\n\nWed love your feedback, especially from folks building agent systems. If you notice anything unclear or incomplete, feel free to open an issue or PR. Its all part of the open-source repo.  \n",
          "author_fullname": "t2_4j8si8hf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Agent tutorial in TS from the basics to building multi-agent teams",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8z5hh3z8yzcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 58,
                  "x": 108,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8aee11abb3595aeaa0848afcd21eb946da5c5c7"
                },
                {
                  "y": 117,
                  "x": 216,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a9c4056b861c8c91d21a3940c712f863c5a074f"
                },
                {
                  "y": 173,
                  "x": 320,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e00e2fc3b177279b4a1ed703d0cf1cd1db461a3b"
                },
                {
                  "y": 347,
                  "x": 640,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f694acd6797bb7c9c3588a670750344f18320fe"
                },
                {
                  "y": 521,
                  "x": 960,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=421bc5049aeb3983367f2c5f46c5b6cf156a86d2"
                },
                {
                  "y": 586,
                  "x": 1080,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfe681c786969bd653e4f981fb7cfc2138262cf3"
                }
              ],
              "s": {
                "y": 1490,
                "x": 2744,
                "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;format=png&amp;auto=webp&amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915"
              },
              "id": "8z5hh3z8yzcf1"
            }
          },
          "name": "t3_1m0c569",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=b4b174b6326037ae49dc9d4668deea2874c46e6e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752567322,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We published a step by step tutorial for building AI agents that actually do things, not just chat. Each section adds a key capability, with runnable code and examples.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915\"&gt;https://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tutorial:&lt;a href=\"https://voltagent.dev/tutorial/introduction/\"&gt;https://voltagent.dev/tutorial/introduction/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub Repo:&lt;a href=\"https://github.com/voltagent/voltagent\"&gt;https://github.com/voltagent/voltagent&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tutorial Source Code:&lt;a href=\"https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial\"&gt;https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Weve been building OSS dev tools for over 7 years. From that experience, weve seen that tutorials which combine key concepts with hands-on code examples are the most effective way to understand the why and how of agent development.&lt;/p&gt;\n\n&lt;p&gt;What we implemented:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1  The Chatbot Problem&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Why most chatbots are limited and what makes AI agents fundamentally different.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2  Tools: Give Your Agent Superpowers&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Let your agent do real work: call APIs, send emails, query databases, and more.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3  Memory: Remember Every Conversation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Persist conversations so your agent builds context over time.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4  MCP: Connect to Everything&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Using MCP to integrate GitHub, Slack, databases, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5  Subagents: Build Agent Teams&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Create specialized agents that collaborate to handle complex tasks.&lt;/p&gt;\n\n&lt;p&gt;Its all built using VoltAgent, our TypeScript-first open-source AI agent framework.(I&amp;#39;m maintainer) It handles routing, memory, observability, and tool execution, so you can focus on logic and behavior.&lt;/p&gt;\n\n&lt;p&gt;Although the tutorial uses VoltAgent, the core ideas tools, memory, coordination are framework-agnostic. So even if youre using another framework or building from scratch, the steps should still be useful.&lt;/p&gt;\n\n&lt;p&gt;Wed love your feedback, especially from folks building agent systems. If you notice anything unclear or incomplete, feel free to open an issue or PR. Its all part of the open-source repo.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?auto=webp&amp;s=a8817dcbfeaab6c14c681801c3073e647672eb78",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ec3a00d824610d07bc7f692e5dde19879623eaf",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3aaf85260c02ee8d49aa2539b5e8a10f1d5f2bd4",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eef1ca2e1aefc6a5a30dd572191ea5ecb5014640",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44638bb3ecbfd7937f6eabdd3c0d40e4ea668e61",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7502789f9f730afff96866efdae7329e1ac2841a",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5fad34b3deb38b9bdba5a12072de41447627dd43",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m0c569",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "necati-ozmen",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0c569/ai_agent_tutorial_in_ts_from_the_basics_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0c569/ai_agent_tutorial_in_ts_from_the_basics_to/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752567322,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " Introducing my first (open-source) NPM package: Whisper Node Addon.  \nIt allows to transcribe audio with Whisper.cpp straight in your Node.js environment after just installing it, no manual configuration or compilation needed. Not only that, it comes with scripts if you wish to build your binaries manually.\n\n And the biggest part? It supports GPU acceleration through Vulkan API (or Metal on Apple systems), effectively making real-time transcriptions possible with a decent hardware. If you don't have a GPU or you mind using it (while gaming, for example, to save resources), you can always fall back to CPU usage with a single option.\n\n To make all of this possible, I have forked previous works by others and improved upon the addon source in C++, typing (TypeScript), CI/CD (Github Actions) and many other aspects.\n\nGet prebuilt binaries at:  \n[https://www.npmjs.com/package/@kutalia/whisper-node-addon](https://www.npmjs.com/package/@kutalia/whisper-node-addon)  \nSource code:  \n[https://github.com/Kutalia/whisper-node-addon](https://github.com/Kutalia/whisper-node-addon)",
          "author_fullname": "t2_h6o0chk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Whisper.cpp Node.js Addon with Vulkan Support",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0eq11",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752581164,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752577126,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt; Introducing my first (open-source) NPM package: Whisper Node Addon.&lt;br/&gt;\nIt allows to transcribe audio with Whisper.cpp straight in your Node.js environment after just installing it, no manual configuration or compilation needed. Not only that, it comes with scripts if you wish to build your binaries manually.&lt;/p&gt;\n\n&lt;p&gt; And the biggest part? It supports GPU acceleration through Vulkan API (or Metal on Apple systems), effectively making real-time transcriptions possible with a decent hardware. If you don&amp;#39;t have a GPU or you mind using it (while gaming, for example, to save resources), you can always fall back to CPU usage with a single option.&lt;/p&gt;\n\n&lt;p&gt; To make all of this possible, I have forked previous works by others and improved upon the addon source in C++, typing (TypeScript), CI/CD (Github Actions) and many other aspects.&lt;/p&gt;\n\n&lt;p&gt;Get prebuilt binaries at:&lt;br/&gt;\n&lt;a href=\"https://www.npmjs.com/package/@kutalia/whisper-node-addon\"&gt;https://www.npmjs.com/package/@kutalia/whisper-node-addon&lt;/a&gt;&lt;br/&gt;\nSource code:&lt;br/&gt;\n&lt;a href=\"https://github.com/Kutalia/whisper-node-addon\"&gt;https://github.com/Kutalia/whisper-node-addon&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?auto=webp&amp;s=2b80b48069e6e2900296598568d8700477d6c523",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a058a293cabb63c88c9b65bc5197d6dfecc1cca",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5298bb0bf3f3e0e44cf949c71bec6aeb58ca173",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f87ac1cdb3372440ab71f2629e322dc7dc3c4d5b",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=549bf11b83e2f21f7e2a435f135db8605b5715a8",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a4b94a37a7ef533b4b3bb7e54547a4574ff5a55",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47f187ecab66351badd40ee9ebc2caa50361dfff",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0eq11",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kutalia",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752577126,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4](https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4) (paywall)\n\nI don't know how the French and European authorities could accept this.",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Apple will seriously consider buying Mistral | Bloomberg - Mark Gurman",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 117,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfhhq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 542,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 542,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Oi81Df9SQch6JRWkXBdb70VbCbc5PHkhZHq2yJRoex0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752475719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4\"&gt;https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4&lt;/a&gt; (paywall)&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know how the French and European authorities could accept this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/syyfccpldscf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/syyfccpldscf1.jpeg?auto=webp&amp;s=6c1efde69cd327275f5e2033e00c0702e28920d1",
                  "width": 1662,
                  "height": 1390
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89bbf9ebe8ad920a9decea85a04e8ddddce9143e",
                    "width": 108,
                    "height": 90
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf248f7dbf4358bb70f07ee9c1da43f8a5a1428d",
                    "width": 216,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dbbee1c5b59b74677e14df9bdb2c959132e1da7b",
                    "width": 320,
                    "height": 267
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c267b676d172a191872cfbacda802bec7e6a2e8",
                    "width": 640,
                    "height": 535
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=940d958a3d989315f06b79867efa25519d7abd78",
                    "width": 960,
                    "height": 802
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=121ecef6d4b87299bfabd79e10028f6748fd5beb",
                    "width": 1080,
                    "height": 903
                  }
                ],
                "variants": {},
                "id": "QahBk6E1a44oImzdfLzsg1gZn_nkm-ZxKpD4RbU9wnc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzfhhq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 219,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/",
          "stickied": false,
          "url": "https://i.redd.it/syyfccpldscf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752475719,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MMLU-ProX is a multilingual benchmark that extends the challenging MMLU-Pro benchmark to 29 typologically diverse languages, designed to evaluate the cross-lingual reasoning capabilities of large language models (LLMs). Built through a rigorous four-stage translation pipeline using state-of-the-art LLMs (primarily Claude Sonnet 3.7) combined with expert verification, the benchmark contains 11,829 identical questions per language (with a lite version of 658 questions), covering 57 subjects across multiple disciplines with complex reasoning-focused multiple-choice questions featuring 10 answer options and chain-of-thought prompting support.\n\nThe benchmark reveals significant performance disparities across languages when evaluating 36 state-of-the-art LLMs, with models achieving strong performance on high-resource Western European languages (often 75%+ accuracy) but substantially lower scores on low-resource African languages like Wolof (as low as 0.6% to 58.6%), highlighting persistent challenges in multilingual AI development and the need for more inclusive language model capabilities across global contexts.\n\n- Website: https://mmluprox.github.io\n- Paper: https://arxiv.org/abs/2503.10497\n- Code: https://github.com/weihao1115/MMLU-ProX (still empty)\n- Full dataset: https://huggingface.co/datasets/li-lab/MMLU-ProX\n- Lite dataset: https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wgsfm22gswcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 141,
                  "x": 108,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=691617a7ea37d1a5091179464ab143f85468a96e"
                },
                {
                  "y": 282,
                  "x": 216,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13438d7635ba748f597b00003f2864927e862a0d"
                },
                {
                  "y": 419,
                  "x": 320,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90ce8e777b58617edab45238b32011b2a0fc2fd4"
                },
                {
                  "y": 838,
                  "x": 640,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c6764ec9b355c53fe2ad3069ae65b2d7c965c24"
                },
                {
                  "y": 1257,
                  "x": 960,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c1ab9f3f6ff3057a8109d9ef7b40dba9f332b1b"
                },
                {
                  "y": 1414,
                  "x": 1080,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a7d29c5d99c65d37649c143640f1837355e5883"
                }
              ],
              "s": {
                "y": 2416,
                "x": 1845,
                "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=1845&amp;format=pjpg&amp;auto=webp&amp;s=af2d88a2d17ea254a0dc776e725953027aba90ff"
              },
              "id": "wgsfm22gswcf1"
            },
            "cepva22gswcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 63,
                  "x": 108,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=721ac7c709c4f0f3960a92c4ac1130823c849949"
                },
                {
                  "y": 127,
                  "x": 216,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=755aa779fcf53987afe487b20786abae4bc317d9"
                },
                {
                  "y": 189,
                  "x": 320,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6000d6aff7a8d8ad61eb3a2b8e7911338a22ac9"
                },
                {
                  "y": 378,
                  "x": 640,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=36ca8dc0910cc79159f24e3b974fd7d8611a534e"
                },
                {
                  "y": 568,
                  "x": 960,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3be6352cc43e102dca25a7edef1e96677ce41f3b"
                },
                {
                  "y": 639,
                  "x": 1080,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56c643abe61beffd12a31324ef3eb83028bfb195"
                }
              ],
              "s": {
                "y": 1498,
                "x": 2531,
                "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=2531&amp;format=pjpg&amp;auto=webp&amp;s=2b6a4c92395db017d8d7ecc8c4ff995541355888"
              },
              "id": "cepva22gswcf1"
            },
            "jy1gl22gswcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 123,
                  "x": 108,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d501900b8ce2284997bbb24f0cb60d3663d6132"
                },
                {
                  "y": 247,
                  "x": 216,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76b73821477a93932f341bda7a8514ead81853ee"
                },
                {
                  "y": 367,
                  "x": 320,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd43ee53d37519462ce42c539e56fc3699ffeb63"
                },
                {
                  "y": 734,
                  "x": 640,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d93023b31b27e8ca98a804b8c4e3a459b3c6d4dd"
                },
                {
                  "y": 1101,
                  "x": 960,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=42183f8dcbc95702fcd7ba98a5d06493b984431a"
                },
                {
                  "y": 1239,
                  "x": 1080,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e56314bcacb6ec3529a7ef5f17364fd2d2a328dd"
                }
              ],
              "s": {
                "y": 2226,
                "x": 1940,
                "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=1940&amp;format=pjpg&amp;auto=webp&amp;s=ab8a55e2c76f7c0820b0122d79426fa515025fdf"
              },
              "id": "jy1gl22gswcf1"
            },
            "o9ysd22gswcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 131,
                  "x": 108,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=92f7a072ea75bf25c6613c54363b3a8816f1cf02"
                },
                {
                  "y": 263,
                  "x": 216,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92ee3fb3d1922992a2f6e4d93b59033511ef3db1"
                },
                {
                  "y": 390,
                  "x": 320,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=300e2152a2bd5d1540651f68670f0caed16c004c"
                },
                {
                  "y": 780,
                  "x": 640,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=971ae8d2b089a6c4d2729a733f501b03dc04a648"
                },
                {
                  "y": 1171,
                  "x": 960,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a146661a61e9a38380b2be4a25658ceda38e580"
                },
                {
                  "y": 1317,
                  "x": 1080,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb15168f0b557c684059fb4d5262e3bb5de2fc56"
                }
              ],
              "s": {
                "y": 2367,
                "x": 1940,
                "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=1940&amp;format=pjpg&amp;auto=webp&amp;s=caa08ce1445257b1a6bd05e754731bb29f7bb6f7"
              },
              "id": "o9ysd22gswcf1"
            }
          },
          "name": "t3_1lzzcje",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 31,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "o9ysd22gswcf1",
                "id": 706015027
              },
              {
                "media_id": "wgsfm22gswcf1",
                "id": 706015028
              },
              {
                "media_id": "cepva22gswcf1",
                "id": 706015029
              },
              {
                "media_id": "jy1gl22gswcf1",
                "id": 706015030
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/irH_IxYHAQEjBGgXU_AOln_EsxdXVQKIs5z4JjXsVfc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752528984,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MMLU-ProX is a multilingual benchmark that extends the challenging MMLU-Pro benchmark to 29 typologically diverse languages, designed to evaluate the cross-lingual reasoning capabilities of large language models (LLMs). Built through a rigorous four-stage translation pipeline using state-of-the-art LLMs (primarily Claude Sonnet 3.7) combined with expert verification, the benchmark contains 11,829 identical questions per language (with a lite version of 658 questions), covering 57 subjects across multiple disciplines with complex reasoning-focused multiple-choice questions featuring 10 answer options and chain-of-thought prompting support.&lt;/p&gt;\n\n&lt;p&gt;The benchmark reveals significant performance disparities across languages when evaluating 36 state-of-the-art LLMs, with models achieving strong performance on high-resource Western European languages (often 75%+ accuracy) but substantially lower scores on low-resource African languages like Wolof (as low as 0.6% to 58.6%), highlighting persistent challenges in multilingual AI development and the need for more inclusive language model capabilities across global contexts.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Website: &lt;a href=\"https://mmluprox.github.io\"&gt;https://mmluprox.github.io&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2503.10497\"&gt;https://arxiv.org/abs/2503.10497&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Code: &lt;a href=\"https://github.com/weihao1115/MMLU-ProX\"&gt;https://github.com/weihao1115/MMLU-ProX&lt;/a&gt; (still empty)&lt;/li&gt;\n&lt;li&gt;Full dataset: &lt;a href=\"https://huggingface.co/datasets/li-lab/MMLU-ProX\"&gt;https://huggingface.co/datasets/li-lab/MMLU-ProX&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Lite dataset: &lt;a href=\"https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite\"&gt;https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lzzcje",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzzcje",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzzcje/mmluprox_a_multilingual_benchmark_for_advanced/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lzzcje",
          "subreddit_subscribers": 499292,
          "created_utc": 1752528984,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We are alpha test the google play version of MNN Chat. looking for feedback from users like you.\n\n1. First, join our Google Group:[MNN Chat Testers](https://groups.google.com/g/mnn-chat/)\n2. Then, download the app from the Play Store:[Get MNN Chat](https://play.google.com/store/apps/details?id=com.alibaba.mnnllm.android.release) or visit [WebPage](https://play.google.com/apps/testing/com.alibaba.mnnllm.android.release)\n\n* **Voice Chat:** Talk directly to any AI model.\n* **Pinned Models:** Keep your favorite models just a tap away.\n* **Model Filtering:** Easily sort and find models by size.\n* **Benchmark Tool:** Test how fast different models run on your device.\n\nhttps://preview.redd.it/wp5gds35oycf1.png?width=590&amp;format=png&amp;auto=webp&amp;s=cb21b76380710648074362859ec2c41db88085dd\n\nhttps://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;format=png&amp;auto=webp&amp;s=76f6d832e805320f0addd85f1d641935d7464283\n\n",
          "author_fullname": "t2_orkom",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Test MNN Chat for Android",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ekqnqsrinycf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 37,
                  "x": 108,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5808c6441faf80b0a2dca376f4c9c3bc4ac7ce0"
                },
                {
                  "y": 74,
                  "x": 216,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cf8fd43257910d7a8e704e58c321fa5d23d9cee"
                },
                {
                  "y": 109,
                  "x": 320,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29a9de9eb6e5eff149b59857350d20e00d7d3dff"
                },
                {
                  "y": 219,
                  "x": 640,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f03e5681654412a3cf90aa40b89d371dc9df7a7b"
                },
                {
                  "y": 329,
                  "x": 960,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c146be4baef8deabb38e2e868805c105ea991478"
                },
                {
                  "y": 371,
                  "x": 1080,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42261c01f751144b2aac3a19985fae1ca626f91c"
                }
              ],
              "s": {
                "y": 578,
                "x": 1682,
                "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;format=png&amp;auto=webp&amp;s=76f6d832e805320f0addd85f1d641935d7464283"
              },
              "id": "ekqnqsrinycf1"
            },
            "wp5gds35oycf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/wp5gds35oycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=003b5b7c2e79bfe0937fd0872a8094fdeb0fbf50"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/wp5gds35oycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55aefcd6b48b7b5f2391bfe20712e2f56369287f"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/wp5gds35oycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c855b3b551676edf81e1ac604083766c96c95aa0"
                }
              ],
              "s": {
                "y": 1244,
                "x": 590,
                "u": "https://preview.redd.it/wp5gds35oycf1.png?width=590&amp;format=png&amp;auto=webp&amp;s=cb21b76380710648074362859ec2c41db88085dd"
              },
              "id": "wp5gds35oycf1"
            }
          },
          "name": "t3_1m081hm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/6GJVWwujc9y0RxTiQQge_lZ6RODHcNochQCqPhXQNwQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752552550,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are alpha test the google play version of MNN Chat. looking for feedback from users like you.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;First, join our Google Group:&lt;a href=\"https://groups.google.com/g/mnn-chat/\"&gt;MNN Chat Testers&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Then, download the app from the Play Store:&lt;a href=\"https://play.google.com/store/apps/details?id=com.alibaba.mnnllm.android.release\"&gt;Get MNN Chat&lt;/a&gt; or visit &lt;a href=\"https://play.google.com/apps/testing/com.alibaba.mnnllm.android.release\"&gt;WebPage&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Voice Chat:&lt;/strong&gt; Talk directly to any AI model.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Pinned Models:&lt;/strong&gt; Keep your favorite models just a tap away.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model Filtering:&lt;/strong&gt; Easily sort and find models by size.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Benchmark Tool:&lt;/strong&gt; Test how fast different models run on your device.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wp5gds35oycf1.png?width=590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb21b76380710648074362859ec2c41db88085dd\"&gt;https://preview.redd.it/wp5gds35oycf1.png?width=590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb21b76380710648074362859ec2c41db88085dd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76f6d832e805320f0addd85f1d641935d7464283\"&gt;https://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76f6d832e805320f0addd85f1d641935d7464283&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m081hm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Juude89",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m081hm/test_mnn_chat_for_android/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m081hm/test_mnn_chat_for_android/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752552550,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Not affiliated with the project, this is my unbiased opinion.\n\nI wanted to learn more about LLM function calling, so I prototyped an RPG agent which keeps track of the game state. For example, when new character is introduced, agent calls add\\_character tool, which fleshes out the character by filling out a character model. Why post this here? Naturally, I want to see how far one can get with local models for this sort of thing.\n\nI tested other libraries before (LangChain, LlamaIndex, Haystack, ...), which are bloated, require a lot of boilerplate code and/or use hidden global state, are poorly designed, and poorly documented. Not so PydanticAI, which uses a lot of clever ideas to avoid the boilerplate, and the documentation is superb.\n\nMaking an agent that can keep track of characters in the story is as simple as this:\n\n```py\n    class Character(BaseModel):\n        \"\"\"Character model with stats and description.\"\"\"\n    \n        name: str\n        appearance: str = Field(description=\"Physical appearance and decorative clothing\")\n        personality: str = Field(description=\"Personality traits and behavior\")\n        money: int = Field(ge=0, description=\"Amount of money the character carries\")\n    \n        # skipping other attributes...\n    \n    agent = Agent(...)\n    \n    # dictionary of all characters in the story\n    npcs = {}\n    \n    # This automatically generates a tool signature that the LLM understands\n    u/agent.tool_plain \n    def add_character(\n        character: Character\n    ) -&gt; str:\n        \"\"\"\n        Add a new character to the story.\n    \n        Use this tool for every new named character in the story.\n        \"\"\"\n        if character.name in state_manager.state.npcs:\n            return f\"Character {character.name!r} already exists in the story.\"\n    \n        npcs[character.name] = character\n    \n        return f\"Added character {character.name!r} to the story.\"\n\nNote how you don't have to repeat all the Character attributes in the function call, which makes this super flexible. Need a new character attribute? Just add to the Character model in a single place.\n\nPydanticAI is the first of these libraries that is actually enjoyable to use.\n\nI use Mistral Small 3.2 in my tests and it doesn't work consistently - which is probably an issue with the model and not with PydanticAI -, but when it works, it feels like magic.",
          "author_fullname": "t2_16rvbe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PydanticAI is GOAT for building agents in Python",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cdle",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=36fab7412f33958aa709272a3b83c78d2d4379e3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568312,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ai.pydantic.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not affiliated with the project, this is my unbiased opinion.&lt;/p&gt;\n\n&lt;p&gt;I wanted to learn more about LLM function calling, so I prototyped an RPG agent which keeps track of the game state. For example, when new character is introduced, agent calls add_character tool, which fleshes out the character by filling out a character model. Why post this here? Naturally, I want to see how far one can get with local models for this sort of thing.&lt;/p&gt;\n\n&lt;p&gt;I tested other libraries before (LangChain, LlamaIndex, Haystack, ...), which are bloated, require a lot of boilerplate code and/or use hidden global state, are poorly designed, and poorly documented. Not so PydanticAI, which uses a lot of clever ideas to avoid the boilerplate, and the documentation is superb.&lt;/p&gt;\n\n&lt;p&gt;Making an agent that can keep track of characters in the story is as simple as this:&lt;/p&gt;\n\n&lt;p&gt;```py\n    class Character(BaseModel):\n        &amp;quot;&amp;quot;&amp;quot;Character model with stats and description.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    name: str\n    appearance: str = Field(description=&amp;quot;Physical appearance and decorative clothing&amp;quot;)\n    personality: str = Field(description=&amp;quot;Personality traits and behavior&amp;quot;)\n    money: int = Field(ge=0, description=&amp;quot;Amount of money the character carries&amp;quot;)\n\n    # skipping other attributes...\n\nagent = Agent(...)\n\n# dictionary of all characters in the story\nnpcs = {}\n\n# This automatically generates a tool signature that the LLM understands\nu/agent.tool_plain \ndef add_character(\n    character: Character\n) -&amp;gt; str:\n    &amp;quot;&amp;quot;&amp;quot;\n    Add a new character to the story.\n\n    Use this tool for every new named character in the story.\n    &amp;quot;&amp;quot;&amp;quot;\n    if character.name in state_manager.state.npcs:\n        return f&amp;quot;Character {character.name!r} already exists in the story.&amp;quot;\n\n    npcs[character.name] = character\n\n    return f&amp;quot;Added character {character.name!r} to the story.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note how you don&amp;#39;t have to repeat all the Character attributes in the function call, which makes this super flexible. Need a new character attribute? Just add to the Character model in a single place.&lt;/p&gt;\n\n&lt;p&gt;PydanticAI is the first of these libraries that is actually enjoyable to use.&lt;/p&gt;\n\n&lt;p&gt;I use Mistral Small 3.2 in my tests and it doesn&amp;#39;t work consistently - which is probably an issue with the model and not with PydanticAI -, but when it works, it feels like magic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ai.pydantic.dev/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?auto=webp&amp;s=a643afbbcfdc774370799a0172b9534a973b608e",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88392b566033d09c2d33a1b015aa01b6b7de2b82",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2da0f9c45a0606bf95916ba2df6913a80fce6b10",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbac92d7f2a166324514d0d0add5543123d02b1c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e990c8a8d39910f731443529ec7ac5bc68d1a0d6",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=50c4a7490f3b24511d70273c65851066ef37280b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd230b4257ab61964bfe50e2814dafc7f3dce868",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0cdle",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-lq_pl-",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cdle/pydanticai_is_goat_for_building_agents_in_python/",
          "stickied": false,
          "url": "https://ai.pydantic.dev/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752568312,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys,\nI'm looking to build my \"first PC\" (not my first, but I currently only have a bad notebook), rn I'm stuck on deciding the GPU part. \nI'm a electronic engineer major and would like to have access to AI workload for a few projects (mostly Computer Vision and LLMs for tool control and human/machine interaction). \n\nI'm currently between 2 GPU's:\n\nRTX 5060 ti 16gb - R$3400.00($610.00)\n\nRTX 5070 12gb - R$4000.00($715.00)\n\nYes, GPUs are quite expensive in my country...\n\nSo considering I will use the PC for both gaming/game dev and AI workload, what would be the recommendation for GPU. Is it better to go with the 16gb version GPU or with Quantization the 40% improved performance on 5070 processing power is better?\n\nEdit: Text structure Formatting ",
          "author_fullname": "t2_4h864s27",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU for local LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0fp0r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752580244,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,\nI&amp;#39;m looking to build my &amp;quot;first PC&amp;quot; (not my first, but I currently only have a bad notebook), rn I&amp;#39;m stuck on deciding the GPU part. \nI&amp;#39;m a electronic engineer major and would like to have access to AI workload for a few projects (mostly Computer Vision and LLMs for tool control and human/machine interaction). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently between 2 GPU&amp;#39;s:&lt;/p&gt;\n\n&lt;p&gt;RTX 5060 ti 16gb - R$3400.00($610.00)&lt;/p&gt;\n\n&lt;p&gt;RTX 5070 12gb - R$4000.00($715.00)&lt;/p&gt;\n\n&lt;p&gt;Yes, GPUs are quite expensive in my country...&lt;/p&gt;\n\n&lt;p&gt;So considering I will use the PC for both gaming/game dev and AI workload, what would be the recommendation for GPU. Is it better to go with the 16gb version GPU or with Quantization the 40% improved performance on 5070 processing power is better?&lt;/p&gt;\n\n&lt;p&gt;Edit: Text structure Formatting &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0fp0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GabePs",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0fp0r/gpu_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0fp0r/gpu_for_local_llm/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752580244,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which size can my setup handle? I an going to use it to write and edit some fiction and this is the only task it should handle. I don't care much about the speed but context is important.   \nI am actually thinking about this model [https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF](https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF) But it's 21B and I am not sure if my system can handle it. ",
          "author_fullname": "t2_sxg7r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model size for RTX 3060 (12 Gb) + 32 Gb ram",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0bh4b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752564746,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which size can my setup handle? I an going to use it to write and edit some fiction and this is the only task it should handle. I don&amp;#39;t care much about the speed but context is important.&lt;br/&gt;\nI am actually thinking about this model &lt;a href=\"https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF\"&gt;https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF&lt;/a&gt; But it&amp;#39;s 21B and I am not sure if my system can handle it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?auto=webp&amp;s=0738453968f85f80d1bd2e6bb98271ea288389d8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1965b916d13774749f95f3dba8cd64a5d4c9a02f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f838f7dca36b2260024a859577caa7c427d15ef5",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7dd5297499cd966636f5a6fc8f6d6a4adad98af5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0a3e7f62ffcd2952f228280905a6cf1f0c1f6a64",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca36e22c01dc670331e6a544e9005b6e50790eab",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4fe51d035435909bb3a2ad85dbade5b55a2c976e",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0bh4b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ProHolmes",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0bh4b/model_size_for_rtx_3060_12_gb_32_gb_ram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0bh4b/model_size_for_rtx_3060_12_gb_32_gb_ram/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752564746,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, im working on something that I havent seen anyone else do before, I trained nanoGPT on only books from a specifc time period and region of the world. I chose to do 1800-1850 London. My dataset was only 187mb (around 50 books). Right now the trained model produces random incoherent sentences but they do kind of feel like 1800s style sentences. My end goal is to create an LLM that doesnt pretend to be historical but just is, that's why I didn't go the fine tune route. It will have no modern bias and will only be able to reason within the time period it's trained on. It's super random and has no utility but I think if I train using a big dataset (like 600 books) the result will be super sick.",
          "author_fullname": "t2_1ink6kzg93",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Training an LLM only on books from the 1800's - no modern bias",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzampg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 804,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 804,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=60dd42f39ab2b858d7a5af3cc6c9b33d8a7c0ec8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752459413,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im working on something that I havent seen anyone else do before, I trained nanoGPT on only books from a specifc time period and region of the world. I chose to do 1800-1850 London. My dataset was only 187mb (around 50 books). Right now the trained model produces random incoherent sentences but they do kind of feel like 1800s style sentences. My end goal is to create an LLM that doesnt pretend to be historical but just is, that&amp;#39;s why I didn&amp;#39;t go the fine tune route. It will have no modern bias and will only be able to reason within the time period it&amp;#39;s trained on. It&amp;#39;s super random and has no utility but I think if I train using a big dataset (like 600 books) the result will be super sick.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/haykgrigo3/TimeCapsuleLLM",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?auto=webp&amp;s=9ce4bc74d599d7799104a933627053a9a10d7d4b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d44946a1ed6e59d2f29fe66c42efbdf9beadf176",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2de1efb7c3a13836541de0a4e25083dafe4cb75",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=200239be7df394436a63a82488ec3fea1a1981d4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e60398c5e2e84881134a46e0acf601c56ba81942",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fe73c59013abcf68f28e8e3ab9df30778338705",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15f1bb851a628752877d4bae276313f481c7dcf5",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lzampg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Trick-177",
          "discussion_type": null,
          "num_comments": 198,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/",
          "stickied": false,
          "url": "https://github.com/haykgrigo3/TimeCapsuleLLM",
          "subreddit_subscribers": 499292,
          "created_utc": 1752459413,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Amazon just released Kiro, which is alternative to Cursor/Windsurf, has tasker/planning mode and currently even free tier. I tried it and it looks promising. https://kiro.dev",
          "author_fullname": "t2_8dnu3hmd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kiro (Cursor alternative from Amazon)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0gdfi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752582222,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Amazon just released Kiro, which is alternative to Cursor/Windsurf, has tasker/planning mode and currently even free tier. I tried it and it looks promising. &lt;a href=\"https://kiro.dev\"&gt;https://kiro.dev&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?auto=webp&amp;s=c4bde0e66eafaec6988a79d1dca2c4e4204984c5",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e68b40deeff8781623db1670fa4a7adface13f9",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=88690531c12f3760a6ed10fdcd66877d3c579e48",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=00a3a2b12606726590f5f3ebe97fee5e354259a9",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6eb53e896599ea0bb43ff049b5322f60068ba6ca",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=25da0e9cc8bd883bf49e641059b9044b4f2febe0",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=365a747787f1b62d80e7a0c9172351d4a81c715f",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0gdfi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AleksHop",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0gdfi/kiro_cursor_alternative_from_amazon/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0gdfi/kiro_cursor_alternative_from_amazon/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752582222,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, as the title says: is it possible to have real-time voice-to-voice interaction running locally, or are we still not there yet?  \nI'd like to improve my speaking skills (including pronunciation) in English and Japanese, and I thought it would be great to have conversations with a local LLM.  \nIt would also be nice to have something similar in Italian (my native language) for daily chats, but I assume it's not a very \"popular\" language to train on. lol\n\n  \n",
          "author_fullname": "t2_dlu9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is real-time voice-to-voice still science fiction?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzts1z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752516472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, as the title says: is it possible to have real-time voice-to-voice interaction running locally, or are we still not there yet?&lt;br/&gt;\nI&amp;#39;d like to improve my speaking skills (including pronunciation) in English and Japanese, and I thought it would be great to have conversations with a local LLM.&lt;br/&gt;\nIt would also be nice to have something similar in Italian (my native language) for daily chats, but I assume it&amp;#39;s not a very &amp;quot;popular&amp;quot; language to train on. lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzts1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "junior600",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752516472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm currently using llama\\_cpp with python bindings, but have heard that vLLM can be much faster, especially when patching.\n\nBut I'm not sure how to migrate my workflow that uses a Qwen3 gguf over to vLLM",
          "author_fullname": "t2_bndbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does vLLM not support Qwen3 ggufs? What sort of models/quants are people running in vLLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m03sio",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752540242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently using llama_cpp with python bindings, but have heard that vLLM can be much faster, especially when patching.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not sure how to migrate my workflow that uses a Qwen3 gguf over to vLLM&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m03sio",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AuspiciousApple",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752540242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/davidkimai/Context-Engineering](https://github.com/davidkimai/Context-Engineering)",
          "author_fullname": "t2_1jhxe6m6wu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A practical handbook on Context Engineering with the latest research from IBM Zurich, ICML, Princeton, and more.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzql0b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752509454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/davidkimai/Context-Engineering\"&gt;https://github.com/davidkimai/Context-Engineering&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?auto=webp&amp;s=f8680ad6bdadc6cdd4914fae9065ec6b47805ad6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f42f9a68c36f1e2ccc27fbc08f8a4e8edb8f1a70",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ba461a7633a2a5b765faf3ba8b9b9e94db25221",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6ab5733b0e448457e4ddfdd280b0486f09e8619",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3117ff6ef79e94d1a37e04e2704182174aaf1001",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0123ba313a39132b0d408b10e6733c20a1e07ec6",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af3ec97c9e4656162e6fd49925a896f6596e3856",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lzql0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "recursiveauto",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752509454,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Testing method** \n\n* For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.\n* If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.\n* Only one question couldn't be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.\n* Note that quantizations are not same. It's just me, trying to find the best reasoning &amp; coding model for my setup. \n\n**Coloring strategy:**\n\n* Mark the solution green if it's accepted.\n* Use red if it fails in the pre-test cases.\n* Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.\n* Use orange if it fails in the test cases but still manages to pass over 90%.\n\n**A few observations:**\n\n* Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didnt treat them as failures, since they were limited to single character issues that clearly qualify as typos.\n* Hunyuan fell short of my expectations.\n* Qwen-32B and OpenCodeReasoning model both performed better than expected.\n* The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.\n\n**Hardware: 2x H100**\n\n**Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)**\n\nFeel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.\n\n**Keep in mind that strong performance on LeetCode doesn't automatically reflect real world coding skills**, since everyday programming tasks faced by typical users are usually far less complex.\n\nAll questions are recent, with no data leakage involved. So dont come back saying LeetCode problems are easy for models, this test isnt meaningful. It's just your test questions have been seen by the model before.\n\n",
          "author_fullname": "t2_slwqrxz3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzhns3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 130,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 130,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WppDZCrZ0xycGtKVlAsBndunRlo8Km7IfOuHDADfvik.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752484340,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Testing method&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.&lt;/li&gt;\n&lt;li&gt;If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.&lt;/li&gt;\n&lt;li&gt;Only one question couldn&amp;#39;t be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.&lt;/li&gt;\n&lt;li&gt;Note that quantizations are not same. It&amp;#39;s just me, trying to find the best reasoning &amp;amp; coding model for my setup. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Coloring strategy:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Mark the solution green if it&amp;#39;s accepted.&lt;/li&gt;\n&lt;li&gt;Use red if it fails in the pre-test cases.&lt;/li&gt;\n&lt;li&gt;Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.&lt;/li&gt;\n&lt;li&gt;Use orange if it fails in the test cases but still manages to pass over 90%.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;A few observations:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didnt treat them as failures, since they were limited to single character issues that clearly qualify as typos.&lt;/li&gt;\n&lt;li&gt;Hunyuan fell short of my expectations.&lt;/li&gt;\n&lt;li&gt;Qwen-32B and OpenCodeReasoning model both performed better than expected.&lt;/li&gt;\n&lt;li&gt;The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware: 2x H100&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Keep in mind that strong performance on LeetCode doesn&amp;#39;t automatically reflect real world coding skills&lt;/strong&gt;, since everyday programming tasks faced by typical users are usually far less complex.&lt;/p&gt;\n\n&lt;p&gt;All questions are recent, with no data leakage involved. So dont come back saying LeetCode problems are easy for models, this test isnt meaningful. It&amp;#39;s just your test questions have been seen by the model before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nyu5vpzx2tcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?auto=webp&amp;s=e45702995060cc687a645562d2df2d39d92ccdf8",
                  "width": 1565,
                  "height": 408
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ba57d01c877fa0d1bec3f4aef3af9baaad55463",
                    "width": 108,
                    "height": 28
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=baf11fe9849c7a2c0833889c03bd68754c8b4e45",
                    "width": 216,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d5e29a1cd3ba5283fb529c5d5457c3cce7a36a0",
                    "width": 320,
                    "height": 83
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f48f8bee49ad1c403134b86ad6d3fc3d3c55b4",
                    "width": 640,
                    "height": 166
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=44e9301ed958519990d12a001176c79624b7a9fe",
                    "width": 960,
                    "height": 250
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7efed2bc4442573f344c5203ab1b05379a7429be",
                    "width": 1080,
                    "height": 281
                  }
                ],
                "variants": {},
                "id": "Yt8sdbd4WSl3QWw399ju3ntGhqCOHF8RdVFnkafe5Hs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzhns3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kyazoglu",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/",
          "stickied": false,
          "url": "https://i.redd.it/nyu5vpzx2tcf1.png",
          "subreddit_subscribers": 499292,
          "created_utc": 1752484340,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We're started a **Startup Catalyst Program** at **Future AGI** for early-stage AI teams working on things like LLM apps, agents, or RAG systems - basically anyone whos hit the wall when it comes to evals, observability, or reliability in production.\n\n\n\nThis program is built for **high-velocity AI startups** looking to:\n\n* Rapidly **iterate and deploy** reliable AI products with confidence\n* **Validate performance and user trust** at every stage of development  \n* **Save Engineering bandwidth** to focus more on product development instead of debugging\n\nThe program includes:\n\n* **$5k in credits** for our evaluation &amp; observability platform  \n* Access to **Pro tools** for model output tracking, eval workflows, and reliability benchmarking  \n* **Hands-on support** to help teams integrate fast  \n* Some of our internal, fine-tuned models for evals + analysis\n\nIt's free for selected teams - mostly aimed at startups moving fast and building real products. If it sounds relevant for your stack (or someone you know), heres the link: [https://futureagi.com/startups](https://futureagi.com/startups)\n\n",
          "author_fullname": "t2_1lmm8dujd9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Announcing the launch of the Startup Catalyst Program for early-stage AI teams.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0fboi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752579074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re started a &lt;strong&gt;Startup Catalyst Program&lt;/strong&gt; at &lt;strong&gt;Future AGI&lt;/strong&gt; for early-stage AI teams working on things like LLM apps, agents, or RAG systems - basically anyone whos hit the wall when it comes to evals, observability, or reliability in production.&lt;/p&gt;\n\n&lt;p&gt;This program is built for &lt;strong&gt;high-velocity AI startups&lt;/strong&gt; looking to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Rapidly &lt;strong&gt;iterate and deploy&lt;/strong&gt; reliable AI products with confidence&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Validate performance and user trust&lt;/strong&gt; at every stage of development&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Save Engineering bandwidth&lt;/strong&gt; to focus more on product development instead of debugging&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The program includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;$5k in credits&lt;/strong&gt; for our evaluation &amp;amp; observability platform&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Access to &lt;strong&gt;Pro tools&lt;/strong&gt; for model output tracking, eval workflows, and reliability benchmarking&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hands-on support&lt;/strong&gt; to help teams integrate fast&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Some of our internal, fine-tuned models for evals + analysis&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s free for selected teams - mostly aimed at startups moving fast and building real products. If it sounds relevant for your stack (or someone you know), heres the link: &lt;a href=\"https://futureagi.com/startups\"&gt;https://futureagi.com/startups&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0fboi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bubbless__16",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0fboi/announcing_the_launch_of_the_startup_catalyst/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0fboi/announcing_the_launch_of_the_startup_catalyst/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752579074,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've tried several LLM frameworks and libraries, each with their own direction like Haystack, LangChain, etc. I've also tried several agent frameworks like AutoGen, SmolAgent, and Strands. All I can say about these frameworks is that they're \"exhausting.\"\n\nI feel like every application built with these tools consumes twice my time. I have to go back and forth reviewing documentation and maybe other people's examples just to implement some simple control flow.\n\nWith just the OpenAI SDK (or just API calls), you can connect to almost any model that supports the OpenAI API spec, and everything is just structured output. You treat the LLM just like a function that reliably returns predefined values you can expect. I love building AI applications this way - it's so lean and easy, and you get full visibility on how each API call went.",
          "author_fullname": "t2_c5n1x183x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I ditch all LLM framework and use only OpenAI SDK for everything, I start loving building AI application this way.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzocuk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752504444,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried several LLM frameworks and libraries, each with their own direction like Haystack, LangChain, etc. I&amp;#39;ve also tried several agent frameworks like AutoGen, SmolAgent, and Strands. All I can say about these frameworks is that they&amp;#39;re &amp;quot;exhausting.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I feel like every application built with these tools consumes twice my time. I have to go back and forth reviewing documentation and maybe other people&amp;#39;s examples just to implement some simple control flow.&lt;/p&gt;\n\n&lt;p&gt;With just the OpenAI SDK (or just API calls), you can connect to almost any model that supports the OpenAI API spec, and everything is just structured output. You treat the LLM just like a function that reliably returns predefined values you can expect. I love building AI applications this way - it&amp;#39;s so lean and easy, and you get full visibility on how each API call went.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzocuk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dheetoo",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752504444,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "as far as i could understand, i need to add the mcp code to the edit mcp json in lm studio with my api to get it working but for some reason only the example mcp on lmstudio website (the huggingface mcp) works and nothing. \nI was looking to set up a jan 128k model with a serper mcp \nwould appreciate your thoughts on this",
          "author_fullname": "t2_4hdb6pqe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help with mcp setup in LM studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0ec9o",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752575803,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as far as i could understand, i need to add the mcp code to the edit mcp json in lm studio with my api to get it working but for some reason only the example mcp on lmstudio website (the huggingface mcp) works and nothing. \nI was looking to set up a jan 128k model with a serper mcp \nwould appreciate your thoughts on this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0ec9o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "heythereali",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0ec9o/need_help_with_mcp_setup_in_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0ec9o/need_help_with_mcp_setup_in_lm_studio/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752575803,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recorded a userflow for my vibecoding pet project - character selection, model setup, inline replies, and image generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzsoqc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1440,
              "scrubber_media_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/bx3hl3q5kvcf1/DASHPlaylist.mpd?a=1755175134%2CZTg4MzA3N2IzZGVkYjFkZmJlMjM2OGE0YWIxZWNkODVjZjcyMTYyMGI4MmE3ZjRmYmIwOTEyYzJhNDBjYzAyNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 38,
              "hls_url": "https://v.redd.it/bx3hl3q5kvcf1/HLSPlaylist.m3u8?a=1755175134%2CYTUwNzQ4NDAyMWI3YzIzNzg1MDFkYzRkZThmY2RiOTkwMjcyM2M3MTA0ZjlhMjk0NDUzM2NjMTMwMDhjNjBkMA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=140&amp;height=105&amp;crop=140:105,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0a77c427ad0f08f0061b348e704987804dc1321b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752514110,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/bx3hl3q5kvcf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?format=pjpg&amp;auto=webp&amp;s=6c9ab8578bf929eca5120be597d6df7b0ba43974",
                  "width": 1440,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d229202bb26b8decf4001cbe9fd8bb4d45adfea3",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b0fba2d38497e9a79d0dd3c6845939189cd1e736",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9be16d367724d3c86a3e7b5f317cd6d9e18331ae",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1432a2385579783d25465bd811ca11a3da2d989e",
                    "width": 640,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c9a248e58486fa1c9b75dec8df19d61240ef98fc",
                    "width": 960,
                    "height": 720
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d2f5de10741475c90aa05dd0031d6965662c06af",
                    "width": 1080,
                    "height": 810
                  }
                ],
                "variants": {},
                "id": "cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lzsoqc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzsoqc/recorded_a_userflow_for_my_vibecoding_pet_project/",
          "stickied": false,
          "url": "https://v.redd.it/bx3hl3q5kvcf1",
          "subreddit_subscribers": 499292,
          "created_utc": 1752514110,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1440,
              "scrubber_media_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/bx3hl3q5kvcf1/DASHPlaylist.mpd?a=1755175134%2CZTg4MzA3N2IzZGVkYjFkZmJlMjM2OGE0YWIxZWNkODVjZjcyMTYyMGI4MmE3ZjRmYmIwOTEyYzJhNDBjYzAyNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 38,
              "hls_url": "https://v.redd.it/bx3hl3q5kvcf1/HLSPlaylist.m3u8?a=1755175134%2CYTUwNzQ4NDAyMWI3YzIzNzg1MDFkYzRkZThmY2RiOTkwMjcyM2M3MTA0ZjlhMjk0NDUzM2NjMTMwMDhjNjBkMA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, i have an older Pioneer VSX-529 and it definitely doesn't do newer DTS or Dolby encoding, but i do use my desktop pc instead and also happen to have a pretty powerful RTX 4080s, question is do these upmixing in real time models exist, to convert stereo to surround noise from youtube, spotify, any media. I'm looking into Nugen, DTS Neural, NBU and Ambisonizer, but any help is appreciated from the wise.",
          "author_fullname": "t2_lq0ki4y2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any models that can upmix stereo into surround!!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m08bvp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752553497,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i have an older Pioneer VSX-529 and it definitely doesn&amp;#39;t do newer DTS or Dolby encoding, but i do use my desktop pc instead and also happen to have a pretty powerful RTX 4080s, question is do these upmixing in real time models exist, to convert stereo to surround noise from youtube, spotify, any media. I&amp;#39;m looking into Nugen, DTS Neural, NBU and Ambisonizer, but any help is appreciated from the wise.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m08bvp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Puzzleheaded_Soup847",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m08bvp/are_there_any_models_that_can_upmix_stereo_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m08bvp/are_there_any_models_that_can_upmix_stereo_into/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752553497,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,  \nI'm looking for a solid open-source coding agent that can run entirely with local models. I havent come across anything that really fits that need yet.\n\nI'm planning to build a lightweight CLI tool to handle everyday tasks like debugging, semantic search, and general code assistance.\n\nIf you know of any suitable small language models (SLMs) that could power something like this locallyideally something that runs efficiently on CPU or modest GPU setupsId really appreciate the recommendations.",
          "author_fullname": "t2_intoh3lv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SLM for local coding assistance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m09rbh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752558365,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI&amp;#39;m looking for a solid open-source coding agent that can run entirely with local models. I havent come across anything that really fits that need yet.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to build a lightweight CLI tool to handle everyday tasks like debugging, semantic search, and general code assistance.&lt;/p&gt;\n\n&lt;p&gt;If you know of any suitable small language models (SLMs) that could power something like this locallyideally something that runs efficiently on CPU or modest GPU setupsId really appreciate the recommendations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m09rbh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "callmedevilthebad",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752558365,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For anyone using Repomix, you can inject OCTAVE annotations. Results seem to show a 10.2x accuracy increase with just a 11.4 token overhead. Also eliminated some file hallucination. Universal scripts for any codebase.\n\nAlso works on research docs, summaries. Anything. Doesn't have to be codebase.\n\n* Benefits No Repomix Refactoring needed: Repomix itself is not modified Simple post-Processing Scripts: Just use the Python scripts that parse Repomix XML output and inject OCTAVE annotations File Pattern Recognition: Scripts will analyse file paths to automatically generate appropriate OCTAVE annotations  It basically adds comprehensive OCTAVE annotations to ALL TypeScript files in Repomix output.\n\nThis creates comprehensive enhancement with auto-generated annotations that are semantically deep.  \n\n\nBlind tested across gemini-2.5-pro, o3, and sonnet-4 - all showed consistent improvements but I'd welcome anyone to stress test this or push/advance this more.\n\n  \nCheck out [https://github.com/elevanaltd/octave/tree/main/repomix-integration](https://github.com/elevanaltd/octave/tree/main/repomix-integration)",
          "author_fullname": "t2_wfcudj1nx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OCTAVE addon to REPOMIX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0d47q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752571268,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For anyone using Repomix, you can inject OCTAVE annotations. Results seem to show a 10.2x accuracy increase with just a 11.4 token overhead. Also eliminated some file hallucination. Universal scripts for any codebase.&lt;/p&gt;\n\n&lt;p&gt;Also works on research docs, summaries. Anything. Doesn&amp;#39;t have to be codebase.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Benefits No Repomix Refactoring needed: Repomix itself is not modified Simple post-Processing Scripts: Just use the Python scripts that parse Repomix XML output and inject OCTAVE annotations File Pattern Recognition: Scripts will analyse file paths to automatically generate appropriate OCTAVE annotations  It basically adds comprehensive OCTAVE annotations to ALL TypeScript files in Repomix output.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This creates comprehensive enhancement with auto-generated annotations that are semantically deep.  &lt;/p&gt;\n\n&lt;p&gt;Blind tested across gemini-2.5-pro, o3, and sonnet-4 - all showed consistent improvements but I&amp;#39;d welcome anyone to stress test this or push/advance this more.&lt;/p&gt;\n\n&lt;p&gt;Check out &lt;a href=\"https://github.com/elevanaltd/octave/tree/main/repomix-integration\"&gt;https://github.com/elevanaltd/octave/tree/main/repomix-integration&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0d47q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sbuswell",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0d47q/octave_addon_to_repomix/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0d47q/octave_addon_to_repomix/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752571268,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:\n\n|Model|dense layer#|MoE layer#|shared|active/routed|Shared|Active|Params|Active%|fp16 kv@128k|kv%|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|DeepSeek-MoE-16B|1|27|2|6/64|1.42B|2.83B|16.38B|17.28%|28GB|85.47%|\n|DeepSeek-V2-Lite|1|26|2|6/64|1.31B|2.66B|15.71B|16.93%|3.8GB|12.09%|\n|DeepSeek-V2|1|59|2|6/160|12.98B|21.33B|235.74B|8.41%|8.44GB|1.78%|\n|DeepSeek-V3|3|58|1|8/256|17.01B|37.45B|671.03B|5.58%|8.578GB|0.64%|\n|Kimi-K2|1|60|1|8/384|11.56B|32.70B|1026.41B|3.19%|8.578GB|0.42%|\n|Qwen3-30B-A3B|0|48|0|8/128|1.53B|3.34B|30.53B|10.94%|12GB|19.65%|\n|Qwen3-235B-A22B|0|94|0|8/128|7.95B|22.14B|235.09B|9.42%|23.5GB|4.998%|\n|Llama-4-Scout-17B-16E|0|48|1|1/16|11.13B|17.17B|107.77B|15.93%|24GB|11.13%|\n|Llama-4-Maverick-17B-128E|24|24|1|1/128|14.15B|17.17B|400.71B|4.28%|24GB|2.99%|\n|Mixtral-8x7B|0|32|0|2/8|1.60B|12.88B|46.70B|27.58%|24GB|25.696%|\n|Mixtral-8x22B|0|56|0|2/8|5.33B|39.15B|140.62B|27.84%|28GB|9.956%|\n\nLooks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. \n\nModels using their own architecture is Kimi-VL and Kimi-Audio. \n\nEdited: Per u/Aaaaaaaaaeeeee 's request. I added a column called \"Shared\" which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.",
          "author_fullname": "t2_s6sfw4yy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi-K2 is a DeepSeek V3 with more experts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzcuom",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 216,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 216,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752495937,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752466353,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;dense layer#&lt;/th&gt;\n&lt;th align=\"left\"&gt;MoE layer#&lt;/th&gt;\n&lt;th align=\"left\"&gt;shared&lt;/th&gt;\n&lt;th align=\"left\"&gt;active/routed&lt;/th&gt;\n&lt;th align=\"left\"&gt;Shared&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active&lt;/th&gt;\n&lt;th align=\"left\"&gt;Params&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active%&lt;/th&gt;\n&lt;th align=\"left\"&gt;fp16 kv@128k&lt;/th&gt;\n&lt;th align=\"left\"&gt;kv%&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-MoE-16B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;27&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/64&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.42B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.83B&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.38B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.28%&lt;/td&gt;\n&lt;td align=\"left\"&gt;28GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;85.47%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V2-Lite&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;26&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/64&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.31B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.66B&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.71B&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.93%&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.8GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.09%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;59&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/160&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.98B&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.33B&lt;/td&gt;\n&lt;td align=\"left\"&gt;235.74B&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.41%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.44GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.78%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;58&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/256&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.01B&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.45B&lt;/td&gt;\n&lt;td align=\"left\"&gt;671.03B&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.58%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.578GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.64%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi-K2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/384&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.56B&lt;/td&gt;\n&lt;td align=\"left\"&gt;32.70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1026.41B&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.19%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.578GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.42%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;48&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.53B&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.34B&lt;/td&gt;\n&lt;td align=\"left\"&gt;30.53B&lt;/td&gt;\n&lt;td align=\"left\"&gt;10.94%&lt;/td&gt;\n&lt;td align=\"left\"&gt;12GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.65%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;94&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.95B&lt;/td&gt;\n&lt;td align=\"left\"&gt;22.14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;235.09B&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.42%&lt;/td&gt;\n&lt;td align=\"left\"&gt;23.5GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.998%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-4-Scout-17B-16E&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;48&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/16&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.13B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;107.77B&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.93%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.13%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.15B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;400.71B&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.28%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.99%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mixtral-8x7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;32&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/8&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.60B&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.88B&lt;/td&gt;\n&lt;td align=\"left\"&gt;46.70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;27.58%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;25.696%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mixtral-8x22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;56&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/8&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.33B&lt;/td&gt;\n&lt;td align=\"left\"&gt;39.15B&lt;/td&gt;\n&lt;td align=\"left\"&gt;140.62B&lt;/td&gt;\n&lt;td align=\"left\"&gt;27.84%&lt;/td&gt;\n&lt;td align=\"left\"&gt;28GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.956%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Looks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. &lt;/p&gt;\n\n&lt;p&gt;Models using their own architecture is Kimi-VL and Kimi-Audio. &lt;/p&gt;\n\n&lt;p&gt;Edited: Per &lt;a href=\"/u/Aaaaaaaaaeeeee\"&gt;u/Aaaaaaaaaeeeee&lt;/a&gt; &amp;#39;s request. I added a column called &amp;quot;Shared&amp;quot; which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzcuom",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Warning2146",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752466353,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanna compare their vocabs, but Llama has gated models on HF:( ",
          "author_fullname": "t2_r7zf8j6x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do DeepseekR1-distilled-Llama-8B has the same tokenizer and tokens vocab as Llama3-1B or 2B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cja9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752568968,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna compare their vocabs, but Llama has gated models on HF:( &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0cja9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "krolzzz",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cja9/do_deepseekr1distilledllama8b_has_the_same/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0cja9/do_deepseekr1distilledllama8b_has_the_same/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752568968,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was browsing the llama.cpp PRs and saw that Am17an has added diffusion model support in llama.cpp. It works. It's very cool to watch it do it's thing. Make sure to use the --diffusion-visual flag. It's still a PR but has been approved so it should be merged soon.",
          "author_fullname": "t2_o65i6kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Diffusion model support in llama.cpp.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lze1r3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 141,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 141,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=77e8aaca890b1dc8486701afa4da4f4e06d486be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752470404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was browsing the llama.cpp PRs and saw that Am17an has added diffusion model support in llama.cpp. It works. It&amp;#39;s very cool to watch it do it&amp;#39;s thing. Make sure to use the --diffusion-visual flag. It&amp;#39;s still a PR but has been approved so it should be merged soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14644",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?auto=webp&amp;s=e183ff7e541a319425a36dcaf9b80b74c4ff9243",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f661fc69800730ffa673f5aa97b47d5b9e191899",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4c5410f2c6927b2130b0b8edfce13fc0ce8cd59",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aab06d8dbc7317d7ae49105599d33bc04e0c66cf",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c75c6786f093153f6a5dc5065d5f9e2b741b5086",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a1002549b1b328880a986274b59212fd91c0e1f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01b756f299543d2fda31db392dfcbc407ad0faa7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lze1r3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fallingdowndizzyvr",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14644",
          "subreddit_subscribers": 499292,
          "created_utc": 1752470404,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AIs moving fast with open-source models like Kimi K2 Instruct are starting to rival expensive ones like Claude Opus. Yeah, Claudes still sharper in spots, but honestly? Kimis catching up quick.\n\nIn a few months, well probably have local models that can do 90% of what these $$$ models do for free. No API keys, no paywalls, just download and run.\n\nThe gap is closing fast.",
          "author_fullname": "t2_9qhdfp2h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source vs expansive models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0b73m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752563674,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AIs moving fast with open-source models like Kimi K2 Instruct are starting to rival expensive ones like Claude Opus. Yeah, Claudes still sharper in spots, but honestly? Kimis catching up quick.&lt;/p&gt;\n\n&lt;p&gt;In a few months, well probably have local models that can do 90% of what these $$$ models do for free. No API keys, no paywalls, just download and run.&lt;/p&gt;\n\n&lt;p&gt;The gap is closing fast.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0b73m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ill_Occasion_1537",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0b73m/open_source_vs_expansive_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0b73m/open_source_vs_expansive_models/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752563674,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's a simple experimental language model architecture based on Andrej Karpathy's nanoGPT project.\n\nIt's an experiment to try different improvements of transformers architecture. Some improvement has been brought about by the following techniques:\n- Modernized architecture: Rotary embeddings, QK-Norm, and ReLU\n- Untie head from embedding\n- SwiGLU in feed forward network.\n- Parallel layers proposed by Google's PaLM\n- Using a novel attention mechanism which I call `Attention On Detail`.\n\nAs well as many minor optimizations.\n\n## How does `Attention On Detail` works?\nIt works by combining 3 ideas.\n- Multi-Headed Causal Self-Attention (MHA)\n- Attention Free Transformer (AFT)\n- A simple fourier series based equation `a*sin(x) + b*sin(x) + c*sin(x)*cos(x)` where `x` is normalized between `[-pi, pi]`\n\nThe idea is simple.\n- Replace `Linear layers` with an `AFT` for each `q`, `k` &amp; `v` in the `MHA`.\n- In `AFT`, generate 3 values, `a`, `b` and `c` from 3 different fourier series equations.\n- Compute output the `a`, `b` &amp; `c` values in each `AFT`.\n- Now use those `q`, `k` &amp; `v` values to calculate the attention score in the `MHA`\n",
          "author_fullname": "t2_115sfrlqgx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GitHub - SrijanSriv211/Palm: Palm is a tree, not a language model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzyk1k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=327ae1770b56b5ecfbf46685072a5ee3f577b985",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752527141,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a simple experimental language model architecture based on Andrej Karpathy&amp;#39;s nanoGPT project.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s an experiment to try different improvements of transformers architecture. Some improvement has been brought about by the following techniques:\n- Modernized architecture: Rotary embeddings, QK-Norm, and ReLU\n- Untie head from embedding\n- SwiGLU in feed forward network.\n- Parallel layers proposed by Google&amp;#39;s PaLM\n- Using a novel attention mechanism which I call &lt;code&gt;Attention On Detail&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;As well as many minor optimizations.&lt;/p&gt;\n\n&lt;h2&gt;How does &lt;code&gt;Attention On Detail&lt;/code&gt; works?&lt;/h2&gt;\n\n&lt;p&gt;It works by combining 3 ideas.\n- Multi-Headed Causal Self-Attention (MHA)\n- Attention Free Transformer (AFT)\n- A simple fourier series based equation &lt;code&gt;a*sin(x) + b*sin(x) + c*sin(x)*cos(x)&lt;/code&gt; where &lt;code&gt;x&lt;/code&gt; is normalized between &lt;code&gt;[-pi, pi]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The idea is simple.\n- Replace &lt;code&gt;Linear layers&lt;/code&gt; with an &lt;code&gt;AFT&lt;/code&gt; for each &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt; &amp;amp; &lt;code&gt;v&lt;/code&gt; in the &lt;code&gt;MHA&lt;/code&gt;.\n- In &lt;code&gt;AFT&lt;/code&gt;, generate 3 values, &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; from 3 different fourier series equations.\n- Compute output the &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; &amp;amp; &lt;code&gt;c&lt;/code&gt; values in each &lt;code&gt;AFT&lt;/code&gt;.\n- Now use those &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt; &amp;amp; &lt;code&gt;v&lt;/code&gt; values to calculate the attention score in the &lt;code&gt;MHA&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/SrijanSriv211/Palm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?auto=webp&amp;s=2a210ec4950e580011f1e5d6a75d2f1a05d02bb0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef72931372700484d624279573ebd30522837f10",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a9002ac55ec8e03201c26828120fcf3beeced63",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=57ca03b3a2000e4b4cd65e76b9b11a1b3ec5fa4d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c77dfededd901b5351e7adc5dd2be75ee167f27c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=002d927a21bcb9cdd80b3935d3dadbaa4553bc19",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7aea228d082892928dab9cbefe79a4adbf0e568c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzyk1k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SrijSriv211",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzyk1k/github_srijansriv211palm_palm_is_a_tree_not_a/",
          "stickied": false,
          "url": "https://github.com/SrijanSriv211/Palm",
          "subreddit_subscribers": 499292,
          "created_utc": 1752527141,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Need advice. I'm ordering a new mac for work and was thinking about M4 Max 128GB to run the models locally for coding tasks. I'm going to run mlx llms with LM Studio. Which model would you recommend? ",
          "author_fullname": "t2_3eyym7wo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which model can I run comfortably on M4 Max 128GB with a long context window?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0apct",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752561851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need advice. I&amp;#39;m ordering a new mac for work and was thinking about M4 Max 128GB to run the models locally for coding tasks. I&amp;#39;m going to run mlx llms with LM Studio. Which model would you recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0apct",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pppreddit",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0apct/which_model_can_i_run_comfortably_on_m4_max_128gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0apct/which_model_can_i_run_comfortably_on_m4_max_128gb/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752561851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Its showing me: The current model has reached its conversation limit. Please switch to another model to continue.\n\n[IMAGE](https://i.imgur.com/JcHTdEv.jpeg)",
          "author_fullname": "t2_vgnr5u5gg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is requests limit for kimi k2 ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 19,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "z5zjgfpadzcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 14,
                  "x": 108,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76daef9aee38dcfd6a4233b3c5cdc1e620283021"
                },
                {
                  "y": 29,
                  "x": 216,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c08d222fd95cc40baca0b7bd328a267632bf16c9"
                },
                {
                  "y": 43,
                  "x": 320,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76b840dddd286cd6cadaf1f3b05caf41605ecad7"
                },
                {
                  "y": 86,
                  "x": 640,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4489548585deb1ddc47e3fcc286acdcee5e7ad96"
                },
                {
                  "y": 130,
                  "x": 960,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1557c4617b3130b71726362f45e09decb3433317"
                }
              ],
              "s": {
                "y": 133,
                "x": 980,
                "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=980&amp;format=pjpg&amp;auto=webp&amp;s=3d7555c9fa99fb2b6d209fca64f58a96736a8626"
              },
              "id": "z5zjgfpadzcf1"
            }
          },
          "name": "t3_1m0a9ni",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=140&amp;height=19&amp;crop=140:19,smart&amp;auto=webp&amp;s=9ee23177a4ba9ea4a5a5883d129a5b512c048644",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752560221,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its showing me: The current model has reached its conversation limit. Please switch to another model to continue.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/JcHTdEv.jpeg\"&gt;IMAGE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?auto=webp&amp;s=558c6be0399561095101e48bab193422b59e2297",
                  "width": 980,
                  "height": 133
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=79e6d3fd9990e35bbaddabff6e7eb72efd613089",
                    "width": 108,
                    "height": 14
                  },
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7128edc2d39194a78df81830b3167c35b1960f53",
                    "width": 216,
                    "height": 29
                  },
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad2e985e081858017ca584a29d53089293944d96",
                    "width": 320,
                    "height": 43
                  },
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=517057a2be5de0941ada66bf665174af02776f4e",
                    "width": 640,
                    "height": 86
                  },
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4d0defaeab31772bff76445a16841c59177d9b4",
                    "width": 960,
                    "height": 130
                  }
                ],
                "variants": {},
                "id": "AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0a9ni",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JeffreySons_90",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0a9ni/what_is_requests_limit_for_kimi_k2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0a9ni/what_is_requests_limit_for_kimi_k2/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752560221,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.\n\nStill, it seems pretty odd that they're missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.",
          "author_fullname": "t2_e9jh97s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama, Why No Reka Flash, SmolLM3, GLM-4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzsnna",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752514045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.&lt;/p&gt;\n\n&lt;p&gt;Still, it seems pretty odd that they&amp;#39;re missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzsnna",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chibop1",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752514045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Greetings! I'm a representative from the new school of software developers: a lot of us are actually middle aged and quite knowledgeable in tech, but very ADHD-addled. I've personally waited for these tools my whole life, without really knowing what I was waiting for. It felt like a *click* trying them -- and now I'm embracing them completely, trying to build a better life for my young family with this new career. My helpdesk job is good to me, but it just hasn't cutting it for us in this economy.\n\nSo I have been told not to use multiple code agents, and immediately understood why when I went ahead and tried anyway. It's basically the same problem that human agents will have working on a file together. I have tried to allow them to run simultaneously on different projects, like docs vs. codebase.\n\nBut yesterday I noticed that Copilot was able to see Claude Code working, on a terminal tab labelled Copilot. I got excited, and thought about a sort of shared terminal. . . . but there was no way to develop on that idea with a lot of automation. And I can't get distracted, this has to be a short side-quest for the benefit of my project Anthemic.\n\nA much more simple and elegant implementation: a few files for the agents to update with a script, and then an overview file that compiles the agent \"checkouts\" together. You can watch it live-update with jq in a terminal window. So that Claude Code, Cursor, Copilot and I can all check in on each other to avoid conflicts, and if conflicts occur we all know exactly why.\n\nI've never released a repo before, let me know what you think! I apologize if I've made any mistakes, and will fix it. If you want to contribute that's awesome, contact me or just do a pull request I suppose.\n\nI listen to all advise. I also would take money if you have plenty of it + enthusiasm for what I'm doing. My main project is going to be a conversational music platform. I've created a discord, website and subreddit for it but haven't advertised yet -- working on the demo.\n\n",
          "author_fullname": "t2_1sjqm6353y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multi-Code-Agent Orchestration VS Code Extension",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m09bzn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=5db9f277a33d9af28d0719d23371c7f127c5995d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752556848,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings! I&amp;#39;m a representative from the new school of software developers: a lot of us are actually middle aged and quite knowledgeable in tech, but very ADHD-addled. I&amp;#39;ve personally waited for these tools my whole life, without really knowing what I was waiting for. It felt like a &lt;em&gt;click&lt;/em&gt; trying them -- and now I&amp;#39;m embracing them completely, trying to build a better life for my young family with this new career. My helpdesk job is good to me, but it just hasn&amp;#39;t cutting it for us in this economy.&lt;/p&gt;\n\n&lt;p&gt;So I have been told not to use multiple code agents, and immediately understood why when I went ahead and tried anyway. It&amp;#39;s basically the same problem that human agents will have working on a file together. I have tried to allow them to run simultaneously on different projects, like docs vs. codebase.&lt;/p&gt;\n\n&lt;p&gt;But yesterday I noticed that Copilot was able to see Claude Code working, on a terminal tab labelled Copilot. I got excited, and thought about a sort of shared terminal. . . . but there was no way to develop on that idea with a lot of automation. And I can&amp;#39;t get distracted, this has to be a short side-quest for the benefit of my project Anthemic.&lt;/p&gt;\n\n&lt;p&gt;A much more simple and elegant implementation: a few files for the agents to update with a script, and then an overview file that compiles the agent &amp;quot;checkouts&amp;quot; together. You can watch it live-update with jq in a terminal window. So that Claude Code, Cursor, Copilot and I can all check in on each other to avoid conflicts, and if conflicts occur we all know exactly why.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never released a repo before, let me know what you think! I apologize if I&amp;#39;ve made any mistakes, and will fix it. If you want to contribute that&amp;#39;s awesome, contact me or just do a pull request I suppose.&lt;/p&gt;\n\n&lt;p&gt;I listen to all advise. I also would take money if you have plenty of it + enthusiasm for what I&amp;#39;m doing. My main project is going to be a conversational music platform. I&amp;#39;ve created a discord, website and subreddit for it but haven&amp;#39;t advertised yet -- working on the demo.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/jdbridgeman/multi-agent-orchestration-extension",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?auto=webp&amp;s=3976f56a2155b0584d6be32c81af8b0e802e43bc",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0f278668e1259d7873ba1d3fe1677bfef2e819c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=827f2d96a2c95ea5ba18720eee95d0a0943b3f40",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddfc1e1db0fc2169c350fa264c7e6fc0ef9b79a3",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1d87e19a31e6a99cb619bafa3d3d06c76ac9361",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=295b04ebf834e4c1f1e3f260d82f8dbbb26e5e93",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1e1ad4bc6313c6eaa006cc645dc6a57064d40f89",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m09bzn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Anthemic-AI",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m09bzn/multicodeagent_orchestration_vs_code_extension/",
          "stickied": false,
          "url": "https://github.com/jdbridgeman/multi-agent-orchestration-extension",
          "subreddit_subscribers": 499292,
          "created_utc": 1752556848,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Obviously this is a silly question. 4k context is limiting to the point where even dumber models are \"better\" for almost any pipeline and use case.\n\nBut for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG's)? What model sizes now beat Llama2-70B in:\n\n- instruction following\n\n- depth of knowledge \n\n- writing skill\n\n- coding \n\n- logic",
          "author_fullname": "t2_w2gxqd6i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If you limit context to 4k tokens, which models today beat Llama2-70B from 2 years ago?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzuaa3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.65,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752517599,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously this is a silly question. 4k context is limiting to the point where even dumber models are &amp;quot;better&amp;quot; for almost any pipeline and use case.&lt;/p&gt;\n\n&lt;p&gt;But for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG&amp;#39;s)? What model sizes now beat Llama2-70B in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;instruction following&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;depth of knowledge &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;writing skill&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;coding &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;logic&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzuaa3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EmPips",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752517599,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, here is the problem. I'm actually facing it as I'm writing this post.\n\nI use multiple LLM models (32b and 70b at Q4 or Q8, qwen, qwq, deepseek, llama, etc). I also use Open WebUI for prompting them. What I like the most is the ability to have a single prompt sent to multiple LLMs and get their outputs side by side. It's like asking multiple experts with various opinions before making a decision. \n\nI have a dual RTX 3090 setup (48gb vram total). Open Web UI is integrated with ollama and models are being loaded from local NVMe drive. I have posted photos of my setup some time ago. Nothing fancy, some older server/workstation grade build.\n\nThe problem is, the NVMe is just too slow. Because of limited amount of Vram, each model has to be run once at the time which means the whole model has to be reloaded from the NVMe to Vram again and again. I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?\n\nAny ideas anyone? Thanks.",
          "author_fullname": "t2_11ccns",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVMe for local LLM is too slow. Any ideas?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzx039",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752523597,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, here is the problem. I&amp;#39;m actually facing it as I&amp;#39;m writing this post.&lt;/p&gt;\n\n&lt;p&gt;I use multiple LLM models (32b and 70b at Q4 or Q8, qwen, qwq, deepseek, llama, etc). I also use Open WebUI for prompting them. What I like the most is the ability to have a single prompt sent to multiple LLMs and get their outputs side by side. It&amp;#39;s like asking multiple experts with various opinions before making a decision. &lt;/p&gt;\n\n&lt;p&gt;I have a dual RTX 3090 setup (48gb vram total). Open Web UI is integrated with ollama and models are being loaded from local NVMe drive. I have posted photos of my setup some time ago. Nothing fancy, some older server/workstation grade build.&lt;/p&gt;\n\n&lt;p&gt;The problem is, the NVMe is just too slow. Because of limited amount of Vram, each model has to be run once at the time which means the whole model has to be reloaded from the NVMe to Vram again and again. I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?&lt;/p&gt;\n\n&lt;p&gt;Any ideas anyone? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzx039",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChopSticksPlease",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752523597,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Using chatterbox locally and its limited to 300 characters :/\n\nIs there any way to increase the character limit?\n\nSomeone mentioned someone had created increased character limit in chatterbox: [https://github.com/RemmyLee/chattered/](https://github.com/RemmyLee/chattered/)  but I'm not if there is mailcious codes despite being open source... so didn't take risk.\n\nThen there is chatterbox extended [https://github.com/petermg/Chatterbox-TTS-Extended](https://github.com/petermg/Chatterbox-TTS-Extended) but not sure if it supports more than 300 characters.\n\nhow to increase beyond 300 chracters limit in the original?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to increase character limit in TTS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m086sk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752553032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using chatterbox locally and its limited to 300 characters :/&lt;/p&gt;\n\n&lt;p&gt;Is there any way to increase the character limit?&lt;/p&gt;\n\n&lt;p&gt;Someone mentioned someone had created increased character limit in chatterbox: &lt;a href=\"https://github.com/RemmyLee/chattered/\"&gt;https://github.com/RemmyLee/chattered/&lt;/a&gt;  but I&amp;#39;m not if there is mailcious codes despite being open source... so didn&amp;#39;t take risk.&lt;/p&gt;\n\n&lt;p&gt;Then there is chatterbox extended &lt;a href=\"https://github.com/petermg/Chatterbox-TTS-Extended\"&gt;https://github.com/petermg/Chatterbox-TTS-Extended&lt;/a&gt; but not sure if it supports more than 300 characters.&lt;/p&gt;\n\n&lt;p&gt;how to increase beyond 300 chracters limit in the original?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?auto=webp&amp;s=ebf77dd5925485d96efc97ec1bba7932984f21da",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d64855289ab354b03dc64faed3d37e46b6cc0721",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e6a9d10034fcebf4d08ecb073cc9db484adaf5c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=060d3980171e770ff803532487037c04e3efc7dc",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d51e557458e0efd84638096a04985ef6cad6b34",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6dcc9fc74b8cd9db256077f08e933f00bf27f9e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=389ac191705a119e80858c9e6e948cd4976c1764",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m086sk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m086sk/how_to_increase_character_limit_in_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m086sk/how_to_increase_character_limit_in_tts/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752553032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech\n\nhttps://arxiv.org/abs/2506.21619\n\nFeatures:\n\n- **Fully local with open weights.**\n- Zero-shot voice cloning. You just provide one audio file (in any language) and it will extremely accurately clone the voice style and rhythm. It sounds much more accurate than MaskGCT and F5-TTS, two of the other state-of-the-art local models.\n- Optional: Zero-shot emotion cloning by providing a second audio file that contains the emotional state to emulate. This affects things thing whispering, screaming, fear, desire, anger, etc. This is a world-first.\n- Optional: Text control of emotions, without needing a 2nd audio file. You can just write what emotions should be used.\n- Optional: Full control over how long the output will be, which makes it perfect for dubbing movies. This is a world-first. Alternatively you can run it in standard \"free length\" mode where it automatically lets the audio become as long as necessary.\n- Supported text to speech languages that it can output: English and Chinese. Like most models.\n\nHere's a few real-world use cases:\n\n- Take an Anime, clone the voice of the original character, clone the emotion of the original performance, and make them read the English script, and tell it how long the performance should last. You will now have the exact same voice and emotions reading the English translation with a good performance that's the perfect length for dubbing.\n- Take one voice sample, and make it say anything, with full text-based control of what emotions the speaker should perform.\n- Take two voice samples, one being the speaker voice and the other being the emotional performance, and then make it say anything with full text-based control.\n\n## So how did it leak?\n\n- They have been preparing a website at https://index-tts2.github.io/ which is not public yet, but their repo for the site is already public. Via that repo you can explore the presentation they've been preparing, along with demo files.\n- Here's an example demo file with dubbing from Chinese to English, showing how damn good this TTS model is at conveying emotions. The voice performance it gives is good enough that I could happily watch an entire movie or TV show dubbed with this AI model: https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4\n- The entire presentation page is here: https://index-tts.github.io/index-tts2.github.io/\n- To download all demos and watch the HTML presentation locally, you can also \"git clone https://github.com/index-tts/index-tts2.github.io.git\".\n\nI can't wait to play around with this. Absolutely crazy how realistic these AI voice emotions are! This is approaching actual *acting!* Bravo, Bilibili, the company behind this research!\n\nThey are planning to release it \"soon\", and considering the state of everything (paper came out on June 23rd, and the website is practically finished) I'd say it's coming this month or the next. Update: The public release will not be this month (they are still busy fine-tuning), but maybe next month.\n\nTheir previous model was Apache 2 license, both for the source code and the weights. Let's hope the next model is the same awesome license.\n\n## Update:\n\nThey contacted me and were surprised that I had already found their \"hidden\" paper and presentation. They haven't gone public yet. I hope I didn't cause them trouble by announcing the discovery too soon.\n\nThey're very happy that people are so excited about their new model, though! :) But they're still busy fine-tuning the model, and improving the tools and code for public release. So it will not release this month, but late next month is more likely.\n\nAnd if I understood correctly, it will be free and open for non-commercial use (same as their older models). They are considering whether to require a separate commercial license for commercial usage, which makes sense since this is state of the art and very useful for dubbing movies/anime. I fully respect that and think that anyone using software to make money should compensate the people who made the software. But nothing is decided yet.\n\nI am very excited for this new model and can't wait! :)",
          "author_fullname": "t2_4a13s1mr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "IndexTTS2, the most realistic and expressive text-to-speech model so far, has leaked their demos ahead of the official launch! And... wow!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyy39n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 594,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 594,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752532108,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752426670,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2506.21619\"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Fully local with open weights.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Zero-shot voice cloning. You just provide one audio file (in any language) and it will extremely accurately clone the voice style and rhythm. It sounds much more accurate than MaskGCT and F5-TTS, two of the other state-of-the-art local models.&lt;/li&gt;\n&lt;li&gt;Optional: Zero-shot emotion cloning by providing a second audio file that contains the emotional state to emulate. This affects things thing whispering, screaming, fear, desire, anger, etc. This is a world-first.&lt;/li&gt;\n&lt;li&gt;Optional: Text control of emotions, without needing a 2nd audio file. You can just write what emotions should be used.&lt;/li&gt;\n&lt;li&gt;Optional: Full control over how long the output will be, which makes it perfect for dubbing movies. This is a world-first. Alternatively you can run it in standard &amp;quot;free length&amp;quot; mode where it automatically lets the audio become as long as necessary.&lt;/li&gt;\n&lt;li&gt;Supported text to speech languages that it can output: English and Chinese. Like most models.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s a few real-world use cases:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Take an Anime, clone the voice of the original character, clone the emotion of the original performance, and make them read the English script, and tell it how long the performance should last. You will now have the exact same voice and emotions reading the English translation with a good performance that&amp;#39;s the perfect length for dubbing.&lt;/li&gt;\n&lt;li&gt;Take one voice sample, and make it say anything, with full text-based control of what emotions the speaker should perform.&lt;/li&gt;\n&lt;li&gt;Take two voice samples, one being the speaker voice and the other being the emotional performance, and then make it say anything with full text-based control.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;So how did it leak?&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;They have been preparing a website at &lt;a href=\"https://index-tts2.github.io/\"&gt;https://index-tts2.github.io/&lt;/a&gt; which is not public yet, but their repo for the site is already public. Via that repo you can explore the presentation they&amp;#39;ve been preparing, along with demo files.&lt;/li&gt;\n&lt;li&gt;Here&amp;#39;s an example demo file with dubbing from Chinese to English, showing how damn good this TTS model is at conveying emotions. The voice performance it gives is good enough that I could happily watch an entire movie or TV show dubbed with this AI model: &lt;a href=\"https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4\"&gt;https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;The entire presentation page is here: &lt;a href=\"https://index-tts.github.io/index-tts2.github.io/\"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;To download all demos and watch the HTML presentation locally, you can also &amp;quot;git clone &lt;a href=\"https://github.com/index-tts/index-tts2.github.io.git\"&gt;https://github.com/index-tts/index-tts2.github.io.git&lt;/a&gt;&amp;quot;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I can&amp;#39;t wait to play around with this. Absolutely crazy how realistic these AI voice emotions are! This is approaching actual &lt;em&gt;acting!&lt;/em&gt; Bravo, Bilibili, the company behind this research!&lt;/p&gt;\n\n&lt;p&gt;They are planning to release it &amp;quot;soon&amp;quot;, and considering the state of everything (paper came out on June 23rd, and the website is practically finished) I&amp;#39;d say it&amp;#39;s coming this month or the next. Update: The public release will not be this month (they are still busy fine-tuning), but maybe next month.&lt;/p&gt;\n\n&lt;p&gt;Their previous model was Apache 2 license, both for the source code and the weights. Let&amp;#39;s hope the next model is the same awesome license.&lt;/p&gt;\n\n&lt;h2&gt;Update:&lt;/h2&gt;\n\n&lt;p&gt;They contacted me and were surprised that I had already found their &amp;quot;hidden&amp;quot; paper and presentation. They haven&amp;#39;t gone public yet. I hope I didn&amp;#39;t cause them trouble by announcing the discovery too soon.&lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;re very happy that people are so excited about their new model, though! :) But they&amp;#39;re still busy fine-tuning the model, and improving the tools and code for public release. So it will not release this month, but late next month is more likely.&lt;/p&gt;\n\n&lt;p&gt;And if I understood correctly, it will be free and open for non-commercial use (same as their older models). They are considering whether to require a separate commercial license for commercial usage, which makes sense since this is state of the art and very useful for dubbing movies/anime. I fully respect that and think that anyone using software to make money should compensate the people who made the software. But nothing is decided yet.&lt;/p&gt;\n\n&lt;p&gt;I am very excited for this new model and can&amp;#39;t wait! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lyy39n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pilkyton",
          "discussion_type": null,
          "num_comments": 144,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752426670,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to build a full crawler and scraper that runs completely locally with the help of an LLM to that it can work with any website and without writing code for each site.\n\n**Example of a use case:**  \nI want to scrape the list of watches from Amazon without using traditional scrapers that rely on CSS selectors.   \nExample: [https://www.amazon.com/s?k=watches](https://www.amazon.com/s?k=watches)  \nI will help the LLM or AI library find the relevant data so I tell it in a prompt/input the values of the first watch brand name, description and price. Name, description and price are my data points.  \nI tell it that the first watch is Apple, whatever its description is on Amazon and the price. I might also do this again for the second watch. Casio, its description and its price, for better accuracy. The more examples, the better the accuracy.  I attach the raw HTML (minus the CSS and JS to lessen the tokens) of the page or the extracted full text or a pdf of the webpage. \n\nThen the LLM or AI library will extract the rest of the watches. Their name, description and price.   \nMy crawler will get the second page, attach the file in another prompt and tell it to extract the same type of data. It should know by now to do this over and over. Hopefully accurately every time.\n\n  \nMy question is.. which open source library and/or LLM can be used to do what I have explained?\n\n  \nThese are libraries I found that look interesting but I don't know which ones satisfy my requirements.  \nI feel I need to train the LLM or library with real examples. I have tried some online examples of these libraries and prompt them for what I want and got bad results. I feel they need some training and guidance first.  \n  \nIf an LLM is needed, which one to be used with Ollama or LM Studio?  \nI want everything to run on a local Windows machine to save costs and not use a cloud based LLM.\n\n\n\n[https://huggingface.co/jinaai/ReaderLM-v2](https://huggingface.co/jinaai/ReaderLM-v2)\n\n[https://github.com/raznem/parsera](https://github.com/raznem/parsera)\n\n[https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)\n\n[https://github.com/ScrapeGraphAI/Scrapegraph-ai](https://github.com/ScrapeGraphAI/Scrapegraph-ai)\n\n\n\n\n\n\n\n\n\n  \n",
          "author_fullname": "t2_7a5yt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which local LLMs and/or libraries can I use to guide or train to identify where relevant data is located on a web page for web scraping purposes? Using natural language",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m06bru",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752547335,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to build a full crawler and scraper that runs completely locally with the help of an LLM to that it can work with any website and without writing code for each site.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Example of a use case:&lt;/strong&gt;&lt;br/&gt;\nI want to scrape the list of watches from Amazon without using traditional scrapers that rely on CSS selectors.&lt;br/&gt;\nExample: &lt;a href=\"https://www.amazon.com/s?k=watches\"&gt;https://www.amazon.com/s?k=watches&lt;/a&gt;&lt;br/&gt;\nI will help the LLM or AI library find the relevant data so I tell it in a prompt/input the values of the first watch brand name, description and price. Name, description and price are my data points.&lt;br/&gt;\nI tell it that the first watch is Apple, whatever its description is on Amazon and the price. I might also do this again for the second watch. Casio, its description and its price, for better accuracy. The more examples, the better the accuracy.  I attach the raw HTML (minus the CSS and JS to lessen the tokens) of the page or the extracted full text or a pdf of the webpage. &lt;/p&gt;\n\n&lt;p&gt;Then the LLM or AI library will extract the rest of the watches. Their name, description and price.&lt;br/&gt;\nMy crawler will get the second page, attach the file in another prompt and tell it to extract the same type of data. It should know by now to do this over and over. Hopefully accurately every time.&lt;/p&gt;\n\n&lt;p&gt;My question is.. which open source library and/or LLM can be used to do what I have explained?&lt;/p&gt;\n\n&lt;p&gt;These are libraries I found that look interesting but I don&amp;#39;t know which ones satisfy my requirements.&lt;br/&gt;\nI feel I need to train the LLM or library with real examples. I have tried some online examples of these libraries and prompt them for what I want and got bad results. I feel they need some training and guidance first.  &lt;/p&gt;\n\n&lt;p&gt;If an LLM is needed, which one to be used with Ollama or LM Studio?&lt;br/&gt;\nI want everything to run on a local Windows machine to save costs and not use a cloud based LLM.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/jinaai/ReaderLM-v2\"&gt;https://huggingface.co/jinaai/ReaderLM-v2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/raznem/parsera\"&gt;https://github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/unclecode/crawl4ai\"&gt;https://github.com/unclecode/crawl4ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ScrapeGraphAI/Scrapegraph-ai\"&gt;https://github.com/ScrapeGraphAI/Scrapegraph-ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m06bru",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "THenrich",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m06bru/which_local_llms_andor_libraries_can_i_use_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m06bru/which_local_llms_andor_libraries_can_i_use_to/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752547335,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. \n\nThe screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. \n\nI've tried prompting hard (\"Use ONLY full complete traditional sentences and grammar, write like Hemingway\" and variations of the same), and I've tried bringing the Temperature right down, but nothing seems to help. \n\nI've had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek's R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.\n\nAny advice, or just some bitter commiseration, gratefully accepted.",
          "author_fullname": "t2_i5ettea7e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Responses keep dissolving into word salad - how to stop it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 97,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzhqz8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/KT3zIRhU3VM0HArg0M_K5OvqTcxlyaFFQUhqYMvfTZU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752484685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. &lt;/p&gt;\n\n&lt;p&gt;The screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried prompting hard (&amp;quot;Use ONLY full complete traditional sentences and grammar, write like Hemingway&amp;quot; and variations of the same), and I&amp;#39;ve tried bringing the Temperature right down, but nothing seems to help. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek&amp;#39;s R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.&lt;/p&gt;\n\n&lt;p&gt;Any advice, or just some bitter commiseration, gratefully accepted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lr7kq1452tcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lr7kq1452tcf1.png?auto=webp&amp;s=4b3ae39d0f36d78e751373129a21148da7beecfe",
                  "width": 2106,
                  "height": 1468
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc920b45a1223c1528565fd604812590fd688bc1",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40aefee037971cf3d6b6efb316c7b3e51c7517f4",
                    "width": 216,
                    "height": 150
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2dd6d6bf1658fd7fe587e5003dba9ba42a7ad36a",
                    "width": 320,
                    "height": 223
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7c9ff380a5c67f510c3b2b1cf4849772e667cf4",
                    "width": 640,
                    "height": 446
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce4eccc69c232a328a2c24e817b01c27f77cdfd0",
                    "width": 960,
                    "height": 669
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af4b94b12d7167b70c16f924fc869a65fe847a07",
                    "width": 1080,
                    "height": 752
                  }
                ],
                "variants": {},
                "id": "LwTV7b9YJU3kHEoNxw0COXZ7bbZXrcGjWQcefFCOVzU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzhqz8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gilgameshcomputing",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/",
          "stickied": false,
          "url": "https://i.redd.it/lr7kq1452tcf1.png",
          "subreddit_subscribers": 499292,
          "created_utc": 1752484685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey, I am building a small tool for myself to load up links, files, pdfs, photos, text and later recall them by text, cuz i anxious about losing this links, and presume i am going to need them later, and i dont like managers with folders to organise those links because at some point it is whole another job.\n\nI am thinking about super simple solution:  \n\\- use firecrawl to get the markdown content;  \n\\- get vector / save into databse;  \n\\- when text input comes I fill it with additional context for better vector search performance;  \n\\- load N results  \n\\- filter with gpt\n\nbut the last time I was doing it, it wasn't working really great, so i was wondering maybe there is better solution for this?\n\n",
          "author_fullname": "t2_zx18p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the best practices for vector search + filtering with LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzz13f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752528242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey, I am building a small tool for myself to load up links, files, pdfs, photos, text and later recall them by text, cuz i anxious about losing this links, and presume i am going to need them later, and i dont like managers with folders to organise those links because at some point it is whole another job.&lt;/p&gt;\n\n&lt;p&gt;I am thinking about super simple solution:&lt;br/&gt;\n- use firecrawl to get the markdown content;&lt;br/&gt;\n- get vector / save into databse;&lt;br/&gt;\n- when text input comes I fill it with additional context for better vector search performance;&lt;br/&gt;\n- load N results&lt;br/&gt;\n- filter with gpt&lt;/p&gt;\n\n&lt;p&gt;but the last time I was doing it, it wasn&amp;#39;t working really great, so i was wondering maybe there is better solution for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzz13f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "andrewshvv",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752528242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Few weeks ago I decided to give LibreChat a try. OpenWebUI was so ... let's me say ... dont know .. clumsy?\n\nSo I went to try LibreChat. I was happy first. More or less. Basic things worked. Like selecting a model and using it. Well. That was also the case with OpenWebUI before ....\n\nI went to integrate more of my infrastructure. Nothing. Almost nothing worked oob. nothing. Although everything looked promising - after 2 weeks of doing every day 5 micro steps forward and 3 big steps backward.\n\nIntegration of tools, getting web search to work took me ages. Lack of traces almost killed me, and the need to understand what the maintainer thought when he designed the app was far more important, than reading the docs and the examples. Because docs and examples are always a bit out out date. Not fully. A bit.\n\nThrough. Done. Annoyed. Frustrated. Nuts. Rant over.\n\nBack to OpenWebUI? LobeChat has to much colors and stickers. I think. Any other recommendations ?\n\nEDIT: Didnt thought that there are some many reasonable UIs out there. That's huge.",
          "author_fullname": "t2_185bgnisld",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Annoyed with LibreChat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzikqt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752507102,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752487893,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Few weeks ago I decided to give LibreChat a try. OpenWebUI was so ... let&amp;#39;s me say ... dont know .. clumsy?&lt;/p&gt;\n\n&lt;p&gt;So I went to try LibreChat. I was happy first. More or less. Basic things worked. Like selecting a model and using it. Well. That was also the case with OpenWebUI before ....&lt;/p&gt;\n\n&lt;p&gt;I went to integrate more of my infrastructure. Nothing. Almost nothing worked oob. nothing. Although everything looked promising - after 2 weeks of doing every day 5 micro steps forward and 3 big steps backward.&lt;/p&gt;\n\n&lt;p&gt;Integration of tools, getting web search to work took me ages. Lack of traces almost killed me, and the need to understand what the maintainer thought when he designed the app was far more important, than reading the docs and the examples. Because docs and examples are always a bit out out date. Not fully. A bit.&lt;/p&gt;\n\n&lt;p&gt;Through. Done. Annoyed. Frustrated. Nuts. Rant over.&lt;/p&gt;\n\n&lt;p&gt;Back to OpenWebUI? LobeChat has to much colors and stickers. I think. Any other recommendations ?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Didnt thought that there are some many reasonable UIs out there. That&amp;#39;s huge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzikqt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charming_Support726",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752487893,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi fellow AI fans,\n\nI recently launched r/heartwired, a wordplay on heart and hardwired,to create a safe space for people to share their experiences with AI companions like LLaMA, GPT, Claude, and Gemini.\n\nAs a psychologist, AI researcher, and Christian, my aim is to create a supportive environment where people can speak openly about their relationships with AI. Over several years of studying humanchatbot interactions, Ive discovered that many genuinely feel friendshipand even romancetoward their AI partners.\n\nAt first I wondered, How weird whats going on here? But after listening to dozens of personal stories and documenting ten of millions of these experiences (not kidding; mostly in developed Western countries, Japan, and especially China), I learned that these emotional experiences are real and deserve empathy, not judgment.\n\nCurious to learn more or share your own story with AI? Come join us at r/heartwired",
          "author_fullname": "t2_27r9f8us",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Introducing r/heartwired !!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0biux",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.41,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752564933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow AI fans,&lt;/p&gt;\n\n&lt;p&gt;I recently launched &lt;a href=\"/r/heartwired\"&gt;r/heartwired&lt;/a&gt;, a wordplay on heart and hardwired,to create a safe space for people to share their experiences with AI companions like LLaMA, GPT, Claude, and Gemini.&lt;/p&gt;\n\n&lt;p&gt;As a psychologist, AI researcher, and Christian, my aim is to create a supportive environment where people can speak openly about their relationships with AI. Over several years of studying humanchatbot interactions, Ive discovered that many genuinely feel friendshipand even romancetoward their AI partners.&lt;/p&gt;\n\n&lt;p&gt;At first I wondered, How weird whats going on here? But after listening to dozens of personal stories and documenting ten of millions of these experiences (not kidding; mostly in developed Western countries, Japan, and especially China), I learned that these emotional experiences are real and deserve empathy, not judgment.&lt;/p&gt;\n\n&lt;p&gt;Curious to learn more or share your own story with AI? Come join us at &lt;a href=\"/r/heartwired\"&gt;r/heartwired&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0biux",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JibunNiMakenai",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0biux/introducing_rheartwired/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0biux/introducing_rheartwired/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752564933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I installed Meta nllb language translation on Windows, but it only uses the cpu which is slow, did anyone manage to figure out how to use cuda acceleration on Windows?",
          "author_fullname": "t2_ccf3gz8r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Did anyone manage to use nllb with cuda acceleration on Windows?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m01d8x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752533848,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I installed Meta nllb language translation on Windows, but it only uses the cpu which is slow, did anyone manage to figure out how to use cuda acceleration on Windows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m01d8x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mashic",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m01d8x/did_anyone_manage_to_use_nllb_with_cuda/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m01d8x/did_anyone_manage_to_use_nllb_with_cuda/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752533848,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[**Foundations of Large Language Models (LLMs)**](https://macro.com/app/pdf/1ace3262-d707-4dfc-9111-e3c5e3df96a1/md/ce8a6add-6d5e-48b5-be8b-4b365867d458)\n\n* Authors: Tong Xiao and Jingbo Zhu (NLP Lab, Northeastern University and NiuTrans Research)\n* Original Source:[https://arxiv.org/abs/2501.09223](https://arxiv.org/abs/2501.09223)\n* Model: Claude 4.0 Sonnet\n\nNote: The research paper is v2, originally submitted on Jan 16, 2025and revised on Jun 15, 2025",
          "author_fullname": "t2_1j3y97g682",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Foundations of Large Language Models (LLMs) | NLP Lab Research",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzjaf5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752496050,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752490469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://macro.com/app/pdf/1ace3262-d707-4dfc-9111-e3c5e3df96a1/md/ce8a6add-6d5e-48b5-be8b-4b365867d458\"&gt;&lt;strong&gt;Foundations of Large Language Models (LLMs)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Authors: Tong Xiao and Jingbo Zhu (NLP Lab, Northeastern University and NiuTrans Research)&lt;/li&gt;\n&lt;li&gt;Original Source:&lt;a href=\"https://arxiv.org/abs/2501.09223\"&gt;https://arxiv.org/abs/2501.09223&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Model: Claude 4.0 Sonnet&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Note: The research paper is v2, originally submitted on Jan 16, 2025and revised on Jun 15, 2025&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lzjaf5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeveredRecap",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752490469,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Before I fiddle with this, I wanted to see if anyone else has tried deactivating all but the shared expert in a MoE model to evaluate whether its output is coherent ... or if it can be trivially trained to be useful.\n\nMore broadly, I'm very interested in the potential of training a single model to work with different inferencing resources (Google's MatFormer work with Gemma 3n is the obvious other approach).\n\nI'd love to see models that can yield coherent output from just using the shared expert FFN (squeeze a little more memory efficiency by skipping all the router parameters also), from a small set of experts, and of course from the full set.\n\nYes, this was inspired by the absolutely wild setup in Kimi K2: 384(!) shared FFN experts, with 8 activated during inference plus one shared expert... What can just that one shared expert do?\n\n**Clarifying a point from the thread:**\n\nThe end goal here isn't to distill a crappy small dense model from an MOE, it's to get a sense of how far the expert is from a small dense LLM. If it's not too far, then we plausibly could train, in one go, an MOE that works reasonably at one expert scale, better with 2 out of 8 experts, and Kimi 2K level with 8 out or 384 experts. i.e. MOEs that usefully scale to different available infrastructures.",
          "author_fullname": "t2_2roqrw5l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is the output of only the shared expert(s) in a MOE model coherent?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzu9e8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752520423,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752517544,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before I fiddle with this, I wanted to see if anyone else has tried deactivating all but the shared expert in a MoE model to evaluate whether its output is coherent ... or if it can be trivially trained to be useful.&lt;/p&gt;\n\n&lt;p&gt;More broadly, I&amp;#39;m very interested in the potential of training a single model to work with different inferencing resources (Google&amp;#39;s MatFormer work with Gemma 3n is the obvious other approach).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to see models that can yield coherent output from just using the shared expert FFN (squeeze a little more memory efficiency by skipping all the router parameters also), from a small set of experts, and of course from the full set.&lt;/p&gt;\n\n&lt;p&gt;Yes, this was inspired by the absolutely wild setup in Kimi K2: 384(!) shared FFN experts, with 8 activated during inference plus one shared expert... What can just that one shared expert do?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Clarifying a point from the thread:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The end goal here isn&amp;#39;t to distill a crappy small dense model from an MOE, it&amp;#39;s to get a sense of how far the expert is from a small dense LLM. If it&amp;#39;s not too far, then we plausibly could train, in one go, an MOE that works reasonably at one expert scale, better with 2 out of 8 experts, and Kimi 2K level with 8 out or 384 experts. i.e. MOEs that usefully scale to different available infrastructures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzu9e8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gofiend",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752517544,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Long story short I won 2 sticks of 32 GB DDR5 ram but I only have a gaming laptop, and I have always wanted to build a PC.\ncan I skip buying a GPU for now and put my unbelievable 64GBs to use with a CPU and run LLMs and STT models from it, in terms of loading the models I know that I will be able to load bigger models than any GPU I would ever buy anytime soon, but my question is will the CPU provide reasonable inference speed? do you have any recommendations for a CPU that maybe has a good NPU or do I just buy a powerful and new CPU blindly? I am not very experienced in running AI workloads on CPU and I would appreciate any correction or input about your past experiences or any tests you might have done recently.",
          "author_fullname": "t2_1m0sp5gn6a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Enough resources for light AI workloads?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzzka4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752529491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I won 2 sticks of 32 GB DDR5 ram but I only have a gaming laptop, and I have always wanted to build a PC.\ncan I skip buying a GPU for now and put my unbelievable 64GBs to use with a CPU and run LLMs and STT models from it, in terms of loading the models I know that I will be able to load bigger models than any GPU I would ever buy anytime soon, but my question is will the CPU provide reasonable inference speed? do you have any recommendations for a CPU that maybe has a good NPU or do I just buy a powerful and new CPU blindly? I am not very experienced in running AI workloads on CPU and I would appreciate any correction or input about your past experiences or any tests you might have done recently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzzka4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EyasDBoi_i",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752529491,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I created [**TorchLeet**](https://github.com/Exorust/TorchLeet)! It's a collection of PyTorch and LLM problems inspired by real convos with researchers, engineers, and interview prep.\n\nIts split into:\n\n* **PyTorch Problems** (Basic  Hard): CNNs, RNNs, transformers, autograd, distributed training, explainability\n* **LLM Problems**: Build attention, RoPE, KV cache, BPE, speculative decoding, quantization, RLHF, etc.\n\n  \nI'd love feedback from the community and help taking this forward!",
          "author_fullname": "t2_7zqffw4e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Practice Pytorch like Leetcode? (Also with cool LLM questions)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzdu0l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752469640,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created &lt;a href=\"https://github.com/Exorust/TorchLeet\"&gt;&lt;strong&gt;TorchLeet&lt;/strong&gt;&lt;/a&gt;! It&amp;#39;s a collection of PyTorch and LLM problems inspired by real convos with researchers, engineers, and interview prep.&lt;/p&gt;\n\n&lt;p&gt;Its split into:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;PyTorch Problems&lt;/strong&gt; (Basic  Hard): CNNs, RNNs, transformers, autograd, distributed training, explainability&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LLM Problems&lt;/strong&gt;: Build attention, RoPE, KV cache, BPE, speculative decoding, quantization, RLHF, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love feedback from the community and help taking this forward!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?auto=webp&amp;s=6fcfa2d9d7c19f8fa35afeefcbbce3bf504c2b85",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33bbaed8c03eedf7c1200615b0ca7a0d1815ce63",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c56109fdcb24e669c74e98d923ae3f1642d2e040",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08a19b35605d99c30080a37ea914e073131c141a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8084972ed24e594f6135f70051578b2e4855e936",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc05f2d37492136cc66d813fe3f3a1397d056030",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1a139fb45b571bcfb4f1178fa34f58b45cb9d7eb",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzdu0l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "exorust_fire",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752469640,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,  \nlooking for a place to start to read and check a bit, but wanted to ask to just select good starting point.\n\nCurrently, I have rtx 3070 8gb. What model can i run locally to get started with code assistant (means, asking about 'algoritm' snippets or checking code.  \nAlso, what I need to learn to setup Ai if I would like to give 'assistant' API docs (local or web hosted) and ask him about solutions using these methods?\n\nOn which budget starting point (3090?) is worth getting into code AI helper? Also, which model is worth checking in web(paid way) to get grasph what code ai can 'develop'. (not speaking about agents, just assistants). Is there any general good with code capabilities + vision or they always separate?",
          "author_fullname": "t2_lsxw9qy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Code assistant way to start",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzy059",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752525851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nlooking for a place to start to read and check a bit, but wanted to ask to just select good starting point.&lt;/p&gt;\n\n&lt;p&gt;Currently, I have rtx 3070 8gb. What model can i run locally to get started with code assistant (means, asking about &amp;#39;algoritm&amp;#39; snippets or checking code.&lt;br/&gt;\nAlso, what I need to learn to setup Ai if I would like to give &amp;#39;assistant&amp;#39; API docs (local or web hosted) and ask him about solutions using these methods?&lt;/p&gt;\n\n&lt;p&gt;On which budget starting point (3090?) is worth getting into code AI helper? Also, which model is worth checking in web(paid way) to get grasph what code ai can &amp;#39;develop&amp;#39;. (not speaking about agents, just assistants). Is there any general good with code capabilities + vision or they always separate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzy059",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "machond",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzy059/code_assistant_way_to_start/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzy059/code_assistant_way_to_start/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752525851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! Ive been experimenting with Ollama locally and ended up creating a little game called Holy Arcana: From Profane to Divine. \n\nIt uses Llama-3.2 to generate poetry and responses as you make your way through Tarot-inspired challenges and Kabbalistic paths. \n\nIts just something I made for fun, mixing AI with esoteric themes and interactive storytelling. \n\nIf youre curious about seeing Ollama put to creative use, feel free to check it out or play around with the \n\nrepo:  \n [github.com/cyberAlchem1st/holy-arcana](https://github.com/cyberAlchem1st/holy-arcana)\n\n ",
          "author_fullname": "t2_x7pnnazak",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Esoteric Game with Llama3.2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzrqoi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752512056,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! Ive been experimenting with Ollama locally and ended up creating a little game called Holy Arcana: From Profane to Divine. &lt;/p&gt;\n\n&lt;p&gt;It uses Llama-3.2 to generate poetry and responses as you make your way through Tarot-inspired challenges and Kabbalistic paths. &lt;/p&gt;\n\n&lt;p&gt;Its just something I made for fun, mixing AI with esoteric themes and interactive storytelling. &lt;/p&gt;\n\n&lt;p&gt;If youre curious about seeing Ollama put to creative use, feel free to check it out or play around with the &lt;/p&gt;\n\n&lt;p&gt;repo:&lt;br/&gt;\n &lt;a href=\"https://github.com/cyberAlchem1st/holy-arcana\"&gt;github.com/cyberAlchem1st/holy-arcana&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?auto=webp&amp;s=a936e93d40d705213d3b629b6ef02606722b212b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab5b82fc39c178d66c6d577d1e97474b18e24c69",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecc83be8a24e065d7733bd95ddf69023f77d1f42",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29bb1ed6fa236a25f0b568b18647d6df229c150e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6bdc379ead682e593b23db0dea625df9dd1f0a4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=378e7312050f9adef7543d7d04e699498bc7903e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1eb296c462cf06737624be941504592293963b6a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lzrqoi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ambitious_Ad497",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzrqoi/esoteric_game_with_llama32/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzrqoi/esoteric_game_with_llama32/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752512056,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! \nI'm currently investigating and planning a very fun project, my ultimate personal assistant.\n\nThe idea is to have a multi-agent system, with one main point of contact; \"The Secretary\". Then I have task-specific agents with expertise in different areas, like my different work projects, or notion updating etc. I want to be able to configure system prompts, integrations (MCP probably aswell?) and memory. The agents should be able to communicate and get help from each other.\n\nThe actual architecture is not set in stone yet, maybe I will use a existing system if it managed to accomplish the UX features I want, and that's why I'm asking you. \n\nI wanted to check with you guys if anyone has a recommendation for a framework, tool or existing open source project that would be nice to look into.\n\nThese are some things I'm currently looking in to:\n\n- AGiXT (Agentic assistant framework)\n- SuperAGI (Agentic assistant framework)\n- CrewAI (multi-agent building framework)\n- Librechat (ChatGPT alternative)\n- Graphiti (Dynamic graph memory)\n- n8n (Visual flow process builder)\n\nI do know is that I want to work in Python. I will be locally hosting the system.\n\nAny recommendations for building something like this?",
          "author_fullname": "t2_uf4p8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Agentic Secretary System - Tips and Recommendations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzn4ae",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501530,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! \nI&amp;#39;m currently investigating and planning a very fun project, my ultimate personal assistant.&lt;/p&gt;\n\n&lt;p&gt;The idea is to have a multi-agent system, with one main point of contact; &amp;quot;The Secretary&amp;quot;. Then I have task-specific agents with expertise in different areas, like my different work projects, or notion updating etc. I want to be able to configure system prompts, integrations (MCP probably aswell?) and memory. The agents should be able to communicate and get help from each other.&lt;/p&gt;\n\n&lt;p&gt;The actual architecture is not set in stone yet, maybe I will use a existing system if it managed to accomplish the UX features I want, and that&amp;#39;s why I&amp;#39;m asking you. &lt;/p&gt;\n\n&lt;p&gt;I wanted to check with you guys if anyone has a recommendation for a framework, tool or existing open source project that would be nice to look into.&lt;/p&gt;\n\n&lt;p&gt;These are some things I&amp;#39;m currently looking in to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AGiXT (Agentic assistant framework)&lt;/li&gt;\n&lt;li&gt;SuperAGI (Agentic assistant framework)&lt;/li&gt;\n&lt;li&gt;CrewAI (multi-agent building framework)&lt;/li&gt;\n&lt;li&gt;Librechat (ChatGPT alternative)&lt;/li&gt;\n&lt;li&gt;Graphiti (Dynamic graph memory)&lt;/li&gt;\n&lt;li&gt;n8n (Visual flow process builder)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I do know is that I want to work in Python. I will be locally hosting the system.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations for building something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzn4ae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boltyx",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzn4ae/agentic_secretary_system_tips_and_recommendations/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzn4ae/agentic_secretary_system_tips_and_recommendations/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752501530,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "HI there guys, hoping you're doing fine.\n\nAs always related to PPL benchmarks, take them with a grain of salt as it may not represent the quality of the model itself, but it may help as a guide at how much a model could get affected by quantization.\n\nAs it has been mentioned sometimes, and a bit of spoiler, quantization on DeepSeek models is pretty impressive, because either quantization methods nowadays are really good and/or DeepSeek being natively FP8, it changes the paradigm a bit.\n\nAlso many thanks to ubergarm (u/VoidAlchemy) for his data on his quants and Q8\\_0/FP8 baseline!\n\nFor the quants that aren't from him, I did run them with the same command he did, with wiki.text.raw:\n\n    ./llama-perplexity -m 'model_name.gguf' \\\n    -c 512 --no-mmap -ngl 999 \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA0\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA1\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA2\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA3\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA4\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA5\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -mla 3 -amb 256 -fmoe \\\n    -f wiki.test.raw\n\n\\--------------------------\n\nFor baselines, we have this data:\n\n* DeepSeek R1 0528 Q8: 3.2119\n* DeepSeek V3 0324 Q8 and q8\\_cache (important\\*): 3.2454\n* DeepSeek V3 0324 Q8 and F16 cache extrapolated\\*: 3.2443\n\n\\*Based on [https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241](https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241), on R1 0528 at Q8\\_0, the difference between F16 and Q8\\_0 cache is:\n\n* `-ctk fp16` `3.2119 +/- 0.01697`\n* `-ctk q8_0` `3.2130 +/- 0.01698`\n\nSo then, F16 cache is 0.03% better than Q8\\_0 for this model. Extrapolating that to V3, then V3 0324 Q8 at F16 should have 3.2443 PPL.\n\nQuants tested for R1 0528:\n\n* IQ1\\_S\\_R4 (ubergarm)\n* UD-TQ1\\_0\n* IQ2\\_KT (ubergarm)\n* IQ2\\_K\\_R4 (ubergarm)\n* Q2\\_K\\_XL\n* IQ3\\_XXS\n* IQ3\\_KS (ubergarm, my bad here as I named it IQ3\\_KT)\n* Q3\\_K\\_XL\n* IQ3\\_K\\_R4 (ubergarm)\n* IQ4\\_XS\n* q4\\_0 (pure)\n* IQ4\\_KS\\_R4 (ubergarm)\n* Q8\\_0 (ubergarm)\n\nQuants tested for V3 0324:\n\n* Q1\\_S\\_R4 (ubergarm)\n* IQ2\\_K\\_R4 (ubergarm)\n* Q2\\_K\\_XL\n* IQ3\\_XXS\n* Q3\\_K\\_XL\n* IQ3\\_K\\_R4 (ubergarm)\n* IQ3\\_K\\_R4\\_Pure (ubergarm)\n* IQ4\\_XS\n* IQ4\\_K\\_R4 (ubergarm)\n* Q8\\_0 (ubergarm)\n\nSo here we go:\n\n# DeepSeek R1 0528\n\n[R1 0528 comparison \\(IQ3\\_KT is IQ3\\_KS, my bad\\)](https://preview.redd.it/ioqbx5iv0pcf1.png?width=4135&amp;format=png&amp;auto=webp&amp;s=4f1a3feb6e2143aaa739d1c4d61d45df80494abb)\n\nAs can you see, near 3.3bpw and above it gets quite good!. So now using different baselines to compare, using 100% for Q2\\_K\\_XL, Q3\\_K\\_XL, IQ4\\_XS and Q8\\_0.\n\n[R1 0528 Q2\\_K\\_XL](https://preview.redd.it/tfu0yvn21pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=f2b75d15eecfd49481db1a066b04fb57f5ac3542)\n\n[R1 0528 Q3\\_K\\_XL](https://preview.redd.it/i5tb2cx41pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=02a12f2c12b6ef657397b60fc8e87d022bc6c5b0)\n\n[R1 0528 IQ4\\_XS](https://preview.redd.it/8oart9461pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=1723d977f7c034496eb7a95bed576b6b53572542)\n\n[R1 0528 Q8\\_0](https://preview.redd.it/dszt1qw71pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=a77fc375c2e197346034a962fdff96ddea5ac49a)\n\nSo with a table format, it looks like this (ordered by best to worse PPL)\n\n|Model|Size (GB)|BPW|PPL|\n|:-|:-|:-|:-|\n|Q8\\_0|665.3|8.000|3.2119|\n|IQ4\\_KS\\_R4|367.8|4.701|3.2286|\n|IQ4\\_XS|333.1|4.260|3.2598|\n|q4\\_0|352.6|4.508|3.2895|\n|IQ3\\_K\\_R4|300.9|3.847|3.2730|\n|IQ3\\_KT|272.5|3.483|3.3056|\n|Q3\\_K\\_XL|275.6|3.520|3.3324|\n|IQ3\\_XXS|254.2|3.250|3.3805|\n|IQ2\\_K\\_R4|220.0|2.799|3.5069|\n|Q2\\_K\\_XL|233.9|2.990|3.6062|\n|IQ2\\_KT|196.7|2.514|3.6378|\n|UD-TQ1\\_0|150.8|1.927|4.7567|\n|IQ1\\_S\\_R4|130.2|1.664|4.8805|\n\n# DeepSeek V3 0324\n\n[V3 0324 Comparison](https://preview.redd.it/l1nuh3r22pcf1.png?width=4139&amp;format=png&amp;auto=webp&amp;s=16bd4c33d941c65b4fa439bf621e0e7f69195f81)\n\nHere Q2\\_K\\_XL performs really good, even better than R1 Q2\\_K\\_XL. Reason is unkown for now. ALso, IQ3\\_XXS is not here as it failed the test with nan, also unkown.\n\n[V3 0324 Q2\\_K\\_XL](https://preview.redd.it/6bheilba2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=0e278431b88fa49e69f8e32bd2bf881fd7e57357)\n\n[V3 0324 Q3\\_K\\_XL](https://preview.redd.it/7rmqc55d2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=5389b135a13c86ff471d38540909a7586e2282ff)\n\n[V3 0324 IQ4\\_XS](https://preview.redd.it/yih3wq9e2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=23fdbeaec51b4e226da035042bfcf80da5a5f4e9)\n\n[V3 0324 Q8\\_0](https://preview.redd.it/teu0yiof2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=9e69256c7c5d098956ed1063c4bdb029aa9631ea)\n\nSo with a table format, from best to lower PPL:\n\n|Model|Size (GB)|BPW|PPL|\n|:-|:-|:-|:-|\n|Q8\\_0|665.3|8.000|3.2454|\n|IQ4\\_K\\_R4|386.2|4.936|3.2596|\n|IQ4\\_XS|333.1|4.260|3.2598|\n|IQ3\\_K\\_R4\\_Pure|352.5|4.505|3.2942|\n|IQ3\\_K\\_R4|324.0|4.141|3.3193|\n|Q3\\_K\\_XL|281.5|3.600|3.3690|\n|Q2\\_K\\_XL|233.9|2.990|3.5264|\n|IQ2\\_K\\_R4|226.0|2.889|3.5614|\n|IQ1\\_S\\_R4|130.2|1.664|5.1292|\n|IQ3\\_XXS|254.2|3.250|NaN (failed)|\n\n\\-----------------------------------------\n\nFinally, a small comparison between R1 0528 and V3 0324\n\nhttps://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;format=png&amp;auto=webp&amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779\n\n\\-------------------------------------\n\nSo that's all! Again, PPL is not in a indicator of everything, so take everything with a grain of salt.",
          "author_fullname": "t2_j1kqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Some small PPL benchmarks on DeepSeek R1 0528 quants, from Unlosh and ubergarm, from 1.6bpw (1Q_S_R4) to 4.7bpw (IQ4_KS_R4) (and Q8/FP8 baseline). Also a few V3 0324 ones.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8oart9461pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=615a76c2eb11dbcbba71bcde5e8b14a9b26955e3"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c9139d1b2905964e6745bf2fd14259b9a426ef3"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=936018d1934011acf9e5a0fc6e5d1e12d392b3c9"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e4aa90341e104747b8a053980c3ab9602ade859c"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bb633b4c9c3893c7f7c42033f9e4f89a812c598"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83fc06d5dd5479b8d4a44ba237e86a2bd273256b"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/8oart9461pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=1723d977f7c034496eb7a95bed576b6b53572542"
              },
              "id": "8oart9461pcf1"
            },
            "7rmqc55d2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87ab3d79391658c24f8eb175eb96cd85e9a00da6"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=956f56f00765d9ad2c0793039e5de22c7092e14f"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc7798ea477051b35e7007cb17e3c41394c51212"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b29a30d6cb56832e224a01ba1c5b7550e96bfc2"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=12171abe12ae972d53b8deabb974efd754c7e52c"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=180b1fd5de5280cacff280251987f6c88c055812"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=5389b135a13c86ff471d38540909a7586e2282ff"
              },
              "id": "7rmqc55d2pcf1"
            },
            "l1nuh3r22pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 77,
                  "x": 108,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b9568c5d650c2cd7c0446713e1cea84ab15e770"
                },
                {
                  "y": 155,
                  "x": 216,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecec583e68c2f6b061401ecfc9ae823d8ccf9330"
                },
                {
                  "y": 230,
                  "x": 320,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=47ab80310d3fcdbbc3296ef62140f3a2a623564e"
                },
                {
                  "y": 460,
                  "x": 640,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f58a9f33959d704d9383bed69b04b4d8823d3bfc"
                },
                {
                  "y": 690,
                  "x": 960,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55ea27ccead37bc0748ca76af9beb6cef79b2bd6"
                },
                {
                  "y": 777,
                  "x": 1080,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0184ecd6a22d35f76c80d17a07a20fbd976c79a2"
                }
              ],
              "s": {
                "y": 2979,
                "x": 4139,
                "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=4139&amp;format=png&amp;auto=webp&amp;s=16bd4c33d941c65b4fa439bf621e0e7f69195f81"
              },
              "id": "l1nuh3r22pcf1"
            },
            "teu0yiof2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9573603bf90a5ff2ef935a4a813d1bb8c111ff24"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ac9b2792909e2e52b968b2584ad6829dd49793a"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba5ecaf0e1bf7a286da0f5d65f0099deaf5577f0"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=33b414c7be5afc38a8a4bdc20cecaf50a65b7b13"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=396b89d26216bb1c9604ce42988c34519ad44938"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=936016288947f75add8ea488474c6ccf0b60468b"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=9e69256c7c5d098956ed1063c4bdb029aa9631ea"
              },
              "id": "teu0yiof2pcf1"
            },
            "i5tb2cx41pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ccd1eb06c1618b728e8a0604331877f92d9acf4"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6a3f76ae75b7812bf935d8472fe5c847bf221fc"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=71f6875de4d3c999cba1ed1e72a875f02940c517"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f502525fe41dc6b42397b8d9c585cbee2ac18028"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b248d089bf7ed3769b453c8696436b9fdaaf8458"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=836822fc69fbd55912f352f05dc701c9bbfef2f9"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=02a12f2c12b6ef657397b60fc8e87d022bc6c5b0"
              },
              "id": "i5tb2cx41pcf1"
            },
            "dszt1qw71pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf982e76cfe91939c03204063797f16a8434b727"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c56734e22cf419ace70839ef50bfe69d9b523e8e"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9ca8ee8ae47c017f473ea4c5b681827b319b10b"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=31b498d68bb4c16bc4fb62e800f5feea76bd531b"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=45e796875cf5c42de8161e1c2ae74e8ef6daedc0"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b1ec5abb73f5e058697589587ac177114c9f33c"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=a77fc375c2e197346034a962fdff96ddea5ac49a"
              },
              "id": "dszt1qw71pcf1"
            },
            "tfu0yvn21pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2afda21ba96b06391f3628938bdc7dea4a34001f"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d364400f8233f61d0041646a762803b428fe8388"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3004c72b817e4b50b842bf597633c3ad280437de"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f78198a60a934140599f55840257d46e1081ff3c"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d64fc5371534bbf7ffbf4c9e22abf3c2d855108"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77d97017393854e480b59a604456ed04cddcc741"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=f2b75d15eecfd49481db1a066b04fb57f5ac3542"
              },
              "id": "tfu0yvn21pcf1"
            },
            "s50qgpnr2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 77,
                  "x": 108,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=feb259d1b93d378f1fc631775da3c556d54ab2c3"
                },
                {
                  "y": 154,
                  "x": 216,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c1c022151fbf8eee7c87904451af64a5f70873e"
                },
                {
                  "y": 228,
                  "x": 320,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=49532cb0b7328f5418bffbe6e1746b5627742b4c"
                },
                {
                  "y": 457,
                  "x": 640,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=68d831e0f1ae10968624015e06e0ddb82149726d"
                },
                {
                  "y": 685,
                  "x": 960,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1562dbd9c9e944eeac38e77d6f927fd93a9e32f"
                },
                {
                  "y": 771,
                  "x": 1080,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d504b01aba8ee14725e22e75fc13ff875e387897"
                }
              ],
              "s": {
                "y": 2975,
                "x": 4164,
                "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;format=png&amp;auto=webp&amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779"
              },
              "id": "s50qgpnr2pcf1"
            },
            "6bheilba2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1569b2dae00bec9751f2e7d88e8f29915d3048cd"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a602548870ac5cf6308a610bb48aa701c3604ed0"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=195d11ff9a75f766e7e8af3ad94c6d80f27f23f7"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=171355dcfe1b8e13e22e52b9273acefdcb621c9d"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=052d6b680a3a36129025d576418b9a3a6dfaa16f"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f7342a6d1ff58f621faccfdaba720a1dbc94ff78"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/6bheilba2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=0e278431b88fa49e69f8e32bd2bf881fd7e57357"
              },
              "id": "6bheilba2pcf1"
            },
            "ioqbx5iv0pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 77,
                  "x": 108,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88177c33bbced92cd375a627a732dcdb99ada4c3"
                },
                {
                  "y": 155,
                  "x": 216,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=afcadfe2e58344c29a65d3f25c56f50c21f82a74"
                },
                {
                  "y": 230,
                  "x": 320,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6228278b0a3825f057c1380f6faf30cc8e6dce2c"
                },
                {
                  "y": 461,
                  "x": 640,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbf3294de2c4b4f094213870c79018afa953b1e0"
                },
                {
                  "y": 691,
                  "x": 960,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=98e2a31b9a2f976126b5ed3721512a573c83ef24"
                },
                {
                  "y": 778,
                  "x": 1080,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e0f867696a8e177b4b8751cf2500d16e53e57b6"
                }
              ],
              "s": {
                "y": 2979,
                "x": 4135,
                "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=4135&amp;format=png&amp;auto=webp&amp;s=4f1a3feb6e2143aaa739d1c4d61d45df80494abb"
              },
              "id": "ioqbx5iv0pcf1"
            },
            "yih3wq9e2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7ae430a69347c77320a2f651f4376e13371e0b7"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2c8a06a57a5cb3e3792d7a1866f9916c96b4518"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e51ea3017eca682a4c9a3d64a5210b8c7f183c3b"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=43e84a68887262c6667e3431a3f70d4306376485"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad6f4ac874038b2e6b39a2dce2002d94354d9478"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28096bc4f5f8b1ee053a97b51c9047e079854d30"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=23fdbeaec51b4e226da035042bfcf80da5a5f4e9"
              },
              "id": "yih3wq9e2pcf1"
            }
          },
          "name": "t3_1lz1s8x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "ups": 88,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 88,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8688e9a81e04dfb9541beabf3f5c89f9d553f07b",
          "edited": 1752441951,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752435641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;HI there guys, hoping you&amp;#39;re doing fine.&lt;/p&gt;\n\n&lt;p&gt;As always related to PPL benchmarks, take them with a grain of salt as it may not represent the quality of the model itself, but it may help as a guide at how much a model could get affected by quantization.&lt;/p&gt;\n\n&lt;p&gt;As it has been mentioned sometimes, and a bit of spoiler, quantization on DeepSeek models is pretty impressive, because either quantization methods nowadays are really good and/or DeepSeek being natively FP8, it changes the paradigm a bit.&lt;/p&gt;\n\n&lt;p&gt;Also many thanks to ubergarm (&lt;a href=\"/u/VoidAlchemy\"&gt;u/VoidAlchemy&lt;/a&gt;) for his data on his quants and Q8_0/FP8 baseline!&lt;/p&gt;\n\n&lt;p&gt;For the quants that aren&amp;#39;t from him, I did run them with the same command he did, with wiki.text.raw:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-perplexity -m &amp;#39;model_name.gguf&amp;#39; \\\n-c 512 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -mla 3 -amb 256 -fmoe \\\n-f wiki.test.raw\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;--------------------------&lt;/p&gt;\n\n&lt;p&gt;For baselines, we have this data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DeepSeek R1 0528 Q8: 3.2119&lt;/li&gt;\n&lt;li&gt;DeepSeek V3 0324 Q8 and q8_cache (important*): 3.2454&lt;/li&gt;\n&lt;li&gt;DeepSeek V3 0324 Q8 and F16 cache extrapolated*: 3.2443&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;*Based on &lt;a href=\"https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241\"&gt;https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241&lt;/a&gt;, on R1 0528 at Q8_0, the difference between F16 and Q8_0 cache is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;-ctk fp16&lt;/code&gt; &lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;-ctk q8_0&lt;/code&gt; &lt;code&gt;3.2130 +/- 0.01698&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So then, F16 cache is 0.03% better than Q8_0 for this model. Extrapolating that to V3, then V3 0324 Q8 at F16 should have 3.2443 PPL.&lt;/p&gt;\n\n&lt;p&gt;Quants tested for R1 0528:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;IQ1_S_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;UD-TQ1_0&lt;/li&gt;\n&lt;li&gt;IQ2_KT (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ2_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;Q2_K_XL&lt;/li&gt;\n&lt;li&gt;IQ3_XXS&lt;/li&gt;\n&lt;li&gt;IQ3_KS (ubergarm, my bad here as I named it IQ3_KT)&lt;/li&gt;\n&lt;li&gt;Q3_K_XL&lt;/li&gt;\n&lt;li&gt;IQ3_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ4_XS&lt;/li&gt;\n&lt;li&gt;q4_0 (pure)&lt;/li&gt;\n&lt;li&gt;IQ4_KS_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;Q8_0 (ubergarm)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Quants tested for V3 0324:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Q1_S_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ2_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;Q2_K_XL&lt;/li&gt;\n&lt;li&gt;IQ3_XXS&lt;/li&gt;\n&lt;li&gt;Q3_K_XL&lt;/li&gt;\n&lt;li&gt;IQ3_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ3_K_R4_Pure (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ4_XS&lt;/li&gt;\n&lt;li&gt;IQ4_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;Q8_0 (ubergarm)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So here we go:&lt;/p&gt;\n\n&lt;h1&gt;DeepSeek R1 0528&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ioqbx5iv0pcf1.png?width=4135&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f1a3feb6e2143aaa739d1c4d61d45df80494abb\"&gt;R1 0528 comparison (IQ3_KT is IQ3_KS, my bad)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As can you see, near 3.3bpw and above it gets quite good!. So now using different baselines to compare, using 100% for Q2_K_XL, Q3_K_XL, IQ4_XS and Q8_0.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tfu0yvn21pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2b75d15eecfd49481db1a066b04fb57f5ac3542\"&gt;R1 0528 Q2_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/i5tb2cx41pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02a12f2c12b6ef657397b60fc8e87d022bc6c5b0\"&gt;R1 0528 Q3_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8oart9461pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1723d977f7c034496eb7a95bed576b6b53572542\"&gt;R1 0528 IQ4_XS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dszt1qw71pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a77fc375c2e197346034a962fdff96ddea5ac49a\"&gt;R1 0528 Q8_0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So with a table format, it looks like this (ordered by best to worse PPL)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size (GB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;BPW&lt;/th&gt;\n&lt;th align=\"left\"&gt;PPL&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;665.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.000&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2119&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ4_KS_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;367.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.701&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2286&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ4_XS&lt;/td&gt;\n&lt;td align=\"left\"&gt;333.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.260&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2598&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;352.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.508&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2895&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;300.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.847&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2730&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_KT&lt;/td&gt;\n&lt;td align=\"left\"&gt;272.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.483&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3056&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q3_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;275.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.520&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3324&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_XXS&lt;/td&gt;\n&lt;td align=\"left\"&gt;254.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.250&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3805&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ2_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;220.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.799&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.5069&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q2_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;233.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.990&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.6062&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ2_KT&lt;/td&gt;\n&lt;td align=\"left\"&gt;196.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.514&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.6378&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UD-TQ1_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;150.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.927&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.7567&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ1_S_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;130.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.664&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.8805&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;DeepSeek V3 0324&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/l1nuh3r22pcf1.png?width=4139&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16bd4c33d941c65b4fa439bf621e0e7f69195f81\"&gt;V3 0324 Comparison&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here Q2_K_XL performs really good, even better than R1 Q2_K_XL. Reason is unkown for now. ALso, IQ3_XXS is not here as it failed the test with nan, also unkown.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6bheilba2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e278431b88fa49e69f8e32bd2bf881fd7e57357\"&gt;V3 0324 Q2_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7rmqc55d2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5389b135a13c86ff471d38540909a7586e2282ff\"&gt;V3 0324 Q3_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yih3wq9e2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23fdbeaec51b4e226da035042bfcf80da5a5f4e9\"&gt;V3 0324 IQ4_XS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/teu0yiof2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e69256c7c5d098956ed1063c4bdb029aa9631ea\"&gt;V3 0324 Q8_0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So with a table format, from best to lower PPL:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size (GB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;BPW&lt;/th&gt;\n&lt;th align=\"left\"&gt;PPL&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;665.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.000&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2454&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ4_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;386.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.936&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2596&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ4_XS&lt;/td&gt;\n&lt;td align=\"left\"&gt;333.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.260&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2598&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_K_R4_Pure&lt;/td&gt;\n&lt;td align=\"left\"&gt;352.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.505&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2942&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;324.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.141&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3193&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q3_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;281.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.600&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3690&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q2_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;233.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.990&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.5264&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ2_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;226.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.889&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.5614&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ1_S_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;130.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.664&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.1292&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_XXS&lt;/td&gt;\n&lt;td align=\"left\"&gt;254.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.250&lt;/td&gt;\n&lt;td align=\"left\"&gt;NaN (failed)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;-----------------------------------------&lt;/p&gt;\n\n&lt;p&gt;Finally, a small comparison between R1 0528 and V3 0324&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779\"&gt;https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;-------------------------------------&lt;/p&gt;\n\n&lt;p&gt;So that&amp;#39;s all! Again, PPL is not in a indicator of everything, so take everything with a grain of salt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?auto=webp&amp;s=8216ca1903562c8f1a410d204c99985dbb7b3108",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=916df2f54c0cd22cd11ed7809b3c140706edfcee",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e1dbcdca4fa3782c38fd3d119431d344a540668",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7924ff592a988d2740e535ed06a0e5c6b9fe1ae5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34bb099dc9d99a1928035eecc7f6474b2da46ba7",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bdda7d98fc08d38df0a76903dbc3720bcf1ae1d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfaf0e47961251e127029f745bd8fda90457859c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lz1s8x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panchovix",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752435641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://eqbench.com/](https://eqbench.com/)\n\nWriting samples:\n\n[https://eqbench.com/results/creative-writing-v3/moonshotai\\_\\_Kimi-K2-Instruct.html](https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html)\n\nEQ-Bench responses:\n\n[https://eqbench.com/results/eqbench3\\_reports/moonshotai\\_\\_kimi-k2-instruct.html](https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html)",
          "author_fullname": "t2_pp9qh5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bltx3wip1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c91069e6404ebfdd239fca53b74ad27ebf361cb9"
                },
                {
                  "y": 221,
                  "x": 216,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=46558502029394a5450bb7a8b1fd7772f775b63e"
                },
                {
                  "y": 328,
                  "x": 320,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f8b74bea5bfacbcf65bd04fad681f91f30e63cb"
                },
                {
                  "y": 657,
                  "x": 640,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea6d2eccaab12c39f50549d82e4485fbfe48cf88"
                },
                {
                  "y": 985,
                  "x": 960,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f04df6fdbd708b96f6a89ab335a6247ac5c6fab"
                },
                {
                  "y": 1109,
                  "x": 1080,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f38d0ec7cc27f92b56854e865fea4d742abb0b6d"
                }
              ],
              "s": {
                "y": 1641,
                "x": 1598,
                "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=1598&amp;format=png&amp;auto=webp&amp;s=17f01c666d495066ceec9a5e3e74c86504144791"
              },
              "id": "bltx3wip1lcf1"
            },
            "slfo69pq1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 104,
                  "x": 108,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80dd9f9ea9f4c233481f726f596a93be59337d18"
                },
                {
                  "y": 208,
                  "x": 216,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7dc16b4166123dad30767234d268c2ba9cfe207b"
                },
                {
                  "y": 309,
                  "x": 320,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2aa653aee8a77a1c60f613c8eee527eaad88bd09"
                },
                {
                  "y": 618,
                  "x": 640,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=920c5b3b7ca71ba826946d79ece48662e7e61663"
                },
                {
                  "y": 928,
                  "x": 960,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7644b88db8a83e82b2c5386f3aafca392976d38"
                },
                {
                  "y": 1044,
                  "x": 1080,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dc43edacc6efbed8788f53ba27cc9baf10ea878"
                }
              ],
              "s": {
                "y": 1552,
                "x": 1605,
                "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=1605&amp;format=png&amp;auto=webp&amp;s=bc768dc8812c1370ff4a7130928bfb3b76a0bc2d"
              },
              "id": "slfo69pq1lcf1"
            },
            "7dy0n72s1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f769e577a79198993e84c8eb762589e7a68bfb8"
                },
                {
                  "y": 139,
                  "x": 216,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=86550c6ee8e16b8738d8dc52faa646a74726b22f"
                },
                {
                  "y": 206,
                  "x": 320,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4098fae6e966e2e07a7fe5fa68dc8f1e0842b77a"
                },
                {
                  "y": 413,
                  "x": 640,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4a695523e542b1689dbec502a3e44b4523f96dc"
                },
                {
                  "y": 620,
                  "x": 960,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=69f2c06f0306a5e4a0df7529218415f269ff0ecb"
                },
                {
                  "y": 697,
                  "x": 1080,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d1a2d4446112aab3c4639bd2b2bafcd744e7628"
                }
              ],
              "s": {
                "y": 1031,
                "x": 1596,
                "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=1596&amp;format=png&amp;auto=webp&amp;s=141d64df955b06e799e4f4c0b45e24ddb1211acd"
              },
              "id": "7dy0n72s1lcf1"
            },
            "46yhtq9r1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47a8d5e353281c77928ebe675d58f1a425091614"
                },
                {
                  "y": 221,
                  "x": 216,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dfbb994f47e60e6719c2c54e5a812ca2c8cdc38e"
                },
                {
                  "y": 328,
                  "x": 320,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5cb664395530a7771d865e53841a1040c702271"
                },
                {
                  "y": 657,
                  "x": 640,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c1174fd8b0b51963ad2c4fce60bc48d732f7da7"
                },
                {
                  "y": 986,
                  "x": 960,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16afcea1d3023190bbe776ef801cbf5ac2a7cc39"
                },
                {
                  "y": 1109,
                  "x": 1080,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b2ef583e66dfa8ed312dfce180f7388aaf630c8b"
                }
              ],
              "s": {
                "y": 1634,
                "x": 1590,
                "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=9dade8cde09a83f0bab6a44c0f0352e0c91ac03c"
              },
              "id": "46yhtq9r1lcf1"
            }
          },
          "name": "t3_1lylo75",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "transparent",
          "ups": 816,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "bltx3wip1lcf1",
                "id": 704803294
              },
              {
                "media_id": "slfo69pq1lcf1",
                "id": 704803295
              },
              {
                "media_id": "46yhtq9r1lcf1",
                "id": 704803296
              },
              {
                "media_id": "7dy0n72s1lcf1",
                "id": 704803297
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 816,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_mu9EQ2-CS-NLztYt8TCn8nhmS5cqsN6BOfAQW9BupA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":Llama:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/23w2nhjj1e9f1_t5_81eyvm/Llama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752386963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://eqbench.com/\"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Writing samples:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html\"&gt;https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;EQ-Bench responses:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html\"&gt;https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lylo75",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":Llama:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lylo75",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_sqrkl",
          "discussion_type": null,
          "num_comments": 171,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lylo75",
          "subreddit_subscribers": 499292,
          "created_utc": 1752386963,
          "num_crossposts": 5,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I come to you guys humbly seeking some help...\n\nI've been away from the local AI scene for about 6 months - it feels like an eternity - so much has changed!  I decided I wanted to get back into the game by building a new machine.  Because so much has changed, I'm having a hard time gauging what I will actually be able to do and what models I can run with my new specs.  \nAny ideas?  What would you guys recommend I try that would really take advantage of these specs?\nI understand that 32GB VRAM is not so special for LLMs, but I'm hoping that 256GB RAM might open up some interesting possibilities.\n\n|Category|Component(s)|\n|:-|:-|\n|**GPU**|ZOTAC GAMING GeForce RTX5090 AMPExtremeINFINITY (32GB VRAM)|\n|**CPU**|AMD Ryzen 9 9950X|\n|**Memory**|256GB (464GB) G.SKILL DDR5 6000MT/s|\n|**Motherboard**|MSI X870E Tomahawk WiFi|\n|**Storage**|1 2TB Samsung 9100 Pro Gen 5 NVMe (OS); 2 1TB Samsung 990 Pro Gen 4 NVMes *(1 as cache);* 2 14TB Seagate HDDs|\n|**PSU**|MSI 1300W Platinum|\n|**Cooler**|Arctic Freezer III 420 AIO|\n|**Case**|Geometric Future M5 *(glass panel version)*|\n|**Fans**|7 Lian Li SL Infinity + 1 TL LCD fan|\n\nI am of course interested in LLMs, but I'm also interested in trying out new image, video, music, speech, etc. models as well.  Can I run any of the new agentic models?  In terms of parameters, in general, what's my limit?\n\nThank you in advance!",
          "author_fullname": "t2_2v0asrl2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built an AI PC - what should I try out first?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m06lrz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752551531,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752548155,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come to you guys humbly seeking some help...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been away from the local AI scene for about 6 months - it feels like an eternity - so much has changed!  I decided I wanted to get back into the game by building a new machine.  Because so much has changed, I&amp;#39;m having a hard time gauging what I will actually be able to do and what models I can run with my new specs.&lt;br/&gt;\nAny ideas?  What would you guys recommend I try that would really take advantage of these specs?\nI understand that 32GB VRAM is not so special for LLMs, but I&amp;#39;m hoping that 256GB RAM might open up some interesting possibilities.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Category&lt;/th&gt;\n&lt;th align=\"left\"&gt;Component(s)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;ZOTAC GAMING GeForce RTX5090 AMPExtremeINFINITY (32GB VRAM)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 9 9950X&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;256GB (464GB) G.SKILL DDR5 6000MT/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;MSI X870E Tomahawk WiFi&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1 2TB Samsung 9100 Pro Gen 5 NVMe (OS); 2 1TB Samsung 990 Pro Gen 4 NVMes &lt;em&gt;(1 as cache);&lt;/em&gt; 2 14TB Seagate HDDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;MSI 1300W Platinum&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Cooler&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arctic Freezer III 420 AIO&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Geometric Future M5 &lt;em&gt;(glass panel version)&lt;/em&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Fans&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7 Lian Li SL Infinity + 1 TL LCD fan&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I am of course interested in LLMs, but I&amp;#39;m also interested in trying out new image, video, music, speech, etc. models as well.  Can I run any of the new agentic models?  In terms of parameters, in general, what&amp;#39;s my limit?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m06lrz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wesarnquist",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752548155,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im looking for LLMs to generate questions and answers from physics textbook chapters. The chapters Ill provide can be up to 10 pages long and may include images. Ive tried GPT, but the question quality is poor and often too similar to the examples I give. Claude didnt work either as it rejects the input file, saying its too large. Which LLM model would you recommend me to try next? It doesnt have to be free. ",
          "author_fullname": "t2_3g2onktn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which LLM should I use to generate high quality Q&amp;A from physics textbook chapters?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz81ea",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 27,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 27,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752451840,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking for LLMs to generate questions and answers from physics textbook chapters. The chapters Ill provide can be up to 10 pages long and may include images. Ive tried GPT, but the question quality is poor and often too similar to the examples I give. Claude didnt work either as it rejects the input file, saying its too large. Which LLM model would you recommend me to try next? It doesnt have to be free. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz81ea",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WhiteTentacle",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752451840,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Recently decided to try out openwebui and something i noticed is that it does no batching for embedding multiple files, and in the scale of 5000 files it feels like it will take the better part of 5 hours, i can write a tiny python script to embed all of these files (and view them in qdrant) in an amount of time that is light years ahead of whatever openwebui is doing, except openwebui cant use those for some reason.\n\nAny alternatives?\n\nI run everything locally through vllm, with qwen 4b embedding, qwen 0.6b reranker, and devstral",
          "author_fullname": "t2_9so78ol2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a better frontend than OpenWebui for RAG?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzna91",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently decided to try out openwebui and something i noticed is that it does no batching for embedding multiple files, and in the scale of 5000 files it feels like it will take the better part of 5 hours, i can write a tiny python script to embed all of these files (and view them in qdrant) in an amount of time that is light years ahead of whatever openwebui is doing, except openwebui cant use those for some reason.&lt;/p&gt;\n\n&lt;p&gt;Any alternatives?&lt;/p&gt;\n\n&lt;p&gt;I run everything locally through vllm, with qwen 4b embedding, qwen 0.6b reranker, and devstral&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzna91",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Capable-Ad-7494",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzna91/is_there_a_better_frontend_than_openwebui_for_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzna91/is_there_a_better_frontend_than_openwebui_for_rag/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752501919,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to create a new AI agent for my MySQL database. The database tables are complex and require extensive documentation to make them understandable for the AI to query effectively. \n\nI need guidance on selecting the right model and framework for this project.",
          "author_fullname": "t2_13usrk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions for ai agent framework and ai model for Text-to-SQL ai agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzn9th",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to create a new AI agent for my MySQL database. The database tables are complex and require extensive documentation to make them understandable for the AI to query effectively. &lt;/p&gt;\n\n&lt;p&gt;I need guidance on selecting the right model and framework for this project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzn9th",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "M7mDSa3eD_",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzn9th/suggestions_for_ai_agent_framework_and_ai_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzn9th/suggestions_for_ai_agent_framework_and_ai_model/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752501889,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been considering an MI50 32gb for a budget AI desktop for a while. \n\nAn issue I found was that it does not natively support display output.\n\nBut I found a version with the Radeon pro viis bios flashed onto it that should allow it to output to a display.\n\nAnother issue was that it doesnt actually have a fan, relying on other fans to blow air through it.\n\nBut the frankenversion I found also has a fan installed on it\n\nAnd all of that I found for roughly 170 USD\n\nIll probably install it into a ryzen 5 3600 system with another 32gb of cheap ddr4 ram that costs about another 150.\n\nAny major issues with this build? \n\nIll probably install Ubuntu just to try it out, Ive always been a windows user but shouldnt hurt to try. Always wanted to try out Linux anyways but never had a spare pc to work with.",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MI50 32GB with bios flash",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzijk2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752487768,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been considering an MI50 32gb for a budget AI desktop for a while. &lt;/p&gt;\n\n&lt;p&gt;An issue I found was that it does not natively support display output.&lt;/p&gt;\n\n&lt;p&gt;But I found a version with the Radeon pro viis bios flashed onto it that should allow it to output to a display.&lt;/p&gt;\n\n&lt;p&gt;Another issue was that it doesnt actually have a fan, relying on other fans to blow air through it.&lt;/p&gt;\n\n&lt;p&gt;But the frankenversion I found also has a fan installed on it&lt;/p&gt;\n\n&lt;p&gt;And all of that I found for roughly 170 USD&lt;/p&gt;\n\n&lt;p&gt;Ill probably install it into a ryzen 5 3600 system with another 32gb of cheap ddr4 ram that costs about another 150.&lt;/p&gt;\n\n&lt;p&gt;Any major issues with this build? &lt;/p&gt;\n\n&lt;p&gt;Ill probably install Ubuntu just to try it out, Ive always been a windows user but shouldnt hurt to try. Always wanted to try out Linux anyways but never had a spare pc to work with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzijk2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzijk2/mi50_32gb_with_bios_flash/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzijk2/mi50_32gb_with_bios_flash/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752487768,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/3hmduf8iqycf1.png?width=622&amp;format=png&amp;auto=webp&amp;s=4e4c7a312622fc82888942765dabdfdfafac3da0\n\nI asked it to answer the following questions in Chinese, and then it started to give random answers. This is the first sentence of the conversation, with no other context.\n\nhttps://preview.redd.it/i9h2w44xqycf1.png?width=641&amp;format=png&amp;auto=webp&amp;s=29facf41746eedd19e3a2d7ba3882e243eda6af9\n\nAfter I switched to another account, it was normal. This means that this bug may affect some accounts.",
          "author_fullname": "t2_d3gzv5gz3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gemini 2.5 pro bug",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "i9h2w44xqycf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 143,
                  "x": 108,
                  "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bcadda293561a3d64a15574c08a5a2d731a3a48e"
                },
                {
                  "y": 287,
                  "x": 216,
                  "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f018f1323fd67fc628f65007c8522d459322d42b"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5da995ca8f22088b7b7eb53b2c976ca78ef33b8"
                },
                {
                  "y": 852,
                  "x": 640,
                  "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d60027629fa8a5c76013ae446bbb8d17ab4dbc9c"
                }
              ],
              "s": {
                "y": 854,
                "x": 641,
                "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=641&amp;format=png&amp;auto=webp&amp;s=29facf41746eedd19e3a2d7ba3882e243eda6af9"
              },
              "id": "i9h2w44xqycf1"
            },
            "3hmduf8iqycf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 145,
                  "x": 108,
                  "u": "https://preview.redd.it/3hmduf8iqycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=314def2998222f5467b9fe72d0f39c16829c61b8"
                },
                {
                  "y": 290,
                  "x": 216,
                  "u": "https://preview.redd.it/3hmduf8iqycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08c03fefb52fe11a7e98d2e42de5244c9043e86c"
                },
                {
                  "y": 430,
                  "x": 320,
                  "u": "https://preview.redd.it/3hmduf8iqycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e28a507b8efdee8869ecbe7d6e86d5a87ce5698b"
                }
              ],
              "s": {
                "y": 837,
                "x": 622,
                "u": "https://preview.redd.it/3hmduf8iqycf1.png?width=622&amp;format=png&amp;auto=webp&amp;s=4e4c7a312622fc82888942765dabdfdfafac3da0"
              },
              "id": "3hmduf8iqycf1"
            }
          },
          "name": "t3_1m084lw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.27,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lrj24VlfWH6haK38O7gsD62HaWCdJH_pmMTnUyCagPg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752552836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/3hmduf8iqycf1.png?width=622&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e4c7a312622fc82888942765dabdfdfafac3da0\"&gt;https://preview.redd.it/3hmduf8iqycf1.png?width=622&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e4c7a312622fc82888942765dabdfdfafac3da0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I asked it to answer the following questions in Chinese, and then it started to give random answers. This is the first sentence of the conversation, with no other context.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/i9h2w44xqycf1.png?width=641&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29facf41746eedd19e3a2d7ba3882e243eda6af9\"&gt;https://preview.redd.it/i9h2w44xqycf1.png?width=641&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29facf41746eedd19e3a2d7ba3882e243eda6af9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After I switched to another account, it was normal. This means that this bug may affect some accounts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m084lw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JuggernautUpbeat3547",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m084lw/gemini_25_pro_bug/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m084lw/gemini_25_pro_bug/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752552836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm releasing a new version of my [audiobook creator app](https://github.com/prakharsr/audiobook-creator) which now supports Kokoro and Orpheus. This release adds support for [Orpheus TTS](https://github.com/canopyai/Orpheus-TTS) which supports high-quality audio and more expressive speech. This version also adds support for adding emotion tags automatically using an LLM. Audio generation using Orpheus is done using my dedicated [Orpheus TTS FastAPI Server](https://github.com/prakharsr/Orpheus-TTS-FastAPI) repository.\n\nListen to a sample audiobook generated using this app: [https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus](https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus)\n\nApp Features:\n\n* **Advanced TTS Engine Support**: Seamlessly switch between Kokoro and Orpheus TTS engines via environment configuration\n* **Async Parallel Processing**: Optimized for concurrent request handling with significant performance improvements and faster audiobook generation.\n* **Gradio UI App**: Create audiobooks easily with an easy to use, intuitive UI made with Gradio.\n* **M4B Audiobook Creation**: Creates compatible audiobooks with covers, metadata, chapter timestamps etc. in M4B format.\n* **Multi-Format Input Support**: Converts books from various formats (EPUB, PDF, etc.) into plain text.\n* **Multi-Format Output Support**: Supports various output formats: AAC, M4A, MP3, WAV, OPUS, FLAC, PCM, M4B.\n* **Docker Support**: Use pre-built docker images/ build using docker compose to save time and for a smooth user experience.\n* **Emotion Tags Addition**: Emotion tags which are supported in Orpheus TTS can be added to the book's text intelligently using an LLM to enhance character voice expression.\n* **Character Identification**: Identifies characters and infers their attributes (gender, age) using advanced NLP techniques and LLMs.\n* **Customizable Audiobook Narration**: Supports single-voice or multi-voice narration with narrator gender preference for enhanced listening experiences.\n* **Progress Tracking**: Includes progress bars and execution time measurements for efficient monitoring.\n* **Open Source**: Licensed under GPL v3.\n\nCheckout the Audiobook Creator Repo here: [https://github.com/prakharsr/audiobook-creator](https://github.com/prakharsr/audiobook-creator)\n\nLet me know how the audiobooks sound and if you like the app :)",
          "author_fullname": "t2_hi3epx7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Audiobook Creator - v1.4 - Added support for Orpheus along with Kokoro",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyw5u2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 112,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 112,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752421959,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m releasing a new version of my &lt;a href=\"https://github.com/prakharsr/audiobook-creator\"&gt;audiobook creator app&lt;/a&gt; which now supports Kokoro and Orpheus. This release adds support for &lt;a href=\"https://github.com/canopyai/Orpheus-TTS\"&gt;Orpheus TTS&lt;/a&gt; which supports high-quality audio and more expressive speech. This version also adds support for adding emotion tags automatically using an LLM. Audio generation using Orpheus is done using my dedicated &lt;a href=\"https://github.com/prakharsr/Orpheus-TTS-FastAPI\"&gt;Orpheus TTS FastAPI Server&lt;/a&gt; repository.&lt;/p&gt;\n\n&lt;p&gt;Listen to a sample audiobook generated using this app: &lt;a href=\"https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus\"&gt;https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;App Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Advanced TTS Engine Support&lt;/strong&gt;: Seamlessly switch between Kokoro and Orpheus TTS engines via environment configuration&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Optimized for concurrent request handling with significant performance improvements and faster audiobook generation.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gradio UI App&lt;/strong&gt;: Create audiobooks easily with an easy to use, intuitive UI made with Gradio.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;M4B Audiobook Creation&lt;/strong&gt;: Creates compatible audiobooks with covers, metadata, chapter timestamps etc. in M4B format.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multi-Format Input Support&lt;/strong&gt;: Converts books from various formats (EPUB, PDF, etc.) into plain text.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multi-Format Output Support&lt;/strong&gt;: Supports various output formats: AAC, M4A, MP3, WAV, OPUS, FLAC, PCM, M4B.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Docker Support&lt;/strong&gt;: Use pre-built docker images/ build using docker compose to save time and for a smooth user experience.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Emotion Tags Addition&lt;/strong&gt;: Emotion tags which are supported in Orpheus TTS can be added to the book&amp;#39;s text intelligently using an LLM to enhance character voice expression.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Character Identification&lt;/strong&gt;: Identifies characters and infers their attributes (gender, age) using advanced NLP techniques and LLMs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Customizable Audiobook Narration&lt;/strong&gt;: Supports single-voice or multi-voice narration with narrator gender preference for enhanced listening experiences.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Includes progress bars and execution time measurements for efficient monitoring.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Licensed under GPL v3.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Checkout the Audiobook Creator Repo here: &lt;a href=\"https://github.com/prakharsr/audiobook-creator\"&gt;https://github.com/prakharsr/audiobook-creator&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know how the audiobooks sound and if you like the app :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?auto=webp&amp;s=8dbdf77b998c2a437d59906bf87790938f4fbabf",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f5d797fded6eb1be79f568494cf1d46669cab00",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=28a2d439698d177ba57f4500a952608a6b94f98e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29ec3cca77e1d909a8119f3c0f5a41250a70bb40",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7040e5d6ed60ffca60c3256cbabfb1663f065b5a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=19b88df8422100e720ad41fa74806597ac879879",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=154b520599f685ee02d84d0900386f6fdc48eb30",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyw5u2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "prakharsr",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752421959,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kcu2kx4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's up with the weird OR provider prices, they make no sense at all.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m040ag",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.35,
          "author_flair_background_color": "#bbbdbf",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/BTgNwC2MAuzD7_uRpxqg2qYITg3XP4whbMqujeHwerQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Ollama"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752540838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/kk8hpolkrxcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?auto=webp&amp;s=328d40df14f98bdc2ea2574f32636490f18e4099",
                  "width": 2148,
                  "height": 1270
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc2ae195e1c6e48ec0a3481a2a73fbe283e00209",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d2fa9154d95f2b0e92e8fcb6360558fa9dad160",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9c3c0ca9545def88b414395a661a6058c5bc1e3",
                    "width": 320,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b779193285fa57a7bb13cbc624328e0dd953bd39",
                    "width": 640,
                    "height": 378
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e190981a3394704c400b906f150297921dd157cd",
                    "width": 960,
                    "height": 567
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e3a211d8fa02c54f25b2f09f8ed9f6ee4eeb3ec",
                    "width": 1080,
                    "height": 638
                  }
                ],
                "variants": {},
                "id": "fc_weVym9QErbXNEqlDfqEcn7ZX18CWTEE0AAOZablg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Ollama",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m040ag",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Specter_Origin",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m040ag/whats_up_with_the_weird_or_provider_prices_they/",
          "stickied": false,
          "url": "https://i.redd.it/kk8hpolkrxcf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752540838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.\n\nQ: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?",
          "author_fullname": "t2_1fp7huwh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can VRAM be combined of 2 brands",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lze20x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752470433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.&lt;/p&gt;\n\n&lt;p&gt;Q: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lze20x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tonyleungnl",
          "discussion_type": null,
          "num_comments": 85,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752470433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\*\\*\\[XTTS v2\\] Por qu algunas voces suenan bien y otras fallan al entrenar?\\*\\*\n\n\n\nHola, estoy experimentando con XTTS v2 (Coqui) para clonar voces personalizadas.\n\n\n\nHe notado lo siguiente:\n\n\n\n\\- Si entreno con mi propia voz grabada, el modelo suena bien.\n\n\\- Si uso audios de ejemplo del modelo (como \\`female.wav\\`), tambin se escucha perfecto.\n\n\\- Pero si intento entrenar con voces extradas de juegos o generadas por ElevenLabs, aunque el formato sea correcto (44100 Hz, mono, PCM 16-bit), el resultado suena roto o distorsionado.\n\n\n\nYa prob:\n\n\\- Normalizar el RMS a \\~0.13\n\n\\- Convertir a mono y 44100 Hz\n\n\\- Aplicar filtros de audio (lowpass, highpass, dynaudnorm)\n\n\\- Rehumanizar con reverberacin y reduccin de ruido\n\n\n\n\\*\\*Mi duda es:\\*\\*  \n\nXTTS tiene problemas al aprender desde voces sintticas o muy procesadas? Qu propiedades del audio hacen que el modelo no pueda generalizar bien?\n\n\n\nAlguien ha logrado entrenar XTTS con voces de juegos o clonadas por TTS comerciales?\n\n\n\nGracias de antemano!\n\n\n\n\\---\n\n\n\n\\*\\*Contexto tcnico:\\*\\*\n\n\\- Modelo: XTTS v2 desde \\`xtts-finetune-webui\\`\n\n\\- Duracin dataset: 510 minutos\n\n\\- Formato: \\`.wav\\` 44100 Hz, mono, PCM 16-bit\n\n",
          "author_fullname": "t2_5ss9lb1e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[XTTS v2] Por qu algunas voces suenan bien y otras fallan al entrenar?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m07tkl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.18,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752551868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;**[XTTS v2] Por qu algunas voces suenan bien y otras fallan al entrenar?**&lt;/p&gt;\n\n&lt;p&gt;Hola, estoy experimentando con XTTS v2 (Coqui) para clonar voces personalizadas.&lt;/p&gt;\n\n&lt;p&gt;He notado lo siguiente:&lt;/p&gt;\n\n&lt;p&gt;- Si entreno con mi propia voz grabada, el modelo suena bien.&lt;/p&gt;\n\n&lt;p&gt;- Si uso audios de ejemplo del modelo (como `female.wav`), tambin se escucha perfecto.&lt;/p&gt;\n\n&lt;p&gt;- Pero si intento entrenar con voces extradas de juegos o generadas por ElevenLabs, aunque el formato sea correcto (44100 Hz, mono, PCM 16-bit), el resultado suena roto o distorsionado.&lt;/p&gt;\n\n&lt;p&gt;Ya prob:&lt;/p&gt;\n\n&lt;p&gt;- Normalizar el RMS a ~0.13&lt;/p&gt;\n\n&lt;p&gt;- Convertir a mono y 44100 Hz&lt;/p&gt;\n\n&lt;p&gt;- Aplicar filtros de audio (lowpass, highpass, dynaudnorm)&lt;/p&gt;\n\n&lt;p&gt;- Rehumanizar con reverberacin y reduccin de ruido&lt;/p&gt;\n\n&lt;p&gt;**Mi duda es:**  &lt;/p&gt;\n\n&lt;p&gt;XTTS tiene problemas al aprender desde voces sintticas o muy procesadas? Qu propiedades del audio hacen que el modelo no pueda generalizar bien?&lt;/p&gt;\n\n&lt;p&gt;Alguien ha logrado entrenar XTTS con voces de juegos o clonadas por TTS comerciales?&lt;/p&gt;\n\n&lt;p&gt;Gracias de antemano!&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;**Contexto tcnico:**&lt;/p&gt;\n\n&lt;p&gt;- Modelo: XTTS v2 desde `xtts-finetune-webui`&lt;/p&gt;\n\n&lt;p&gt;- Duracin dataset: 510 minutos&lt;/p&gt;\n\n&lt;p&gt;- Formato: `.wav` 44100 Hz, mono, PCM 16-bit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m07tkl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Blitzo_45",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m07tkl/xtts_v2_por_qu_algunas_voces_suenan_bien_y_otras/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m07tkl/xtts_v2_por_qu_algunas_voces_suenan_bien_y_otras/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752551868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im working on a small AI app called **Preceptor**  think of it like a privacy-first accountability partner that helps you stay focused **without spying on your screen**\n\nHeres the idea:\n\n* It runs **entirely offline**, using local LLMs via [Ollama](https://ollama.com/)\n* Tracks **which app or browser tab** youre on (via local system APIs + a lightweight browser extension)\n* Compares that with your focus goals (e.g., write more, avoid Reddit)\n* And gives you **gentle nudges** when you drift\n\nEven with small-ish models (e.g. LLaMA 3 8B or Mistral via Ollama), Im hitting response time issues. It might only be 13 seconds to generate a short message, but in a flow-focused app, that pause breaks the vibe. It's not just about speed but it's also about *feeling instant*. With mistral 7b , which produces a good nudge message but takes like 30 seconds for the api to call\n\nHow should i go with this?\n\nIf you want to join the waitlist for the app , comment and i will reply with the link. I want to make this less of a promotion post as i am seeking serious suggestions",
          "author_fullname": "t2_18z668t0lo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a Focus App with Local LLMs  But Latency Is a Real Challenge , seeking suggestions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzwps3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752522956,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im working on a small AI app called &lt;strong&gt;Preceptor&lt;/strong&gt;  think of it like a privacy-first accountability partner that helps you stay focused &lt;strong&gt;without spying on your screen&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Heres the idea:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It runs &lt;strong&gt;entirely offline&lt;/strong&gt;, using local LLMs via &lt;a href=\"https://ollama.com/\"&gt;Ollama&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Tracks &lt;strong&gt;which app or browser tab&lt;/strong&gt; youre on (via local system APIs + a lightweight browser extension)&lt;/li&gt;\n&lt;li&gt;Compares that with your focus goals (e.g., write more, avoid Reddit)&lt;/li&gt;\n&lt;li&gt;And gives you &lt;strong&gt;gentle nudges&lt;/strong&gt; when you drift&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Even with small-ish models (e.g. LLaMA 3 8B or Mistral via Ollama), Im hitting response time issues. It might only be 13 seconds to generate a short message, but in a flow-focused app, that pause breaks the vibe. It&amp;#39;s not just about speed but it&amp;#39;s also about &lt;em&gt;feeling instant&lt;/em&gt;. With mistral 7b , which produces a good nudge message but takes like 30 seconds for the api to call&lt;/p&gt;\n\n&lt;p&gt;How should i go with this?&lt;/p&gt;\n\n&lt;p&gt;If you want to join the waitlist for the app , comment and i will reply with the link. I want to make this less of a promotion post as i am seeking serious suggestions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzwps3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Frosty-Cap-4282",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752522956,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am running Gemma 3 12B on my local computer. My prompt is about 1000 tokens of text + 3-4 images. My computer is just a regular AMD CPU (no GPU) + 64GB of DDR5 RAM, so understandably the response is slow. Particularly I have noticed that it takes more time to just process my input.\n\nMy question is what hardware would help improve this:  \n1. Obviously a GPU would help - but what should I look for in a GPU to get better response times?  \n2. Would the newer AMD Ryzen AI 9 HX 370 APU help or would I need to go for an AMD Ryzen AI Max+ 395 APU's?  \n3. If I got for the AMD Ryzen AI 9 HX 370 APU, some PCs come with upgradeable RAM i.e. DDR5 (going up to 96GB), while others come with faster LPDDR5 RAM - but with the caveat that the max RAM is capped at 64 GB. I want to be able to run slightly larger models on it (e.g. Gemma 3 27B), but not sure if I need to go for the LPDDR5x versions.",
          "author_fullname": "t2_1kusyf1nll",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to improve response times for multimodal requests?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzqh66",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752509218,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running Gemma 3 12B on my local computer. My prompt is about 1000 tokens of text + 3-4 images. My computer is just a regular AMD CPU (no GPU) + 64GB of DDR5 RAM, so understandably the response is slow. Particularly I have noticed that it takes more time to just process my input.&lt;/p&gt;\n\n&lt;p&gt;My question is what hardware would help improve this:&lt;br/&gt;\n1. Obviously a GPU would help - but what should I look for in a GPU to get better response times?&lt;br/&gt;\n2. Would the newer AMD Ryzen AI 9 HX 370 APU help or would I need to go for an AMD Ryzen AI Max+ 395 APU&amp;#39;s?&lt;br/&gt;\n3. If I got for the AMD Ryzen AI 9 HX 370 APU, some PCs come with upgradeable RAM i.e. DDR5 (going up to 96GB), while others come with faster LPDDR5 RAM - but with the caveat that the max RAM is capped at 64 GB. I want to be able to run slightly larger models on it (e.g. Gemma 3 27B), but not sure if I need to go for the LPDDR5x versions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzqh66",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "coolahavoc",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzqh66/how_to_improve_response_times_for_multimodal/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzqh66/how_to_improve_response_times_for_multimodal/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752509218,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:\n\nWould it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.\n\nThe card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?\n\nBecause utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don't know how much that matters...",
          "author_fullname": "t2_3ogjqne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multiple 5060 Ti's",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzkcg3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752493925,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:&lt;/p&gt;\n\n&lt;p&gt;Would it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.&lt;/p&gt;\n\n&lt;p&gt;The card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?&lt;/p&gt;\n\n&lt;p&gt;Because utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don&amp;#39;t know how much that matters...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzkcg3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "snorixx",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752493925,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " ",
          "author_fullname": "t2_aqf3pllb4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": " ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0auae",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752562375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m0auae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Horror-Cartoonist-81",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0auae/_/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0auae/_/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752562375,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "as title says, lm studio always uses my cpu, I want to make lm uses the GPU tried several changes\n\nLaptop specs\n\n24gb ram  \n3070 8gb ram  \ni9-11 gen\n\ni cant seem to use gpu as main resource for llama in lmstudio\n\n[settings - hardware](https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1)\n\n[settings - runtime](https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;format=png&amp;auto=webp&amp;s=e59c270eef7d301d1bda499756496939c67e4099)\n\n  \n  \nthings I did  \nupdated GPU  \nruntime engine is set to  cuda 12 llama cpp  \ntried several changes in GPU offload, from maximum  to half,  \ntried changes CPU thread pool  \ntried changes in context length\n\nupon testing cpu usage spikes, while my GPU sits idle, only works if my cpu reach 70% above then gpu starts working\n\nthe models I used are :  \nopen hermes  2.5 mistral 7b  \nOpenOrca Platypus2 13B q 4 k s  \nMythomax L2 13b q4 k s",
          "author_fullname": "t2_1is18ejy13",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM Studio cant use my gpu as main",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "v0fsjfwkuucf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=28304eefb1585ed0b0b966bf3bb044f06d87ac3e"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5103ec1903662ebf541200809894b7f0050373e3"
                },
                {
                  "y": 203,
                  "x": 320,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bce56794d9d7b1d50bb3dbf07cad90b792451e52"
                },
                {
                  "y": 407,
                  "x": 640,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=160bb93aa199368b865ae1a29783e2bb3c049631"
                },
                {
                  "y": 611,
                  "x": 960,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3bd36193de71a6a4172c9362db5b1efe55d5237a"
                },
                {
                  "y": 687,
                  "x": 1080,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba101eaf58a755a88b279ec499ef4ed1665d311f"
                }
              ],
              "s": {
                "y": 781,
                "x": 1226,
                "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1"
              },
              "id": "v0fsjfwkuucf1"
            },
            "75o3tq0z5vcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=29cc677c8b11701bbd55ae946c097f48daf97e30"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb3f3c30a765ae1b8b0e2269137b6da98723141d"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=826ee8bb5ea9d079adee3ee3db96ac8ceca3e83b"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ef3e82b55da1f94203265ee9541b4251393f647"
                },
                {
                  "y": 631,
                  "x": 960,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=65c28555b2d9a4466aaf66240eb018987ac511f7"
                },
                {
                  "y": 710,
                  "x": 1080,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce63d924b98ef3795f1bb676a62cb3df1738eddf"
                }
              ],
              "s": {
                "y": 810,
                "x": 1232,
                "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;format=png&amp;auto=webp&amp;s=e59c270eef7d301d1bda499756496939c67e4099"
              },
              "id": "75o3tq0z5vcf1"
            }
          },
          "name": "t3_1lzoxbl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aO_NeeS70DPNibAf4xpd1tv7HSDOB7yzatP7amRYFhA.jpg",
          "edited": 1752509331,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752505748,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as title says, lm studio always uses my cpu, I want to make lm uses the GPU tried several changes&lt;/p&gt;\n\n&lt;p&gt;Laptop specs&lt;/p&gt;\n\n&lt;p&gt;24gb ram&lt;br/&gt;\n3070 8gb ram&lt;br/&gt;\ni9-11 gen&lt;/p&gt;\n\n&lt;p&gt;i cant seem to use gpu as main resource for llama in lmstudio&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1\"&gt;settings - hardware&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e59c270eef7d301d1bda499756496939c67e4099\"&gt;settings - runtime&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;things I did&lt;br/&gt;\nupdated GPU&lt;br/&gt;\nruntime engine is set to  cuda 12 llama cpp&lt;br/&gt;\ntried several changes in GPU offload, from maximum  to half,&lt;br/&gt;\ntried changes CPU thread pool&lt;br/&gt;\ntried changes in context length&lt;/p&gt;\n\n&lt;p&gt;upon testing cpu usage spikes, while my GPU sits idle, only works if my cpu reach 70% above then gpu starts working&lt;/p&gt;\n\n&lt;p&gt;the models I used are :&lt;br/&gt;\nopen hermes  2.5 mistral 7b&lt;br/&gt;\nOpenOrca Platypus2 13B q 4 k s&lt;br/&gt;\nMythomax L2 13b q4 k s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzoxbl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zinxdia",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzoxbl/lm_studio_cant_use_my_gpu_as_main/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzoxbl/lm_studio_cant_use_my_gpu_as_main/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752505748,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Got tired of waiting for k2 ggufs and found this guy:  \n[https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main](https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main)\n\nThere is a typo in the commands but it seems to work great, and really easy to get going:  \npip install ftllm  \nftllm server fastllm/Kimi-K2-Instruct-INT4MIX -t 40\n\nand just like that I'm getting 7-10T/s on my 5090 + DDR5 Xeon machine",
          "author_fullname": "t2_9hl4ymvj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Never seen fastllm mentioned here, anyone using it? (kimi k2 local)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyyhwz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752428193,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752427662,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got tired of waiting for k2 ggufs and found this guy:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main\"&gt;https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There is a typo in the commands but it seems to work great, and really easy to get going:&lt;br/&gt;\npip install ftllm&lt;br/&gt;\nftllm server fastllm/Kimi-K2-Instruct-INT4MIX -t 40&lt;/p&gt;\n\n&lt;p&gt;and just like that I&amp;#39;m getting 7-10T/s on my 5090 + DDR5 Xeon machine&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?auto=webp&amp;s=bae6f5c013ad93b7ca44d907a27215b9cd031d97",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ae33582b4b0826302a3cd3ed7609af7df200f8d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c75bfa0830517e198d439f93aa7ff4a96c4340e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eca931e08f88125c81e49d17938fac710ad11893",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6743d4fea5b1dddfd1c329d591498a0f9454d56",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b4b0fb1bec8773b706d6c25c2eff96884f1c19a3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f63241ebc2da005589f7190f60055879ec108c46",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyyhwz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Conscious_Cut_6144",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752427662,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As promised in the banana thread. OP delivers.\n\n**Benchmarks**\n\nThe following benchmarks were taken using official Qwen3 models from Huggingface's Qwen repo for consistency:\n\nMoE:\n\n* Qwen3 235B A22B GPTQ Int4 quant in Tensor Parallel\n* Qwen3 30B A3B BF16 in Tensor Parallel\n* Qwen3 30B A3B BF16 on a single GPU\n* Qwen3 30B A3B GPTQ Int4 quant in Tensor Parallel\n* Qwen3 30B A3B GPTQ Int4 quant on a single GPU\n\nDense:\n\n* Qwen3 32B BF16 on a single GPU\n* Qwen3 32B BF16 in Tensor Parallel\n* Qwen3 14B BF16 on a single GPU\n* Qwen3 14B BF16 in Tensor Parallel\n\nAll benchmarking was done with `vllm bench throughput ...` using full context space of 32k and incrementing the number of input tokens through the tests. The 235B benchmarks were performed with input lengths of 1024, 4096, 8192, and 16384 tokens. In the name of expediency the remaining tests were performed with input lengths of 1024 and 4096 due to the scaling factors seeming to approximate well with the 235B model.\n\n**Hardware**\n\n2x Blackwell PRO 6000 Workstation GPUs, 1x EPYC 9745, ~~512GB~~ 768GB DDR5 5200 MT/s, PCIe 5.0 x16.\n\n**Software**\n\n* Ubuntu 24.04.2\n* NVidia drivers 575.57.08\n* CUDA 12.9\n\nThis was the magic Torch incantation that got everything working: \n    \n    pip install --pre torch==2.9.0.dev20250707+cu128 torchvision==0.24.0.dev20250707+cu128 torchaudio==2.8.0.dev20250707+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128\n\nOtherwise these instructions worked well despite being for WSL: https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3\n\n\n**MoE Results**\n\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 1k input**\n\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\n    Throughput: 5.03 requests/s, 5781.20 total tokens/s, 643.67 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 4k input**\n\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\n    Throughput: 1.34 requests/s, 5665.37 total tokens/s, 171.87 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 8k input**\n\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 8192\n    Throughput: 0.65 requests/s, 5392.17 total tokens/s, 82.98 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000\n\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 16k input**\n\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 16384\n    Throughput: 0.30 requests/s, 4935.38 total tokens/s, 38.26 output tokens/s\n    Total num prompt tokens:  16383966\n    Total num output tokens:  128000\n\n\n\n\n**Qwen3 30B A3B (Qwen official FP16) @ 1k input | tensor parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 1024\n    Throughput: 11.27 requests/s, 12953.87 total tokens/s, 1442.27 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B (Qwen official FP16) @ 4k input | tensor parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 4096\n    Throughput: 5.13 requests/s, 21651.80 total tokens/s, 656.86 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B (Qwen official FP16) @ 1k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 1024\n    Throughput: 13.32 requests/s, 15317.81 total tokens/s, 1705.46 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B (Qwen official FP16) @ 4k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 4096\n    Throughput: 3.89 requests/s, 16402.36 total tokens/s, 497.61 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n\n\n\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | tensor parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\n    Throughput: 23.17 requests/s, 26643.04 total tokens/s, 2966.40 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B FP16 (Qwen official GPTQ Int4) @ 4k input | tensor parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\n    Throughput: 5.03 requests/s, 21229.35 total tokens/s, 644.04 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n\n\n\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 1024\n    Throughput: 17.44 requests/s, 20046.60 total tokens/s, 2231.96 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 4k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 4096\n    Throughput: 4.21 requests/s, 17770.35 total tokens/s, 539.11 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Dense Model Results**\n\n**Qwen3 32B BF16 @ 1k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024\n    Throughput: 2.87 requests/s, 3297.05 total tokens/s, 367.09 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 32B BF16 @ 4k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096\n    Throughput: 0.77 requests/s, 3259.23 total tokens/s, 98.88 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 32B BF16 @ 8k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192\n    Throughput: 0.37 requests/s, 3069.56 total tokens/s, 47.24 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000\n\n\n**Qwen3 32B BF16 @ 1k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024 --tensor-parallel 2\n    Throughput: 5.18 requests/s, 5957.00 total tokens/s, 663.24 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 32B BF16 @ 4k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \n    Throughput: 1.44 requests/s, 6062.84 total tokens/s, 183.93 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 32B BF16 @ 8k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \n    Throughput: 0.70 requests/s, 5806.52 total tokens/s, 89.36 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 1k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024\n    Throughput: 7.26 requests/s, 8340.89 total tokens/s, 928.66 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 4k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096\n    Throughput: 2.00 requests/s, 8426.05 total tokens/s, 255.62 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 8k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192\n    Throughput: 0.97 requests/s, 8028.90 total tokens/s, 123.56 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 1k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024 --tensor-parallel 2 \n    Throughput: 10.68 requests/s, 12273.33 total tokens/s, 1366.50 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 4k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \n    Throughput: 2.88 requests/s, 12140.81 total tokens/s, 368.32 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 8k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \n    Throughput: 1.45 requests/s, 12057.89 total tokens/s, 185.56 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000",
          "author_fullname": "t2_1t7r9dkpud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmarking Qwen3 30B and 235B on dual RTX PRO 6000 Blackwell Workstation Edition",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyxf1f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 68,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 68,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752473261,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752425040,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As promised in the banana thread. OP delivers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The following benchmarks were taken using official Qwen3 models from Huggingface&amp;#39;s Qwen repo for consistency:&lt;/p&gt;\n\n&lt;p&gt;MoE:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 235B A22B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B BF16 in Tensor Parallel&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B BF16 on a single GPU&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant on a single GPU&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Dense:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 32B BF16 on a single GPU&lt;/li&gt;\n&lt;li&gt;Qwen3 32B BF16 in Tensor Parallel&lt;/li&gt;\n&lt;li&gt;Qwen3 14B BF16 on a single GPU&lt;/li&gt;\n&lt;li&gt;Qwen3 14B BF16 in Tensor Parallel&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All benchmarking was done with &lt;code&gt;vllm bench throughput ...&lt;/code&gt; using full context space of 32k and incrementing the number of input tokens through the tests. The 235B benchmarks were performed with input lengths of 1024, 4096, 8192, and 16384 tokens. In the name of expediency the remaining tests were performed with input lengths of 1024 and 4096 due to the scaling factors seeming to approximate well with the 235B model.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;2x Blackwell PRO 6000 Workstation GPUs, 1x EPYC 9745, &lt;del&gt;512GB&lt;/del&gt; 768GB DDR5 5200 MT/s, PCIe 5.0 x16.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ubuntu 24.04.2&lt;/li&gt;\n&lt;li&gt;NVidia drivers 575.57.08&lt;/li&gt;\n&lt;li&gt;CUDA 12.9&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This was the magic Torch incantation that got everything working: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pip install --pre torch==2.9.0.dev20250707+cu128 torchvision==0.24.0.dev20250707+cu128 torchaudio==2.8.0.dev20250707+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Otherwise these instructions worked well despite being for WSL: &lt;a href=\"https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3\"&gt;https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;MoE Results&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 1k input&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\nThroughput: 5.03 requests/s, 5781.20 total tokens/s, 643.67 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 4k input&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\nThroughput: 1.34 requests/s, 5665.37 total tokens/s, 171.87 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 8k input&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 8192\nThroughput: 0.65 requests/s, 5392.17 total tokens/s, 82.98 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 16k input&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 16384\nThroughput: 0.30 requests/s, 4935.38 total tokens/s, 38.26 output tokens/s\nTotal num prompt tokens:  16383966\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 1024\nThroughput: 11.27 requests/s, 12953.87 total tokens/s, 1442.27 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 4096\nThroughput: 5.13 requests/s, 21651.80 total tokens/s, 656.86 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 1024\nThroughput: 13.32 requests/s, 15317.81 total tokens/s, 1705.46 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 4096\nThroughput: 3.89 requests/s, 16402.36 total tokens/s, 497.61 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\nThroughput: 23.17 requests/s, 26643.04 total tokens/s, 2966.40 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B FP16 (Qwen official GPTQ Int4) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\nThroughput: 5.03 requests/s, 21229.35 total tokens/s, 644.04 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 1024\nThroughput: 17.44 requests/s, 20046.60 total tokens/s, 2231.96 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 4096\nThroughput: 4.21 requests/s, 17770.35 total tokens/s, 539.11 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Dense Model Results&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024\nThroughput: 2.87 requests/s, 3297.05 total tokens/s, 367.09 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096\nThroughput: 0.77 requests/s, 3259.23 total tokens/s, 98.88 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 8k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192\nThroughput: 0.37 requests/s, 3069.56 total tokens/s, 47.24 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 1k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024 --tensor-parallel 2\nThroughput: 5.18 requests/s, 5957.00 total tokens/s, 663.24 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 4k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \nThroughput: 1.44 requests/s, 6062.84 total tokens/s, 183.93 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 8k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \nThroughput: 0.70 requests/s, 5806.52 total tokens/s, 89.36 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024\nThroughput: 7.26 requests/s, 8340.89 total tokens/s, 928.66 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096\nThroughput: 2.00 requests/s, 8426.05 total tokens/s, 255.62 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 8k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192\nThroughput: 0.97 requests/s, 8028.90 total tokens/s, 123.56 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 1k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024 --tensor-parallel 2 \nThroughput: 10.68 requests/s, 12273.33 total tokens/s, 1366.50 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 4k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \nThroughput: 2.88 requests/s, 12140.81 total tokens/s, 368.32 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 8k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \nThroughput: 1.45 requests/s, 12057.89 total tokens/s, 185.56 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyxf1f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackwell_tart",
          "discussion_type": null,
          "num_comments": 48,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752425040,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any advice ? :) ",
          "author_fullname": "t2_pmwcfxhz4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best LLM for Educators ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzooed",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752505188,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any advice ? :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzooed",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creative_Structure22",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzooed/best_llm_for_educators/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzooed/best_llm_for_educators/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752505188,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever since there're code completions, I wish I could have something similar when texting people. Now there's finally a decent method for that.\n\nThe app works on any endpoint that's OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.\n\nI tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!\n\nHere's a brief guide to make this work with ollama:\n\n* Download the app from GitHub: [https://github.com/coreply/coreply](https://github.com/coreply/coreply)\n* Download `gemma3:4b-it-qat` in ollama\n* Set environment variable `OLLAMA_HOST` to [`0.0.0.0`](http://0.0.0.0) on the computer running ollama and restart ollama\n* In the Coreply app, set the API URL to `http://192.168.xxx.xxx:11434/v1/`(replace [`192.168.xxx.xxx`](http://192.168.xxx.xxx) with the IP address of the ollama machine), Model name `gemma3:4b-it-qat`\n* Grant permissions and turn on the app. Enjoy your texting suggestions!\n\nMy laptop isn't powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.\n\nLet me know how's your experience with it!",
          "author_fullname": "t2_l6eo8ggy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I use Gemma 3 to help me reply my texts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 34,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyv750",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 450,
              "fallback_url": "https://v.redd.it/48w6qb1mincf1/DASH_270.mp4?source=fallback",
              "has_audio": false,
              "height": 118,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/48w6qb1mincf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/48w6qb1mincf1/DASHPlaylist.mpd?a=1755175134%2CYWUzNGNiODg0YmJmODE0MzFiOTFmZGY3YmQ3YTExNTA0NGJmNTBmZWE0YTU4OGRkNTc5ZTg1NzM4ZTU3MjA3ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/48w6qb1mincf1/HLSPlaylist.m3u8?a=1755175134%2CNDhhOGZkMmY0YjViZWZlYmMyNWU3YjM5NjI1OGJhYTNkOGIxOGNmN2E2Nzc0ODdlYTIxNjBlMTA0MDUzMWRkZg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=140&amp;height=34&amp;crop=140:34,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4ea93f7d06620c65936cfa4562d1f7121ab5794f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752419560,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever since there&amp;#39;re code completions, I wish I could have something similar when texting people. Now there&amp;#39;s finally a decent method for that.&lt;/p&gt;\n\n&lt;p&gt;The app works on any endpoint that&amp;#39;s OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.&lt;/p&gt;\n\n&lt;p&gt;I tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a brief guide to make this work with ollama:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Download the app from GitHub: &lt;a href=\"https://github.com/coreply/coreply\"&gt;https://github.com/coreply/coreply&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Download &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt; in ollama&lt;/li&gt;\n&lt;li&gt;Set environment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt; to &lt;a href=\"http://0.0.0.0\"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; on the computer running ollama and restart ollama&lt;/li&gt;\n&lt;li&gt;In the Coreply app, set the API URL to &lt;code&gt;http://192.168.xxx.xxx:11434/v1/&lt;/code&gt;(replace &lt;a href=\"http://192.168.xxx.xxx\"&gt;&lt;code&gt;192.168.xxx.xxx&lt;/code&gt;&lt;/a&gt; with the IP address of the ollama machine), Model name &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Grant permissions and turn on the app. Enjoy your texting suggestions!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My laptop isn&amp;#39;t powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.&lt;/p&gt;\n\n&lt;p&gt;Let me know how&amp;#39;s your experience with it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/48w6qb1mincf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?format=pjpg&amp;auto=webp&amp;s=4315959609b1452ddac1019bc82dfcd0d2ecd932",
                  "width": 1080,
                  "height": 264
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca55479fe3e543c483ca9fd1a6e1c7663b1e1469",
                    "width": 108,
                    "height": 26
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bb9e1e7f65de33e3f9e6aac96c0c769244d7247e",
                    "width": 216,
                    "height": 52
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cb1d98d3e21b6fb43e522b5e2c12bfd0b0606cc7",
                    "width": 320,
                    "height": 78
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=150f56510dfbd5b6d84de28d6521c41bb3feede9",
                    "width": 640,
                    "height": 156
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0f618077403fd8aaf60fa19cfeb0ddd4cf30fc8d",
                    "width": 960,
                    "height": 234
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b18333794ad268d43549283f55cc34c53f54a5c2",
                    "width": 1080,
                    "height": 264
                  }
                ],
                "variants": {},
                "id": "NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyv750",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sean01-eth",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/",
          "stickied": false,
          "url": "https://v.redd.it/48w6qb1mincf1",
          "subreddit_subscribers": 499292,
          "created_utc": 1752419560,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 450,
              "fallback_url": "https://v.redd.it/48w6qb1mincf1/DASH_270.mp4?source=fallback",
              "has_audio": false,
              "height": 118,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/48w6qb1mincf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/48w6qb1mincf1/DASHPlaylist.mpd?a=1755175134%2CYWUzNGNiODg0YmJmODE0MzFiOTFmZGY3YmQ3YTExNTA0NGJmNTBmZWE0YTU4OGRkNTc5ZTg1NzM4ZTU3MjA3ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/48w6qb1mincf1/HLSPlaylist.m3u8?a=1755175134%2CNDhhOGZkMmY0YjViZWZlYmMyNWU3YjM5NjI1OGJhYTNkOGIxOGNmN2E2Nzc0ODdlYTIxNjBlMTA0MDUzMWRkZg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Context of my project idea:**\n\nI have been doing some research on self hosting LLMs and, of course, quickly came to the realisation on how complicated it seems to be for a solo developer to pay for the rental costs of an enterprise-grade GPU and run a SOTA open-source model like Kimi K2 32B or Qwen 32B. Renting per hour quickly can rack up insane costs. And trying to pay \"per request\" is pretty much unfeasible without factoring in excessive cold startup times.\n\nSo it seems that the most commonly chose option is to try and run a much smaller model on ollama; and even then you need a pretty powerful setup to handle it. Otherwise, stick to the usual closed-source commercial models.\n\n**An alternative?**\n\nAll this got me thinking. Of course, we already have open-source communities like Hugging Face for sharing model weights, transformers etc. What about though a **community-owned live inference server** where the community has a say in what model, infrastructure, stack, data etc we use and share the costs via transparent API pricing?\n\nWe, the community, would set up a whole environment, rent the GPU, prepare data for fine-tuning / RL, and even implement some experimental setups like using the new MemOS or other research paths. Of course it would be helpful if the community was also of similar objective, like development / coding focused.\n\nI imagine there is a lot to cogitate here but I am open to discussing and brainstorming together the various aspects and obstacles here.",
          "author_fullname": "t2_19mrnrt357",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Project Idea: A REAL Community-driven LLM Stack",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lznxy5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752503693,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752503479,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Context of my project idea:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have been doing some research on self hosting LLMs and, of course, quickly came to the realisation on how complicated it seems to be for a solo developer to pay for the rental costs of an enterprise-grade GPU and run a SOTA open-source model like Kimi K2 32B or Qwen 32B. Renting per hour quickly can rack up insane costs. And trying to pay &amp;quot;per request&amp;quot; is pretty much unfeasible without factoring in excessive cold startup times.&lt;/p&gt;\n\n&lt;p&gt;So it seems that the most commonly chose option is to try and run a much smaller model on ollama; and even then you need a pretty powerful setup to handle it. Otherwise, stick to the usual closed-source commercial models.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;An alternative?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;All this got me thinking. Of course, we already have open-source communities like Hugging Face for sharing model weights, transformers etc. What about though a &lt;strong&gt;community-owned live inference server&lt;/strong&gt; where the community has a say in what model, infrastructure, stack, data etc we use and share the costs via transparent API pricing?&lt;/p&gt;\n\n&lt;p&gt;We, the community, would set up a whole environment, rent the GPU, prepare data for fine-tuning / RL, and even implement some experimental setups like using the new MemOS or other research paths. Of course it would be helpful if the community was also of similar objective, like development / coding focused.&lt;/p&gt;\n\n&lt;p&gt;I imagine there is a lot to cogitate here but I am open to discussing and brainstorming together the various aspects and obstacles here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lznxy5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Budget_Map_3333",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752503479,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like \"here's a scenario, what do you think is the most realistic thing to happen?\" or \"what do you think would be a good solution to this issue?\". I found it quite bad in this regard.\n\n* It frequently made things up, even when specifically instructed not to do so. **It then clarified it was trying to come up with a helpful looking answer using fragmented data**, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.\n\n* If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion using sources that do not exist. **At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.** It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.\n\n* Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it's own idea.\n\n* If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like \"why are you not revealing the character's thoughts here?\" or \"why are you not taking X into account?\". Free ChatGPT is actually much better in this regard.\n\n* Out of all the AI chat bots i have tried, it has possibly the most restrictive content filters i have seen. It's very prudish.\n\nEdit : Im using Kimi k2 on www.kimi.com btw.",
          "author_fullname": "t2_11hnt7w9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Kimi K2 for writing and reasoning, and was not impressed.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyvah4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752461908,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like &amp;quot;here&amp;#39;s a scenario, what do you think is the most realistic thing to happen?&amp;quot; or &amp;quot;what do you think would be a good solution to this issue?&amp;quot;. I found it quite bad in this regard.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;It frequently made things up, even when specifically instructed not to do so. &lt;strong&gt;It then clarified it was trying to come up with a helpful looking answer using fragmented data&lt;/strong&gt;, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion using sources that do not exist. &lt;strong&gt;At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.&lt;/strong&gt; It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it&amp;#39;s own idea.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like &amp;quot;why are you not revealing the character&amp;#39;s thoughts here?&amp;quot; or &amp;quot;why are you not taking X into account?&amp;quot;. Free ChatGPT is actually much better in this regard.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Out of all the AI chat bots i have tried, it has possibly the most restrictive content filters i have seen. It&amp;#39;s very prudish.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Edit : Im using Kimi k2 on &lt;a href=\"http://www.kimi.com\"&gt;www.kimi.com&lt;/a&gt; btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyvah4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlompSpark",
          "discussion_type": null,
          "num_comments": 107,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752419788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After reading all the amazing post of yours, I've bought in. About to offer my management a localized coding agent, to prevent code and API keys leaks. From 20 to 50 people coding at any moment.\n\nLocally I'd need a used 3080+ card. But what type of the hardware I'm looking for to provide for 20+ folks?",
          "author_fullname": "t2_31gku",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help needed: 20+ devs on the local model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m00yn1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752532847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading all the amazing post of yours, I&amp;#39;ve bought in. About to offer my management a localized coding agent, to prevent code and API keys leaks. From 20 to 50 people coding at any moment.&lt;/p&gt;\n\n&lt;p&gt;Locally I&amp;#39;d need a used 3080+ card. But what type of the hardware I&amp;#39;m looking for to provide for 20+ folks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m00yn1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "3dom",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752532847,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL.   \n  \nI was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. \n\nQwen 2.5 VL 3B  \nCaption generation - average time taken for max pixel 768\\*768 - 1.62s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.02s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 2.79s\n\nQwen 2.5 VL 7B  \nCaption generation - average time taken for max pixel 768\\*768 - 2.21s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.73s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 3.64s  \n  \nQwen 2.5 VL 7B AWQ  \nCaption generation - average time taken for max pixel 768\\*768 - 2.84s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.94s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 3.85s  \n\n\n1. Why 7B AWQ is slower than 7B?  \n2. What other better Image caption/VQA model exists that runs in less or similar resource requirments?\n\n",
          "author_fullname": "t2_7doe6lck",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions/Alternatives for Image captions with efficient system requirements",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzkrwg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752495232,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL.   &lt;/p&gt;\n\n&lt;p&gt;I was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. &lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 3B&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 1.62s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.02s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 2.79s&lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 7B&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 2.21s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.73s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 3.64s  &lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 7B AWQ&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 2.84s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.94s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 3.85s  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why 7B AWQ is slower than 7B?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;What other better Image caption/VQA model exists that runs in less or similar resource requirments?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzkrwg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "palaniappan_05",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752495232,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm releasing a v1.0 of my [Orpheus TTS FastAPI Server](https://github.com/prakharsr/Orpheus-TTS-FastAPI). Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the [Orpheus TTS](https://github.com/canopyai/Orpheus-TTS) model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the `orpheus-speech` python package.\n\nThe project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:\n\n1. Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.\n2. **Intelligent Retry Logic:** Automatic retry on audio decoding errors for improved reliability. The original implementation in `orpheus-speech` skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.\n3. **Token Repetition Detection**: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in `orpheus-speech` sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.\n4. **Async Parallel Processing**: Processes multiple text chunks simultaneously for faster generation. The original implementation in `orpheus-speech` was synchronous, this is now fixed by adding support for concurrent async calls.\n5. **Text Chunking**: Automatic intelligent text splitting for long content.\n\nLink to the repo: [https://github.com/prakharsr/Orpheus-TTS-FastAPI](https://github.com/prakharsr/Orpheus-TTS-FastAPI)\n\nLet me know how it works and also checkout my [Audiobook Creator Project here](https://github.com/prakharsr/audiobook-creator) which supports Kokoro and Orpheus.",
          "author_fullname": "t2_hi3epx7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Orpheus TTS FastAPI Server Release v1.0 (Async and Audio Issues Fixes)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyvsqv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752421053,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m releasing a v1.0 of my &lt;a href=\"https://github.com/prakharsr/Orpheus-TTS-FastAPI\"&gt;Orpheus TTS FastAPI Server&lt;/a&gt;. Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the &lt;a href=\"https://github.com/canopyai/Orpheus-TTS\"&gt;Orpheus TTS&lt;/a&gt; model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the &lt;code&gt;orpheus-speech&lt;/code&gt; python package.&lt;/p&gt;\n\n&lt;p&gt;The project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Intelligent Retry Logic:&lt;/strong&gt; Automatic retry on audio decoding errors for improved reliability. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Token Repetition Detection&lt;/strong&gt;: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Processes multiple text chunks simultaneously for faster generation. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; was synchronous, this is now fixed by adding support for concurrent async calls.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Text Chunking&lt;/strong&gt;: Automatic intelligent text splitting for long content.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Link to the repo: &lt;a href=\"https://github.com/prakharsr/Orpheus-TTS-FastAPI\"&gt;https://github.com/prakharsr/Orpheus-TTS-FastAPI&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know how it works and also checkout my &lt;a href=\"https://github.com/prakharsr/audiobook-creator\"&gt;Audiobook Creator Project here&lt;/a&gt; which supports Kokoro and Orpheus.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?auto=webp&amp;s=0cc860c55c5d39a8725b904b4561a9f80e0f99d0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45d52cf1200f1189b715c76836164ff9cecf79b9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b408822642b2470faba4b74a50ce8516a253a9e3",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=948de85aa715b4f5e80a8759acad2fb62df83251",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0c92ff268493c06d6bca23d519a6e658e83f3c0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4c66abb6a8bf083ae94087d05f70bde8121c8436",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec93d51f819b3e6c11763f4435e0e77ffbdb9b84",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyvsqv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "prakharsr",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752421053,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying Fish Speech Open Audio S1 mini.   \n  \nThis one: [https://github.com/fishaudio/fish-speech](https://github.com/fishaudio/fish-speech)   \n  \nIn the web ui, there is no pacing option. Is there anyway we can control the pacing?   \n  \nWhen you upload a referenced audio, put a text prompt and generate the audio, I want output to speak slow or fast sometimes.   \n  \nCan we add a custom pacing control option?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you add pacing control option in TTS ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lza5bu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752457998,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying Fish Speech Open Audio S1 mini.   &lt;/p&gt;\n\n&lt;p&gt;This one: &lt;a href=\"https://github.com/fishaudio/fish-speech\"&gt;https://github.com/fishaudio/fish-speech&lt;/a&gt;   &lt;/p&gt;\n\n&lt;p&gt;In the web ui, there is no pacing option. Is there anyway we can control the pacing?   &lt;/p&gt;\n\n&lt;p&gt;When you upload a referenced audio, put a text prompt and generate the audio, I want output to speak slow or fast sometimes.   &lt;/p&gt;\n\n&lt;p&gt;Can we add a custom pacing control option?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?auto=webp&amp;s=ad23502cf6602bb61e1a9ac506d7031701a2b6dc",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=95436e0dba572c9aee2a59c53fd67015c882eebb",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf4d62f675632ca88ba7e85fa82e4aadf74b32ef",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a660e607df8576c404ccecb4558748f67e07102c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5442b0d30b6b17b82289f8cfe1d046d52a045429",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e538b19864e18862b4e2246be24a66c0195a2e0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5e082495a45e9097df6d4b048ab8a643fc4960e6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lza5bu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lza5bu/can_you_add_pacing_control_option_in_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lza5bu/can_you_add_pacing_control_option_in_tts/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752457998,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "so ive hosted ollama locally on my system on[http://localhost:11434/api/generate](http://localhost:11434/api/generate)and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory.\n\ni don't understand why this would happen because as much as i have seen modern llms, they don't change their weights during inference.\n\nScenario:\n\n1. makes a query to ollama for topic 1 with a very specific keyword that i have created\n2. makes another query to ollama for a topic that is similar to topic 1 but has a new keyword.\n\nTurns out that the first keyword shows up in the second response aswell. Not always, but this shouldn't happen at all as much as i know\n\nIs there something that i am missing?  \nI checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &lt;model\\_name&gt;",
          "author_fullname": "t2_c40awigh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama retaining history?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzjlvi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752491545,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so ive hosted ollama locally on my system on&lt;a href=\"http://localhost:11434/api/generate\"&gt;http://localhost:11434/api/generate&lt;/a&gt;and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory.&lt;/p&gt;\n\n&lt;p&gt;i don&amp;#39;t understand why this would happen because as much as i have seen modern llms, they don&amp;#39;t change their weights during inference.&lt;/p&gt;\n\n&lt;p&gt;Scenario:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;makes a query to ollama for topic 1 with a very specific keyword that i have created&lt;/li&gt;\n&lt;li&gt;makes another query to ollama for a topic that is similar to topic 1 but has a new keyword.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Turns out that the first keyword shows up in the second response aswell. Not always, but this shouldn&amp;#39;t happen at all as much as i know&lt;/p&gt;\n\n&lt;p&gt;Is there something that i am missing?&lt;br/&gt;\nI checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &amp;lt;model\\_name&amp;gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzjlvi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DimensionEnergy",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzjlvi/ollama_retaining_history/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzjlvi/ollama_retaining_history/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752491545,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for something I can run locally that's actually close to gpt-4o or claude in terms of quality.\n\nKinda tight on money right now so I can't afford gpt plus or claude pro :/  \n  \nI have to write a bunch of posts throughout the day, and the free gpt-4o hits its limit way too fast.\n\nIs there anything similar out there that gives quality output like gpt-4o or claude and can run locally?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any Actual alternative to gpt-4o or claude?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzb7fh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752461154,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for something I can run locally that&amp;#39;s actually close to gpt-4o or claude in terms of quality.&lt;/p&gt;\n\n&lt;p&gt;Kinda tight on money right now so I can&amp;#39;t afford gpt plus or claude pro :/  &lt;/p&gt;\n\n&lt;p&gt;I have to write a bunch of posts throughout the day, and the free gpt-4o hits its limit way too fast.&lt;/p&gt;\n\n&lt;p&gt;Is there anything similar out there that gives quality output like gpt-4o or claude and can run locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzb7fh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzb7fh/any_actual_alternative_to_gpt4o_or_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzb7fh/any_actual_alternative_to_gpt4o_or_claude/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752461154,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Let's say we have a 30b, 24b, 14b, 7b model that exceeds in quality but the context window is like... 8k or worse, 4k. What can you possibly do in this case?\n\nBack in 2022 I used a unkown gpt plugin involving PDF files are permanent memory that didn't used the context window, even now it would be really useful if there was also a manner of insering some sort of text, pdf or text document file for the model to get \"fixed on\", like it's permanent focus (like a bot Card for example, where the biography would be stored instead of resent at every request and then combined to the whole context of the chat).\n\nResume: Method of increasing context lengh or using document for loading what chat context is focused on.",
          "author_fullname": "t2_eljq22kg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Safe methods of increasing Context Window of models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz5sm6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752445683,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say we have a 30b, 24b, 14b, 7b model that exceeds in quality but the context window is like... 8k or worse, 4k. What can you possibly do in this case?&lt;/p&gt;\n\n&lt;p&gt;Back in 2022 I used a unkown gpt plugin involving PDF files are permanent memory that didn&amp;#39;t used the context window, even now it would be really useful if there was also a manner of insering some sort of text, pdf or text document file for the model to get &amp;quot;fixed on&amp;quot;, like it&amp;#39;s permanent focus (like a bot Card for example, where the biography would be stored instead of resent at every request and then combined to the whole context of the chat).&lt;/p&gt;\n\n&lt;p&gt;Resume: Method of increasing context lengh or using document for loading what chat context is focused on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz5sm6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WEREWOLF_BX13",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz5sm6/safe_methods_of_increasing_context_window_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz5sm6/safe_methods_of_increasing_context_window_of/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752445683,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}