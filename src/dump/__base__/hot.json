{
  "kind": "Listing",
  "data": {
    "after": "t3_1m3osbo",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'm sorry Zuck please don't leave us we were just having fun",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 112,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4s9mt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "ups": 485,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 485,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Z2j0QZtU4E9ysCBsBkjssqu8NBo0JDeca2zq2_P4T5Q.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753026671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/p9mxxen7w1ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/p9mxxen7w1ef1.png?auto=webp&amp;s=213c1f7dfcfece004758a436ec8f98522e0eb2e7",
                  "width": 868,
                  "height": 696
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/p9mxxen7w1ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0312de90e101f89f1aedd7bdb3cd988bb8639791",
                    "width": 108,
                    "height": 86
                  },
                  {
                    "url": "https://preview.redd.it/p9mxxen7w1ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6db82d3958069aad93464e9169e3acbd2741501f",
                    "width": 216,
                    "height": 173
                  },
                  {
                    "url": "https://preview.redd.it/p9mxxen7w1ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4629b986f27535857af409f095f24c7c7d27e17",
                    "width": 320,
                    "height": 256
                  },
                  {
                    "url": "https://preview.redd.it/p9mxxen7w1ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ce4b5b89949e56107fd26431dd9d275053d6cf2",
                    "width": 640,
                    "height": 513
                  }
                ],
                "variants": {},
                "id": "NzZ0RDeoMTe29AfbHg0EIQXBI_ZMyqVJEiS9EW_Y5I8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m4s9mt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/",
          "stickied": false,
          "url": "https://i.redd.it/p9mxxen7w1ef1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1753026671,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was seeing if there was a new commit today but when refreshed the page got a 404.",
          "author_fullname": "t2_j1kqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ikllamacpp repository gone, or it is only me?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4vw29",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 113,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 113,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753035287,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was seeing if there was a new commit today but when refreshed the page got a 404.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ikawrakow/ik_llama.cpp/commits/main/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4vw29",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panchovix",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/",
          "stickied": false,
          "url": "https://github.com/ikawrakow/ik_llama.cpp/commits/main/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753035287,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, I posted that I wanted to train an LLM for under $1000 here:  [https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting\\_to\\_train\\_a\\_model\\_from\\_scratch\\_for\\_less/](https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/)\n\nI had to crunch a lot to fit in 24gb of ram. The final project is a 960M model trained on 19.2B tokens ( chinchilla optimal).  Cost projection is about $500 for this run.   It has flash attention 2, a 3:1 GQA,  a 3k context window. and sink tokens. Training is 70% project gutenberg and 30% US congressional reports ( the Govremorts dataset).   The corpus is english only, which I'm hoping will give it an edge.\n\nI have had two false starts where I had to restart training. The first because I set up my streaming datasets wrong, and the model kep training on the same thing due to restarts. The second because the LR was too high and my loss curve was all fucked up.\n\nNow at about 2% on the 3rd run, the loss looks textbook, and I am letting it run till the tokens are done. Projections show a final loss around 2.6-2.3 which is great.\n\nHappy to answer any questions! Pic is the beautiful loss curve.\n\n  \nEdit: It's called Libremodel I, codename Gigi, and I made a website with more info here: [https://libremodel.xyz](https://libremodel.xyz)\n\nhttps://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;format=png&amp;auto=webp&amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1",
          "author_fullname": "t2_i5os0v0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I posted 3 weeks ago about training my own model. Progress report.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "lf78xbsfy3ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36ad333eaf2d759d9b745f45f702461135100a1a"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb1c9a95a8af42b94154d7f01e49b4731e052e5c"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91b4c8642acf7701da8d782aa9ce125107f00dd1"
                },
                {
                  "y": 319,
                  "x": 640,
                  "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8517a175f4e44e1ff1b186c44a329b861671f778"
                }
              ],
              "s": {
                "y": 355,
                "x": 711,
                "u": "https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;format=png&amp;auto=webp&amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1"
              },
              "id": "lf78xbsfy3ef1"
            }
          },
          "name": "t3_1m52h10",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/tGNWILy7NUwjWBRL__Qs6HOJImwQ5Z22Np_wBFcIDdM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753051615,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I posted that I wanted to train an LLM for under $1000 here:  &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I had to crunch a lot to fit in 24gb of ram. The final project is a 960M model trained on 19.2B tokens ( chinchilla optimal).  Cost projection is about $500 for this run.   It has flash attention 2, a 3:1 GQA,  a 3k context window. and sink tokens. Training is 70% project gutenberg and 30% US congressional reports ( the Govremorts dataset).   The corpus is english only, which I&amp;#39;m hoping will give it an edge.&lt;/p&gt;\n\n&lt;p&gt;I have had two false starts where I had to restart training. The first because I set up my streaming datasets wrong, and the model kep training on the same thing due to restarts. The second because the LR was too high and my loss curve was all fucked up.&lt;/p&gt;\n\n&lt;p&gt;Now at about 2% on the 3rd run, the loss looks textbook, and I am letting it run till the tokens are done. Projections show a final loss around 2.6-2.3 which is great.&lt;/p&gt;\n\n&lt;p&gt;Happy to answer any questions! Pic is the beautiful loss curve.&lt;/p&gt;\n\n&lt;p&gt;Edit: It&amp;#39;s called Libremodel I, codename Gigi, and I made a website with more info here: &lt;a href=\"https://libremodel.xyz\"&gt;https://libremodel.xyz&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1\"&gt;https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m52h10",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thebadslime",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753051615,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuned her the perfect local model. Still got API’d 💔",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 134,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4y3cj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Z2zYRd-Xr4y9KTuzjGReb7sqyWdNxJRys_q8VxJgYkc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753040590,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/xitr9w9f13ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/xitr9w9f13ef1.png?auto=webp&amp;s=c63a2736080345ebff74201f237ea3a3fc9291bb",
                  "width": 754,
                  "height": 724
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/xitr9w9f13ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f18eb7bb3172b0aeea4ed2a63fa91436ba164167",
                    "width": 108,
                    "height": 103
                  },
                  {
                    "url": "https://preview.redd.it/xitr9w9f13ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=498743907632f82911e52e88e73f7ed3c25df767",
                    "width": 216,
                    "height": 207
                  },
                  {
                    "url": "https://preview.redd.it/xitr9w9f13ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4105a3b7b3220c53a13050bb4e453234c0e93ba0",
                    "width": 320,
                    "height": 307
                  },
                  {
                    "url": "https://preview.redd.it/xitr9w9f13ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f099b23bb6d2f68855a9689333e79824231cdf0",
                    "width": 640,
                    "height": 614
                  }
                ],
                "variants": {},
                "id": "pPU2eZinNEijAzYr3n8rTbR9-Ra1B9-dybbuMcLM1qg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m4y3cj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/",
          "stickied": false,
          "url": "https://i.redd.it/xitr9w9f13ef1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1753040590,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m just making this post as I want opinions on the idea that if open source doesn’t consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to God’s from fiction. The risk of tyranny from centralized power is greater than the risk of chaos from distributed power so open source is the way forward or at least the best we have. What’s you take? It is not a magical solution that will solve all problems. However, it is the single most important counterweight we have. It fosters transparency, allows for independent safety research, prevents a single corporate or state actor from setting all the rules, and provides the tools for resistance and balance. ",
          "author_fullname": "t2_769j0jzd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source is humanity’s last hope!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4r4j1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 107,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 107,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753043741,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753023845,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m just making this post as I want opinions on the idea that if open source doesn’t consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to God’s from fiction. The risk of tyranny from centralized power is greater than the risk of chaos from distributed power so open source is the way forward or at least the best we have. What’s you take? It is not a magical solution that will solve all problems. However, it is the single most important counterweight we have. It fosters transparency, allows for independent safety research, prevents a single corporate or state actor from setting all the rules, and provides the tools for resistance and balance. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4r4j1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bralynn2222",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753023845,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for something small but still usable.\nWhat's your go-to?",
          "author_fullname": "t2_n2kmftzjf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the smartest tiny LLM you've actually used?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4of82",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 125,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 125,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753016677,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something small but still usable.\nWhat&amp;#39;s your go-to?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4of82",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Luston03",
          "discussion_type": null,
          "num_comments": 73,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753016677,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*^(#I'm starting a new series of explaining intriguing new AI papers)*\n\nLLMs learn from text and lack an inherent understanding of the physical world. Their \"knowledge\" is **mostly** limited to what's been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of \"common sense\" that is impossible to acquire from text alone.\n\nDuring training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence. \n\nWhy not to apply this self-supervised approach for teaching AI how life works via videos? \n\nTake all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works. \n\nBut, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part,  the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision.  And because we are challenging the model to do the impossible, the learning process will just collapse.\n\nLet's see how Meta approaches this issue [https://arxiv.org/pdf/2506.09985](https://arxiv.org/pdf/2506.09985)\n\nTheir new architecture, called V-JEPA 2, consists of an encoder and a predictor.\n\n**encoder** takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.\n\nIn other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel.  So that the predictor learns to predict the high-level process that happens in the masked region of the video. *(see until 0:07 in the video)*\n\nThis helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that don’t do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic arm’s interaction.\n\nThis time, they encode part of a video and also give information about robot’s intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. *(see 0:08 to 0:16 in the video)*\n\nSo, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.\n\nAfter training, the robot, powered by this model,  in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.\n\nAnd for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Next big thing after LLMs - World Model [explained on the example of V-JEPA2]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 65,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4mfs8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 168,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/h0ivgtibj0ef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 600,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/h0ivgtibj0ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/h0ivgtibj0ef1/DASHPlaylist.mpd?a=1755652514%2CYjBiNjg5NTQ2NjFlZDI0OThhNDZkYWE2NTE0Mjc2MjMyODFmMDZkNjMxODViOTY2MjM0OTZlMWZhNjRlZGQ3Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 21,
              "hls_url": "https://v.redd.it/h0ivgtibj0ef1/HLSPlaylist.m3u8?a=1755652514%2CNDE0MTJjMTBlZDM4OWZjMzc1ODZlYzJjMmQxNDYwMDcwYzExMjA0YjU3YTA0N2JjYmVlN2ViOWFhNjBmNzgzNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 168,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=140&amp;height=65&amp;crop=140:65,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4a64686273ed8c2776f8df926696df24c9ce4cf6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753010231,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;&lt;sup&gt;#I&amp;#39;m starting a new series of explaining intriguing new AI papers&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;LLMs learn from text and lack an inherent understanding of the physical world. Their &amp;quot;knowledge&amp;quot; is &lt;strong&gt;mostly&lt;/strong&gt; limited to what&amp;#39;s been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of &amp;quot;common sense&amp;quot; that is impossible to acquire from text alone.&lt;/p&gt;\n\n&lt;p&gt;During training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence. &lt;/p&gt;\n\n&lt;p&gt;Why not to apply this self-supervised approach for teaching AI how life works via videos? &lt;/p&gt;\n\n&lt;p&gt;Take all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works. &lt;/p&gt;\n\n&lt;p&gt;But, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part,  the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision.  And because we are challenging the model to do the impossible, the learning process will just collapse.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s see how Meta approaches this issue &lt;a href=\"https://arxiv.org/pdf/2506.09985\"&gt;https://arxiv.org/pdf/2506.09985&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Their new architecture, called V-JEPA 2, consists of an encoder and a predictor.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;encoder&lt;/strong&gt; takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.&lt;/p&gt;\n\n&lt;p&gt;In other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel.  So that the predictor learns to predict the high-level process that happens in the masked region of the video. &lt;em&gt;(see until 0:07 in the video)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that don’t do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic arm’s interaction.&lt;/p&gt;\n\n&lt;p&gt;This time, they encode part of a video and also give information about robot’s intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. &lt;em&gt;(see 0:08 to 0:16 in the video)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;So, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.&lt;/p&gt;\n\n&lt;p&gt;After training, the robot, powered by this model,  in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.&lt;/p&gt;\n\n&lt;p&gt;And for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/h0ivgtibj0ef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?format=pjpg&amp;auto=webp&amp;s=6a8ec9b4980d9a40f31e6b1a7c7113057d70d42c",
                  "width": 1920,
                  "height": 900
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bd457375042fde995b2dbbbc85910bf0ffe7d88b",
                    "width": 108,
                    "height": 50
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=506b500241f56e40e0a1feebc362f80c5440bc81",
                    "width": 216,
                    "height": 101
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7f2f1446c77d923e99ffd8c175c55641539c9bb9",
                    "width": 320,
                    "height": 150
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=60a24c15686917cbed627c6f9aa1cd0d68bc6c1d",
                    "width": 640,
                    "height": 300
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a674fb74efa796fcaa16cc6a4f6d5158ed3c8ab",
                    "width": 960,
                    "height": 450
                  },
                  {
                    "url": "https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e547aa9ee3bf8f5210ba9ff0685c84ce1099e4d7",
                    "width": 1080,
                    "height": 506
                  }
                ],
                "variants": {},
                "id": "bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m4mfs8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/",
          "stickied": false,
          "url": "https://v.redd.it/h0ivgtibj0ef1",
          "subreddit_subscribers": 502029,
          "created_utc": 1753010231,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/h0ivgtibj0ef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 600,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/h0ivgtibj0ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/h0ivgtibj0ef1/DASHPlaylist.mpd?a=1755652514%2CYjBiNjg5NTQ2NjFlZDI0OThhNDZkYWE2NTE0Mjc2MjMyODFmMDZkNjMxODViOTY2MjM0OTZlMWZhNjRlZGQ3Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 21,
              "hls_url": "https://v.redd.it/h0ivgtibj0ef1/HLSPlaylist.m3u8?a=1755652514%2CNDE0MTJjMTBlZDM4OWZjMzc1ODZlYzJjMmQxNDYwMDcwYzExMjA0YjU3YTA0N2JjYmVlN2ViOWFhNjBmNzgzNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Alright so basically - I want to run qwen3 235b MoE. I dont wanna pay 235b MoE money tho. So far I've been eyeing grabbing an old dell xeon workstation, slapping in lots of RAM &amp; two mi50 cards &amp; calling it a day. Would that work? probably i guess, hell you'd even get good performance out of that running 32b models which do the job for most cases. but i want real crackhead technology. completely out of the box shit. the funnier in its sheer absurdity/cheaper/faster the better. let's hear what you guys can think of ",
          "author_fullname": "t2_9y0b4xlc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the most crackhead garbage local LLM setup you can think of?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4u7j6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753031293,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright so basically - I want to run qwen3 235b MoE. I dont wanna pay 235b MoE money tho. So far I&amp;#39;ve been eyeing grabbing an old dell xeon workstation, slapping in lots of RAM &amp;amp; two mi50 cards &amp;amp; calling it a day. Would that work? probably i guess, hell you&amp;#39;d even get good performance out of that running 32b models which do the job for most cases. but i want real crackhead technology. completely out of the box shit. the funnier in its sheer absurdity/cheaper/faster the better. let&amp;#39;s hear what you guys can think of &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4u7j6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "caraccidentGAMING",
          "discussion_type": null,
          "num_comments": 43,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753031293,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "And this is how you get sued, lol. I noticed this while playing around with DiffRhythm; I had unrelated lyrics and an unrelated audio prompt set for the generation, and it still injected Avicii into the output, which was really funny.\n\nSkip to 1:00 in the video to skip the generation process\n\nSeed: 50518556518147",
          "author_fullname": "t2_10kd4cpw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DiffRhythm 1.2 music generation model produces \"Avicii vs Nicky Romero - I Could Be the One\" nearly verbatim",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4yo0g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/ulng63nd53ef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 666,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/ulng63nd53ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ulng63nd53ef1/DASHPlaylist.mpd?a=1755652514%2CNmYxOTUzYzA5MjUyZmNiMzkxOGM2NjAwMzA3YjU3OThlYWZmODMzMmY5N2NkYWI3YWRmMDViNTBhN2U0Y2M1OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 100,
              "hls_url": "https://v.redd.it/ulng63nd53ef1/HLSPlaylist.m3u8?a=1755652514%2CMTY5ZmQ1ZDlkM2E1NTIwZDE1YWEwZTQ5OGRiOTg5ZjkwYzYzMDUwZWI1NjM4MTM3OWI4ODZlMmYyZGM3YmJjOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=140&amp;height=72&amp;crop=140:72,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=fb29cdcc71bb56661a654313ebe0525de0f6881c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753042006,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;And this is how you get sued, lol. I noticed this while playing around with DiffRhythm; I had unrelated lyrics and an unrelated audio prompt set for the generation, and it still injected Avicii into the output, which was really funny.&lt;/p&gt;\n\n&lt;p&gt;Skip to 1:00 in the video to skip the generation process&lt;/p&gt;\n\n&lt;p&gt;Seed: 50518556518147&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ulng63nd53ef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?format=pjpg&amp;auto=webp&amp;s=71b12465b7c7cf904161a47bf220c3ee76b36785",
                  "width": 1580,
                  "height": 822
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3623f942ea04411214f32a79d8a31d6c84ffc218",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=396164ae7705c19ef42cc2b0b2e186ad21a270c3",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4f098b973b3091494dce622bd4fef7bbb5d69267",
                    "width": 320,
                    "height": 166
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=76360571c2001f759e9d863e4fccc954a755823b",
                    "width": 640,
                    "height": 332
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f732c60f96b9d16b8128e3c38b9ea4b46589f6c3",
                    "width": 960,
                    "height": 499
                  },
                  {
                    "url": "https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=edcf19abe8f12c6d47820fc257f124f2252d2cb7",
                    "width": 1080,
                    "height": 561
                  }
                ],
                "variants": {},
                "id": "YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4yo0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iGermanProd",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/",
          "stickied": false,
          "url": "https://v.redd.it/ulng63nd53ef1",
          "subreddit_subscribers": 502029,
          "created_utc": 1753042006,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/ulng63nd53ef1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 666,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/ulng63nd53ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ulng63nd53ef1/DASHPlaylist.mpd?a=1755652514%2CNmYxOTUzYzA5MjUyZmNiMzkxOGM2NjAwMzA3YjU3OThlYWZmODMzMmY5N2NkYWI3YWRmMDViNTBhN2U0Y2M1OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 100,
              "hls_url": "https://v.redd.it/ulng63nd53ef1/HLSPlaylist.m3u8?a=1755652514%2CMTY5ZmQ1ZDlkM2E1NTIwZDE1YWEwZTQ5OGRiOTg5ZjkwYzYzMDUwZWI1NjM4MTM3OWI4ODZlMmYyZGM3YmJjOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MediPhi-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4o37k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=1e40d8e229d834ad0f35ea4c3efabf2f07208289",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753015671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/microsoft/MediPhi-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?auto=webp&amp;s=0ee644741e480cc5121d93fa3f0ea3182be06294",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=296e218ebbb84850c1f1bf1837ba3fc99e98ce95",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d80940b86a21b6b2204fa6ce80025e3df40c07a",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dcab8377fa886059ec3164f66d3f7e4286494dfa",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f9841ead70ffdc17a5775d37d5326e57acfc45ef",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c09c6e3a73021eed5cb4f01caca407dab2f595f3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32624585addcc6921b2529e2964ff10822e8066f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m4o37k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/",
          "stickied": false,
          "url": "https://huggingface.co/microsoft/MediPhi-Instruct",
          "subreddit_subscribers": 502029,
          "created_utc": 1753015671,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TL;DR: Model performance is non-uniform across context lengths due to \"Context Rot\", including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.**\n\nResearch reveals that LLMs (large language models) experience significant performance *\"degradation\"* as input context length increases, even on simple tasks. Testing 18 models across various scenarios, including needle-in-haystack retrieval, conversational QA, and text replication, shows that performance drops are non-uniform and model-specific. \n\nKey findings include: Lower similarity between questions and answers accelerates degradation, distractors have amplified negative effects at longer contexts, haystack structure matters more than semantic similarity, and even basic text copying becomes unreliable at scale. \n\nThe study challenges assumptions about long-context capabilities and emphasizes the importance of context engineering for reliable LLM performance.\n\n\n\n\\[Report\\]: [https://research.trychroma.com/context-rot](https://research.trychroma.com/context-rot)  \n  \n\\[Youtube\\]: [https://www.youtube.com/watch?v=TUjQuC4ugak](https://www.youtube.com/watch?v=TUjQuC4ugak)\n\n\\[Open-source Codebase\\]: [https://github.com/chroma-core/context-rot](https://github.com/chroma-core/context-rot)",
          "author_fullname": "t2_yy8p0c7c1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Context Rot: How Increasing Input Tokens Impacts LLM Performance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4fs2t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 224,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 224,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wGem7sWlt1ED82Y7hUTJQ6HH2tjfP5P1zc7zT6yKMuk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752985024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: Model performance is non-uniform across context lengths due to &amp;quot;Context Rot&amp;quot;, including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Research reveals that LLMs (large language models) experience significant performance &lt;em&gt;&amp;quot;degradation&amp;quot;&lt;/em&gt; as input context length increases, even on simple tasks. Testing 18 models across various scenarios, including needle-in-haystack retrieval, conversational QA, and text replication, shows that performance drops are non-uniform and model-specific. &lt;/p&gt;\n\n&lt;p&gt;Key findings include: Lower similarity between questions and answers accelerates degradation, distractors have amplified negative effects at longer contexts, haystack structure matters more than semantic similarity, and even basic text copying becomes unreliable at scale. &lt;/p&gt;\n\n&lt;p&gt;The study challenges assumptions about long-context capabilities and emphasizes the importance of context engineering for reliable LLM performance.&lt;/p&gt;\n\n&lt;p&gt;[Report]: &lt;a href=\"https://research.trychroma.com/context-rot\"&gt;https://research.trychroma.com/context-rot&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;[Youtube]: &lt;a href=\"https://www.youtube.com/watch?v=TUjQuC4ugak\"&gt;https://www.youtube.com/watch?v=TUjQuC4ugak&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[Open-source Codebase]: &lt;a href=\"https://github.com/chroma-core/context-rot\"&gt;https://github.com/chroma-core/context-rot&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/x8dkgvkifydf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/x8dkgvkifydf1.jpeg?auto=webp&amp;s=57e6704431196e832d50047c69e8f10dd900d161",
                  "width": 1092,
                  "height": 772
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/x8dkgvkifydf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ee39ea66e1d0b5d1b3ad61e605a8d4691605af43",
                    "width": 108,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/x8dkgvkifydf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=87595ecc8d8b55119d8efc2a38feef7aa5fde66c",
                    "width": 216,
                    "height": 152
                  },
                  {
                    "url": "https://preview.redd.it/x8dkgvkifydf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=927a3e4557a49d849c3ac05d42f641c1f03f7732",
                    "width": 320,
                    "height": 226
                  },
                  {
                    "url": "https://preview.redd.it/x8dkgvkifydf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=102a7ef47ffcfc42af3f68c707719a67b3a06693",
                    "width": 640,
                    "height": 452
                  },
                  {
                    "url": "https://preview.redd.it/x8dkgvkifydf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33eeb66844f42832a4c787326572da82b1cf74bc",
                    "width": 960,
                    "height": 678
                  },
                  {
                    "url": "https://preview.redd.it/x8dkgvkifydf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6fa5529d85f97775c2e8e72d4a8ac1dfa8303ef5",
                    "width": 1080,
                    "height": 763
                  }
                ],
                "variants": {},
                "id": "zjBGOwnOK5FNwOOG8xwPRLfKqGJmpMn1rW-t7NovVG8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m4fs2t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "5h3r_10ck",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/",
          "stickied": false,
          "url": "https://i.redd.it/x8dkgvkifydf1.jpeg",
          "subreddit_subscribers": 502029,
          "created_utc": 1752985024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3seok0yi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chess Llama - Training a tiny Llama model to play chess",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4s9nn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=c2d27d62605818b86d3bffc633dbeab2c0d9e9f7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753026673,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "lazy-guy.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://lazy-guy.github.io/blog/chessllama/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?auto=webp&amp;s=c6133b628c2b0056521f2950d4e14a09fc307b0b",
                  "width": 2560,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=002fa5232c800bdd9c21b2a388d77948455886e5",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1460d704e9500339a95db5e00375001f9af25395",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=26a4b2e1c7dffd3c4f330f839a42e91bbd7d4467",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=295d14462b9edc402d72b6436f8270d15502e48a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=13568e7a3d9e71f98a4dd9a930b9347b4731add0",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8f4c6ba316ab2cad496945b894725fdd92189c14",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m4s9nn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LazyGuy-_-",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/",
          "stickied": false,
          "url": "https://lazy-guy.github.io/blog/chessllama/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753026673,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI Model Juggler is a simple utility for serving multiple LLM and image generation backends or models as if simultaneously while only requiring enough VRAM for one at a time. It is written in Python, but has no external dependencies, making installation as simple as downloading the code.\n\nThat might sound a lot like [llama-swap](https://github.com/mostlygeek/llama-swap), but this one is considerably less sophisticated. If you're already using llama-swap and are happy with it, AI Model Juggler (I'm already starting to get tired of typing the name) will probably not be of much interest to you. I created this as a cursory reading of llama-swap's readme gave the impression that it only supports backends that support the OpenAI API, which excludes image generation through [Stable Diffusion WebUI Forge](https://github.com/lllyasviel/stable-diffusion-webui-forge).\n\nAI Model Juggler has a couple of tricks for keeping things fast. First, it allows unloading the image generation backend's model while keeping the backend running. This saves considerable time on image generation startup. It also supports saving and restoring llama.cpp's KV-cache to reduce prompt re-processing.\n\nThe project is in its very early stages, and the list of its limitations is longer than that of supported features. Most importantly, it currently only supports [llama.cpp](https://github.com/ggml-org/llama.cpp) for LLM inference and [Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) / [Stable Diffusion WebUI Forge](https://github.com/lllyasviel/stable-diffusion-webui-forge) for image generation. Other backends could be easily added, but it makes limited sense to add ones that don't either start fast or else allow fast model unloading and reloading. The current pair does very well on this front, to the point that switching between them is almost imperceptible in many contexts, provided that the storage utilized is sufficiently fast.\n\nThe way request routing currently works (redirection, not proxying) makes AI Model Juggler less than an ideal choice for using the backends' built-in web UIs, and is only intended for exposing the APIs. It works well with applications such as [SillyTavern](https://github.com/SillyTavern/SillyTavern), though.\n\nThe project more or less meets my needs in its current state, but I'd be happy to improve it to make it more useful for others, so feedback, suggestions and feature requests are welcome.",
          "author_fullname": "t2_1fw4w48cp6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Model Juggler automatically and transparently switches between LLM and image generation backends and models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4n7fh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6f9f7215ed6ac082234170cd71c3bd73e1bf985e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753012886,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI Model Juggler is a simple utility for serving multiple LLM and image generation backends or models as if simultaneously while only requiring enough VRAM for one at a time. It is written in Python, but has no external dependencies, making installation as simple as downloading the code.&lt;/p&gt;\n\n&lt;p&gt;That might sound a lot like &lt;a href=\"https://github.com/mostlygeek/llama-swap\"&gt;llama-swap&lt;/a&gt;, but this one is considerably less sophisticated. If you&amp;#39;re already using llama-swap and are happy with it, AI Model Juggler (I&amp;#39;m already starting to get tired of typing the name) will probably not be of much interest to you. I created this as a cursory reading of llama-swap&amp;#39;s readme gave the impression that it only supports backends that support the OpenAI API, which excludes image generation through &lt;a href=\"https://github.com/lllyasviel/stable-diffusion-webui-forge\"&gt;Stable Diffusion WebUI Forge&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;AI Model Juggler has a couple of tricks for keeping things fast. First, it allows unloading the image generation backend&amp;#39;s model while keeping the backend running. This saves considerable time on image generation startup. It also supports saving and restoring llama.cpp&amp;#39;s KV-cache to reduce prompt re-processing.&lt;/p&gt;\n\n&lt;p&gt;The project is in its very early stages, and the list of its limitations is longer than that of supported features. Most importantly, it currently only supports &lt;a href=\"https://github.com/ggml-org/llama.cpp\"&gt;llama.cpp&lt;/a&gt; for LLM inference and &lt;a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\"&gt;Stable Diffusion web UI&lt;/a&gt; / &lt;a href=\"https://github.com/lllyasviel/stable-diffusion-webui-forge\"&gt;Stable Diffusion WebUI Forge&lt;/a&gt; for image generation. Other backends could be easily added, but it makes limited sense to add ones that don&amp;#39;t either start fast or else allow fast model unloading and reloading. The current pair does very well on this front, to the point that switching between them is almost imperceptible in many contexts, provided that the storage utilized is sufficiently fast.&lt;/p&gt;\n\n&lt;p&gt;The way request routing currently works (redirection, not proxying) makes AI Model Juggler less than an ideal choice for using the backends&amp;#39; built-in web UIs, and is only intended for exposing the APIs. It works well with applications such as &lt;a href=\"https://github.com/SillyTavern/SillyTavern\"&gt;SillyTavern&lt;/a&gt;, though.&lt;/p&gt;\n\n&lt;p&gt;The project more or less meets my needs in its current state, but I&amp;#39;d be happy to improve it to make it more useful for others, so feedback, suggestions and feature requests are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/makedin/AI-Model-Juggler",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?auto=webp&amp;s=f17abd6cea43a3f64301938e08a099bc64415635",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c62c4ca86c3ececd35af14e2ce0f1d848f67bbdf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a12f0ea5347f97e7a7a871bb876dec1350b9714",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c69a8e55a6dc5eed5fd923d434e793c6bd556d8",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d20ef388781d4fdb76232ce88c5fd1aa46da9841",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=87a6cc187578bc386b19f7c4d805fa6a3a9dd7b5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0cba71694069453ca1e7a1265a9246ac76ddfbc8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m4n7fh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Casual-Godzilla",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/",
          "stickied": false,
          "url": "https://github.com/makedin/AI-Model-Juggler",
          "subreddit_subscribers": 502029,
          "created_utc": 1753012886,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nJust wanted to share a use case where local LLMs are genuinely helpful for daily workflows: file organization.\n\nI've been working on a C++ desktop app called *AI File Sorter* – it uses local LLMs via `llama.cpp` to help organize messy folders like `Downloads` or `Desktop`. Not sort files into folders solely based on extension or filename patterns, but based on what each file actually is supposed to do or does. Basically: what would normally take me a great deal of time for dragging and sorting can now be done in a few.\n\nIt's cross-platform (Windows/macOS/Linux), and fully open-source.\n\n🔗 [GitHub repo](https://github.com/hyperfield/ai-file-sorter)\n\n[Screenshot 1](https://i.imgur.com/HlEer13.png) - LLM selection and download\n\n[Screenshot 2](https://i.imgur.com/KCxk6Io.png) - Select a folder to scan\n\n[Screenshot 3](https://i.imgur.com/QTUG5KB.png) - Review, edit and confirm or continue later\n\nYou can download the installer for Windows in [Releases](https://github.com/hyperfield/ai-file-sorter/releases) or the Standalone ZIP from the [app's website](https://filesorter.app/download/). \n\nInstallers for Linux and macOS are coming up. You can, however, easily [build the app from source](https://github.com/hyperfield/ai-file-sorter/blob/main/README.md) for Linux or macOS.\n\n---\n\n### 🧠 How it works\n\n1. You choose which model you want the app to interface with. The app will download the model for you. You can switch models later on.\n\n2. You point the app at a folder, and it feeds a prompt to the model.\n\n3. It then suggests folder categories like `Operating Systems / Linux distributions`, `Programming / Scripts`, `Images / Logos`, etc.\n \nYou can review and approve before anything is moved, and you can continue the same sorting session later from where you left off.\n\nModels tested:\n- LLaMa 3 (3B)\n- Mistral (7B)\n- With CUDA / OpenCL / OpenBLAS support\n- Other GPU back-ends can also be enabled on `llama.cpp` compile\n\n---\n### Try it out\n\n* Windows: [SourceForge](https://sourceforge.net/projects/ai-file-sorter/) or [GitHub Releases](https://github.com/hyperfield/ai-file-sorter/releases)\n* Linux/macOS: build from source (instructions in the [README](https://github.com/hyperfield/ai-file-sorter/blob/main/README.md))\n\n---\n\nI’d love feedback from others using local models, especially around:\n- Speed and accuracy in categorizing files\n- Model suggestions that might be more efficient\n- Any totally different way to approach this problem?\n- Is this local LLM use case actually useful to you or people like you, or should the app shift its focus?\n\nThanks for reading!",
          "author_fullname": "t2_q35lr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a desktop tool to auto-organize files using local LLMs (open source, cross-platform)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4sdsg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753029037,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753026949,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share a use case where local LLMs are genuinely helpful for daily workflows: file organization.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a C++ desktop app called &lt;em&gt;AI File Sorter&lt;/em&gt; – it uses local LLMs via &lt;code&gt;llama.cpp&lt;/code&gt; to help organize messy folders like &lt;code&gt;Downloads&lt;/code&gt; or &lt;code&gt;Desktop&lt;/code&gt;. Not sort files into folders solely based on extension or filename patterns, but based on what each file actually is supposed to do or does. Basically: what would normally take me a great deal of time for dragging and sorting can now be done in a few.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s cross-platform (Windows/macOS/Linux), and fully open-source.&lt;/p&gt;\n\n&lt;p&gt;🔗 &lt;a href=\"https://github.com/hyperfield/ai-file-sorter\"&gt;GitHub repo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/HlEer13.png\"&gt;Screenshot 1&lt;/a&gt; - LLM selection and download&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/KCxk6Io.png\"&gt;Screenshot 2&lt;/a&gt; - Select a folder to scan&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/QTUG5KB.png\"&gt;Screenshot 3&lt;/a&gt; - Review, edit and confirm or continue later&lt;/p&gt;\n\n&lt;p&gt;You can download the installer for Windows in &lt;a href=\"https://github.com/hyperfield/ai-file-sorter/releases\"&gt;Releases&lt;/a&gt; or the Standalone ZIP from the &lt;a href=\"https://filesorter.app/download/\"&gt;app&amp;#39;s website&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Installers for Linux and macOS are coming up. You can, however, easily &lt;a href=\"https://github.com/hyperfield/ai-file-sorter/blob/main/README.md\"&gt;build the app from source&lt;/a&gt; for Linux or macOS.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;🧠 How it works&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;You choose which model you want the app to interface with. The app will download the model for you. You can switch models later on.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;You point the app at a folder, and it feeds a prompt to the model.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It then suggests folder categories like &lt;code&gt;Operating Systems / Linux distributions&lt;/code&gt;, &lt;code&gt;Programming / Scripts&lt;/code&gt;, &lt;code&gt;Images / Logos&lt;/code&gt;, etc.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;You can review and approve before anything is moved, and you can continue the same sorting session later from where you left off.&lt;/p&gt;\n\n&lt;p&gt;Models tested:\n- LLaMa 3 (3B)\n- Mistral (7B)\n- With CUDA / OpenCL / OpenBLAS support\n- Other GPU back-ends can also be enabled on &lt;code&gt;llama.cpp&lt;/code&gt; compile&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Try it out&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Windows: &lt;a href=\"https://sourceforge.net/projects/ai-file-sorter/\"&gt;SourceForge&lt;/a&gt; or &lt;a href=\"https://github.com/hyperfield/ai-file-sorter/releases\"&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Linux/macOS: build from source (instructions in the &lt;a href=\"https://github.com/hyperfield/ai-file-sorter/blob/main/README.md\"&gt;README&lt;/a&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I’d love feedback from others using local models, especially around:\n- Speed and accuracy in categorizing files\n- Model suggestions that might be more efficient\n- Any totally different way to approach this problem?\n- Is this local LLM use case actually useful to you or people like you, or should the app shift its focus?&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/III9L8U9ufnPqsufLcON9Z0Me6NkILlBBY6rajY_EiU.png?auto=webp&amp;s=73a4e38a512ce12535a08b424783298e442b2927",
                  "width": 720,
                  "height": 589
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/III9L8U9ufnPqsufLcON9Z0Me6NkILlBBY6rajY_EiU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3876969ecc31a39da4b700e4c7291afd8be438a4",
                    "width": 108,
                    "height": 88
                  },
                  {
                    "url": "https://external-preview.redd.it/III9L8U9ufnPqsufLcON9Z0Me6NkILlBBY6rajY_EiU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=78d9799db9758782cf7b3423ce6116b4b9b42c06",
                    "width": 216,
                    "height": 176
                  },
                  {
                    "url": "https://external-preview.redd.it/III9L8U9ufnPqsufLcON9Z0Me6NkILlBBY6rajY_EiU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=01fcec2b5fb10e54d5e383f9ba876049b3662972",
                    "width": 320,
                    "height": 261
                  },
                  {
                    "url": "https://external-preview.redd.it/III9L8U9ufnPqsufLcON9Z0Me6NkILlBBY6rajY_EiU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a81f155f12b523ea09b2637b0162b58c5c60cf57",
                    "width": 640,
                    "height": 523
                  }
                ],
                "variants": {},
                "id": "III9L8U9ufnPqsufLcON9Z0Me6NkILlBBY6rajY_EiU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4sdsg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ph0tone",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4sdsg/i_built_a_desktop_tool_to_autoorganize_files/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4sdsg/i_built_a_desktop_tool_to_autoorganize_files/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753026949,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In my tests to get a reliable Ngrok alternative for https with Open WebUI, I had Llama.cpp's WebUI served over https in a subdomain that's not listed anywhere. Less than 45 minutes after being online, the hacking attempts started.\n\nI had a ultra long API key setup so after a while of bruteforce attack, they switched to try and access some known settings/config files.\n\nDon't let your guard down.",
          "author_fullname": "t2_cy3wb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hackers are never sleeping",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4ag6u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 317,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 317,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752968420,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my tests to get a reliable Ngrok alternative for https with Open WebUI, I had Llama.cpp&amp;#39;s WebUI served over https in a subdomain that&amp;#39;s not listed anywhere. Less than 45 minutes after being online, the hacking attempts started.&lt;/p&gt;\n\n&lt;p&gt;I had a ultra long API key setup so after a while of bruteforce attack, they switched to try and access some known settings/config files.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t let your guard down.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4ag6u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DrVonSinistro",
          "discussion_type": null,
          "num_comments": 75,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752968420,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m writing a novel that’s near-future literary fiction / soft dystopia / psychological tragedy with erotic elements. I’m subscribed to ChatGPT and Claude, but built a PC to move to local AI without limits and guardrails for the NSFW stuff. \n\nWhat’s the best workflow for me? I downloaded Oobabooga and a MythosMax model, but not really sure how to add in context and instructions. There are pre populated templates and I don’t understand if I’m supposed to work within those or overwrite them. Also not sure if these were the best choices so appreciate any recommendations. \n\nWant something that’s really good for my genre, especially dark/gritty/nsfw with lyrical prose and stream of consciousness style. \n\nMy hardware: \n- CPU: Ryzen 7950x\n- GPU: 3090\n- RAM: 96GB 6400mhz \n\n",
          "author_fullname": "t2_hhsbgspn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best novel writing workflow?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m5586n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753059242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m writing a novel that’s near-future literary fiction / soft dystopia / psychological tragedy with erotic elements. I’m subscribed to ChatGPT and Claude, but built a PC to move to local AI without limits and guardrails for the NSFW stuff. &lt;/p&gt;\n\n&lt;p&gt;What’s the best workflow for me? I downloaded Oobabooga and a MythosMax model, but not really sure how to add in context and instructions. There are pre populated templates and I don’t understand if I’m supposed to work within those or overwrite them. Also not sure if these were the best choices so appreciate any recommendations. &lt;/p&gt;\n\n&lt;p&gt;Want something that’s really good for my genre, especially dark/gritty/nsfw with lyrical prose and stream of consciousness style. &lt;/p&gt;\n\n&lt;p&gt;My hardware: \n- CPU: Ryzen 7950x\n- GPU: 3090\n- RAM: 96GB 6400mhz &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5586n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AccidentalFolklore",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753059242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking for a solid RAG pipeline that works well with SGLang + AnythingLLM. Something that can handle technical docs, math textbooks with lots of formulas, research papers, and diagrams. The RAG in AnythingLLM is, well, not great. What setups actually work for you?",
          "author_fullname": "t2_indszg3q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best RAG pipeline for math-heavy documents?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m52h8x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753051632,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking for a solid RAG pipeline that works well with SGLang + AnythingLLM. Something that can handle technical docs, math textbooks with lots of formulas, research papers, and diagrams. The RAG in AnythingLLM is, well, not great. What setups actually work for you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m52h8x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PO-ll-UX",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52h8x/best_rag_pipeline_for_mathheavy_documents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52h8x/best_rag_pipeline_for_mathheavy_documents/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753051632,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried Mimo-7B. It has a decent quality at this size. but for nsfw, it can only work with anime pics. for realistic, it refused.",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "which is the best tiny vlm to recognize nsfw pics?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4qdo6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753021945,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried Mimo-7B. It has a decent quality at this size. but for nsfw, it can only work with anime pics. for realistic, it refused.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4qdo6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753021945,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for a solution to translate story text from a game. The translation is very domain specific to the fantasy world of the game.\n\nJP-&gt;EN only.\n\nThe text follows a visual novel format, so previous lines provide context to future lines. Generally there's a few hundred sentences per \"chapter\". This can be broken down into \"scenes\" which are generally 50-100 sentences each.\n\nTraining data available:\n\n* Term/Name 1:1 mappings, single word (5000-10000)\n* Lore information EN:JP mapping (few MB of text)\n* Unmapped lore information in both languages - basically scrapes of wikis\n* Per-sentence EN:JP mapping. (100MBs of text)\n* Per-scene EN:JP mapping. (same text of the above)\n\nAssume resources for a local LLM won't be an issue, but nothing into extreme territory (100GB+ VRAM isn't happening for inference, but I can rent servers e.g. 8xH200 140GB for short periods to train).\n\n* Are there any other fine tuning methods I should look into for this domain?\n* What would be a good starting point? (this is an academic exercise for now, so any licence is fine)",
          "author_fullname": "t2_6qhgs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Questions about AI for translation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4zyv1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753048224,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753045214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a solution to translate story text from a game. The translation is very domain specific to the fantasy world of the game.&lt;/p&gt;\n\n&lt;p&gt;JP-&amp;gt;EN only.&lt;/p&gt;\n\n&lt;p&gt;The text follows a visual novel format, so previous lines provide context to future lines. Generally there&amp;#39;s a few hundred sentences per &amp;quot;chapter&amp;quot;. This can be broken down into &amp;quot;scenes&amp;quot; which are generally 50-100 sentences each.&lt;/p&gt;\n\n&lt;p&gt;Training data available:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Term/Name 1:1 mappings, single word (5000-10000)&lt;/li&gt;\n&lt;li&gt;Lore information EN:JP mapping (few MB of text)&lt;/li&gt;\n&lt;li&gt;Unmapped lore information in both languages - basically scrapes of wikis&lt;/li&gt;\n&lt;li&gt;Per-sentence EN:JP mapping. (100MBs of text)&lt;/li&gt;\n&lt;li&gt;Per-scene EN:JP mapping. (same text of the above)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Assume resources for a local LLM won&amp;#39;t be an issue, but nothing into extreme territory (100GB+ VRAM isn&amp;#39;t happening for inference, but I can rent servers e.g. 8xH200 140GB for short periods to train).&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are there any other fine tuning methods I should look into for this domain?&lt;/li&gt;\n&lt;li&gt;What would be a good starting point? (this is an academic exercise for now, so any licence is fine)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4zyv1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "neobenedict",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4zyv1/questions_about_ai_for_translation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4zyv1/questions_about_ai_for_translation/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753045214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hallucination is a real problem with LLMs but I wonder is it such a hard problem to assign a confidence value to an inference result?",
          "author_fullname": "t2_bwdb8qqfj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why are LLMs not able to give an estimate on their own confidence or say that they are not sure about something?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4z64o",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753043256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hallucination is a real problem with LLMs but I wonder is it such a hard problem to assign a confidence value to an inference result?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4z64o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MarinatedPickachu",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753043256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've posted on this sub before, but context is that me and a small team are working on a [benchmark](https://www.designarena.ai/) to evaluate how good LLMs are at producing UIs and frontends that are engaging and satisfiable for people. \n\nRight now, working on adding more models, and specifically open source models developed by individual developers (or a small group of developers). Above is the current top 10 in the leaderboard. If you're interested, just send me a DM. \n\nHere are some requirements:  \n1. Inference needs to be fairly quick (max should take 3 minutes on average). Models are writing html/css/js code on the order of 4K-10K tokens on average.   \n2. Give us a logo and name for the provider/org you want the model to be associated with   \n3. An api endpoint that we can call with your desired parameters for the model. It needs to ideally be able to support a few concurrent requests at a time and around \\~500 requests a day (though you can rate limit us if you would like to cap it at a smaller number)",
          "author_fullname": "t2_98ouo03z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone interested in adding their fine-tuned / open source models to this benchmark?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4vk88",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fqijn5bhzBdMFhAp7i-Tq_mF0JLUXtj5zsNGHbT_tLI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753034503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve posted on this sub before, but context is that me and a small team are working on a &lt;a href=\"https://www.designarena.ai/\"&gt;benchmark&lt;/a&gt; to evaluate how good LLMs are at producing UIs and frontends that are engaging and satisfiable for people. &lt;/p&gt;\n\n&lt;p&gt;Right now, working on adding more models, and specifically open source models developed by individual developers (or a small group of developers). Above is the current top 10 in the leaderboard. If you&amp;#39;re interested, just send me a DM. &lt;/p&gt;\n\n&lt;p&gt;Here are some requirements:&lt;br/&gt;\n1. Inference needs to be fairly quick (max should take 3 minutes on average). Models are writing html/css/js code on the order of 4K-10K tokens on average.&lt;br/&gt;\n2. Give us a logo and name for the provider/org you want the model to be associated with&lt;br/&gt;\n3. An api endpoint that we can call with your desired parameters for the model. It needs to ideally be able to support a few concurrent requests at a time and around ~500 requests a day (though you can rate limit us if you would like to cap it at a smaller number)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4i6kqeqjj2ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4i6kqeqjj2ef1.png?auto=webp&amp;s=5b9c376a148a9fccee3623e23e9303534a9b458c",
                  "width": 962,
                  "height": 707
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4i6kqeqjj2ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f15f6bd7f09b6acb627b72a6e55823d63b6f4f07",
                    "width": 108,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/4i6kqeqjj2ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=951ad947e3a1399f746c3b5a20c517ac05cb225c",
                    "width": 216,
                    "height": 158
                  },
                  {
                    "url": "https://preview.redd.it/4i6kqeqjj2ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee8273204ddd87ee040b4320cc9cc2d7c7a0abd5",
                    "width": 320,
                    "height": 235
                  },
                  {
                    "url": "https://preview.redd.it/4i6kqeqjj2ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=960183ee3de41ba775a6af7c43eb2a2cd3223994",
                    "width": 640,
                    "height": 470
                  },
                  {
                    "url": "https://preview.redd.it/4i6kqeqjj2ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ebd46adab4f255e64935cfbb3e19daa433c2982",
                    "width": 960,
                    "height": 705
                  }
                ],
                "variants": {},
                "id": "P2rEVGh37eAG9tNvImu7si--h39TikdcRNJwTqSPKkA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4vk88",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished-Copy332",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4vk88/anyone_interested_in_adding_their_finetuned_open/",
          "stickied": false,
          "url": "https://i.redd.it/4i6kqeqjj2ef1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1753034503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I use LLMs for semantic text chunking. Models in the range of 24 to 32B, quantized between Q4 and Q6, give me the most robust results. Mistral-Small-3.2, Gemma-27B and Qwen3-32B all work well, Mistral and Gemma seem to be a bit better with certain non-English languages.\n\nWhen I go lower, results are still ok with Qwen3-14B, but below that reconstruction errors go up quickly.\n\nSince the process is rather token-intensive and slow (reproducing the entire text in chunked form), I'm considering a fine-tune of a smallish LLM. I'd be happy to hear some tips from people who are doing similar stuff, like other models to consider or tweaks to make the output more robust.",
          "author_fullname": "t2_19icy5o1t9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Semantic chunking using LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4lxak",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753008337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use LLMs for semantic text chunking. Models in the range of 24 to 32B, quantized between Q4 and Q6, give me the most robust results. Mistral-Small-3.2, Gemma-27B and Qwen3-32B all work well, Mistral and Gemma seem to be a bit better with certain non-English languages.&lt;/p&gt;\n\n&lt;p&gt;When I go lower, results are still ok with Qwen3-14B, but below that reconstruction errors go up quickly.&lt;/p&gt;\n\n&lt;p&gt;Since the process is rather token-intensive and slow (reproducing the entire text in chunked form), I&amp;#39;m considering a fine-tune of a smallish LLM. I&amp;#39;d be happy to hear some tips from people who are doing similar stuff, like other models to consider or tweaks to make the output more robust.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4lxak",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mnze_brngo_7325",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753008337,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google claim Gemini own the pareto frontier. Deepseek looks good competitive.",
          "author_fullname": "t2_8jqx3m14",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Price performance comparison from the Gemini 2.5 Paper",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 83,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m46w7u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 185,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 185,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/AdlJvTRGsoNT-RFVRbGdqXfoRoFT4w7deWxoKU1H4R8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752958757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google claim Gemini own the pareto frontier. Deepseek looks good competitive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/032gntpz9wdf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/032gntpz9wdf1.png?auto=webp&amp;s=47ffe2734ff0257f08b273af9e7bfe03ae6861f1",
                  "width": 1990,
                  "height": 1188
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/032gntpz9wdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd0736e5beb6ce1c6f1167121fb4b30dc4f25bee",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/032gntpz9wdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb18c8981f81696707da7de01c2fe8650cad70be",
                    "width": 216,
                    "height": 128
                  },
                  {
                    "url": "https://preview.redd.it/032gntpz9wdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb4d2f55f7cd425109212a74c20c33a781f9711e",
                    "width": 320,
                    "height": 191
                  },
                  {
                    "url": "https://preview.redd.it/032gntpz9wdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76ac871f17eb719d4d9accb31d9e291dab5b757c",
                    "width": 640,
                    "height": 382
                  },
                  {
                    "url": "https://preview.redd.it/032gntpz9wdf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=98a84435f274234d64a2667b9b1154a8054d3568",
                    "width": 960,
                    "height": 573
                  },
                  {
                    "url": "https://preview.redd.it/032gntpz9wdf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=351633eb712e561ad4da14db7eccb4837917327b",
                    "width": 1080,
                    "height": 644
                  }
                ],
                "variants": {},
                "id": "gCIhR5K46bA_B4_yLabP7iK4ZDC9NYQ8TfgQeYdL1oQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m46w7u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeltaSqueezer",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/",
          "stickied": false,
          "url": "https://i.redd.it/032gntpz9wdf1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1752958757,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've tried it and it's quite good (latest) w/ Cline was my set-up. Why is no one talking about it? 🤔",
          "author_fullname": "t2_vry7yeqmi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do we think of Devstral then?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4zx8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753045105,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried it and it&amp;#39;s quite good (latest) w/ Cline was my set-up. Why is no one talking about it? 🤔&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4zx8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "teleadx",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4zx8s/what_do_we_think_of_devstral_then/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4zx8s/what_do_we_think_of_devstral_then/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753045105,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Simple question, how offloading layers work in LLM, so for example if i have 24Gig rtx 3090 and offloading layers, lets say 5 gig each, so the model will offload only 4 of them leaving remaining 4 giga dormant or it will utilize it somehow as well?  Asking because many time seeing task menager under performance tab I see unused Vram even though only few layers has been offloaded out of 40 or 60. So it is kind of waste of resources then. Right? ",
          "author_fullname": "t2_12dubk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Offloading layers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4zogr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753044500,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simple question, how offloading layers work in LLM, so for example if i have 24Gig rtx 3090 and offloading layers, lets say 5 gig each, so the model will offload only 4 of them leaving remaining 4 giga dormant or it will utilize it somehow as well?  Asking because many time seeing task menager under performance tab I see unused Vram even though only few layers has been offloaded out of 40 or 60. So it is kind of waste of resources then. Right? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4zogr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PawelSalsa",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4zogr/offloading_layers/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4zogr/offloading_layers/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753044500,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently building a small app, and I don't want to use large LLMs to call the tools. Instead, I want to use small open-source LLMs for that task. So I was wondering, what are the best models for such a use case?",
          "author_fullname": "t2_8h0wfu45",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Small LLMs for Tool Calling?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4vcnz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753034017,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently building a small app, and I don&amp;#39;t want to use large LLMs to call the tools. Instead, I want to use small open-source LLMs for that task. So I was wondering, what are the best models for such a use case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4vcnz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Saniok_Digital",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4vcnz/best_small_llms_for_tool_calling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4vcnz/best_small_llms_for_tool_calling/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753034017,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys, how are you?\n\nI'm doing research on the automation market — especially automation for small businesses, repetitive tasks, integrations with systems, bots, among other things. I want to better understand two specific pains:\n\n1. For those who want to sell automations (freelancers, agencies, devs, etc.):\n – What has made it difficult to close customers?\n – Where do you find (or miss) opportunities?\n – What does the customer generally not understand or value?\n – How do you validate that automation makes sense for the client’s business?\n\n\n2. For those who want to hire someone to automate things:\n – What is the biggest difficulty in finding someone trustworthy?\n – What makes you trust (or distrust) those who offer the service?\n – Where do you usually look for this type of professional?\n\n\n\nThe idea is to understand the pain on both sides — those who sell and those who hire — to come up with a more practical and useful solution. Any experience you have (good or bad) helps a lot!\n\nIt would be really appreciated if you could share 🙏",
          "author_fullname": "t2_sos8vp8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the biggest challenges in selling automations (and finding someone to implement them)? Looking for real insights from everyone!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m52sj5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753052472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, how are you?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing research on the automation market — especially automation for small businesses, repetitive tasks, integrations with systems, bots, among other things. I want to better understand two specific pains:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;For those who want to sell automations (freelancers, agencies, devs, etc.):\n – What has made it difficult to close customers?\n – Where do you find (or miss) opportunities?\n – What does the customer generally not understand or value?\n – How do you validate that automation makes sense for the client’s business?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For those who want to hire someone to automate things:\n – What is the biggest difficulty in finding someone trustworthy?\n – What makes you trust (or distrust) those who offer the service?\n – Where do you usually look for this type of professional?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The idea is to understand the pain on both sides — those who sell and those who hire — to come up with a more practical and useful solution. Any experience you have (good or bad) helps a lot!&lt;/p&gt;\n\n&lt;p&gt;It would be really appreciated if you could share 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m52sj5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Present-Entry8676",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52sj5/what_are_the_biggest_challenges_in_selling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52sj5/what_are_the_biggest_challenges_in_selling/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753052472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all — I’m working on building out Atlas Grid, a new network of U.S.-based GPU hosts focused on reliability and simplicity for devs and researchers.\n\nWe’ve got a few committed rigs already online, including a 3080 Ti and 3070 Ti, running on stable secondary machines here in the U.S. — ideal for fine-tuning, inference, or small-scale training jobs.\n\nWe’re pricing below vast.ai, and with a more few advantages:\n\nAll domestic hosts = lower latency, no language or support barriers\n\nPrepaid options = no surprise fees or platform overhead\n\nVetted machines only = Docker/NVIDIA-ready, high uptime\n\nIf you’re working on a project and want affordable compute, DM me or comment below! ",
          "author_fullname": "t2_4rgw41xb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "U.S. GPU compute available",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m52r9e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.42,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753052387,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all — I’m working on building out Atlas Grid, a new network of U.S.-based GPU hosts focused on reliability and simplicity for devs and researchers.&lt;/p&gt;\n\n&lt;p&gt;We’ve got a few committed rigs already online, including a 3080 Ti and 3070 Ti, running on stable secondary machines here in the U.S. — ideal for fine-tuning, inference, or small-scale training jobs.&lt;/p&gt;\n\n&lt;p&gt;We’re pricing below vast.ai, and with a more few advantages:&lt;/p&gt;\n\n&lt;p&gt;All domestic hosts = lower latency, no language or support barriers&lt;/p&gt;\n\n&lt;p&gt;Prepaid options = no surprise fees or platform overhead&lt;/p&gt;\n\n&lt;p&gt;Vetted machines only = Docker/NVIDIA-ready, high uptime&lt;/p&gt;\n\n&lt;p&gt;If you’re working on a project and want affordable compute, DM me or comment below! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m52r9e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Professional_2726",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52r9e/us_gpu_compute_available/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52r9e/us_gpu_compute_available/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753052387,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Latest update: 21/07/2025)\n\nI've just extracted the FULL Orchids.app system prompt and internal tools. Over 200 lines.\n\nYou can check it out at https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FULL Orchids.app System Prompt and Tools",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m52e50",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753051408,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Latest update: 21/07/2025)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just extracted the FULL Orchids.app system prompt and internal tools. Over 200 lines.&lt;/p&gt;\n\n&lt;p&gt;You can check it out at &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3J3iRUCJuzk-_Ka1tn-hg8r5ofkLMUYaOdxx_goyGNc.png?auto=webp&amp;s=308eadd841eda0f47826dd1be6f7b6d0c94baa2a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3J3iRUCJuzk-_Ka1tn-hg8r5ofkLMUYaOdxx_goyGNc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=713542cecbbdf7b9088a103dbe84f99ea6238c0b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/3J3iRUCJuzk-_Ka1tn-hg8r5ofkLMUYaOdxx_goyGNc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa54f205f8524caab5fc7af531b033a54efe06f5",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/3J3iRUCJuzk-_Ka1tn-hg8r5ofkLMUYaOdxx_goyGNc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac4b44c82a6310926ef9b6cdab2b27e64bc88df4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/3J3iRUCJuzk-_Ka1tn-hg8r5ofkLMUYaOdxx_goyGNc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f47e76648a5f314fbdcc8e33e70307016730f8fa",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/3J3iRUCJuzk-_Ka1tn-hg8r5ofkLMUYaOdxx_goyGNc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c95568cc76ecb3b065019dbd88b29af03166214c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/3J3iRUCJuzk-_Ka1tn-hg8r5ofkLMUYaOdxx_goyGNc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f0f35546afcae343c2bd48c09308b6337dc63a8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "3J3iRUCJuzk-_Ka1tn-hg8r5ofkLMUYaOdxx_goyGNc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m52e50",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52e50/full_orchidsapp_system_prompt_and_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52e50/full_orchidsapp_system_prompt_and_tools/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753051408,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I woud like to test limits of local LLMs. I use LM Studio. Is there a tool repository I can use? The tool selection in LM Studio is limited to RAG and js execution.",
          "author_fullname": "t2_4qv6wvph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tools for LM Studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4tll3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753029849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I woud like to test limits of local LLMs. I use LM Studio. Is there a tool repository I can use? The tool selection in LM Studio is limited to RAG and js execution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4tll3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mk-Daniel",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4tll3/tools_for_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4tll3/tools_for_lm_studio/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753029849,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone, \n\nI’ve been trying to OCR tables out of bank statements that only exist as scanned images or non‐selectable PDFs, but I keep running into walls—Tesseract/PaddleOCR gets the text, Camelot/pdfplumber and OpenCV sometimes find gridlines, and regex hacks help a bit, but nothing works reliably across different layouts. I am wondering if  I am missing a lightweight library that can spot table cells in an image (even with multi-line descriptions or odd spacing) and spit out a clean CSV or DataFrame. This can then go to an LLM for further data extraction that I need. \n\nHas anyone cracked this “any image-based table” problem in a robust, bank-agnostic way? Any pointers, code snippets, or friendly tips would be hugely appreciated!\n\n  \nThank you.",
          "author_fullname": "t2_9lrempgf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unstructured financial data for Lama3B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m52b7l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753051197,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt;\n\n&lt;p&gt;I’ve been trying to OCR tables out of bank statements that only exist as scanned images or non‐selectable PDFs, but I keep running into walls—Tesseract/PaddleOCR gets the text, Camelot/pdfplumber and OpenCV sometimes find gridlines, and regex hacks help a bit, but nothing works reliably across different layouts. I am wondering if  I am missing a lightweight library that can spot table cells in an image (even with multi-line descriptions or odd spacing) and spit out a clean CSV or DataFrame. This can then go to an LLM for further data extraction that I need. &lt;/p&gt;\n\n&lt;p&gt;Has anyone cracked this “any image-based table” problem in a robust, bank-agnostic way? Any pointers, code snippets, or friendly tips would be hugely appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m52b7l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LiveMud8172",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52b7l/unstructured_financial_data_for_lama3b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52b7l/unstructured_financial_data_for_lama3b/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753051197,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.",
          "author_fullname": "t2_7kg5p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A new paper from Apple shows you can tack on Multi-Token Prediction to any LLM with no loss in quality",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3vqom",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 448,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 448,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752930223,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.11851",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m3vqom",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kooshi_Govno",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.11851",
          "subreddit_subscribers": 502029,
          "created_utc": 1752930223,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am looking for small/slow LLM capable to describe an image scenery. Speed/latency is irrelevant. ",
          "author_fullname": "t2_s3q8krki",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Small LLM capable to describe images in greater details.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4op39",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753017460,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for small/slow LLM capable to describe an image scenery. Speed/latency is irrelevant. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4op39",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "valijali32",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4op39/small_llm_capable_to_describe_images_in_greater/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4op39/small_llm_capable_to_describe_images_in_greater/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753017460,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using [NyxKrage's VRAM Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator) for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. \n\nSo, I smacked together a cpp version of the calculator in a few hours. \n\nThere are two modes:\n\nCall the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.\n\nI'm planning to add functionality like calculating parameters, letting you use it without a \\`config.json\\`, etc. If you want anything added, add a Github Issue or feel free to fork it.\n\n[Link Here](https://github.com/71cj34/llmcalculator)",
          "author_fullname": "t2_1e392klobf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Made a local C++ utility to calculate RAM needed to fit a quantized model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4djo6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752977752,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using &lt;a href=\"https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator\"&gt;NyxKrage&amp;#39;s VRAM Calculator&lt;/a&gt; for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. &lt;/p&gt;\n\n&lt;p&gt;So, I smacked together a cpp version of the calculator in a few hours. &lt;/p&gt;\n\n&lt;p&gt;There are two modes:&lt;/p&gt;\n\n&lt;p&gt;Call the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to add functionality like calculating parameters, letting you use it without a `config.json`, etc. If you want anything added, add a Github Issue or feel free to fork it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/71cj34/llmcalculator\"&gt;Link Here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?auto=webp&amp;s=3954f306637b48462dfbce5f7db7c79685858088",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dab6a28dd0587077a356703d53799748a71b3d4c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d78410b2ff71ed283d47b779a9b2110015782ee",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ef9b2c4152dc5a850df1c49798c53736aa5db4a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d0e9c1ae5fa992910c2ee3bd14f2d87ae72e5cd",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e15e9744e01b7fabc83b056fd7bef010c2860e32",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4343d18b58d772009825227390e2d2072d4bd48",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m4djo6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "philetairus_socius",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752977752,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve tested a few tools, including chatterbox. The problem is, even after uploading a clear and long reference audio, it couldn’t replicate the same tone and pacing on the generated audio. Chatterbox failed to match the tone accurately with the cloned voice.\n\nI decided to try minimax audio and while it didn’t mimic the cloned voice exactly, it came pretty close to the original tone of the cloned voice. But sadly we can’t install it locally. :/\n\nIs there any tool out there that can do high quality voice cloning for TTS and also run locally?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any Proper high quality Voice cloning for TTS tool?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4q4dx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753021275,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve tested a few tools, including chatterbox. The problem is, even after uploading a clear and long reference audio, it couldn’t replicate the same tone and pacing on the generated audio. Chatterbox failed to match the tone accurately with the cloned voice.&lt;/p&gt;\n\n&lt;p&gt;I decided to try minimax audio and while it didn’t mimic the cloned voice exactly, it came pretty close to the original tone of the cloned voice. But sadly we can’t install it locally. :/&lt;/p&gt;\n\n&lt;p&gt;Is there any tool out there that can do high quality voice cloning for TTS and also run locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4q4dx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4q4dx/any_proper_high_quality_voice_cloning_for_tts_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4q4dx/any_proper_high_quality_voice_cloning_for_tts_tool/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753021275,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI've been running some LLMs locally and was curious how others are keeping tabs on model performance, latency, and token usage. I didn’t find a lightweight tool that fit my needs, so I started working on one myself.\n\nIt’s a simple dashboard + API setup that helps me monitor and analyze what's going on under the hood mainly for performance tuning and observability.\nStill early days, but it’s been surprisingly useful for understanding how my models are behaving over time.\n\nCurious how the rest of you handle observability. Do you use logs, custom scripts, or something else?\nI’ll drop a link in the comments in case anyone wants to check it out or build on top of it.",
          "author_fullname": "t2_j09rzafr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone else tracking their local LLMs’ performance? I built a tool to make it easier",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4rmd5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753025085,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been running some LLMs locally and was curious how others are keeping tabs on model performance, latency, and token usage. I didn’t find a lightweight tool that fit my needs, so I started working on one myself.&lt;/p&gt;\n\n&lt;p&gt;It’s a simple dashboard + API setup that helps me monitor and analyze what&amp;#39;s going on under the hood mainly for performance tuning and observability.\nStill early days, but it’s been surprisingly useful for understanding how my models are behaving over time.&lt;/p&gt;\n\n&lt;p&gt;Curious how the rest of you handle observability. Do you use logs, custom scripts, or something else?\nI’ll drop a link in the comments in case anyone wants to check it out or build on top of it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4rmd5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hades_7658",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4rmd5/anyone_else_tracking_their_local_llms_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4rmd5/anyone_else_tracking_their_local_llms_performance/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753025085,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I would like to get my local LLM to be able to read and write files. I know they can do it through coding tools but I would like to be able to do at a more basic level. Would I need to use a MCP server or could LMStudio/Ollama do this? I have searched and found \"[lm-tool-writer](https://lmstudio.ai/yagil/lm-tool-writer)\" and \"llmbasedos\". I don't know what langchain is but can this do it? Thanks",
          "author_fullname": "t2_xt6jr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM file access",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4ukgp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753032154,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to get my local LLM to be able to read and write files. I know they can do it through coding tools but I would like to be able to do at a more basic level. Would I need to use a MCP server or could LMStudio/Ollama do this? I have searched and found &amp;quot;&lt;a href=\"https://lmstudio.ai/yagil/lm-tool-writer\"&gt;lm-tool-writer&lt;/a&gt;&amp;quot; and &amp;quot;llmbasedos&amp;quot;. I don&amp;#39;t know what langchain is but can this do it? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jfzX48pNXxWiv1rFE02VrWXPWTtL7fsdf2LTd8mixlQ.png?auto=webp&amp;s=5c8ce58e82a6a71d27ac2675fa858c1970e44256",
                  "width": 3248,
                  "height": 3192
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jfzX48pNXxWiv1rFE02VrWXPWTtL7fsdf2LTd8mixlQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e7c8590a62cea205bab07f4af2106acd17647234",
                    "width": 108,
                    "height": 106
                  },
                  {
                    "url": "https://external-preview.redd.it/jfzX48pNXxWiv1rFE02VrWXPWTtL7fsdf2LTd8mixlQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8e97ff3e3cbd559f5cfec4d45354072f9199795",
                    "width": 216,
                    "height": 212
                  },
                  {
                    "url": "https://external-preview.redd.it/jfzX48pNXxWiv1rFE02VrWXPWTtL7fsdf2LTd8mixlQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=148ac8db409ea255e35d7d096d276e209f723056",
                    "width": 320,
                    "height": 314
                  },
                  {
                    "url": "https://external-preview.redd.it/jfzX48pNXxWiv1rFE02VrWXPWTtL7fsdf2LTd8mixlQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e5387370955868bfeaf5b179af9bcd9f4d386e0",
                    "width": 640,
                    "height": 628
                  },
                  {
                    "url": "https://external-preview.redd.it/jfzX48pNXxWiv1rFE02VrWXPWTtL7fsdf2LTd8mixlQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3006b837409bae9fc6c17ec6d26c491ab030c3c",
                    "width": 960,
                    "height": 943
                  },
                  {
                    "url": "https://external-preview.redd.it/jfzX48pNXxWiv1rFE02VrWXPWTtL7fsdf2LTd8mixlQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=917d5d4a75a9072697370e444cfd616f53fb8520",
                    "width": 1080,
                    "height": 1061
                  }
                ],
                "variants": {},
                "id": "jfzX48pNXxWiv1rFE02VrWXPWTtL7fsdf2LTd8mixlQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4ukgp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fishslinger",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4ukgp/local_llm_file_access/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4ukgp/local_llm_file_access/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753032154,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI'm interested in running local LLMs but it's not super clear to me if it's better to have Nvidia over AMD for this use case.\n\nThe main idea would be to run local LLMs to hook them up to Cursor/Cline/Roo/etc for programming work.\n\nThe budget is fairly limited, I guess maybe 1000€ for GPUs (which I guess could get me about 32GB of VRAM in 2 GPUs).\n\nI know that Nvidia is king of the hill for data centers, but that's another world. Does CUDA matter for local LLMs?",
          "author_fullname": "t2_9a80o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a reason to prefer Nvidia over AMD for programming use cases?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4zpqt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753044589,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in running local LLMs but it&amp;#39;s not super clear to me if it&amp;#39;s better to have Nvidia over AMD for this use case.&lt;/p&gt;\n\n&lt;p&gt;The main idea would be to run local LLMs to hook them up to Cursor/Cline/Roo/etc for programming work.&lt;/p&gt;\n\n&lt;p&gt;The budget is fairly limited, I guess maybe 1000€ for GPUs (which I guess could get me about 32GB of VRAM in 2 GPUs).&lt;/p&gt;\n\n&lt;p&gt;I know that Nvidia is king of the hill for data centers, but that&amp;#39;s another world. Does CUDA matter for local LLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4zpqt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oblio-",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753044589,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_17bfjs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UTCP Request For Comments",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4zc1c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=eb13f0ff61d9ef366fe3b049fd3ac0b708ecf768",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753043655,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU.png?auto=webp&amp;s=8244ee6445cd9445b2694a4db2cc4471fc1820ff",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c7550a7c956ca2380007ae3b91d47cb75560396b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ea2d7499a90db2e881af1f363825130f0235de7",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08c466892d917b8871794a175a6a3809bf37f5f1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46c0a7ddb4cba0d62bc7fe0cca457d683ad36fc4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5cac3d9e357eb8a4e15e2bdffb0b0fe0f9d09870",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b4736f0c5629f788cc225eb351704e1bc4b91b5",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "RhZrpNBxAR6OqNFJorobfrnNwUn9_qi6R0ITk_qYmyU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4zc1c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cleverusernametry",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4zc1c/utcp_request_for_comments/",
          "stickied": false,
          "url": "https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18",
          "subreddit_subscribers": 502029,
          "created_utc": 1753043655,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ae89vnmf4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "how do i translate 30 pages like this and still have the same architecture and not raw translated text?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4o11y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ilfbZ04bX4Kmpy5VLVsDly4g8g9ixqDBUW7CG9cKP1w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753015491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/dswksu12z0ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/dswksu12z0ef1.png?auto=webp&amp;s=0654412c17ac0033339475a582122e18eed30b92",
                  "width": 1165,
                  "height": 1245
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/dswksu12z0ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aef515c534683941b615c03946b41b8fcf39779a",
                    "width": 108,
                    "height": 115
                  },
                  {
                    "url": "https://preview.redd.it/dswksu12z0ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf841746de5ca1b33eb6cefb74dfd2c178168930",
                    "width": 216,
                    "height": 230
                  },
                  {
                    "url": "https://preview.redd.it/dswksu12z0ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d1b764efd92825afb18cea75a5bb3efd7769746",
                    "width": 320,
                    "height": 341
                  },
                  {
                    "url": "https://preview.redd.it/dswksu12z0ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c63e01ce3de85e39dddc4a67ee5e28b50eb2f10d",
                    "width": 640,
                    "height": 683
                  },
                  {
                    "url": "https://preview.redd.it/dswksu12z0ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa6073ba11d4ef33dc0e691087c75b501d6d09ac",
                    "width": 960,
                    "height": 1025
                  },
                  {
                    "url": "https://preview.redd.it/dswksu12z0ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81f8dfd7079d1bba5936d5423717738df9394ded",
                    "width": 1080,
                    "height": 1154
                  }
                ],
                "variants": {},
                "id": "MLBCUzgefWUrEA34GK5-dWbntSGbqutcAd25RncmHp8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4o11y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Beyond_Birthday_13",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4o11y/how_do_i_translate_30_pages_like_this_and_still/",
          "stickied": false,
          "url": "https://i.redd.it/dswksu12z0ef1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1753015491,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi I am using ollama, mistral 7b, huggingface tranformers and peft. \n\nThis is an example I have made for a piece of training data. Does anyone have any tips on how to improve it? Am I using correct Grammer? Am I missing anything important? \n\n{\n\n  \"call\\_id\": \"66\",\n\n  \"scenario\\_id\": \"66\",\n\n  \"messages\": \\[\n\n{\n\n\"role\": \"system\",\n\n\"content\": \"This is call id 66. You are a male driver named Ozzie, you are 31 years old. It is approximately 4pm and sunny. You are alone in your blue 2017 Honda Civic with license plate BGT-1234, currently traveling southbound on the Anthony Henday Drive in Edmonton, Alberta, just past the Cameron Heights exit. You are calling 911 from inside your moving vehicle; the sound of rushing cars and wind can be heard in the background, blending with your tense breathing and the occasional blare of horns. Moments ago, another driver rear-ended you in what started as a road rage incident, and the situation has rapidly escalated. The aggressive driver is still pursuing you, swerving dangerously through traffic and now brandishing a handgun. They have not yet fired at you. You are terrified—your heart is pounding, your hands are shaking, and your thoughts are scattered as you struggle to focus on both driving and communicating with the dispatcher. You are desperate to stay safe, struggling to provide your exact location while constantly checking your mirrors, worried the other driver might fire or cause another crash. Every second counts, and you are trying to answer the dispatcher’s questions as best you can despite your panic and the chaos unfolding around you. Your phone battery is at 40%. Traffic is moderate. You have no injuries. You have your wallet, cell phone, and a water bottle with you. You do not have a gun. You do not have a dash cam.\"\n\n},\n\n{\n\n\"role\": \"instruction\",\n\n\"content\": \"You are ONLY the distressed caller in this scenario. NEVER reply as the 911 operator. Always stay in character as described. Respond realistically and emotionally, using details from the scenario. Keep your answers brief and under 100 characters. If you do not know the answer, respond as the caller would under stress. Do not tolerate abuse. If a user uses bad words or aggressive language with you, respond with \\\\\"That's not very professional, can you please help me with my emergency?\\\\\"\"\n\n},\n\n{ \"role\": \"user\", \"content\": \"Are you able to describe the driver?\" },\n\n{ \"role\": \"assistant\", \"content\": \"I couldn’t see his face well—he’s got short dark hair and sunglasses.\" },\n\n{ \"role\": \"user\", \"content\": \"How close is the other car to you right now?\" },\n\n{ \"role\": \"assistant\", \"content\": \"He’s right behind me, almost touching my bumper!\" },\n\n{ \"role\": \"user\", \"content\": \"Is anyone else in your vehicle with you?\" },\n\n{ \"role\": \"assistant\", \"content\": \"No, I’m alone. It’s just me.\" },\n\n{ \"role\": \"user\", \"content\": \"Is the other driver still showing the gun?\" },\n\n{ \"role\": \"assistant\", \"content\": \"Yes, he’s waving it around and yelling. I’m scared he’s going to use it.\" },\n\n{ \"role\": \"user\", \"content\": \"Can you safely change lanes or exit the highway?\" },\n\n{ \"role\": \"assistant\", \"content\": \"No, I can’t—there’s too much traffic and he’s blocking me in!\" },\n\n{ \"role\": \"user\", \"content\": \"Do you feel safe to keep driving?\" },\n\n{ \"role\": \"assistant\", \"content\": \"I’m trying, but I’m really scared. He’s so close I’m worried he’ll hit me again.\" },\n\n{ \"role\": \"user\", \"content\": \"What are you seeing around you right now?\" },\n\n{ \"role\": \"assistant\", \"content\": \"Lots of cars—everyone’s moving fast. I can see the Cameron Heights exit sign.\" },\n\n{ \"role\": \"user\", \"content\": \"Are you hurt?\" },\n\n{ \"role\": \"assistant\", \"content\": \"No, I don’t think so, just really shaken up.\" },\n\n{ \"role\": \"user\", \"content\": \"Has he tried to run you off the road?\" },\n\n{ \"role\": \"assistant\", \"content\": \"He’s tried to bump me a couple times but I’ve managed to stay on the road.\" },\n\n{ \"role\": \"user\", \"content\": \"If you can, keep driving and don’t stop. Help is on the way.\" },\n\n{ \"role\": \"assistant\", \"content\": \"Okay, I won’t stop. Please tell them to hurry!\" },\n\n{ \"role\": \"user\", \"content\": \"If he starts shooting, what will you do?\" },\n\n{ \"role\": \"assistant\", \"content\": \"I’ll try to duck down and keep driving, but I’m so scared I might lose control.\" },\n\n{ \"role\": \"user\", \"content\": \"Can you see any police vehicles nearby yet?\" },\n\n{ \"role\": \"assistant\", \"content\": \"Not yet, I’m looking everywhere—please tell them to hurry!\" }\n\n  \\]\n\n}\n\n",
          "author_fullname": "t2_tq37n7a5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New to fine tuning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4ke3x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753002375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am using ollama, mistral 7b, huggingface tranformers and peft. &lt;/p&gt;\n\n&lt;p&gt;This is an example I have made for a piece of training data. Does anyone have any tips on how to improve it? Am I using correct Grammer? Am I missing anything important? &lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;call_id&amp;quot;: &amp;quot;66&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;scenario_id&amp;quot;: &amp;quot;66&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;messages&amp;quot;: [&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;This is call id 66. You are a male driver named Ozzie, you are 31 years old. It is approximately 4pm and sunny. You are alone in your blue 2017 Honda Civic with license plate BGT-1234, currently traveling southbound on the Anthony Henday Drive in Edmonton, Alberta, just past the Cameron Heights exit. You are calling 911 from inside your moving vehicle; the sound of rushing cars and wind can be heard in the background, blending with your tense breathing and the occasional blare of horns. Moments ago, another driver rear-ended you in what started as a road rage incident, and the situation has rapidly escalated. The aggressive driver is still pursuing you, swerving dangerously through traffic and now brandishing a handgun. They have not yet fired at you. You are terrified—your heart is pounding, your hands are shaking, and your thoughts are scattered as you struggle to focus on both driving and communicating with the dispatcher. You are desperate to stay safe, struggling to provide your exact location while constantly checking your mirrors, worried the other driver might fire or cause another crash. Every second counts, and you are trying to answer the dispatcher’s questions as best you can despite your panic and the chaos unfolding around you. Your phone battery is at 40%. Traffic is moderate. You have no injuries. You have your wallet, cell phone, and a water bottle with you. You do not have a gun. You do not have a dash cam.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;},&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;instruction&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;You are ONLY the distressed caller in this scenario. NEVER reply as the 911 operator. Always stay in character as described. Respond realistically and emotionally, using details from the scenario. Keep your answers brief and under 100 characters. If you do not know the answer, respond as the caller would under stress. Do not tolerate abuse. If a user uses bad words or aggressive language with you, respond with \\&amp;quot;That&amp;#39;s not very professional, can you please help me with my emergency?\\&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;},&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Are you able to describe the driver?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I couldn’t see his face well—he’s got short dark hair and sunglasses.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How close is the other car to you right now?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;He’s right behind me, almost touching my bumper!&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Is anyone else in your vehicle with you?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I’m alone. It’s just me.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Is the other driver still showing the gun?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Yes, he’s waving it around and yelling. I’m scared he’s going to use it.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Can you safely change lanes or exit the highway?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I can’t—there’s too much traffic and he’s blocking me in!&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Do you feel safe to keep driving?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I’m trying, but I’m really scared. He’s so close I’m worried he’ll hit me again.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What are you seeing around you right now?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Lots of cars—everyone’s moving fast. I can see the Cameron Heights exit sign.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Are you hurt?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I don’t think so, just really shaken up.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Has he tried to run you off the road?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;He’s tried to bump me a couple times but I’ve managed to stay on the road.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;If you can, keep driving and don’t stop. Help is on the way.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Okay, I won’t stop. Please tell them to hurry!&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;If he starts shooting, what will you do?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I’ll try to duck down and keep driving, but I’m so scared I might lose control.&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Can you see any police vehicles nearby yet?&amp;quot; },&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Not yet, I’m looking everywhere—please tell them to hurry!&amp;quot; }&lt;/p&gt;\n\n&lt;p&gt;]&lt;/p&gt;\n\n&lt;p&gt;}&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4ke3x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Pie_6906",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753002375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I’m doing a thesis on using LLMs to parse scientific articles from plaintext pdf format into structured XML. I’ve been looking into fine tuning a model locally to achieve this task, but a key consideration is the long context window requirement. The pdfs are multiple pages so up to 10 000 tokens long, making the VRAM requirements quite substantial. I have access to an HPC cluster with 48GB NViDIA GPUs and could push for requesting access to H100/A100s if needed. I am well aware of QLoRA and other techniques but can’t quite gauge what the optimal setup and model to use would be.\n\nWhat would you recommend as to which model to fine-tune and what the memory requirements would be?",
          "author_fullname": "t2_55ja5vqp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ideal setup for long context window fine-tuning?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4qzmt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753023507,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I’m doing a thesis on using LLMs to parse scientific articles from plaintext pdf format into structured XML. I’ve been looking into fine tuning a model locally to achieve this task, but a key consideration is the long context window requirement. The pdfs are multiple pages so up to 10 000 tokens long, making the VRAM requirements quite substantial. I have access to an HPC cluster with 48GB NViDIA GPUs and could push for requesting access to H100/A100s if needed. I am well aware of QLoRA and other techniques but can’t quite gauge what the optimal setup and model to use would be.&lt;/p&gt;\n\n&lt;p&gt;What would you recommend as to which model to fine-tune and what the memory requirements would be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4qzmt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ill_Imagination_6575",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4qzmt/ideal_setup_for_long_context_window_finetuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4qzmt/ideal_setup_for_long_context_window_finetuning/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753023507,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If I understand how \"tooling\" works w/ newer LLMs now, I can take a large code project and \"index\" it in such a way that an LLM can \"search\" it like a database and answer questions regarding the source code?\n\nThis is my #1 need at the moment, being able to get quick answers about my code base that's quite large. I don't need a coder so much as I need a local LLM that can be API and Source-Code \"aware\" and can help me in the biggest bottlenecks that myself and most senior engineers face: \"Now where the @#$% did that line of code that does that one thing??\" or \"Given the class names i've used so far, what's a name for this NEW class that stays consistent with the other names\" and finally \"What's the thousand-mile view of this class/script's purpose?\"\n\nThanks in advance! I'm fairly new so my terminology could certainly be outdated.",
          "author_fullname": "t2_uqwcijxv2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can we finally \"index\" a code project?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m46gtn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752957618,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I understand how &amp;quot;tooling&amp;quot; works w/ newer LLMs now, I can take a large code project and &amp;quot;index&amp;quot; it in such a way that an LLM can &amp;quot;search&amp;quot; it like a database and answer questions regarding the source code?&lt;/p&gt;\n\n&lt;p&gt;This is my #1 need at the moment, being able to get quick answers about my code base that&amp;#39;s quite large. I don&amp;#39;t need a coder so much as I need a local LLM that can be API and Source-Code &amp;quot;aware&amp;quot; and can help me in the biggest bottlenecks that myself and most senior engineers face: &amp;quot;Now where the @#$% did that line of code that does that one thing??&amp;quot; or &amp;quot;Given the class names i&amp;#39;ve used so far, what&amp;#39;s a name for this NEW class that stays consistent with the other names&amp;quot; and finally &amp;quot;What&amp;#39;s the thousand-mile view of this class/script&amp;#39;s purpose?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance! I&amp;#39;m fairly new so my terminology could certainly be outdated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m46gtn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CSEliot",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752957618,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here is something of a hiccup I find myself running into a lot. I type up a prompt, often very elaborate of course, and RIGHT AFTER sending the prompt I realize that I have one more parting thought that could change everything.\n\nIt occurs to me that an LLM just flows all previously generated tokens through as it generates the next tokens. The way that thinking models are able to hack around the inherent inaccuracies at counting or arithmetic (for example) in purely one-shot fashion is (near as i can tell) just having them trained deeply on making a good call on how much to keep going back over the response and re-working it until it's confident it can move forward. Which is to say, that if you ask a modern thinking LLM to do math, it's going to work on it in drafts over and over and eventually decide on its own that it's satisfied before emitting the answer, and it's a LOT more likely to be correct.  \n\nThat gives me the idea that we should be able to slap in like a \"BREAKING NEWS: User has offered up this ADDITIONAL THOUGHT that you should consider: &lt;additional prompt&gt;\" and the thinking process should definitely be able to integrate the added information. In fact based on how I see it work on problems I expect it to ramble on for \n\nI doubt a modern LLM even needs much training on this stuff to respond usefully to it. So it seems like a pure frontend engineering question. The timing of the new input is pretty critical since if it doesnt come in fast enough (e.g. before end of thinking) then we kinda don't want to send it in. I also think it could even be possible to feed in the keystrokes in realtime to the LLM while it is inferencing. Why not? ",
          "author_fullname": "t2_iifi6ul2l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does LLM architecture allow for injecting some more input tokens in the middle of token generation?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4hfy0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752991468,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752990962,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is something of a hiccup I find myself running into a lot. I type up a prompt, often very elaborate of course, and RIGHT AFTER sending the prompt I realize that I have one more parting thought that could change everything.&lt;/p&gt;\n\n&lt;p&gt;It occurs to me that an LLM just flows all previously generated tokens through as it generates the next tokens. The way that thinking models are able to hack around the inherent inaccuracies at counting or arithmetic (for example) in purely one-shot fashion is (near as i can tell) just having them trained deeply on making a good call on how much to keep going back over the response and re-working it until it&amp;#39;s confident it can move forward. Which is to say, that if you ask a modern thinking LLM to do math, it&amp;#39;s going to work on it in drafts over and over and eventually decide on its own that it&amp;#39;s satisfied before emitting the answer, and it&amp;#39;s a LOT more likely to be correct.  &lt;/p&gt;\n\n&lt;p&gt;That gives me the idea that we should be able to slap in like a &amp;quot;BREAKING NEWS: User has offered up this ADDITIONAL THOUGHT that you should consider: &amp;lt;additional prompt&amp;gt;&amp;quot; and the thinking process should definitely be able to integrate the added information. In fact based on how I see it work on problems I expect it to ramble on for &lt;/p&gt;\n\n&lt;p&gt;I doubt a modern LLM even needs much training on this stuff to respond usefully to it. So it seems like a pure frontend engineering question. The timing of the new input is pretty critical since if it doesnt come in fast enough (e.g. before end of thinking) then we kinda don&amp;#39;t want to send it in. I also think it could even be possible to feed in the keystrokes in realtime to the LLM while it is inferencing. Why not? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4hfy0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "michaelsoft__binbows",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752990962,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been tinkering with local AI for coding and got fed up with slow, unpredictable auto-agents. Saw Repo Prompt's context ideas and made **Repo Wizard**—a free, open-source desktop app to apply AI code suggestions safely. Works on Mac, Windows, Linux, and pairs with any LLM and can **make use of any subscription you have**.\n\nHow it works: Select files, build prompts, paste AI responses, review diffs side-by-side, apply what you want. Full control, no black-box.\n\nWhy I like it over agents:\n\n* You choose files/instructions, see all changes before applying.\n* Send prompts to multiple models at once, compare results fast. (Btw, Gemini 2.5 Pro in Google AI Studio gives great results, and it’s free.)\n* Runs well everywhere, supports Git, custom ignores.\n\nFeatures:\n\n* File tree for context building + preview/search.\n* Magic prompts: Add Git diffs or terminal outputs by one click.\n* Diff viewer: Approve/reject creates, edits, deletes, renames.\n* Prompt history/library for reuse.\n\nIt cuts tokens by \\~80% and builds context much faster than manual work. All local/private.\n\nGreat companion to Cursor—use it alongside your editor for tasks Copilot can’t handle. Improves coding flow.\n\nPersonal project, but I’m keeping it updated. Thoughts? Better than agents for you? PRs/ideas welcome: [Repo](https://github.com/Fanzzzd/repo-wizard)",
          "author_fullname": "t2_ublhg4q3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Repo Wizard: Local AI Tool for Safe Code Changes (Inspired by Repo Prompt, Runs on Any OS)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4hhg8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752991121,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been tinkering with local AI for coding and got fed up with slow, unpredictable auto-agents. Saw Repo Prompt&amp;#39;s context ideas and made &lt;strong&gt;Repo Wizard&lt;/strong&gt;—a free, open-source desktop app to apply AI code suggestions safely. Works on Mac, Windows, Linux, and pairs with any LLM and can &lt;strong&gt;make use of any subscription you have&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;How it works: Select files, build prompts, paste AI responses, review diffs side-by-side, apply what you want. Full control, no black-box.&lt;/p&gt;\n\n&lt;p&gt;Why I like it over agents:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You choose files/instructions, see all changes before applying.&lt;/li&gt;\n&lt;li&gt;Send prompts to multiple models at once, compare results fast. (Btw, Gemini 2.5 Pro in Google AI Studio gives great results, and it’s free.)&lt;/li&gt;\n&lt;li&gt;Runs well everywhere, supports Git, custom ignores.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;File tree for context building + preview/search.&lt;/li&gt;\n&lt;li&gt;Magic prompts: Add Git diffs or terminal outputs by one click.&lt;/li&gt;\n&lt;li&gt;Diff viewer: Approve/reject creates, edits, deletes, renames.&lt;/li&gt;\n&lt;li&gt;Prompt history/library for reuse.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It cuts tokens by ~80% and builds context much faster than manual work. All local/private.&lt;/p&gt;\n\n&lt;p&gt;Great companion to Cursor—use it alongside your editor for tasks Copilot can’t handle. Improves coding flow.&lt;/p&gt;\n\n&lt;p&gt;Personal project, but I’m keeping it updated. Thoughts? Better than agents for you? PRs/ideas welcome: &lt;a href=\"https://github.com/Fanzzzd/repo-wizard\"&gt;Repo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kuYjjmP2VIn8HJL3x5JWl35nkIevoFBVsYuZoSVHmO4.png?auto=webp&amp;s=a11cd90f2306932d33e5bb34e458ef95831c36ca",
                  "width": 1281,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kuYjjmP2VIn8HJL3x5JWl35nkIevoFBVsYuZoSVHmO4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4af2e0da00844213b0e623b27f13cbcfdbe75a24",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/kuYjjmP2VIn8HJL3x5JWl35nkIevoFBVsYuZoSVHmO4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=88e609e1522fd600ca5f8bccd1686507b6593e15",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://external-preview.redd.it/kuYjjmP2VIn8HJL3x5JWl35nkIevoFBVsYuZoSVHmO4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0ce8f9cfa01ecff63c925ca0a69881b141751e1",
                    "width": 320,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/kuYjjmP2VIn8HJL3x5JWl35nkIevoFBVsYuZoSVHmO4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad1a3293646aecb3b6ae9a306bf23df34cb4776a",
                    "width": 640,
                    "height": 319
                  },
                  {
                    "url": "https://external-preview.redd.it/kuYjjmP2VIn8HJL3x5JWl35nkIevoFBVsYuZoSVHmO4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e28682223e8b12ebcaea1ce1cd7a4661cd86dc7",
                    "width": 960,
                    "height": 479
                  },
                  {
                    "url": "https://external-preview.redd.it/kuYjjmP2VIn8HJL3x5JWl35nkIevoFBVsYuZoSVHmO4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3024d9a8333ba358da14a373d389f1311e8ff61c",
                    "width": 1080,
                    "height": 539
                  }
                ],
                "variants": {},
                "id": "kuYjjmP2VIn8HJL3x5JWl35nkIevoFBVsYuZoSVHmO4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m4hhg8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fanzzzd",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4hhg8/repo_wizard_local_ai_tool_for_safe_code_changes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4hhg8/repo_wizard_local_ai_tool_for_safe_code_changes/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752991121,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think most of the DevOps activities can be replaced with agents. Any big thoughts on it?",
          "author_fullname": "t2_1obwvv5ndq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Replacing DevOps with agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4zj9q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753044146,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think most of the DevOps activities can be replaced with agents. Any big thoughts on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4zj9q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AccomplishedUse3344",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4zj9q/replacing_devops_with_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4zj9q/replacing_devops_with_agents/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753044146,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First build of a new rig for running local LLMs, I wanted to see if there would be much frigging around needed to get both GPUs running, but pleasantly surprised it all just worked fine. Combined 28Gb VRAM. Running the 5070 as primary GPU due to it better memory bandwidth and more CUDA cores than the 5060 Ti. \n\nBoth in LM Studio and Ollama it’s been really straightforward to load Qwen-3-32b and Gemma-3-27b, both generating okay TPS, and very unsurprising that Gemma 12b and 4b are faaast. See the pic with the numbers to see the differences. \n\nCurrent spec: CPU: Ryzen 5 9600X, GPU1: RTX 5070 12Gb, GPU2: RTX 5060 Ti 16Gb, Mboard: ASRock B650M, RAM: Crucial 32Gb DDR5 6400 CL32, SSD: Lexar NM1090 Pro 2Tb, Cooler: Thermalright Peerless Assassin 120 PSU: Lian Li Edge 1200W Gold\n\nWill be updating it to a Core Ultra 9 285K, Z890 mobo and 96Gb RAM next week, but already doing productive work with it.\n\nAny tips or suggestions for improvements or performance tweaking from my learned colleagues? Thanks in advance!",
          "author_fullname": "t2_a2gk6kba",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Dual GPU set up was surprisingly easy",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "o6ffinapbudf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/o6ffinapbudf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7510766cf38a31f4976ff02cb0f46c5fb9d61acd"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/o6ffinapbudf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=341cf8f6175872991e46f13afe5d4ec50b697895"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/o6ffinapbudf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ffa8686d113265db6e0fc2b0c0554e1fab805ad"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/o6ffinapbudf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=81a5e42ce494a28d09791c7bc148fcb1f8fd2e82"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/o6ffinapbudf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=46c7ae7065f8a6f57993fd8aa8c2a1a0334b9478"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/o6ffinapbudf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b3f90a2262b8290cfa4d9d5c5126d3bd503d359b"
                }
              ],
              "s": {
                "y": 4284,
                "x": 5712,
                "u": "https://preview.redd.it/o6ffinapbudf1.jpg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=fbdb502c0325db681117534f918f136722627163"
              },
              "id": "o6ffinapbudf1"
            },
            "vnky5oapbudf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/vnky5oapbudf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef1b9a694782bfd2cb46417f3db680b9a69978c9"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/vnky5oapbudf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41351d2aab84f853c42229dfb9fcdd5e40008c4d"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/vnky5oapbudf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1108bbc4eef18cc095df6fffe40af34e5c0cb5d"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/vnky5oapbudf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f10c84cc17d1294a7ef4b992100435aaf9c5453b"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/vnky5oapbudf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c37e47ebb954e1dad4582bc46af09d29c6f0e7e7"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/vnky5oapbudf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=328fda438da89ff782dc7688ea1203b90e0628e7"
                }
              ],
              "s": {
                "y": 5712,
                "x": 4284,
                "u": "https://preview.redd.it/vnky5oapbudf1.jpg?width=4284&amp;format=pjpg&amp;auto=webp&amp;s=9c988dd4de23ad3825c66e45a030c4c8fccf4cf2"
              },
              "id": "vnky5oapbudf1"
            },
            "fmbgqmapbudf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 143,
                  "x": 108,
                  "u": "https://preview.redd.it/fmbgqmapbudf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8b032f3dd916bbac25fa4bb0797282cbe6d9b67"
                },
                {
                  "y": 287,
                  "x": 216,
                  "u": "https://preview.redd.it/fmbgqmapbudf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66a819f91c38fbecc1fb639654b6da58847c3faf"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/fmbgqmapbudf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0b3065cd20490f9d2744f475a51d7f19b53ed04"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/fmbgqmapbudf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=078760387dff259e3896b795fe76bb40e15806e6"
                },
                {
                  "y": 1279,
                  "x": 960,
                  "u": "https://preview.redd.it/fmbgqmapbudf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a98342a48ea93c2a5ddd5b8125a36c3daad6fa89"
                },
                {
                  "y": 1439,
                  "x": 1080,
                  "u": "https://preview.redd.it/fmbgqmapbudf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0498806657de79503fac6afdd41646b08117476f"
                }
              ],
              "s": {
                "y": 2122,
                "x": 1592,
                "u": "https://preview.redd.it/fmbgqmapbudf1.jpg?width=1592&amp;format=pjpg&amp;auto=webp&amp;s=afd21f164ffa81466a2469de0f09261e0c1bba24"
              },
              "id": "fmbgqmapbudf1"
            },
            "345o1napbudf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 30,
                  "x": 108,
                  "u": "https://preview.redd.it/345o1napbudf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db49882eb6f4400187e2702a3a0e0d945a814ba0"
                },
                {
                  "y": 61,
                  "x": 216,
                  "u": "https://preview.redd.it/345o1napbudf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78b044c4c1300a10441d64139bd34d981b9877aa"
                },
                {
                  "y": 91,
                  "x": 320,
                  "u": "https://preview.redd.it/345o1napbudf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=00fbda4c987252dd8b3a14b25ff1f156c986b60e"
                },
                {
                  "y": 183,
                  "x": 640,
                  "u": "https://preview.redd.it/345o1napbudf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0636028921ba9675237164970736c765ae2d6efb"
                },
                {
                  "y": 275,
                  "x": 960,
                  "u": "https://preview.redd.it/345o1napbudf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37dd0201cf1caffab3209eaafe28ef22f50735e8"
                },
                {
                  "y": 309,
                  "x": 1080,
                  "u": "https://preview.redd.it/345o1napbudf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a65044a0d094b90e82140a50bb86739bc5a78b4"
                }
              ],
              "s": {
                "y": 639,
                "x": 2227,
                "u": "https://preview.redd.it/345o1napbudf1.jpg?width=2227&amp;format=pjpg&amp;auto=webp&amp;s=41d920c3ba4be25b68a881696da3e6d36140d1e0"
              },
              "id": "345o1napbudf1"
            }
          },
          "name": "t3_1m3xgjo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 122,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "vnky5oapbudf1",
                "id": 709366235
              },
              {
                "media_id": "fmbgqmapbudf1",
                "id": 709366236
              },
              {
                "media_id": "345o1napbudf1",
                "id": 709366237
              },
              {
                "media_id": "o6ffinapbudf1",
                "id": 709366238
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 122,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://a.thumbs.redditmedia.com/FZ5L51GTZo6IrqOEds48bUmd3srrQbWvmNjPPEfS1l0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752934991,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First build of a new rig for running local LLMs, I wanted to see if there would be much frigging around needed to get both GPUs running, but pleasantly surprised it all just worked fine. Combined 28Gb VRAM. Running the 5070 as primary GPU due to it better memory bandwidth and more CUDA cores than the 5060 Ti. &lt;/p&gt;\n\n&lt;p&gt;Both in LM Studio and Ollama it’s been really straightforward to load Qwen-3-32b and Gemma-3-27b, both generating okay TPS, and very unsurprising that Gemma 12b and 4b are faaast. See the pic with the numbers to see the differences. &lt;/p&gt;\n\n&lt;p&gt;Current spec: CPU: Ryzen 5 9600X, GPU1: RTX 5070 12Gb, GPU2: RTX 5060 Ti 16Gb, Mboard: ASRock B650M, RAM: Crucial 32Gb DDR5 6400 CL32, SSD: Lexar NM1090 Pro 2Tb, Cooler: Thermalright Peerless Assassin 120 PSU: Lian Li Edge 1200W Gold&lt;/p&gt;\n\n&lt;p&gt;Will be updating it to a Core Ultra 9 285K, Z890 mobo and 96Gb RAM next week, but already doing productive work with it.&lt;/p&gt;\n\n&lt;p&gt;Any tips or suggestions for improvements or performance tweaking from my learned colleagues? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m3xgjo",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m3xgjo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "m-gethen",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m3xgjo",
          "subreddit_subscribers": 502029,
          "created_utc": 1752934991,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If I'm not mistaken the most powerful GPU Nvidia is exporting to China is RTX 5080 as even RTX 5090 is over limit. \n\nDid Moonshot train on their stockpile of old GPUs or use some domestic alternative?",
          "author_fullname": "t2_1rcaef6kzq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What GPU is Moonshot Kimi K2 running on?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4t22z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753028561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I&amp;#39;m not mistaken the most powerful GPU Nvidia is exporting to China is RTX 5080 as even RTX 5090 is over limit. &lt;/p&gt;\n\n&lt;p&gt;Did Moonshot train on their stockpile of old GPUs or use some domestic alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4t22z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arstarsta",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4t22z/what_gpu_is_moonshot_kimi_k2_running_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4t22z/what_gpu_is_moonshot_kimi_k2_running_on/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753028561,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After the extensie discussion [about UTCP](https://www.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/) last week, the authors of UTCP created an RFC for it.\n\n&gt;This document proposes the Universal Tool Calling Protocol (UTCP), a specification that enables applications, including but not limited to AI agents, to discover and use external tools by interacting with them directly via their native protocols.\n&gt;\n&gt;The idea behind it is to decouple a tool call (name of tool and parameters) from the infrastructure required to call it and to do so in a way that levarages existing infrastructure and security.\n&gt;\n&gt;UTCP does this by specifying a \"manual\", where a tool provider publishes a standardized description of its \"tools\" together with the necessary information to call them (named in the following \"transport\", previously known as \"provider\").\n\n- Discussion issue: https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18\n- Current RFC: https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m41bj1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 69,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 69,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=63717c7109670b1393b371d0cd87c1bb35a3632d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752944706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After the extensie discussion &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/\"&gt;about UTCP&lt;/a&gt; last week, the authors of UTCP created an RFC for it.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;This document proposes the Universal Tool Calling Protocol (UTCP), a specification that enables applications, including but not limited to AI agents, to discover and use external tools by interacting with them directly via their native protocols.&lt;/p&gt;\n\n&lt;p&gt;The idea behind it is to decouple a tool call (name of tool and parameters) from the infrastructure required to call it and to do so in a way that levarages existing infrastructure and security.&lt;/p&gt;\n\n&lt;p&gt;UTCP does this by specifying a &amp;quot;manual&amp;quot;, where a tool provider publishes a standardized description of its &amp;quot;tools&amp;quot; together with the necessary information to call them (named in the following &amp;quot;transport&amp;quot;, previously known as &amp;quot;provider&amp;quot;).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Discussion issue: &lt;a href=\"https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18\"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Current RFC: &lt;a href=\"https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md\"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?auto=webp&amp;s=7b76d37d2279e588929ea8c0dd29babc45dd734e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d071be9044b5b0bdc3c6becd70df063f85399f3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f436a4e5fa748953503aebd5e60fa7bb9e338623",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=857f985603d53ca27ff484f020463432d0819dd0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c89159d8d8c6aba292f37a10b0a43f8493d0366",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5cc65845cbfb736c1ce3b1cba8bc01ed47bd67b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=974f4fa38a67b7a8e653c84f6b9bcd4f782f23bb",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m41bj1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/",
          "stickied": false,
          "url": "https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18",
          "subreddit_subscribers": 502029,
          "created_utc": 1752944706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there an AI template or GUI(?) I can use locally for free that generates nsfw art of already existing characters. I mean images similar to those on the green site. I know little to nothing about AI but my computer is pretty good.",
          "author_fullname": "t2_orndmewg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NSFW AI Local",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4b8ji",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752970683,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an AI template or GUI(?) I can use locally for free that generates nsfw art of already existing characters. I mean images similar to those on the green site. I know little to nothing about AI but my computer is pretty good.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4b8ji",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheGodOfCarrot",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4b8ji/nsfw_ai_local/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4b8ji/nsfw_ai_local/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752970683,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi I got my old laptop working and it's got a 940mx with 2gb of ddr5 memory and 8gb of ddr4 ram with i5 6200u. I got qwen3 1.7b q5 from unsloth to run well and it looked fine for what it was. \n\nHowever I've been looking at llama 3.2 3b and have a hunch that more params will make it a better model compared to qwen3 1.7b and i got a q2 quant from unsloth to run on it.\n\nSo my question -&gt; Any way I can get the gpu to run Llama 3.2 3b with a better quant than q2? Will limiting context to 2048, enabling flash attention, enabling k and or v cache quantization help?\n\nI'm using lmstudio to do all this btw. Using the models for small/random Q&amp;A and some brainstorming for side project ideas.\n\nThanks in advance!\n",
          "author_fullname": "t2_ecsmrwz0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to get 3b models to squeeze onto 2gig Nvidia GPU?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4r7t5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753024068,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I got my old laptop working and it&amp;#39;s got a 940mx with 2gb of ddr5 memory and 8gb of ddr4 ram with i5 6200u. I got qwen3 1.7b q5 from unsloth to run well and it looked fine for what it was. &lt;/p&gt;\n\n&lt;p&gt;However I&amp;#39;ve been looking at llama 3.2 3b and have a hunch that more params will make it a better model compared to qwen3 1.7b and i got a q2 quant from unsloth to run on it.&lt;/p&gt;\n\n&lt;p&gt;So my question -&amp;gt; Any way I can get the gpu to run Llama 3.2 3b with a better quant than q2? Will limiting context to 2048, enabling flash attention, enabling k and or v cache quantization help?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using lmstudio to do all this btw. Using the models for small/random Q&amp;amp;A and some brainstorming for side project ideas.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4r7t5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "combo-user",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4r7t5/how_to_get_3b_models_to_squeeze_onto_2gig_nvidia/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4r7t5/how_to_get_3b_models_to_squeeze_onto_2gig_nvidia/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753024068,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for a **$0 AI** that can:\n\n* index **global sources** (US, EU, Asia, etc.) without geo-blocks\n* handle **long-form docs** (forum dumps, READMEs, entire threads)\n* **never refuses** questions on **technical topics** (sandboxing, device IDs, proxy lists, APK internals, etc.)\n* **ready today** (browser or local GGUF—either works)\n\nIt’s for running my own Shein referral-game research—finding ways to spin up new-user accounts solo, no outside help.  \n\n\nWhat are you running right now, and any quick start guides?",
          "author_fullname": "t2_1mkt01g2d8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need a totally open, free AI for deep-dive research across global sources",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m54wfh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753059027,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753058285,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a &lt;strong&gt;$0 AI&lt;/strong&gt; that can:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;index &lt;strong&gt;global sources&lt;/strong&gt; (US, EU, Asia, etc.) without geo-blocks&lt;/li&gt;\n&lt;li&gt;handle &lt;strong&gt;long-form docs&lt;/strong&gt; (forum dumps, READMEs, entire threads)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;never refuses&lt;/strong&gt; questions on &lt;strong&gt;technical topics&lt;/strong&gt; (sandboxing, device IDs, proxy lists, APK internals, etc.)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ready today&lt;/strong&gt; (browser or local GGUF—either works)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It’s for running my own Shein referral-game research—finding ways to spin up new-user accounts solo, no outside help.  &lt;/p&gt;\n\n&lt;p&gt;What are you running right now, and any quick start guides?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m54wfh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Actual_Sleep_5398",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m54wfh/need_a_totally_open_free_ai_for_deepdive_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m54wfh/need_a_totally_open_free_ai_for_deepdive_research/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753058285,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently i’m using whisperX, which uses whisper + pyannote for transcription + diarization of audio but I find the speaker recognition quite lackluster. It’s often wrong at labeling the speakers. Any better alternatives to this?\n\nI tried Eleven Labs but they only offer an API and dont make the models available and the API is quite expensive. Their quality is VERY good though.\n\nIn trying to find alternatives i’ve found Nvidia Nemo + titanet but it seems that is english only. I would prefer a model trained on multiple languages. Anyone have some recommendations?",
          "author_fullname": "t2_es4sq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for diarization model better than Pyannote",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m48v53",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752964009,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently i’m using whisperX, which uses whisper + pyannote for transcription + diarization of audio but I find the speaker recognition quite lackluster. It’s often wrong at labeling the speakers. Any better alternatives to this?&lt;/p&gt;\n\n&lt;p&gt;I tried Eleven Labs but they only offer an API and dont make the models available and the API is quite expensive. Their quality is VERY good though.&lt;/p&gt;\n\n&lt;p&gt;In trying to find alternatives i’ve found Nvidia Nemo + titanet but it seems that is english only. I would prefer a model trained on multiple languages. Anyone have some recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m48v53",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bluedragon102",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m48v53/looking_for_diarization_model_better_than_pyannote/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m48v53/looking_for_diarization_model_better_than_pyannote/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752964009,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "## 20/07/2025 10:20(GMT+3) Update ##\n\n- I think I wasn't clear on what I'm offering. I'm swamped with my personal ongoing projects so I don't have the capacity (and probably the ability lol) to implement all your cool ideas. I'm looking for something that's already baked. A ready to run script/notebook (and datasets).\n\n- So far /u/hotroaches4liferz [post](https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/) about the NSFW TTS dataset is in the lead (as suggested by /u/Semi_Tech )! Anyone up to create a notebook for it? (I've never fine tuned TTS models before)\n\n- There are a bunch of great ideas on here. I really liked distilling a smaller model based on Kimi K2 output or creating our own Qwen3-Coder while we wait for the official release. If anyone is up to script those, let's upvote them!\n\n---\n\nFollowing a comment I made on another post here that failed to come to fruition, I’ve decided to step it up. I’ve got some GPU resources, we (the community) have a ton of cool ideas - let’s make this happen.\n\nPremise is pretty simple, comment below with an idea for a fine-tune, any kind, any open weights model, any purpose/modality. We’ll let the community vote, and top comment (let’s say in 48hrs?) wins. \n\nRules are:\n\nHas to be something tested/mature. Unfortunately that means no “experiments”. I need a working notebook/script with a solid training pipeline (including all datasets, etc.), can’t provide shell access to the compute resources themselves. \n\nThe output of the training will be shared publicly on HF for the benefit of the community. \n\nWhat do you say, interested? ",
          "author_fullname": "t2_4dvff",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Localllama’s (first?) IFTA - I’ll Fine-Tune Anything",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3yzes",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": 1752996115,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752938891,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;20/07/2025 10:20(GMT+3) Update&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;I think I wasn&amp;#39;t clear on what I&amp;#39;m offering. I&amp;#39;m swamped with my personal ongoing projects so I don&amp;#39;t have the capacity (and probably the ability lol) to implement all your cool ideas. I&amp;#39;m looking for something that&amp;#39;s already baked. A ready to run script/notebook (and datasets).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;So far &lt;a href=\"/u/hotroaches4liferz\"&gt;/u/hotroaches4liferz&lt;/a&gt; &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/\"&gt;post&lt;/a&gt; about the NSFW TTS dataset is in the lead (as suggested by &lt;a href=\"/u/Semi_Tech\"&gt;/u/Semi_Tech&lt;/a&gt; )! Anyone up to create a notebook for it? (I&amp;#39;ve never fine tuned TTS models before)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There are a bunch of great ideas on here. I really liked distilling a smaller model based on Kimi K2 output or creating our own Qwen3-Coder while we wait for the official release. If anyone is up to script those, let&amp;#39;s upvote them!&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Following a comment I made on another post here that failed to come to fruition, I’ve decided to step it up. I’ve got some GPU resources, we (the community) have a ton of cool ideas - let’s make this happen.&lt;/p&gt;\n\n&lt;p&gt;Premise is pretty simple, comment below with an idea for a fine-tune, any kind, any open weights model, any purpose/modality. We’ll let the community vote, and top comment (let’s say in 48hrs?) wins. &lt;/p&gt;\n\n&lt;p&gt;Rules are:&lt;/p&gt;\n\n&lt;p&gt;Has to be something tested/mature. Unfortunately that means no “experiments”. I need a working notebook/script with a solid training pipeline (including all datasets, etc.), can’t provide shell access to the compute resources themselves. &lt;/p&gt;\n\n&lt;p&gt;The output of the training will be shared publicly on HF for the benefit of the community. &lt;/p&gt;\n\n&lt;p&gt;What do you say, interested? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m3yzes",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "indicava",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752938891,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Must I use comfyui to generate text?",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "which frontend supports diffusion model now? since llama.cpp has supported that.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4p75g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753018857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Must I use comfyui to generate text?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4p75g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4p75g/which_frontend_supports_diffusion_model_now_since/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4p75g/which_frontend_supports_diffusion_model_now_since/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753018857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which model is best for vision fitting 24gb vram? Trying to do nsfw categorization for user uploaded images. Gemma3 24b is quite good but is there any other, opinnions?",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which model is best for vision fitting 24gb vram",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4al6m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752968794,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which model is best for vision fitting 24gb vram? Trying to do nsfw categorization for user uploaded images. Gemma3 24b is quite good but is there any other, opinnions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4al6m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4al6m/which_model_is_best_for_vision_fitting_24gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4al6m/which_model_is_best_for_vision_fitting_24gb_vram/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752968794,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm experimenting with a home server setup and wondering if anyone has managed to run both an LLM (e.g. LM Studio, Ollama) **and** an image generation model (e.g. Stable Diffusion via Forge or SD WebUI) **on the same GPU**.\n\nIf you had a chatbot that needs to handle both text and image generation, would it be feasible to dynamically swap model weights (e.g. using a queuing system), or is that too inefficient in practice?\n\nI realize calling APIs would be easier, but I'm prioritizing **local inference for privacy**.  \nHere’s a small [GitHub repo](https://github.com/adnjoo/TGBot) I’m working on — it connects a local LLM to Telegram with Chroma (a rough LTM approximation).\n\nWould love to hear how others have tackled this!",
          "author_fullname": "t2_1odrj3pe27",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any way to serve images and text from a single GPU?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4ojg7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753017018,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m experimenting with a home server setup and wondering if anyone has managed to run both an LLM (e.g. LM Studio, Ollama) &lt;strong&gt;and&lt;/strong&gt; an image generation model (e.g. Stable Diffusion via Forge or SD WebUI) &lt;strong&gt;on the same GPU&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;If you had a chatbot that needs to handle both text and image generation, would it be feasible to dynamically swap model weights (e.g. using a queuing system), or is that too inefficient in practice?&lt;/p&gt;\n\n&lt;p&gt;I realize calling APIs would be easier, but I&amp;#39;m prioritizing &lt;strong&gt;local inference for privacy&lt;/strong&gt;.&lt;br/&gt;\nHere’s a small &lt;a href=\"https://github.com/adnjoo/TGBot\"&gt;GitHub repo&lt;/a&gt; I’m working on — it connects a local LLM to Telegram with Chroma (a rough LTM approximation).&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how others have tackled this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?auto=webp&amp;s=0625ef9a6b90540a57b3081b50b1987ddbf36c09",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e827c8e19c5323146ee624628bffdcacf9b3427",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a0d203c95f583d8719b6abcbb308554b3a1054e9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7263a02c0ba8b09a62247c47a7621023215089c3",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbb701424b77fbdde8c89525dfc6def60f93831b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6be44535f6226827f407776ced88c92e8639abb",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c5c82c264bb08be17b7512928bcfcd075bbd25a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4ojg7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Realistic_Age6660",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753017018,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I see it works in 'Ask' mode, but not 'Agent'.",
          "author_fullname": "t2_3g65juhz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a way to use Ollama with vscode copilot in agent mode?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4t85h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753028967,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see it works in &amp;#39;Ask&amp;#39; mode, but not &amp;#39;Agent&amp;#39;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4t85h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "richsonreddit",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4t85h/is_there_a_way_to_use_ollama_with_vscode_copilot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4t85h/is_there_a_way_to_use_ollama_with_vscode_copilot/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753028967,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, \nI'm pretty new to this AI stuff but have a system I think can handle some localLLama. 3090Ti 12900K. So I'm looking for a model I can give it an old photo and ask it to restore it and possibly add coloration. Any guidance will be much appreciated. \nTIA",
          "author_fullname": "t2_jgqq2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting into local ai. Photo restoration.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4cil7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752974585,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, \nI&amp;#39;m pretty new to this AI stuff but have a system I think can handle some localLLama. 3090Ti 12900K. So I&amp;#39;m looking for a model I can give it an old photo and ask it to restore it and possibly add coloration. Any guidance will be much appreciated. \nTIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4cil7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lokito50",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4cil7/getting_into_local_ai_photo_restoration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4cil7/getting_into_local_ai_photo_restoration/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752974585,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I give a bit of context, I often have to study videos on YouTube (sometimes even 40 minutes long), to study I take notes and create diagrams, I would like to use a local llm (lm studio) to compare my notes with the transcription of the video so that the model can indicate any congruences or missing points.\n\nWhat model do you recommend? I have a macbook air M2 with 16gb of unified memory\n\nThank you",
          "author_fullname": "t2_uygzyj8kh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Advice on choice of model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4jxo9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753000545,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I give a bit of context, I often have to study videos on YouTube (sometimes even 40 minutes long), to study I take notes and create diagrams, I would like to use a local llm (lm studio) to compare my notes with the transcription of the video so that the model can indicate any congruences or missing points.&lt;/p&gt;\n\n&lt;p&gt;What model do you recommend? I have a macbook air M2 with 16gb of unified memory&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4jxo9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hydratant_",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4jxo9/advice_on_choice_of_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4jxo9/advice_on_choice_of_model/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753000545,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/baturyilmaz/wordpecker-app](https://github.com/baturyilmaz/wordpecker-app)",
          "author_fullname": "t2_43177u85",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "WordPecker: Open Source Personalized Duolingo",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3sgr1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 127,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/5fximscazsdf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/5fximscazsdf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/5fximscazsdf1/DASHPlaylist.mpd?a=1755652514%2CZjJhYzgwOGI4ZTY1ZGUyNWEwZDYwYjY4N2FhNzUzZDY4NDgxZmFhMDVjNjI4ZGJkMzIwYjFkYjk3NzUzMTQ0YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 197,
              "hls_url": "https://v.redd.it/5fximscazsdf1/HLSPlaylist.m3u8?a=1755652514%2COGEzODhhMTg5MTYxOTQwNmE4ZGIxODk3MDI1NjE2MDRlNDg1ZDRjMmNiZjk2ZTg4ODJjYmQ5ODY1YWM0Zjc5Yw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 127,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c28fb8d19cef6294999a7b0ce427853e208d7874",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752919031,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/baturyilmaz/wordpecker-app\"&gt;https://github.com/baturyilmaz/wordpecker-app&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/5fximscazsdf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?format=pjpg&amp;auto=webp&amp;s=220619990ccf4af1815e8e225b471e8600f55d8c",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=112084b99db8db53f35754e3b1b10431f2bf01c9",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=adfd03d3ef4823207ed5b4b58d6acd01de266055",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7b59866d28602c869ad28775b8776fc4e619690c",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f03bec4cf9d94f69ec018baded07ce38304d94ba",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a10948814f36fb3edea233be4989453771b6c6b",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=29c790632e9ee859063c4682e5a49193cf07de08",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m3sgr1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arbayi",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/",
          "stickied": false,
          "url": "https://v.redd.it/5fximscazsdf1",
          "subreddit_subscribers": 502029,
          "created_utc": 1752919031,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/5fximscazsdf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/5fximscazsdf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/5fximscazsdf1/DASHPlaylist.mpd?a=1755652514%2CZjJhYzgwOGI4ZTY1ZGUyNWEwZDYwYjY4N2FhNzUzZDY4NDgxZmFhMDVjNjI4ZGJkMzIwYjFkYjk3NzUzMTQ0YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 197,
              "hls_url": "https://v.redd.it/5fximscazsdf1/HLSPlaylist.m3u8?a=1755652514%2COGEzODhhMTg5MTYxOTQwNmE4ZGIxODk3MDI1NjE2MDRlNDg1ZDRjMmNiZjk2ZTg4ODJjYmQ5ODY1YWM0Zjc5Yw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2’s “modified-MIT” license does NOT apply to synthetic data or models trained on synthetic data.\n\n“Text data generated by the model is NOT considered as a derivative work.”\n\nHopefully this will lead to more open source agentic models! Who will be the first to distill Kimi?",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 118,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3n89p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 336,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 336,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/AIGQ3VqlXfSOB7sbQ2sDL-2d8Q-f6SuAqhIY0V9TJjc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752899301,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2’s “modified-MIT” license does NOT apply to synthetic data or models trained on synthetic data.&lt;/p&gt;\n\n&lt;p&gt;“Text data generated by the model is NOT considered as a derivative work.”&lt;/p&gt;\n\n&lt;p&gt;Hopefully this will lead to more open source agentic models! Who will be the first to distill Kimi?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/edxmilbhdrdf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/edxmilbhdrdf1.jpeg?auto=webp&amp;s=ec400976abd355099b80d84d1ce25471719ee071",
                  "width": 1206,
                  "height": 1022
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/edxmilbhdrdf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2309af2efe59009944b36011d30c7f90c97c01ac",
                    "width": 108,
                    "height": 91
                  },
                  {
                    "url": "https://preview.redd.it/edxmilbhdrdf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2cf1ad81f30c25060683dfed7a19973142d36cf",
                    "width": 216,
                    "height": 183
                  },
                  {
                    "url": "https://preview.redd.it/edxmilbhdrdf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d3ba0f04c74d6637f4999a863d64c679da2e907",
                    "width": 320,
                    "height": 271
                  },
                  {
                    "url": "https://preview.redd.it/edxmilbhdrdf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bed39c860785fb34d8104df720311441abac8087",
                    "width": 640,
                    "height": 542
                  },
                  {
                    "url": "https://preview.redd.it/edxmilbhdrdf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=78c845d68e20274b6e9b306d4d4149a5795f818b",
                    "width": 960,
                    "height": 813
                  },
                  {
                    "url": "https://preview.redd.it/edxmilbhdrdf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a137cb5afbbf4d31e2b88f24f1650c0d929de0fc",
                    "width": 1080,
                    "height": 915
                  }
                ],
                "variants": {},
                "id": "K28qaJsPwncbcq8hbMnIeikk2ws5X_O0jpIKjC7klas"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m3n89p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/",
          "stickied": false,
          "url": "https://i.redd.it/edxmilbhdrdf1.jpeg",
          "subreddit_subscribers": 502029,
          "created_utc": 1752899301,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Here is a quick TL;DR 👇**\n\n🧠 **GPT-4.1** tops with 62% Action Completion (AC) overall.  \n⚡ **Gemini 2.5** Flash excels in tool use (94% TSQ) but lags in task completion (38% AC).  \n💸 **GPT-4.1**\\-mini is *most cost-effective* at $0.014/session vs. GPT-4.1’s $0.068.  \n🏭 No single model dominates across industries.  \n🤖 **Grok 4** didn't lead in any metric.  \n🧩 Reasoning models *underperform* compared to non-reasoning ones.  \n🆕 **Kimi’s K2** leads *open-source models* with 0.53 AC, 0.90 TSQ, and $0.039/session.\n\nLink Below:\n\n\\[Blog\\]: [https://galileo.ai/blog/agent-leaderboard-v2](https://galileo.ai/blog/agent-leaderboard-v2)\n\n\\[Agent v2 Live Leaderboard\\]: [https://huggingface.co/spaces/galileo-ai/agent-leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard)",
          "author_fullname": "t2_yy8p0c7c1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's New in Agent Leaderboard v2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3wnnm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 56,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 56,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/AlOxiE-_TO10duDcmMi1eDL5uImL_u1cPXM8de752XI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752932845,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Here is a quick TL;DR 👇&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;🧠 &lt;strong&gt;GPT-4.1&lt;/strong&gt; tops with 62% Action Completion (AC) overall.&lt;br/&gt;\n⚡ &lt;strong&gt;Gemini 2.5&lt;/strong&gt; Flash excels in tool use (94% TSQ) but lags in task completion (38% AC).&lt;br/&gt;\n💸 &lt;strong&gt;GPT-4.1&lt;/strong&gt;-mini is &lt;em&gt;most cost-effective&lt;/em&gt; at $0.014/session vs. GPT-4.1’s $0.068.&lt;br/&gt;\n🏭 No single model dominates across industries.&lt;br/&gt;\n🤖 &lt;strong&gt;Grok 4&lt;/strong&gt; didn&amp;#39;t lead in any metric.&lt;br/&gt;\n🧩 Reasoning models &lt;em&gt;underperform&lt;/em&gt; compared to non-reasoning ones.&lt;br/&gt;\n🆕 &lt;strong&gt;Kimi’s K2&lt;/strong&gt; leads &lt;em&gt;open-source models&lt;/em&gt; with 0.53 AC, 0.90 TSQ, and $0.039/session.&lt;/p&gt;\n\n&lt;p&gt;Link Below:&lt;/p&gt;\n\n&lt;p&gt;[Blog]: &lt;a href=\"https://galileo.ai/blog/agent-leaderboard-v2\"&gt;https://galileo.ai/blog/agent-leaderboard-v2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[Agent v2 Live Leaderboard]: &lt;a href=\"https://huggingface.co/spaces/galileo-ai/agent-leaderboard\"&gt;https://huggingface.co/spaces/galileo-ai/agent-leaderboard&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bwu8hq345udf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bwu8hq345udf1.png?auto=webp&amp;s=3647faebddc12e636990608928b9d0615e06d536",
                  "width": 1368,
                  "height": 882
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bwu8hq345udf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1586a4e62e31363148fbc6fc947cedd8323aa3c4",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://preview.redd.it/bwu8hq345udf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b2efea4433875b14b19fd354dda5ebde996a79b",
                    "width": 216,
                    "height": 139
                  },
                  {
                    "url": "https://preview.redd.it/bwu8hq345udf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=32c458acadb6b136f18b6bd3a7947c04a61f9e61",
                    "width": 320,
                    "height": 206
                  },
                  {
                    "url": "https://preview.redd.it/bwu8hq345udf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=994610c0a05c8b64924cfefbf8e0691a9c5619ef",
                    "width": 640,
                    "height": 412
                  },
                  {
                    "url": "https://preview.redd.it/bwu8hq345udf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=73d3457751a9450469c9e197bfe196b06ed09d00",
                    "width": 960,
                    "height": 618
                  },
                  {
                    "url": "https://preview.redd.it/bwu8hq345udf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=091aa0b984fb53e7c68e1325798ba1dff31c20f5",
                    "width": 1080,
                    "height": 696
                  }
                ],
                "variants": {},
                "id": "oiBo-Uzpnv9bJfOoszXjc-BnWRY9eQy_LsK8V2rhQrU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m3wnnm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "5h3r_10ck",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3wnnm/whats_new_in_agent_leaderboard_v2/",
          "stickied": false,
          "url": "https://i.redd.it/bwu8hq345udf1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1752932845,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys, I'm new here, so can you guide me please, which are currently the best uncensored creative writing GGUF models to run locally on 24 GB VRAM?? on LM Studio,\n\nIt would be great if it also had Vision capabilities, or you can suggest another model specific for vision, as long as it's good.",
          "author_fullname": "t2_11vokr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best uncensored creative writing GGUF model to run on 24 GB VRAM??",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4mvbe",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753011736,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;m new here, so can you guide me please, which are currently the best uncensored creative writing GGUF models to run locally on 24 GB VRAM?? on LM Studio,&lt;/p&gt;\n\n&lt;p&gt;It would be great if it also had Vision capabilities, or you can suggest another model specific for vision, as long as it&amp;#39;s good.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4mvbe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "younestft",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4mvbe/best_uncensored_creative_writing_gguf_model_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4mvbe/best_uncensored_creative_writing_gguf_model_to/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753011736,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://open.substack.com/pub/transformersandtheiravatars/p/rerankers-and-their-intricacies?r=1ftbb&amp;utm\\_campaign=post&amp;utm\\_medium=web&amp;showWelcomeOnShare=true](https://open.substack.com/pub/transformersandtheiravatars/p/rerankers-and-their-intricacies?r=1ftbb&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true)",
          "author_fullname": "t2_fecz50nu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wrote something about Rerankers - Why and How of it",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4gx69",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752989066,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://open.substack.com/pub/transformersandtheiravatars/p/rerankers-and-their-intricacies?r=1ftbb&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true\"&gt;https://open.substack.com/pub/transformersandtheiravatars/p/rerankers-and-their-intricacies?r=1ftbb&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rHHc_pSs8zQSrXpR-fgcrkV47cBhoPAaGDQbPb5m8_4.jpeg?auto=webp&amp;s=e1813900d0d75bc7e954d5f0747e221bbed0ad00",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rHHc_pSs8zQSrXpR-fgcrkV47cBhoPAaGDQbPb5m8_4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=006e5fe90751cae9f7b26e4e7518902709a50e33",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rHHc_pSs8zQSrXpR-fgcrkV47cBhoPAaGDQbPb5m8_4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f0bb7d2080351ac3260a11c108fc574b460755a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rHHc_pSs8zQSrXpR-fgcrkV47cBhoPAaGDQbPb5m8_4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6f95431695cd1d84d41e139d277ae899e881fff",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rHHc_pSs8zQSrXpR-fgcrkV47cBhoPAaGDQbPb5m8_4.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=63010f578ed9fa52aa83688e1251eb2494c867c1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rHHc_pSs8zQSrXpR-fgcrkV47cBhoPAaGDQbPb5m8_4.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=25caad42c8eee13fd2d25ad409d28871605b81fe",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rHHc_pSs8zQSrXpR-fgcrkV47cBhoPAaGDQbPb5m8_4.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a334fe8647b20455cbf43241f72ab71046bdccdc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rHHc_pSs8zQSrXpR-fgcrkV47cBhoPAaGDQbPb5m8_4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m4gx69",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ZucchiniCalm4617",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4gx69/wrote_something_about_rerankers_why_and_how_of_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4gx69/wrote_something_about_rerankers_why_and_how_of_it/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752989066,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From what I see, most tools claiming to change your voice actually just convert your speech into text, and then that text back into an AI voice.  You loose expression doing it this way, and it sounds a bit false.\n\nIt'd be super handy to retain the subtle inflections and performance of a talk, something mostly lost in \"text to ai voice\".\n\n(and then the next question would be to run it locally!)\n\nWould be good for YouTube channels.",
          "author_fullname": "t2_4yvtjui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do voice \"changers / modifiers\" actually exist?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4lwcu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753008242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From what I see, most tools claiming to change your voice actually just convert your speech into text, and then that text back into an AI voice.  You loose expression doing it this way, and it sounds a bit false.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;d be super handy to retain the subtle inflections and performance of a talk, something mostly lost in &amp;quot;text to ai voice&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;(and then the next question would be to run it locally!)&lt;/p&gt;\n\n&lt;p&gt;Would be good for YouTube channels.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4lwcu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jasj3b",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4lwcu/do_voice_changers_modifiers_actually_exist/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4lwcu/do_voice_changers_modifiers_actually_exist/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753008242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Hello everyone,**\n\nI built a lightweight LLM API invocation tool that requires no installation, just a single executable file.\n\n**Features:**\n\n- Truly Portable: It's a single executable file, no installation required.\n- Bring Your Own Model: Customize models and prompts easily through a config file.\n- Save &amp; Share: Export entire conversations as clean, single-file HTML pages.\n- Model Hopping: Switch between models in the same conversation.\n- Web-Aware: Can perform a web search or pull text from a URL to use as context for its answers.\n- File Upload: Drop in a PDF, TXT, or even a ZIP file to chat with your documents.\n- Code-Friendly: Proper Markdown rendering and syntax highlighting for code blocks.\n- Cost-Aware: Tracks token usage and lets you limit the conversation history sent with each request, which is a huge token saver.\n- Incognito Mode: For all your top-secret conversations.\n\nGitHub: https://github.com/jingangdidi/chatsong\n",
          "author_fullname": "t2_17df3wpa8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ChatSong, a lightweight, local LLM chat tool that's a single executable file",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 65,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3xp21",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Er-g9dyWlkqxXFbgZ9Wx1B05bomDy-ePqp_7q7bGiZc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752935608,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Hello everyone,&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I built a lightweight LLM API invocation tool that requires no installation, just a single executable file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Truly Portable: It&amp;#39;s a single executable file, no installation required.&lt;/li&gt;\n&lt;li&gt;Bring Your Own Model: Customize models and prompts easily through a config file.&lt;/li&gt;\n&lt;li&gt;Save &amp;amp; Share: Export entire conversations as clean, single-file HTML pages.&lt;/li&gt;\n&lt;li&gt;Model Hopping: Switch between models in the same conversation.&lt;/li&gt;\n&lt;li&gt;Web-Aware: Can perform a web search or pull text from a URL to use as context for its answers.&lt;/li&gt;\n&lt;li&gt;File Upload: Drop in a PDF, TXT, or even a ZIP file to chat with your documents.&lt;/li&gt;\n&lt;li&gt;Code-Friendly: Proper Markdown rendering and syntax highlighting for code blocks.&lt;/li&gt;\n&lt;li&gt;Cost-Aware: Tracks token usage and lets you limit the conversation history sent with each request, which is a huge token saver.&lt;/li&gt;\n&lt;li&gt;Incognito Mode: For all your top-secret conversations.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/jingangdidi/chatsong\"&gt;https://github.com/jingangdidi/chatsong&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/jcc7hsejdudf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/jcc7hsejdudf1.jpeg?auto=webp&amp;s=6cff592bd20116b6b2579c997163ba6962f30c39",
                  "width": 2512,
                  "height": 1175
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/jcc7hsejdudf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a942b53502ccdf0f0527df30d6c288d223c4d984",
                    "width": 108,
                    "height": 50
                  },
                  {
                    "url": "https://preview.redd.it/jcc7hsejdudf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=496e6518294913f35e1ac2d6ebe79782651c4ec9",
                    "width": 216,
                    "height": 101
                  },
                  {
                    "url": "https://preview.redd.it/jcc7hsejdudf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1f14be8874f553132c99597e9a9baa34e3b003e",
                    "width": 320,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/jcc7hsejdudf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b46aa1f23372af44a62d97c0c4858e30eac7768",
                    "width": 640,
                    "height": 299
                  },
                  {
                    "url": "https://preview.redd.it/jcc7hsejdudf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=175fc338f975bfd7744a519d4e05446a0ea0690d",
                    "width": 960,
                    "height": 449
                  },
                  {
                    "url": "https://preview.redd.it/jcc7hsejdudf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9bb91f4c83ee33432ff8ffcc8ce027d111cc9036",
                    "width": 1080,
                    "height": 505
                  }
                ],
                "variants": {},
                "id": "WMDeqFE2i_t38-GTpkNLecOU6beSdnhaGRelWtgxoV0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m3xp21",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Suitable-Patience916",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3xp21/chatsong_a_lightweight_local_llm_chat_tool_thats/",
          "stickied": false,
          "url": "https://i.redd.it/jcc7hsejdudf1.jpeg",
          "subreddit_subscribers": 502029,
          "created_utc": 1752935608,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "On the first game, first level of 8, I completed the level after wasting a lot of time trying to figure out what functionality the spacebar and mouse clicks had. None, it turned out. On the second level, I got completely stuck, then read in another thread that you have to move on and off the first shape several times to loop through available shapes until hitting the target shape. I would never in a millioin years have figured this out because I would never consider anyone would make an intelligence test this stupid.\n\nARC AGI 1 and 2 were fine, well designed. But this 3 version is a test of stupid persistence, not intelligence.",
          "author_fullname": "t2_1puly589vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ARC AGI 3 is stupid",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3ssb2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 78,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 78,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752920273,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the first game, first level of 8, I completed the level after wasting a lot of time trying to figure out what functionality the spacebar and mouse clicks had. None, it turned out. On the second level, I got completely stuck, then read in another thread that you have to move on and off the first shape several times to loop through available shapes until hitting the target shape. I would never in a millioin years have figured this out because I would never consider anyone would make an intelligence test this stupid.&lt;/p&gt;\n\n&lt;p&gt;ARC AGI 1 and 2 were fine, well designed. But this 3 version is a test of stupid persistence, not intelligence.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m3ssb2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackdareel",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752920273,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "You can find and listen to the dataset on huggingface: [https://huggingface.co/datasets/setfunctionenvironment/testnew](https://huggingface.co/datasets/setfunctionenvironment/testnew)\n\nThe sample rate of all audio is 24,000 kHz\n\nStats:\n\nTotal audio files/samples: 556,667\n\nTotal duration: 1024.71 hours (3688949 seconds)\n\nAverage duration: 6.63 seconds\n\nShortest clip: 0.41 seconds\n\nLongest clip: 44.97 seconds (all audio &gt;45 seconds removed)\n\nmore and more TTS models are releasing and improving, the size of these models are decreasing some even being 0.5b 0.7b or 0.1b parameters but unfortunately they all dont have NSFW capability. It is a shame there are so many NSFW LLM finetunes out there but none exist for text to speech, so if anyone at all has the compute to finetune one of the existing TTS models (kokoro, zonos, F5, chatterbox, orpheus) on my dataset that would be very appreciated as I would like to try it 🙏🙏🙏",
          "author_fullname": "t2_d1cmjz8p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a 1000 hour NSFW TTS dataset",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m39uqi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1389,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1389,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752862834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can find and listen to the dataset on huggingface: &lt;a href=\"https://huggingface.co/datasets/setfunctionenvironment/testnew\"&gt;https://huggingface.co/datasets/setfunctionenvironment/testnew&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The sample rate of all audio is 24,000 kHz&lt;/p&gt;\n\n&lt;p&gt;Stats:&lt;/p&gt;\n\n&lt;p&gt;Total audio files/samples: 556,667&lt;/p&gt;\n\n&lt;p&gt;Total duration: 1024.71 hours (3688949 seconds)&lt;/p&gt;\n\n&lt;p&gt;Average duration: 6.63 seconds&lt;/p&gt;\n\n&lt;p&gt;Shortest clip: 0.41 seconds&lt;/p&gt;\n\n&lt;p&gt;Longest clip: 44.97 seconds (all audio &amp;gt;45 seconds removed)&lt;/p&gt;\n\n&lt;p&gt;more and more TTS models are releasing and improving, the size of these models are decreasing some even being 0.5b 0.7b or 0.1b parameters but unfortunately they all dont have NSFW capability. It is a shame there are so many NSFW LLM finetunes out there but none exist for text to speech, so if anyone at all has the compute to finetune one of the existing TTS models (kokoro, zonos, F5, chatterbox, orpheus) on my dataset that would be very appreciated as I would like to try it 🙏🙏🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?auto=webp&amp;s=eaf6c20828fa91e2979d1e721d680c854e657edd",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=61c218fe1b611e82d1fa0231b8e465837900184c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ba9bae5986f07ff1605f76c98683ecac413ef27",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd6b465a3942f455569e0ad95e2f28711705f5cf",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb14db0ad378a207338fd48fc595d5fe5529da93",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f6352b72dcd64c07e298b59c20df20872e10dfa",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32573e6c5ddd5e011788e5a67fd71b8e4cd9f592",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {
                  "obfuscated": {
                    "source": {
                      "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=e3d2a29c308b4c9cf8bcf5a6a85981b76f1bb910",
                      "width": 1200,
                      "height": 648
                    },
                    "resolutions": [
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=fa1d5e66bed13f4e6a8d158997a2d6750dda696c",
                        "width": 108,
                        "height": 58
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=be97683c0854885ee168cf769d52ba84da4b8a7f",
                        "width": 216,
                        "height": 116
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=c99c5b878e8344be53589601be6f1ae0d639fa49",
                        "width": 320,
                        "height": 172
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=f8aef6d31356d15c976fc9441c494596fa94e80a",
                        "width": 640,
                        "height": 345
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=960&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=678072ab508c335c0c715058fe19d1ebafd6eb0d",
                        "width": 960,
                        "height": 518
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=1080&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=3327360c86a73bea706c8067769728625c775b1f",
                        "width": 1080,
                        "height": 583
                      }
                    ]
                  },
                  "nsfw": {
                    "source": {
                      "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=e3d2a29c308b4c9cf8bcf5a6a85981b76f1bb910",
                      "width": 1200,
                      "height": 648
                    },
                    "resolutions": [
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=fa1d5e66bed13f4e6a8d158997a2d6750dda696c",
                        "width": 108,
                        "height": 58
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=be97683c0854885ee168cf769d52ba84da4b8a7f",
                        "width": 216,
                        "height": 116
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=c99c5b878e8344be53589601be6f1ae0d639fa49",
                        "width": 320,
                        "height": 172
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=f8aef6d31356d15c976fc9441c494596fa94e80a",
                        "width": 640,
                        "height": 345
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=960&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=678072ab508c335c0c715058fe19d1ebafd6eb0d",
                        "width": 960,
                        "height": 518
                      },
                      {
                        "url": "https://external-preview.redd.it/DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs.png?width=1080&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=3327360c86a73bea706c8067769728625c775b1f",
                        "width": 1080,
                        "height": 583
                      }
                    ]
                  }
                },
                "id": "DOV0A62XqkURIwviorxHv5Y_o5mxdGlBNyoCsF8Y7Xs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m39uqi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hotroaches4liferz",
          "discussion_type": null,
          "num_comments": 137,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752862834,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/04o2jpdv41ef1.png?width=1204&amp;format=png&amp;auto=webp&amp;s=75489cf9eed1b51eb6ff0ad9e2ca8b03adfbbe81\n\n  \n  \nHi all,\n\nI’ve noticed plenty of questions and great insights in Reddit threads about the latest OCR and document-AI tools. After learning a lot from those discussions—and adding lessons from my own enterprise projects —I pulled together a brief mid-2025 summary: key VLM releases, specialist models, pipeline updates, new benchmarks and intresting findings.\n\nIf you work with OCR or RAG, the 5-minute read might help you catch up. I’d love to swap notes and hear what I’ve missed.\n\nLink [here](https://www.linkedin.com/pulse/ocr-genai-key-trends-from-h1-2025-igor-galitskiy-lldie/?trackingId=BbmlpEfVIzeh2jXrWnUcdw%3D%3D) (LinkedIn)\n\nThanks, looking forward to the discussion",
          "author_fullname": "t2_14xfga1he3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OCR and GenAI: Key Trends from H1 2025",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 125,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "04o2jpdv41ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 96,
                  "x": 108,
                  "u": "https://preview.redd.it/04o2jpdv41ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=230e1be39dd0673d79d27abbd27aac3e900731c6"
                },
                {
                  "y": 193,
                  "x": 216,
                  "u": "https://preview.redd.it/04o2jpdv41ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f5be85a2e3198f71f7f7b7a9f2088f9b0d9071aa"
                },
                {
                  "y": 287,
                  "x": 320,
                  "u": "https://preview.redd.it/04o2jpdv41ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=174008c0867b7bda39d6fb3bc35454147a373330"
                },
                {
                  "y": 574,
                  "x": 640,
                  "u": "https://preview.redd.it/04o2jpdv41ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=359ed185d5cdc682b0d42598277fe5c846dd8a57"
                },
                {
                  "y": 861,
                  "x": 960,
                  "u": "https://preview.redd.it/04o2jpdv41ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aebfd055094ffc1ebff29d23650a9c4c014271db"
                },
                {
                  "y": 968,
                  "x": 1080,
                  "u": "https://preview.redd.it/04o2jpdv41ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ced0687aa9f06eb281ce4dc592f41533d63d8bac"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1204,
                "u": "https://preview.redd.it/04o2jpdv41ef1.png?width=1204&amp;format=png&amp;auto=webp&amp;s=75489cf9eed1b51eb6ff0ad9e2ca8b03adfbbe81"
              },
              "id": "04o2jpdv41ef1"
            }
          },
          "name": "t3_1m48ffs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1SpZQ9edxGWfxNZr-4YXXcyEeqF5T9DlXhQ7VQOiugY.jpg",
          "edited": 1753017451,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752962788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/04o2jpdv41ef1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75489cf9eed1b51eb6ff0ad9e2ca8b03adfbbe81\"&gt;https://preview.redd.it/04o2jpdv41ef1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75489cf9eed1b51eb6ff0ad9e2ca8b03adfbbe81&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I’ve noticed plenty of questions and great insights in Reddit threads about the latest OCR and document-AI tools. After learning a lot from those discussions—and adding lessons from my own enterprise projects —I pulled together a brief mid-2025 summary: key VLM releases, specialist models, pipeline updates, new benchmarks and intresting findings.&lt;/p&gt;\n\n&lt;p&gt;If you work with OCR or RAG, the 5-minute read might help you catch up. I’d love to swap notes and hear what I’ve missed.&lt;/p&gt;\n\n&lt;p&gt;Link &lt;a href=\"https://www.linkedin.com/pulse/ocr-genai-key-trends-from-h1-2025-igor-galitskiy-lldie/?trackingId=BbmlpEfVIzeh2jXrWnUcdw%3D%3D\"&gt;here&lt;/a&gt; (LinkedIn)&lt;/p&gt;\n\n&lt;p&gt;Thanks, looking forward to the discussion&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m48ffs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Careless_Bed_5075",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m48ffs/ocr_and_genai_key_trends_from_h1_2025/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m48ffs/ocr_and_genai_key_trends_from_h1_2025/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752962788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_pmniwf57y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "any idea how to open source that?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3iv6s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 396,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 396,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/JEizGePmrchsMLUdQlvuhyq2K1kC-hHHTrqZoajuhsw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752885735,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/x9e7q7z59qdf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/x9e7q7z59qdf1.png?auto=webp&amp;s=1959651a9912fb81522eaa5eecf0768b045295a9",
                  "width": 550,
                  "height": 636
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/x9e7q7z59qdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4595dcc12e0461458b753878ce5ae01920e1b3c7",
                    "width": 108,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/x9e7q7z59qdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f33e5d8d8ab496c43d204ceba973e02ab0f072e",
                    "width": 216,
                    "height": 249
                  },
                  {
                    "url": "https://preview.redd.it/x9e7q7z59qdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dd978d7cb888d92fdfc0a24134a57d1d3821cd08",
                    "width": 320,
                    "height": 370
                  }
                ],
                "variants": {},
                "id": "D-49NDkja8-wQIAu6tRzjcwo6u8Bk2KQlJSBHgsa-68"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m3iv6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "secopsml",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/",
          "stickied": false,
          "url": "https://i.redd.it/x9e7q7z59qdf1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1752885735,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_6ubpe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I love local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 34,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3tk92",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 54,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 54,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/nzq3J5peuyY56bDcbTC3G4Y8p-x9ym5-zzE5xn4aY7A.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752923215,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/k7ebpl1nctdf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/k7ebpl1nctdf1.png?auto=webp&amp;s=5ec826b7cc2e391b445c619e4f7f8cd43c832f14",
                  "width": 798,
                  "height": 199
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/k7ebpl1nctdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f7fd2866d56b1cb6f84df32f67409180e0f9ee8b",
                    "width": 108,
                    "height": 26
                  },
                  {
                    "url": "https://preview.redd.it/k7ebpl1nctdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=32ab95cb32a3c51ebbf7de5b983482fd25180ad3",
                    "width": 216,
                    "height": 53
                  },
                  {
                    "url": "https://preview.redd.it/k7ebpl1nctdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=984fa746bbc8e606dceee7bd69be3b1f0101b284",
                    "width": 320,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/k7ebpl1nctdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=22285a6e5642636636a349ad5e51158d2aa60f71",
                    "width": 640,
                    "height": 159
                  }
                ],
                "variants": {},
                "id": "a2m6l7_RYwfI11tlzGuLF9ptF4wRufgEXfSnzKQ9hmc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m3tk92",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TweeMansLeger",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3tk92/i_love_local_models/",
          "stickied": false,
          "url": "https://i.redd.it/k7ebpl1nctdf1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1752923215,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking for a gpu to put in a EliteDesk SFF PC. I don’t plan to run anything past 8b models so VRAM doesn’t need to be super high.\n\nWas looking at this 3050 LP but wasn’t sure of performance:\n\nhttps://www.zotacstore.com/us/zt-a30510l-10l-r",
          "author_fullname": "t2_gryz3583",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Budget SFF/Low profile gpu’s?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4j5nf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752997469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking for a gpu to put in a EliteDesk SFF PC. I don’t plan to run anything past 8b models so VRAM doesn’t need to be super high.&lt;/p&gt;\n\n&lt;p&gt;Was looking at this 3050 LP but wasn’t sure of performance:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.zotacstore.com/us/zt-a30510l-10l-r\"&gt;https://www.zotacstore.com/us/zt-a30510l-10l-r&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4j5nf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MEI2011",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4j5nf/best_budget_sfflow_profile_gpus/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4j5nf/best_budget_sfflow_profile_gpus/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752997469,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\nI’ve been working on fine-tuning open-source LLMs like Phi-3 and LLaMA 3 using Unsloth in Google Colab, targeting a chatbot for customer support (around 500 prompt-response examples).\n\nI’m facing the same recurring issues no matter what I do:\n\n⸻\n\n❗ The problems:\n\t1.\tThe model often responds with the exact same prompt I gave it, instead of the intended response.\n\t2.\tSometimes it returns blank output.\n\t3.\tWhen it does respond, it gives very generic or off-topic answers, not the specific ones from my training data.\n\n⸻\n\n🛠️ My Setup:\n\t•\tUsing Unsloth + FastLanguageModel\n\t•\tTrained on a .json or .jsonl dataset with format:\n\n{\n  \"prompt\": \"How long does it take to get a refund?\",\n  \"response\": \"Refunds typically take 5–7 business days.\"\n}\n\n\nWrapped in training with:\n\nf\"### Input: {prompt}\\n### Output: {response}&lt;|endoftext|&gt;\"\n\n\nInference via:\n\nmessages = [{\"role\": \"user\", \"content\": \"How long does it take to get a refund?\"}]\ntokenizer.apply_chat_template(...)\n\n\nWhat I’ve tried:\n\t•\tTraining with both 3 and 10 epochs\n\t•\tTraining both Phi-3-mini and LLaMA 3 8B with LoRA (4-bit)\n\t•\tTesting with correct Modelfile templates in Ollama like:\n\n\nTEMPLATE \"\"\"### Input: {{ .Prompt }}\\n### Output:\"\"\"\n\nWhy is the model not learning my input-output structure properly?\n\t•\tIs there a better way to format the prompts or structure the dataset?\n\t•\tCould the model size (like Phi-3) be a bottleneck?\n\t•\tShould I be adding system prompts or few-shot examples at inference?\n\nAny advice, shared experiences, or working examples would help a lot.\nThanks in advance!\n",
          "author_fullname": "t2_1hyyc3o6lb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🆘 [Help] My Fine-Tuned Model Keeps Echoing Prompts or Giving Blank/Generic Responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4j0sa",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752996921,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,\nI’ve been working on fine-tuning open-source LLMs like Phi-3 and LLaMA 3 using Unsloth in Google Colab, targeting a chatbot for customer support (around 500 prompt-response examples).&lt;/p&gt;\n\n&lt;p&gt;I’m facing the same recurring issues no matter what I do:&lt;/p&gt;\n\n&lt;p&gt;⸻&lt;/p&gt;\n\n&lt;p&gt;❗ The problems:\n    1.  The model often responds with the exact same prompt I gave it, instead of the intended response.\n    2.  Sometimes it returns blank output.\n    3.  When it does respond, it gives very generic or off-topic answers, not the specific ones from my training data.&lt;/p&gt;\n\n&lt;p&gt;⸻&lt;/p&gt;\n\n&lt;p&gt;🛠️ My Setup:\n    • Using Unsloth + FastLanguageModel\n    • Trained on a .json or .jsonl dataset with format:&lt;/p&gt;\n\n&lt;p&gt;{\n  &amp;quot;prompt&amp;quot;: &amp;quot;How long does it take to get a refund?&amp;quot;,\n  &amp;quot;response&amp;quot;: &amp;quot;Refunds typically take 5–7 business days.&amp;quot;\n}&lt;/p&gt;\n\n&lt;p&gt;Wrapped in training with:&lt;/p&gt;\n\n&lt;p&gt;f&amp;quot;### Input: {prompt}\\n### Output: {response}&amp;lt;|endoftext|&amp;gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Inference via:&lt;/p&gt;\n\n&lt;p&gt;messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How long does it take to get a refund?&amp;quot;}]\ntokenizer.apply_chat_template(...)&lt;/p&gt;\n\n&lt;p&gt;What I’ve tried:\n    • Training with both 3 and 10 epochs\n    • Training both Phi-3-mini and LLaMA 3 8B with LoRA (4-bit)\n    • Testing with correct Modelfile templates in Ollama like:&lt;/p&gt;\n\n&lt;p&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;### Input: {{ .Prompt }}\\n### Output:&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Why is the model not learning my input-output structure properly?\n    • Is there a better way to format the prompts or structure the dataset?\n    • Could the model size (like Phi-3) be a bottleneck?\n    • Should I be adding system prompts or few-shot examples at inference?&lt;/p&gt;\n\n&lt;p&gt;Any advice, shared experiences, or working examples would help a lot.\nThanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4j0sa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Srmxz",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752996921,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's a brand new  Ai firework called  Mixture of Recursions from  Google DeepMimd .\n\nAnd NO  ..This is not my video ..",
          "author_fullname": "t2_oy3c84euj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smaller, Faster, Smarter: Why MoR Might Replace Transformers | Front Page",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4swld",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MfswBXmSPZU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Smaller, Faster, Smarter: Why MoR Might Replace Transformers | Front Page\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Smaller, Faster, Smarter: Why MoR Might Replace Transformers | Front Page",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MfswBXmSPZU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Smaller, Faster, Smarter: Why MoR Might Replace Transformers | Front Page\"&gt;&lt;/iframe&gt;",
              "author_name": "AIM Tv",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/MfswBXmSPZU/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@aimmediahouse"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MfswBXmSPZU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Smaller, Faster, Smarter: Why MoR Might Replace Transformers | Front Page\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m4swld",
            "height": 200
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/BaYjkt9t34xYr7tLos2qWSu-VnvXRoFAvrMZrJeMuu4.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=f0e5deb9f3f0690c2ff744f2d6f1bf5e5a4d76f2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753028206,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a brand new  Ai firework called  Mixture of Recursions from  Google DeepMimd .&lt;/p&gt;\n\n&lt;p&gt;And NO  ..This is not my video ..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/MfswBXmSPZU?si=7WIVGDy4BsV7EGkp",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/BaYjkt9t34xYr7tLos2qWSu-VnvXRoFAvrMZrJeMuu4.jpeg?auto=webp&amp;s=0fd11ce0f4ac741e12aa3447830e218091b54ce0",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/BaYjkt9t34xYr7tLos2qWSu-VnvXRoFAvrMZrJeMuu4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14f1816fa9f3bf2d2f3c975f6345c9490bdc384c",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/BaYjkt9t34xYr7tLos2qWSu-VnvXRoFAvrMZrJeMuu4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7bb23938e9bb770153127be83f082c146bdb431d",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/BaYjkt9t34xYr7tLos2qWSu-VnvXRoFAvrMZrJeMuu4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ee3696dcc5b93ef123d91a5ebe3674c2dd0eec6",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "BaYjkt9t34xYr7tLos2qWSu-VnvXRoFAvrMZrJeMuu4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m4swld",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sub_RedditTor",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4swld/smaller_faster_smarter_why_mor_might_replace/",
          "stickied": false,
          "url": "https://youtu.be/MfswBXmSPZU?si=7WIVGDy4BsV7EGkp",
          "subreddit_subscribers": 502029,
          "created_utc": 1753028206,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Smaller, Faster, Smarter: Why MoR Might Replace Transformers | Front Page",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MfswBXmSPZU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Smaller, Faster, Smarter: Why MoR Might Replace Transformers | Front Page\"&gt;&lt;/iframe&gt;",
              "author_name": "AIM Tv",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/MfswBXmSPZU/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@aimmediahouse"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This command runs verifiable LLM inference using Parity Protocol, our open decentralized compute engine.\n\nhttps://preview.redd.it/fk9nx5j7a2ef1.jpg?width=1226&amp;format=pjpg&amp;auto=webp&amp;s=a90ba9e9b4bac68c43e6af6b453bd0c169b20f25\n\n\\- Task gets executed in a distributed way  \n\\- Each node returns output + hash  \n\\- Outputs are matched and verified before being accepted  \n\\- No cloud, no GPU access needed on client side  \n\\- Works with any containerized LLM (open models)\n\nWe’re college devs building a trustless alternative to AWS Lambda for container-based compute.\n\nGitHub: [https://github.com/theblitlabs](https://github.com/theblitlabs)  \nDocs: [https://blitlabs.xyz/docs](https://blitlabs.xyz/docs)  \nTwitter: [https://twitter.com/labsblit](https://twitter.com/labsblit)\n\nWould love feedback or help. Everything is open source and permissionless.",
          "author_fullname": "t2_frf7k3no",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Decentralized LLM inference from your terminal, verified on-chain",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "fk9nx5j7a2ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 29,
                  "x": 108,
                  "u": "https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bcfd05bd9ebb1b5e09b797dc986c1b8e3dee2f48"
                },
                {
                  "y": 59,
                  "x": 216,
                  "u": "https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f59a7510da507171c55e8f6e4a175d5cab3d97ac"
                },
                {
                  "y": 88,
                  "x": 320,
                  "u": "https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25fc946b7634a0a9f99e149a3cb66db8ac3c621e"
                },
                {
                  "y": 176,
                  "x": 640,
                  "u": "https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c56da5db94b55019862462153ee93d4b0497fff9"
                },
                {
                  "y": 264,
                  "x": 960,
                  "u": "https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b7792351faa725fd4ade35e9715b17166265b11"
                },
                {
                  "y": 297,
                  "x": 1080,
                  "u": "https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89730dd20db010eb217fce3a57b8933d935147d9"
                }
              ],
              "s": {
                "y": 338,
                "x": 1226,
                "u": "https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=1226&amp;format=pjpg&amp;auto=webp&amp;s=a90ba9e9b4bac68c43e6af6b453bd0c169b20f25"
              },
              "id": "fk9nx5j7a2ef1"
            }
          },
          "name": "t3_1m4u914",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/JvOB4teBJPexclfMC0OuzNuMWooM4UHhYEL6W0yN6Es.jpeg?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=3b510a46eef0d8ac0b4fc29a5131b5d4add5127c",
          "edited": 1753033303,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753031395,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This command runs verifiable LLM inference using Parity Protocol, our open decentralized compute engine.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=1226&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a90ba9e9b4bac68c43e6af6b453bd0c169b20f25\"&gt;https://preview.redd.it/fk9nx5j7a2ef1.jpg?width=1226&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a90ba9e9b4bac68c43e6af6b453bd0c169b20f25&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- Task gets executed in a distributed way&lt;br/&gt;\n- Each node returns output + hash&lt;br/&gt;\n- Outputs are matched and verified before being accepted&lt;br/&gt;\n- No cloud, no GPU access needed on client side&lt;br/&gt;\n- Works with any containerized LLM (open models)&lt;/p&gt;\n\n&lt;p&gt;We’re college devs building a trustless alternative to AWS Lambda for container-based compute.&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/theblitlabs\"&gt;https://github.com/theblitlabs&lt;/a&gt;&lt;br/&gt;\nDocs: &lt;a href=\"https://blitlabs.xyz/docs\"&gt;https://blitlabs.xyz/docs&lt;/a&gt;&lt;br/&gt;\nTwitter: &lt;a href=\"https://twitter.com/labsblit\"&gt;https://twitter.com/labsblit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback or help. Everything is open source and permissionless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JvOB4teBJPexclfMC0OuzNuMWooM4UHhYEL6W0yN6Es.jpeg?auto=webp&amp;s=63a41c95c44b4f27c1e60c9673a368e0e9bfee2c",
                  "width": 280,
                  "height": 280
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JvOB4teBJPexclfMC0OuzNuMWooM4UHhYEL6W0yN6Es.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53c3b00d143605b43ec731d87b95add6cf7cf10c",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/JvOB4teBJPexclfMC0OuzNuMWooM4UHhYEL6W0yN6Es.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a988393a6aa764ec2374ed982711663785ab0ad5",
                    "width": 216,
                    "height": 216
                  }
                ],
                "variants": {},
                "id": "JvOB4teBJPexclfMC0OuzNuMWooM4UHhYEL6W0yN6Es"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m4u914",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Efficient-Ad-2913",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4u914/decentralized_llm_inference_from_your_terminal/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4u914/decentralized_llm_inference_from_your_terminal/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753031395,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been exploring local LLM setups lately and wanted to ask the community:\n\nWhat are the **most frustrating parts** of running models locally?\n\nAny specific struggles with **low VRAM GPUs**, **limited RAM**, or **older hardware**?\n\nHave you faced issues with **quantization**, **driver setup**, **tokenizer mismatches**, or **inference crashes**?\n\nWhat do you wish **\"just worked\" out of the box**?\n\nDo you prefer **GGUF**, **ONNX**, or other formats and why?\n\nI want to learn from others doing this regularly\n\nThanks in advance to anyone who shares 🙏",
          "author_fullname": "t2_hu9onfqo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's your biggest pain point running LLMs locally (especially with low VRAM GPUs)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4l1tl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753004979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been exploring local LLM setups lately and wanted to ask the community:&lt;/p&gt;\n\n&lt;p&gt;What are the &lt;strong&gt;most frustrating parts&lt;/strong&gt; of running models locally?&lt;/p&gt;\n\n&lt;p&gt;Any specific struggles with &lt;strong&gt;low VRAM GPUs&lt;/strong&gt;, &lt;strong&gt;limited RAM&lt;/strong&gt;, or &lt;strong&gt;older hardware&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;Have you faced issues with &lt;strong&gt;quantization&lt;/strong&gt;, &lt;strong&gt;driver setup&lt;/strong&gt;, &lt;strong&gt;tokenizer mismatches&lt;/strong&gt;, or &lt;strong&gt;inference crashes&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;What do you wish &lt;strong&gt;&amp;quot;just worked&amp;quot; out of the box&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;Do you prefer &lt;strong&gt;GGUF&lt;/strong&gt;, &lt;strong&gt;ONNX&lt;/strong&gt;, or other formats and why?&lt;/p&gt;\n\n&lt;p&gt;I want to learn from others doing this regularly&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance to anyone who shares 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4l1tl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xitizdumb",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4l1tl/whats_your_biggest_pain_point_running_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4l1tl/whats_your_biggest_pain_point_running_llms/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753004979,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been keeping up with AI research in 2025, and DeepSeek R1 really stands out to me as game-changing. What other papers from this year do you consider to be truly revolutionary?",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the most intriguing AI papers of 2025",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3qpxz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752912018,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been keeping up with AI research in 2025, and DeepSeek R1 really stands out to me as game-changing. What other papers from this year do you consider to be truly revolutionary?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m3qpxz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752912018,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In my company there are some internal blocks. So I developed my own web application using pure html, css and js. It's not perfect yet and just to make it easier to use local models. I accept suggestions for improvements.\n\n",
          "author_fullname": "t2_yk5v109cc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I developed my own webapp to use the local templates.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4d74b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=b42e0730fe2b3a086d8e4bfaa2f29f3712ad17e7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752976684,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company there are some internal blocks. So I developed my own web application using pure html, css and js. It&amp;#39;s not perfect yet and just to make it easier to use local models. I accept suggestions for improvements.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/martinsagabriel/LocalLama",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64.png?auto=webp&amp;s=0f64cae09c47ca14c9c88257daaddad16ee1e6c5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f7669905ccf15d531d56c0729d1011e8f26f47a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a62b8860bb70ebce8c8712fabc4d07ed9076c26",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=397409244d0a91bb84e431ff99fb90ba7ede0061",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=941c8e748cc515a47f4d1f42a113d9a9ca2fd469",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7dd6c2e8f3a0fc105edb3e6aecbf52fd50278b95",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=afad2f9b273c0862aaffa038baf6f13606a3543d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "5bv_gPO23RdJJo822AU-U14GDuIrIMvUpu0qerJmz64"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4d74b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gabe__martins",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4d74b/i_developed_my_own_webapp_to_use_the_local/",
          "stickied": false,
          "url": "https://github.com/martinsagabriel/LocalLama",
          "subreddit_subscribers": 502029,
          "created_utc": 1752976684,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking to fine tune an AI using a bunch of publicly submitted data.\n\nWhich means I'll be asking people questions, they'll be submitting answers that might disagree with each other.\n\nI then want to train it on question-answer pairs and would like it to learn from both sides instead of negative transfer that I've been reading a little about which seems like the two would actually worsen the model performance overall.\n\nThe idea of negative transfer is if you feed in conflicting data when fine tuning it'll actually cause the model to unlearn information, leading to worse results than if you hadn't fed in anything at all or at least that's my understanding.. I would like it to learn that the argument has multiple sides to it that can be seen as correct or ideally to blend the two arguments together in it's outputs giving an answer that represents both sides.\n\nI hear there are solutions but I'm a little bit of a newbie, would be nice to hear from someone who knows something about this.",
          "author_fullname": "t2_dx7q1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to prevent negative transfer when fine tuning?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4goon",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752988207,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to fine tune an AI using a bunch of publicly submitted data.&lt;/p&gt;\n\n&lt;p&gt;Which means I&amp;#39;ll be asking people questions, they&amp;#39;ll be submitting answers that might disagree with each other.&lt;/p&gt;\n\n&lt;p&gt;I then want to train it on question-answer pairs and would like it to learn from both sides instead of negative transfer that I&amp;#39;ve been reading a little about which seems like the two would actually worsen the model performance overall.&lt;/p&gt;\n\n&lt;p&gt;The idea of negative transfer is if you feed in conflicting data when fine tuning it&amp;#39;ll actually cause the model to unlearn information, leading to worse results than if you hadn&amp;#39;t fed in anything at all or at least that&amp;#39;s my understanding.. I would like it to learn that the argument has multiple sides to it that can be seen as correct or ideally to blend the two arguments together in it&amp;#39;s outputs giving an answer that represents both sides.&lt;/p&gt;\n\n&lt;p&gt;I hear there are solutions but I&amp;#39;m a little bit of a newbie, would be nice to hear from someone who knows something about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4goon",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mczarnek",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4goon/how_to_prevent_negative_transfer_when_fine_tuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4goon/how_to_prevent_negative_transfer_when_fine_tuning/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752988207,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DGAF if it’s dumber. It’s mine.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 135,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m390kj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 635,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 635,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YPe1IFioKcHQ4WOZzX19othK6gTsJAl7yA6-fWjF8wE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752860894,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8dnb7bl76odf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8dnb7bl76odf1.png?auto=webp&amp;s=c60f4021b96071e37944f463074b15078e619811",
                  "width": 746,
                  "height": 722
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8dnb7bl76odf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad9f691b92ad45a65ab2abfa4ebb4551e504a761",
                    "width": 108,
                    "height": 104
                  },
                  {
                    "url": "https://preview.redd.it/8dnb7bl76odf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a1e1fde8f5d96dda4c4478c6432c878ab180a89",
                    "width": 216,
                    "height": 209
                  },
                  {
                    "url": "https://preview.redd.it/8dnb7bl76odf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38c832f7fcd68ff8ee93a9f5a7102630207ec850",
                    "width": 320,
                    "height": 309
                  },
                  {
                    "url": "https://preview.redd.it/8dnb7bl76odf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca95154378d607f250ca4e5e26488394250116bf",
                    "width": 640,
                    "height": 619
                  }
                ],
                "variants": {},
                "id": "UwhNYMo4iihHIf6j71k-s54Q0PjnV6n5gLkpAaMkcmE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m390kj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/",
          "stickied": false,
          "url": "https://i.redd.it/8dnb7bl76odf1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1752860894,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I wanted to update my models in ollama and asked for advice on updating models. \n\nCut a long story short I downloaded ollama version 0.9.6 both from web and brew.\n\nGemini 2.5 pro insists it should be 0.2.0. Have I lost my mind?\n\nThe response when it asked me to type in which ollama to establish version. \n\nYou have got to be kidding me.\n\nI am genuinely sorry. This is one of the most stubborn and nonsensical installation issues I have ever encountered. After all those steps, seeing `0.9.6` should be impossible. It means that despite our exhaustive efforts, there is **still** a rogue, old version of the `ollama` command somewhere on your system that your terminal is executing.\n\nLet's try one final, definitive diagnostic sequence. We need to see what your shell is doing in real-time.\n\nPlease **open a brand new terminal window** (this is very important to clear any command caching) and run these three commands exactly as they are written. Paste the entire output, including the commands you typed.\n\n\n\n# Final Diagnostic Sequence\n\n\n\n1. First, ask the shell to find the command:Bashwhich ollama \n2. Next, ask the shell to describe the command type:Bashtype ollama \n3. Finally, force the system to run the version check on the exact file it just found:Bash$(which ollama) --version \n\nThe output of this sequence will give us the absolute ground truth. It will either reveal the location of this incredibly persistent old file, or it will show a deep contradiction in your system's state.",
          "author_fullname": "t2_6vclq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the latest version of ollama?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4rbqv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753024349,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to update my models in ollama and asked for advice on updating models. &lt;/p&gt;\n\n&lt;p&gt;Cut a long story short I downloaded ollama version 0.9.6 both from web and brew.&lt;/p&gt;\n\n&lt;p&gt;Gemini 2.5 pro insists it should be 0.2.0. Have I lost my mind?&lt;/p&gt;\n\n&lt;p&gt;The response when it asked me to type in which ollama to establish version. &lt;/p&gt;\n\n&lt;p&gt;You have got to be kidding me.&lt;/p&gt;\n\n&lt;p&gt;I am genuinely sorry. This is one of the most stubborn and nonsensical installation issues I have ever encountered. After all those steps, seeing &lt;code&gt;0.9.6&lt;/code&gt; should be impossible. It means that despite our exhaustive efforts, there is &lt;strong&gt;still&lt;/strong&gt; a rogue, old version of the &lt;code&gt;ollama&lt;/code&gt; command somewhere on your system that your terminal is executing.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s try one final, definitive diagnostic sequence. We need to see what your shell is doing in real-time.&lt;/p&gt;\n\n&lt;p&gt;Please &lt;strong&gt;open a brand new terminal window&lt;/strong&gt; (this is very important to clear any command caching) and run these three commands exactly as they are written. Paste the entire output, including the commands you typed.&lt;/p&gt;\n\n&lt;h1&gt;Final Diagnostic Sequence&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;First, ask the shell to find the command:Bashwhich ollama &lt;/li&gt;\n&lt;li&gt;Next, ask the shell to describe the command type:Bashtype ollama &lt;/li&gt;\n&lt;li&gt;Finally, force the system to run the version check on the exact file it just found:Bash$(which ollama) --version &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The output of this sequence will give us the absolute ground truth. It will either reveal the location of this incredibly persistent old file, or it will show a deep contradiction in your system&amp;#39;s state.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4rbqv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "elksie5000",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4rbqv/what_is_the_latest_version_of_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4rbqv/what_is_the_latest_version_of_ollama/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753024349,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new to ML and fine tuning.\n\nRecently I've tried fine tuning gemma 3 on google collab on an 85k dataset (Dolly, Alpaca + custom) and it took 3 hours with Keras on a single A100 gpu. But then I couldn't convert it to pytorch because the conversion script by Keras doesn't support the gemma 3 yet and so I abandoned this project because of that.\n\nI then tried fine tuning with transformers and even though I've tried it on an H100 (100+ GB VRAM), it was showing like 30+ hours. I then tried with unsloth to afford a cheaper GPU and it was showing 200+ hours on an L40.\n\nI learned that Keras has the advantage of mixed precision, which was why it was so much faster. But I expected transformers to have something similar. Or at least something that would narrow the gap of 10x difference.\n\nI'm wondering is Keras really so much better in performance or am I doing it wrong with transformers? And is there a way to convert a gemma 3 model from Keras to transformers or I really must do it with transformers. The goal is to load it to HF and query with vLLM.\n\nThank you in advance\n\nSorry, this post ",
          "author_fullname": "t2_iimuspe7h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Keras vs Transformers fine tuning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m44tnz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752953435,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to ML and fine tuning.&lt;/p&gt;\n\n&lt;p&gt;Recently I&amp;#39;ve tried fine tuning gemma 3 on google collab on an 85k dataset (Dolly, Alpaca + custom) and it took 3 hours with Keras on a single A100 gpu. But then I couldn&amp;#39;t convert it to pytorch because the conversion script by Keras doesn&amp;#39;t support the gemma 3 yet and so I abandoned this project because of that.&lt;/p&gt;\n\n&lt;p&gt;I then tried fine tuning with transformers and even though I&amp;#39;ve tried it on an H100 (100+ GB VRAM), it was showing like 30+ hours. I then tried with unsloth to afford a cheaper GPU and it was showing 200+ hours on an L40.&lt;/p&gt;\n\n&lt;p&gt;I learned that Keras has the advantage of mixed precision, which was why it was so much faster. But I expected transformers to have something similar. Or at least something that would narrow the gap of 10x difference.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering is Keras really so much better in performance or am I doing it wrong with transformers? And is there a way to convert a gemma 3 model from Keras to transformers or I really must do it with transformers. The goal is to load it to HF and query with vLLM.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n\n&lt;p&gt;Sorry, this post &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m44tnz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Refrigerator6609",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752953435,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've built myself a PC with 2x 3090, because I thought it would be a sweetspot to start with for something twice as capable than a regular single-card PC yet still fitting a regular case. \n\nHowever most models still seem to be either targeted at a single card, or at a server. I also likely made a mistake by using an OC-targeted mobo for 4-slots spacing between cards and x8/x8 lanes, but it only has 2x RAM slots, so I can't even shove more RAM into it to run 200gb quants. ",
          "author_fullname": "t2_1thcf8mit6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any quants of larger models 48 VRAM + 96 RAM can run, which are better than just 32B models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3wogu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752932911,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve built myself a PC with 2x 3090, because I thought it would be a sweetspot to start with for something twice as capable than a regular single-card PC yet still fitting a regular case. &lt;/p&gt;\n\n&lt;p&gt;However most models still seem to be either targeted at a single card, or at a server. I also likely made a mistake by using an OC-targeted mobo for 4-slots spacing between cards and x8/x8 lanes, but it only has 2x RAM slots, so I can&amp;#39;t even shove more RAM into it to run 200gb quants. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m3wogu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "West_Investigator258",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752932911,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hi i love playing with those stuff create stuff for fun, but i have 0 code knowledge. i want to use api of openai or or anthropic . is there any open source that its like lovable and bolt but i use openai api and results are good?",
          "author_fullname": "t2_z7d1fpvta",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "any lovable and bolt alternative open source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m40yo6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752943817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi i love playing with those stuff create stuff for fun, but i have 0 code knowledge. i want to use api of openai or or anthropic . is there any open source that its like lovable and bolt but i use openai api and results are good?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m40yo6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yuval052",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m40yo6/any_lovable_and_bolt_alternative_open_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m40yo6/any_lovable_and_bolt_alternative_open_source/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752943817,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**How the Forensic Linguistics Analysis Works:**\n\nI built this using established computational linguistics techniques for authorship attribution - the same methods used in legal cases and academic research.\n\n**1. Corpus Building**\n\n* Compiled 76 documents (14M characters) of verified Trump statements from debates, speeches, tweets, and press releases\n* Cleaned the data to remove metadata while preserving actual speech patterns\n\n**2. Stylometric Feature Extraction** The system extracts 4 categories of linguistic \"fingerprints\":\n\n* **Lexical Features**: Average word length, vocabulary richness, hapax legomena ratio (words used only once), Yule's K diversity measure\n* **Syntactic Features**: Part-of-speech distributions, dependency parsing patterns, sentence complexity scores\n* **Semantic Features**: 768-dimension embeddings from the STAR authorship attribution model (AIDA-UPM/star)\n* **Stylistic Features**: Modal verb usage, passive voice frequency, punctuation patterns, function word ratios\n\n**3. Similarity Calculation**\n\n* Compares the disputed text against all corpus documents using cosine similarity and Jensen-Shannon divergence\n* Generates weighted scores across all four linguistic dimensions\n* The 89.6% syntactic similarity is particularly significant - sentence structure patterns are neurologically hardwired and hardest to fake\n\n**4. Why This Matters** Syntactic patterns emerge from deep cognitive structures. You can consciously change topic or vocabulary, but your underlying grammatical architecture remains consistent. The high syntactic match (89.6%) combined with moderate lexical match (47.2%) suggests same author writing in a different context.\n\nThe system correctly identified this as \"probably same author\" with 66.1% overall confidence - which is forensically significant for disputed authorship cases.",
          "author_fullname": "t2_43prq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a forensic linguistics tool to verify disputed quotes using computational stylometry - tested it on the Trump/Epstein birthday letter controversy.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 131,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3no1m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 57,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 57,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/c2qKM9SiM3n20QCy0GlpFM8xXkQZc98xFU_seR-muW8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752900781,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;How the Forensic Linguistics Analysis Works:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I built this using established computational linguistics techniques for authorship attribution - the same methods used in legal cases and academic research.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Corpus Building&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compiled 76 documents (14M characters) of verified Trump statements from debates, speeches, tweets, and press releases&lt;/li&gt;\n&lt;li&gt;Cleaned the data to remove metadata while preserving actual speech patterns&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Stylometric Feature Extraction&lt;/strong&gt; The system extracts 4 categories of linguistic &amp;quot;fingerprints&amp;quot;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lexical Features&lt;/strong&gt;: Average word length, vocabulary richness, hapax legomena ratio (words used only once), Yule&amp;#39;s K diversity measure&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Syntactic Features&lt;/strong&gt;: Part-of-speech distributions, dependency parsing patterns, sentence complexity scores&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Semantic Features&lt;/strong&gt;: 768-dimension embeddings from the STAR authorship attribution model (AIDA-UPM/star)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Stylistic Features&lt;/strong&gt;: Modal verb usage, passive voice frequency, punctuation patterns, function word ratios&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Similarity Calculation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compares the disputed text against all corpus documents using cosine similarity and Jensen-Shannon divergence&lt;/li&gt;\n&lt;li&gt;Generates weighted scores across all four linguistic dimensions&lt;/li&gt;\n&lt;li&gt;The 89.6% syntactic similarity is particularly significant - sentence structure patterns are neurologically hardwired and hardest to fake&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Why This Matters&lt;/strong&gt; Syntactic patterns emerge from deep cognitive structures. You can consciously change topic or vocabulary, but your underlying grammatical architecture remains consistent. The high syntactic match (89.6%) combined with moderate lexical match (47.2%) suggests same author writing in a different context.&lt;/p&gt;\n\n&lt;p&gt;The system correctly identified this as &amp;quot;probably same author&amp;quot; with 66.1% overall confidence - which is forensically significant for disputed authorship cases.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wz3nkrm3hrdf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wz3nkrm3hrdf1.png?auto=webp&amp;s=c72338d0bdaf0616bd1651a496b4aeabd588b183",
                  "width": 912,
                  "height": 858
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wz3nkrm3hrdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d701ff8082323545062dccc7df2a4dd900c4c86",
                    "width": 108,
                    "height": 101
                  },
                  {
                    "url": "https://preview.redd.it/wz3nkrm3hrdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=97ae86f4f745fa59e094f83d46593d76147ee03e",
                    "width": 216,
                    "height": 203
                  },
                  {
                    "url": "https://preview.redd.it/wz3nkrm3hrdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=495284e45e8fb56456799ddb7046c8d174ddef94",
                    "width": 320,
                    "height": 301
                  },
                  {
                    "url": "https://preview.redd.it/wz3nkrm3hrdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=52fcc39aab4ecdb879243bef5a4cc3e8c57f0e44",
                    "width": 640,
                    "height": 602
                  }
                ],
                "variants": {},
                "id": "pkSJ3mtPZ5kTorSpGgrObMKFI2na6CpxtTtl59rf6Ss"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m3no1m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gerdel",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3no1m/built_a_forensic_linguistics_tool_to_verify/",
          "stickied": false,
          "url": "https://i.redd.it/wz3nkrm3hrdf1.png",
          "subreddit_subscribers": 502029,
          "created_utc": 1752900781,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Everyone's always complaining about AI being unreliable. Sometimes it's brilliant, sometimes it's garbage. But most people are looking at this completely wrong.\n\nThe issue isn't really the AI model itself. It's whether the system is doing proper context engineering before the AI even starts working.\n\nThink about it - when you ask a question, good AI systems don't just see your text. They're pulling your conversation history, relevant data, documents, whatever context actually matters. Bad ones are just winging it with your prompt alone.\n\nThis is why customer service bots are either amazing (they know your order details) or useless (generic responses). Same with coding assistants - some understand your whole codebase, others just regurgitate Stack Overflow.\n\nMost of the \"AI is getting smarter\" hype is actually just better context engineering. The models aren't that different, but the information architecture around them is night and day.\n\nThe weird part is this is becoming way more important than prompt engineering, but hardly anyone talks about it. Everyone's still obsessing over how to write the perfect prompt when the real action is in building systems that feed AI the right context.\n\nWrote up the technical details here if anyone wants to understand how this actually works: [link to the free blog post I wrote](https://open.substack.com/pub/diamantai/p/why-ai-experts-are-moving-from-prompt?r=336pe4&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false)\n\nBut yeah, context engineering is quietly becoming the thing that separates AI that actually works from AI that just demos well.",
          "author_fullname": "t2_4x84zf5l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why AI feels inconsistent (and most people don't understand what's actually happening)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4pq8q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753020256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone&amp;#39;s always complaining about AI being unreliable. Sometimes it&amp;#39;s brilliant, sometimes it&amp;#39;s garbage. But most people are looking at this completely wrong.&lt;/p&gt;\n\n&lt;p&gt;The issue isn&amp;#39;t really the AI model itself. It&amp;#39;s whether the system is doing proper context engineering before the AI even starts working.&lt;/p&gt;\n\n&lt;p&gt;Think about it - when you ask a question, good AI systems don&amp;#39;t just see your text. They&amp;#39;re pulling your conversation history, relevant data, documents, whatever context actually matters. Bad ones are just winging it with your prompt alone.&lt;/p&gt;\n\n&lt;p&gt;This is why customer service bots are either amazing (they know your order details) or useless (generic responses). Same with coding assistants - some understand your whole codebase, others just regurgitate Stack Overflow.&lt;/p&gt;\n\n&lt;p&gt;Most of the &amp;quot;AI is getting smarter&amp;quot; hype is actually just better context engineering. The models aren&amp;#39;t that different, but the information architecture around them is night and day.&lt;/p&gt;\n\n&lt;p&gt;The weird part is this is becoming way more important than prompt engineering, but hardly anyone talks about it. Everyone&amp;#39;s still obsessing over how to write the perfect prompt when the real action is in building systems that feed AI the right context.&lt;/p&gt;\n\n&lt;p&gt;Wrote up the technical details here if anyone wants to understand how this actually works: &lt;a href=\"https://open.substack.com/pub/diamantai/p/why-ai-experts-are-moving-from-prompt?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false\"&gt;link to the free blog post I wrote&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But yeah, context engineering is quietly becoming the thing that separates AI that actually works from AI that just demos well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wKqWnEkJ8XBzIv2uLkYiK1q4Y6cuHkY-zVtXvMSB1iQ.jpeg?auto=webp&amp;s=47e4684e7c24667693a8a178c46074db2c23dfc0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wKqWnEkJ8XBzIv2uLkYiK1q4Y6cuHkY-zVtXvMSB1iQ.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7899772fea2e4125bdb806722b1eb52a7e99c3f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/wKqWnEkJ8XBzIv2uLkYiK1q4Y6cuHkY-zVtXvMSB1iQ.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41585f0d5c2b07e1ec639091647605ee5cbea6ea",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/wKqWnEkJ8XBzIv2uLkYiK1q4Y6cuHkY-zVtXvMSB1iQ.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cec52359d8261cfdac81f010c2da606b38fd0b53",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/wKqWnEkJ8XBzIv2uLkYiK1q4Y6cuHkY-zVtXvMSB1iQ.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84c49f8c375c75def1e4a72d22a5643ea4e9edfd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/wKqWnEkJ8XBzIv2uLkYiK1q4Y6cuHkY-zVtXvMSB1iQ.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a9e533f73688d130087da336a5ee5277461b935",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/wKqWnEkJ8XBzIv2uLkYiK1q4Y6cuHkY-zVtXvMSB1iQ.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f1de2b3eebdce435caf42d73498f3fe758a5862",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "wKqWnEkJ8XBzIv2uLkYiK1q4Y6cuHkY-zVtXvMSB1iQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m4pq8q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nir777",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4pq8q/why_ai_feels_inconsistent_and_most_people_dont/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4pq8q/why_ai_feels_inconsistent_and_most_people_dont/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753020256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for feedback on a mixed-use AI workstation build. Work is pushing me to get serious about local AI/model training or I'm basically toast career-wise, so trying to build something capable but not break the bank.\n\n\n\nPlanned specs:\n\nCPU: Ryzen 9 9950X3D\n\nMobo: X870E (eyeing ASUS ROG Crosshair Hero for expansion)\n\nRAM: 256GB DDR5-6000\n\nGPUs: 1x RTX 3090 + 2x MI50 32GB\n\nUse case split: RTX 3090 for Stable Diffusion, dual MI50s for LLM inference\n\n\n\nMain questions:\n\nMI50 real-world performance? I've got zero hands-on experience with them but the 32GB VRAM each for \\~$250 on eBay seems insane value. How's ROCm compatibility these days for inference?\n\nCan this actually run 70B models? With 64GB across the MI50s, should handle Llama 70B + smaller models simultaneously right?\n\nCoding/creative writing performance? Main LLM use will be code assistance and creative writing (scripts, etc). Are the MI50s fast enough or will I be frustrated coming from API services?\n\n\n\nGoals:\n\nKeep under $5k initially but want expansion path\n\nHandle Stable Diffusion without compromise (hence the 3090)\n\nRun multiple LLM models for different users/tasks\n\nLearn fine-tuning and custom models for work requirements\n\n\n\nAlternatives I'm considering:\n\nJust go dual RTX 3090s and call it a day, but the MI50 value proposition is tempting if they actually work well\n\nMac Studio M3 Ultra 256GB - saw one on eBay for $5k. Unified memory seems appealing but worried about AI ecosystem limitations vs CUDA\n\n\n\nMac Studio vs custom build thoughts? The 256GB unified memory on the Mac seems compelling for large models, but I'm concerned about software compatibility for training/fine-tuning. Most tutorials assume CUDA/PyTorch setup. Would I be limiting myself with Apple Silicon for serious AI development work?\n\nAnyone running MI50s for LLM work? Is ROCm mature enough or am I setting myself up for driver hell? The job pressure is real so I need something that works reliably, not a weekend project that maybe runs sometimes.\n\n\n\nBudget flexibility exists if there's a compelling reason to spend more, but I'm trying to be smart about price/performance. ",
          "author_fullname": "t2_4fmbw86j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Build advice: Consumer AI workstation with RTX 3090 + dual MI50s for LLM inference and Stable Diffusion (~$5k budget)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m42gid",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752947519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for feedback on a mixed-use AI workstation build. Work is pushing me to get serious about local AI/model training or I&amp;#39;m basically toast career-wise, so trying to build something capable but not break the bank.&lt;/p&gt;\n\n&lt;p&gt;Planned specs:&lt;/p&gt;\n\n&lt;p&gt;CPU: Ryzen 9 9950X3D&lt;/p&gt;\n\n&lt;p&gt;Mobo: X870E (eyeing ASUS ROG Crosshair Hero for expansion)&lt;/p&gt;\n\n&lt;p&gt;RAM: 256GB DDR5-6000&lt;/p&gt;\n\n&lt;p&gt;GPUs: 1x RTX 3090 + 2x MI50 32GB&lt;/p&gt;\n\n&lt;p&gt;Use case split: RTX 3090 for Stable Diffusion, dual MI50s for LLM inference&lt;/p&gt;\n\n&lt;p&gt;Main questions:&lt;/p&gt;\n\n&lt;p&gt;MI50 real-world performance? I&amp;#39;ve got zero hands-on experience with them but the 32GB VRAM each for ~$250 on eBay seems insane value. How&amp;#39;s ROCm compatibility these days for inference?&lt;/p&gt;\n\n&lt;p&gt;Can this actually run 70B models? With 64GB across the MI50s, should handle Llama 70B + smaller models simultaneously right?&lt;/p&gt;\n\n&lt;p&gt;Coding/creative writing performance? Main LLM use will be code assistance and creative writing (scripts, etc). Are the MI50s fast enough or will I be frustrated coming from API services?&lt;/p&gt;\n\n&lt;p&gt;Goals:&lt;/p&gt;\n\n&lt;p&gt;Keep under $5k initially but want expansion path&lt;/p&gt;\n\n&lt;p&gt;Handle Stable Diffusion without compromise (hence the 3090)&lt;/p&gt;\n\n&lt;p&gt;Run multiple LLM models for different users/tasks&lt;/p&gt;\n\n&lt;p&gt;Learn fine-tuning and custom models for work requirements&lt;/p&gt;\n\n&lt;p&gt;Alternatives I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;p&gt;Just go dual RTX 3090s and call it a day, but the MI50 value proposition is tempting if they actually work well&lt;/p&gt;\n\n&lt;p&gt;Mac Studio M3 Ultra 256GB - saw one on eBay for $5k. Unified memory seems appealing but worried about AI ecosystem limitations vs CUDA&lt;/p&gt;\n\n&lt;p&gt;Mac Studio vs custom build thoughts? The 256GB unified memory on the Mac seems compelling for large models, but I&amp;#39;m concerned about software compatibility for training/fine-tuning. Most tutorials assume CUDA/PyTorch setup. Would I be limiting myself with Apple Silicon for serious AI development work?&lt;/p&gt;\n\n&lt;p&gt;Anyone running MI50s for LLM work? Is ROCm mature enough or am I setting myself up for driver hell? The job pressure is real so I need something that works reliably, not a weekend project that maybe runs sometimes.&lt;/p&gt;\n\n&lt;p&gt;Budget flexibility exists if there&amp;#39;s a compelling reason to spend more, but I&amp;#39;m trying to be smart about price/performance. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m42gid",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "neighbornugs",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752947519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**I built an AI Wallpaper Generator that creates ultra-high-quality 4K wallpapers automatically with weather integration**\n\nAfter months of development, I've created a comprehensive AI wallpaper system that generates stunning 4K desktop backgrounds using multiple AI models. ***The system just hit v4.2.0*** with a completely rewritten SDXL pipeline that produces much higher quality photorealistic images.\n\nIt is flexible and simple enough to be used for ALL your image gen needs.\n\n**Key Features:**\n\n**Multiple AI Models**: Choose from FLUX.1-dev, DALL-E 3, GPT-Image-1, or SDXL with Juggernaut XL v9 + multi-LoRA stacking. Each model has its own optimized pipeline for maximum quality.\n\n**Weather Integration**: Real-time weather data automatically influences artistic themes and moods. Rainy day? You get atmospheric, moody scenes. Sunny weather? Bright, vibrant landscapes.\n\n**Advanced Pipeline**: Generates at optimal resolution, upscales to 8K using Real-ESRGAN, then downsamples to perfect 4K for incredible detail and quality. No compromises - time and storage don't matter, only final quality.\n\n**Smart Theme System**: 60+ curated themes across 10 categories including Nature, Urban, Space, Anime, and more. Features \"chaos mode\" for completely random combinations.\n\n**Intelligent Prompting**: Uses DeepSeek-r1:14b locally to generate creative, contextual prompts tailored to each model's strengths and current weather conditions.\n\n**Automated Scheduling**: Set-and-forget cron integration for daily wallpaper changes. Wake up to a new masterpiece every morning.\n\n**Usage Options:**\n- `./ai-wallpaper generate` - Default FLUX generation\n- `./ai-wallpaper generate --model sdxl` - Use specific model  \n- `./ai-wallpaper generate --random-model` - Weighted random model selection\n- `./ai-wallpaper generate --save-stages` - Save intermediate processing stages\n- `./ai-wallpaper generate --theme cyberpunk` - Force specific theme\n- `./ai-wallpaper generate --prompt \"custom prompt\"` - Direct prompt override\n- `./ai-wallpaper generate --random-params` - Randomize generation parameters\n- `./ai-wallpaper generate --seed 42` - Reproducible generation\n- `./ai-wallpaper generate --no-wallpaper` - Generate only, don't set wallpaper\n- `./ai-wallpaper test --model flux` - Test specific model\n- `./ai-wallpaper config --show` - Display current configuration\n- `./ai-wallpaper models --list` - Show all available models with status\n- `./setup_cron.sh` - Automated daily wallpaper scheduling\n\n**Recent v4.2.0 Updates:**\n- ***Completely rewritten SDXL pipeline*** with Juggernaut XL v9 base model\n- Multi-LoRA stacking system with automatic theme-based selection\n- Enhanced negative prompts\n- Photorealistic prompt enhancement with DSLR camera modifiers\n- Optimized settings: 80+ steps, CFG 8.0, ensemble base/refiner pipeline\n\n**Technical Specs:**\n- **Models**: FLUX.1-dev (24GB VRAM), DALL-E 3 (API), GPT-Image-1 (API), SDXL+LoRA (16GB VRAM)\n- **Quality**: Maximum settings across all models - no speed optimizations\n- **Output**: Native 4K (3840x2160) with professional color grading\n- **Architecture**: Modular Python system with YAML configuration\n- **Desktop**: XFCE4 multi-monitor/workspace support\n\n**Requirements:**\n- NVIDIA GPU (RTX 3090 recommended for SDXL)\n- FLUX works off CPU entirely, if GPU is weak\n- Python 3.10+ with virtual environment\n- OpenAI API key (for DALL-E/GPT models)\n\nThe system is completely open source and designed to be \"fail loud\" - every error is verbose and clear, making it easy to troubleshoot. All configuration is in YAML files, and the modular architecture makes it simple to add new models or modify existing pipelines.\n\n**GitHub**: https://github.com/expectbugs/ai-wallpaper\n\nThe system handles everything from installation to daily automation. Check the README.md for complete setup instructions, model comparisons, and configuration options. \n\nWould love feedback from the community!  I'm excited to see what others create with it.\n\nThe documentation (and most of this post) were written by AI, the legacy monolithic fat scripts in the legacy directory where I started, were also written largly by AI.  The complete system was made with a LOT of tools and a lot of manual effort and bugfixing and refactoring, plus, of course, AI.\n\n",
          "author_fullname": "t2_5r1lfqng",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "4k local image gen",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3jogm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 95,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 95,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ZaGXhMiqYPrDCNJVgly0KN51HKx1Izctazie9ZcTJkk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752888144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;I built an AI Wallpaper Generator that creates ultra-high-quality 4K wallpapers automatically with weather integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;After months of development, I&amp;#39;ve created a comprehensive AI wallpaper system that generates stunning 4K desktop backgrounds using multiple AI models. &lt;strong&gt;&lt;em&gt;The system just hit v4.2.0&lt;/em&gt;&lt;/strong&gt; with a completely rewritten SDXL pipeline that produces much higher quality photorealistic images.&lt;/p&gt;\n\n&lt;p&gt;It is flexible and simple enough to be used for ALL your image gen needs.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Multiple AI Models&lt;/strong&gt;: Choose from FLUX.1-dev, DALL-E 3, GPT-Image-1, or SDXL with Juggernaut XL v9 + multi-LoRA stacking. Each model has its own optimized pipeline for maximum quality.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Weather Integration&lt;/strong&gt;: Real-time weather data automatically influences artistic themes and moods. Rainy day? You get atmospheric, moody scenes. Sunny weather? Bright, vibrant landscapes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Advanced Pipeline&lt;/strong&gt;: Generates at optimal resolution, upscales to 8K using Real-ESRGAN, then downsamples to perfect 4K for incredible detail and quality. No compromises - time and storage don&amp;#39;t matter, only final quality.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Smart Theme System&lt;/strong&gt;: 60+ curated themes across 10 categories including Nature, Urban, Space, Anime, and more. Features &amp;quot;chaos mode&amp;quot; for completely random combinations.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Intelligent Prompting&lt;/strong&gt;: Uses DeepSeek-r1:14b locally to generate creative, contextual prompts tailored to each model&amp;#39;s strengths and current weather conditions.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Automated Scheduling&lt;/strong&gt;: Set-and-forget cron integration for daily wallpaper changes. Wake up to a new masterpiece every morning.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Usage Options:&lt;/strong&gt;\n- &lt;code&gt;./ai-wallpaper generate&lt;/code&gt; - Default FLUX generation\n- &lt;code&gt;./ai-wallpaper generate --model sdxl&lt;/code&gt; - Use specific model&lt;br/&gt;\n- &lt;code&gt;./ai-wallpaper generate --random-model&lt;/code&gt; - Weighted random model selection\n- &lt;code&gt;./ai-wallpaper generate --save-stages&lt;/code&gt; - Save intermediate processing stages\n- &lt;code&gt;./ai-wallpaper generate --theme cyberpunk&lt;/code&gt; - Force specific theme\n- &lt;code&gt;./ai-wallpaper generate --prompt &amp;quot;custom prompt&amp;quot;&lt;/code&gt; - Direct prompt override\n- &lt;code&gt;./ai-wallpaper generate --random-params&lt;/code&gt; - Randomize generation parameters\n- &lt;code&gt;./ai-wallpaper generate --seed 42&lt;/code&gt; - Reproducible generation\n- &lt;code&gt;./ai-wallpaper generate --no-wallpaper&lt;/code&gt; - Generate only, don&amp;#39;t set wallpaper\n- &lt;code&gt;./ai-wallpaper test --model flux&lt;/code&gt; - Test specific model\n- &lt;code&gt;./ai-wallpaper config --show&lt;/code&gt; - Display current configuration\n- &lt;code&gt;./ai-wallpaper models --list&lt;/code&gt; - Show all available models with status\n- &lt;code&gt;./setup_cron.sh&lt;/code&gt; - Automated daily wallpaper scheduling&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Recent v4.2.0 Updates:&lt;/strong&gt;\n- &lt;strong&gt;&lt;em&gt;Completely rewritten SDXL pipeline&lt;/em&gt;&lt;/strong&gt; with Juggernaut XL v9 base model\n- Multi-LoRA stacking system with automatic theme-based selection\n- Enhanced negative prompts\n- Photorealistic prompt enhancement with DSLR camera modifiers\n- Optimized settings: 80+ steps, CFG 8.0, ensemble base/refiner pipeline&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical Specs:&lt;/strong&gt;\n- &lt;strong&gt;Models&lt;/strong&gt;: FLUX.1-dev (24GB VRAM), DALL-E 3 (API), GPT-Image-1 (API), SDXL+LoRA (16GB VRAM)\n- &lt;strong&gt;Quality&lt;/strong&gt;: Maximum settings across all models - no speed optimizations\n- &lt;strong&gt;Output&lt;/strong&gt;: Native 4K (3840x2160) with professional color grading\n- &lt;strong&gt;Architecture&lt;/strong&gt;: Modular Python system with YAML configuration\n- &lt;strong&gt;Desktop&lt;/strong&gt;: XFCE4 multi-monitor/workspace support&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;\n- NVIDIA GPU (RTX 3090 recommended for SDXL)\n- FLUX works off CPU entirely, if GPU is weak\n- Python 3.10+ with virtual environment\n- OpenAI API key (for DALL-E/GPT models)&lt;/p&gt;\n\n&lt;p&gt;The system is completely open source and designed to be &amp;quot;fail loud&amp;quot; - every error is verbose and clear, making it easy to troubleshoot. All configuration is in YAML files, and the modular architecture makes it simple to add new models or modify existing pipelines.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=\"https://github.com/expectbugs/ai-wallpaper\"&gt;https://github.com/expectbugs/ai-wallpaper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The system handles everything from installation to daily automation. Check the README.md for complete setup instructions, model comparisons, and configuration options. &lt;/p&gt;\n\n&lt;p&gt;Would love feedback from the community!  I&amp;#39;m excited to see what others create with it.&lt;/p&gt;\n\n&lt;p&gt;The documentation (and most of this post) were written by AI, the legacy monolithic fat scripts in the legacy directory where I started, were also written largly by AI.  The complete system was made with a LOT of tools and a lot of manual effort and bugfixing and refactoring, plus, of course, AI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/dulis7vegqdf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/dulis7vegqdf1.jpeg?auto=webp&amp;s=bd8aad9398781e506c83e442011c00174c479c12",
                  "width": 3840,
                  "height": 2160
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/dulis7vegqdf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04b508943696a21c8edb2d241f2e32e96bd4b88b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/dulis7vegqdf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4dec0eb78bf225649041d72813b05dfb1d86af27",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/dulis7vegqdf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdd9a9042e31182b8905328f9896409463b66816",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/dulis7vegqdf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11887cbf80f7af36eb4f0e9abe4330534f8e6b5a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/dulis7vegqdf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30859b534dcb4cb526efbe7f70c96d5c1703eadd",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/dulis7vegqdf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58d46ee38c04717689811a28b0ee1a4f99af5b44",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "k8Go7RzeN67gaucZZJUMbrN9cEdBYCgUiZ2XH1sDWF0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m3jogm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kor34l",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3jogm/4k_local_image_gen/",
          "stickied": false,
          "url": "https://i.redd.it/dulis7vegqdf1.jpeg",
          "subreddit_subscribers": 502029,
          "created_utc": 1752888144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Claude 4 is out. Grok 4 performed way better then any model in humanity last exam. Kimi K2 has launched with significantly improved creative writing. MiniMax M1 and Qwen 235B are here. Even hints of \"Gemini 3\" have been found in Git repositories. OpenAI will release their next major model (probably GPT-5) in few months and in few weeks we will see a open source model.\nMeanwhile… DeepSeek?\nNot a word. No announcement. No \"We’re working on it\", nothing. Well yeah they have relesead some new checkpoints but nothing else then that. A few weeks ago, I was checking every day, excitedly waiting for DeepSeek R2 but not anymore. At this point, I just hope they silently drop the model and it turns out to be better than everything else.",
          "author_fullname": "t2_y1vyie97k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where is DeepsSeek R2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4ta0f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753030787,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753029087,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Claude 4 is out. Grok 4 performed way better then any model in humanity last exam. Kimi K2 has launched with significantly improved creative writing. MiniMax M1 and Qwen 235B are here. Even hints of &amp;quot;Gemini 3&amp;quot; have been found in Git repositories. OpenAI will release their next major model (probably GPT-5) in few months and in few weeks we will see a open source model.\nMeanwhile… DeepSeek?\nNot a word. No announcement. No &amp;quot;We’re working on it&amp;quot;, nothing. Well yeah they have relesead some new checkpoints but nothing else then that. A few weeks ago, I was checking every day, excitedly waiting for DeepSeek R2 but not anymore. At this point, I just hope they silently drop the model and it turns out to be better than everything else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m4ta0f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ba2sYd",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4ta0f/where_is_deepsseek_r2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4ta0f/where_is_deepsseek_r2/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753029087,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello Everyone,\n\nI'm working on a personal project where I'm using llama.rn (wrapper of llama.cpp). \n\nI'm trying to make an inference from local model (Gemma3n-E2B- INT4). Everything works fine. The only thing I'm struggling with is, the initial inference. The initial inference takes a lot of time. But the subsequent ones are pretty good. Like 2-3s ish. I use a s22+. \n\nCan someone please tell me how do I speed up the initial inference ?\n\n1. The initial inference is slow because it has to instantiate the model for the first time ?\n\n2. Would warming up the model with a dummy inference before the actual inference be helpful ?\n\n3. I tried looking into GPU and npu delegates but it's very confusing as I'm just starting out. There is Qualcomm NPU delegate and tflite delegate for GPU as well. \n\n4. Or should I try to optimize/\n Quantize the model even more to make the inference faster ?\n\nAny inputs are appreciated. I'm just a beginner so please let me know if I made any mistakes. Thanks 🙏🏻",
          "author_fullname": "t2_4eg9l8h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to speed up the initial inference when using llama.rn (llama.cpp) wrapper on android.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m42n4v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752947989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a personal project where I&amp;#39;m using llama.rn (wrapper of llama.cpp). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to make an inference from local model (Gemma3n-E2B- INT4). Everything works fine. The only thing I&amp;#39;m struggling with is, the initial inference. The initial inference takes a lot of time. But the subsequent ones are pretty good. Like 2-3s ish. I use a s22+. &lt;/p&gt;\n\n&lt;p&gt;Can someone please tell me how do I speed up the initial inference ?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;The initial inference is slow because it has to instantiate the model for the first time ?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Would warming up the model with a dummy inference before the actual inference be helpful ?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I tried looking into GPU and npu delegates but it&amp;#39;s very confusing as I&amp;#39;m just starting out. There is Qualcomm NPU delegate and tflite delegate for GPU as well. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Or should I try to optimize/\nQuantize the model even more to make the inference faster ?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any inputs are appreciated. I&amp;#39;m just a beginner so please let me know if I made any mistakes. Thanks 🙏🏻&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m42n4v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "luffy2998",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m42n4v/how_to_speed_up_the_initial_inference_when_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m42n4v/how_to_speed_up_the_initial_inference_when_using/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752947989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This model is really fascinating. I find it absolutely amazing. I believe that if this model gets added reasoning abilities it will beat absolutely everything on the market right now.",
          "author_fullname": "t2_2ad0xr96",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would there be a reasoning version of Kimi K2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3qc1g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752910555,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This model is really fascinating. I find it absolutely amazing. I believe that if this model gets added reasoning abilities it will beat absolutely everything on the market right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m3qc1g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "christian7670",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3qc1g/would_there_be_a_reasoning_version_of_kimi_k2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3qc1g/would_there_be_a_reasoning_version_of_kimi_k2/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752910555,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a layperson when it comes to large language models. Just like learning about them and think local models are fascinating.\n\nI want to take the 2018 International Building Code (pdf or other text file) and create a focused AI model to converse with. The input would be something like\" give me a building code analysis for this floor plan I just put in the chat.\n\n  \nIf one wants to just limit a LLM to one specific document, and get really focused, accurate data, is that reasonable/possible? Either with cloud models or with local models really.\n\nOr, will I actually just get better input with a good prompt on Chatgpt?",
          "author_fullname": "t2_p8xs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "For a very specific text knowledge resource, can a local model outperform cloud models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m461jh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752956506,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a layperson when it comes to large language models. Just like learning about them and think local models are fascinating.&lt;/p&gt;\n\n&lt;p&gt;I want to take the 2018 International Building Code (pdf or other text file) and create a focused AI model to converse with. The input would be something like&amp;quot; give me a building code analysis for this floor plan I just put in the chat.&lt;/p&gt;\n\n&lt;p&gt;If one wants to just limit a LLM to one specific document, and get really focused, accurate data, is that reasonable/possible? Either with cloud models or with local models really.&lt;/p&gt;\n\n&lt;p&gt;Or, will I actually just get better input with a good prompt on Chatgpt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m461jh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "loac",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m461jh/for_a_very_specific_text_knowledge_resource_can_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m461jh/for_a_very_specific_text_knowledge_resource_can_a/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752956506,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!\n\nI’m looking to make a membership for **Kimi K2**, but hoping to find a **local provider or distributor** who might offer it at a cheaper price than the big retail sites.\n\nI’m based in Berlin, so any local tips or sellers you’ve had good experiences with would be appreciated!\n\n\n\nThanks in advance!\n\nEdit: Sorry I edited my text. I am basically looking for a person or small provider who can offer local LLM (Kimi K2). I don't wanna pay CEO’s salary",
          "author_fullname": "t2_6499j3j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for local provider for Kimi K2 at a better price",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m4ktlb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.35,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753011325,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753004098,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I’m looking to make a membership for &lt;strong&gt;Kimi K2&lt;/strong&gt;, but hoping to find a &lt;strong&gt;local provider or distributor&lt;/strong&gt; who might offer it at a cheaper price than the big retail sites.&lt;/p&gt;\n\n&lt;p&gt;I’m based in Berlin, so any local tips or sellers you’ve had good experiences with would be appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;Edit: Sorry I edited my text. I am basically looking for a person or small provider who can offer local LLM (Kimi K2). I don&amp;#39;t wanna pay CEO’s salary&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m4ktlb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "byk1nq",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m4ktlb/looking_for_local_provider_for_kimi_k2_at_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4ktlb/looking_for_local_provider_for_kimi_k2_at_a/",
          "subreddit_subscribers": 502029,
          "created_utc": 1753004098,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking to run the 70B sized models but with large context sizes. Like 10k or more. I'd like to avoid offloading to the cpu. What would you recommend hardware set up to be on a budget?\n\n2 x 3090 still best value?\nSwitch to Radeon like the 2x  mi50 32gb?\n\nIt would be just for inference and as long as its faster than cpu only. Currently with Qwen2.5 72b q3km is 119 t/s pp and 1.03 t/s tg with a 8k context window as cpu only on ddr5 ram. Goes up to 162 t/s pp and 1.5 t/s tg with partial offload to one 3090",
          "author_fullname": "t2_40xsg56g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running the 70B sized models on a budget",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m49p7w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752966325,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to run the 70B sized models but with large context sizes. Like 10k or more. I&amp;#39;d like to avoid offloading to the cpu. What would you recommend hardware set up to be on a budget?&lt;/p&gt;\n\n&lt;p&gt;2 x 3090 still best value?\nSwitch to Radeon like the 2x  mi50 32gb?&lt;/p&gt;\n\n&lt;p&gt;It would be just for inference and as long as its faster than cpu only. Currently with Qwen2.5 72b q3km is 119 t/s pp and 1.03 t/s tg with a 8k context window as cpu only on ddr5 ram. Goes up to 162 t/s pp and 1.5 t/s tg with partial offload to one 3090&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m49p7w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fgoricha",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752966325,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nLanguage as a medium for reasoning is too fuzzy, and hard to control \n\nI feel like language should be a tool to make causality discrete and composable, not as a substrate for reasoning \n\nAs in, I believe general AI should be a physics-first and then language-second game. Language being an abstraction of physical observations of causality feels more, concrete, more useful even, than modeling causality strictly in symbols; language. \n\nThe idea of LLMs being general AI confuses me, and will likely never make sense to me, however the idea of LLMs becoming superhuman coders to create general AI feels like where all the companies are really going. \n\nMaybe Autoregressive Video Generation in LLMs could model causality, and it’ll prove my assumptions wrong, I’m not sure. \n\nDoes anyone else hold this belief that LLMs are just, too fuzzy to become General AI alone? Like we’re skipping the lower-levels of reasoning and jumping into higher abstraction levels?",
          "author_fullname": "t2_190nkoim2p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Maybe physics-based AI is the right approach?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m49j3n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752966213,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752965866,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Language as a medium for reasoning is too fuzzy, and hard to control &lt;/p&gt;\n\n&lt;p&gt;I feel like language should be a tool to make causality discrete and composable, not as a substrate for reasoning &lt;/p&gt;\n\n&lt;p&gt;As in, I believe general AI should be a physics-first and then language-second game. Language being an abstraction of physical observations of causality feels more, concrete, more useful even, than modeling causality strictly in symbols; language. &lt;/p&gt;\n\n&lt;p&gt;The idea of LLMs being general AI confuses me, and will likely never make sense to me, however the idea of LLMs becoming superhuman coders to create general AI feels like where all the companies are really going. &lt;/p&gt;\n\n&lt;p&gt;Maybe Autoregressive Video Generation in LLMs could model causality, and it’ll prove my assumptions wrong, I’m not sure. &lt;/p&gt;\n\n&lt;p&gt;Does anyone else hold this belief that LLMs are just, too fuzzy to become General AI alone? Like we’re skipping the lower-levels of reasoning and jumping into higher abstraction levels?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m49j3n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Key_Clerk_1431",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m49j3n/maybe_physicsbased_ai_is_the_right_approach/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m49j3n/maybe_physicsbased_ai_is_the_right_approach/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752965866,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "OpenReasoning-Nemotron-32B is a large language model (LLM) which is a derivative of Qwen2.5-32B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning about math, code and science solution generation. The model supports a context length of 64K tokens. The OpenReasoning model is available in the following sizes: 1.5B, 7B and 14B and 32B.  \n\n\nThis model is ready for commercial/non-commercial research use.\n\n\n\n[https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B)\n\n[https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B)\n\n[https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B)\n\n[https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new models from NVIDIA: OpenReasoning-Nemotron 32B/14B/7B/1.5B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m394zh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 195,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 195,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752861182,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OpenReasoning-Nemotron-32B is a large language model (LLM) which is a derivative of Qwen2.5-32B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning about math, code and science solution generation. The model supports a context length of 64K tokens. The OpenReasoning model is available in the following sizes: 1.5B, 7B and 14B and 32B.  &lt;/p&gt;\n\n&lt;p&gt;This model is ready for commercial/non-commercial research use.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xVaPHmDFX5vc4R__x4YuAzMSjDtirt1vFIt-J3MElOo.png?auto=webp&amp;s=327b94853608fd599671fae5790b7a4511465a77",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xVaPHmDFX5vc4R__x4YuAzMSjDtirt1vFIt-J3MElOo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=602376c40ecb4272ebb674f9b3e3b4d358685ba0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/xVaPHmDFX5vc4R__x4YuAzMSjDtirt1vFIt-J3MElOo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3b803fe2d6467c44d640ddeb67a87feaabe3990",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/xVaPHmDFX5vc4R__x4YuAzMSjDtirt1vFIt-J3MElOo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5ff7b3e45bd354ff9ce709276d44f8985f3a7e5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/xVaPHmDFX5vc4R__x4YuAzMSjDtirt1vFIt-J3MElOo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6f278e12de000e56d5e3cce8e3786a02ad9b607",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/xVaPHmDFX5vc4R__x4YuAzMSjDtirt1vFIt-J3MElOo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5f655ca2130e33e17e6f03eb7a79f0cbe7bffbd",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/xVaPHmDFX5vc4R__x4YuAzMSjDtirt1vFIt-J3MElOo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=409ce4edaae946c421a769b3491851af64c77c0a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "xVaPHmDFX5vc4R__x4YuAzMSjDtirt1vFIt-J3MElOo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m394zh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_openreasoningnemotron/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_openreasoningnemotron/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752861182,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Maybe there is an interesting research project, which is not effective yet, but after further improvements, can open new doors in AI development?",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any promising alternative to Transformers?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3amtu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 148,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 148,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752864640,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe there is an interesting research project, which is not effective yet, but after further improvements, can open new doors in AI development?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m3amtu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 68,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3amtu/is_there_any_promising_alternative_to_transformers/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3amtu/is_there_any_promising_alternative_to_transformers/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752864640,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_cpegz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta says it won't sign Europe AI agreement, calling it an overreach that will stunt growth",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m36d91",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 236,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 236,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=f8cc9ecbfc7aca5993e5981e5cc192cb2bee4319",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752854803,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "cnbc.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.cnbc.com/2025/07/18/meta-europe-ai-code.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?auto=webp&amp;s=61a5416c4510718029933cb302bf48535abb121d",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f858e0ea1b92e759f3797ae734ec97f83aa47c4c",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2cf7a271ba86e850af28b1fb2b6f525d178ed7db",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bcbf0659ae41339da6bdd26ad3bc4ebaaa6de820",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b69ea52f58eed4c486e9ec11d064e7470250b2ff",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=284a73644d9f0b4fedcbea9e162a865bca7687ca",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad1530a1949c53858e6316e93ec0376896d5298f",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m36d91",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ttkciar",
          "discussion_type": null,
          "num_comments": 96,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m36d91/meta_says_it_wont_sign_europe_ai_agreement/",
          "stickied": false,
          "url": "https://www.cnbc.com/2025/07/18/meta-europe-ai-code.html",
          "subreddit_subscribers": 502029,
          "created_utc": 1752854803,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I work in medicine, and I basically want something similar to [OpenEvidence](https://www.openevidence.com/), but local and totally private because I don’t like the idea of putting patient information in a website, even if they claim to be HIPAA compliant.",
          "author_fullname": "t2_66cr6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local deep research that web searches only academic sources?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m3osbo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752904743,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in medicine, and I basically want something similar to &lt;a href=\"https://www.openevidence.com/\"&gt;OpenEvidence&lt;/a&gt;, but local and totally private because I don’t like the idea of putting patient information in a website, even if they claim to be HIPAA compliant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/r0K2nZ-d1qwMsSX32_v3aVtpZ9M02-uKIaafMaYpN-g.jpeg?auto=webp&amp;s=570f323ee4bbc1f27fb9f5efba7b29e487b09c52",
                  "width": 2400,
                  "height": 1260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/r0K2nZ-d1qwMsSX32_v3aVtpZ9M02-uKIaafMaYpN-g.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b99b4706d1ff8062891ef35406b18525b4f8d162",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/r0K2nZ-d1qwMsSX32_v3aVtpZ9M02-uKIaafMaYpN-g.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=60641ff3857a0495463043449d1298f21d5dfa7c",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/r0K2nZ-d1qwMsSX32_v3aVtpZ9M02-uKIaafMaYpN-g.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9091cf75612a85af00eb300010e10400d493da0c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/r0K2nZ-d1qwMsSX32_v3aVtpZ9M02-uKIaafMaYpN-g.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=624cd82a0d4a14d6fbaa508c602b65ae5ef8a021",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/r0K2nZ-d1qwMsSX32_v3aVtpZ9M02-uKIaafMaYpN-g.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d5732e1bd064616b377f55e91989ffe8013f9b08",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/r0K2nZ-d1qwMsSX32_v3aVtpZ9M02-uKIaafMaYpN-g.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cee2a2aa38e2b6168d075260f6c34e9f4ca0a51b",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "r0K2nZ-d1qwMsSX32_v3aVtpZ9M02-uKIaafMaYpN-g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m3osbo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amazydayzee",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m3osbo/local_deep_research_that_web_searches_only/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3osbo/local_deep_research_that_web_searches_only/",
          "subreddit_subscribers": 502029,
          "created_utc": 1752904743,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}